{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DataRec Documentation","text":"<p>This is the official documentation for \"DataRec: A Python Library for Standardized and Reproducible Data Management in Recommender Systems\" (SIGIR 2025).</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>What is DataRec</li> <li>Quickstart</li> <li>Filtering Strategies</li> <li>Splitting Strategies</li> <li>Authors</li> </ul>"},{"location":"#what-is-datarec","title":"What is DataRec","text":"<p>DataRec is a Python library that focuses on the data management phase of recommendation systems. It promotes standardization, interoperability, and best practices for processing and analyzing recommendation datasets.</p> <p>Key links: - Get started - Datasets - Pipeline - Processing - Splitters</p>"},{"location":"#quickstart","title":"Quickstart","text":"<pre><code>from datarec.datasets import AmazonOffice\nfrom datarec.processing import FilterOutDuplicatedInteractions, UserItemIterativeKCore\nfrom datarec.splitters import RandomHoldOut\n\ndata = AmazonOffice(version=\"2014\").prepare_and_load()\ndata = FilterOutDuplicatedInteractions().run(data)\ndata = UserItemIterativeKCore(cores=5).run(data)\n\nsplits = RandomHoldOut(test_ratio=0.2, val_ratio=0.1, seed=42).run(data)\ntrain, val, test = splits[\"train\"], splits[\"val\"], splits[\"test\"]\n</code></pre>"},{"location":"#filtering-strategies","title":"Filtering Strategies","text":"<p>DataRec provides preprocessing filters to clean and shape interaction data before modeling. See the <code>Processing</code> section for the full list of filtering operations.</p>"},{"location":"#splitting-strategies","title":"Splitting Strategies","text":"<p>DataRec includes multiple splitting strategies (uniform, user\u2011stratified, temporal) to build train/validation/test splits. See the <code>Splitters</code> section for details.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Dataset Management: Supports reading and writing various data formats and allows dynamic format specification.</li> <li>Reference Datasets: Include commonly used recommendation datasets with traceable sources and versioning.</li> <li>Filtering Strategies: Implements popular filtering techniques.</li> <li>Splitting Strategies: Implements widely used data splitting strategies.</li> <li>Data Characteristics Analysis: Enables computing data characteristics that impact recommendation performance.</li> <li>Interoperability: Designed to be modular and compatible with existing recommendation frameworks by allowing dataset export in various formats.</li> </ul>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Alberto Carlo Maria Mancino (alberto.mancino@poliba.it)</li> <li>Salvatore Bufi (salvatore.bufi@poliba.it)</li> <li>Angela Di Fazio (angela.difazio@poliba.it)</li> <li>Daniele Malitesta (daniele.malitesta@centralesupelec.fr)</li> <li>Antonio Ferrara (antonio.ferrara@poliba.it)</li> <li>Claudio Pomo (claudio.pomo@poliba.it)</li> <li>Tommaso Di Noia (tommaso.dinoia@poliba.it)</li> </ul>"},{"location":"#contributors","title":"Contributors","text":"<ul> <li>Alberto C. M. Mancino</li> <li>Angela Di Fazio</li> <li>Salvatore Bufi</li> <li>Giuseppe Fasano</li> <li>Gianluca Colonna</li> <li>Maria L. N. De Bonis</li> <li>Marco Valentini</li> </ul>"},{"location":"datasets_nav/","title":"Datasets","text":"<p>DataRec includes several commonly used recommendation datasets to facilitate reproducibility and standardization. These datasets have been carefully curated, with traceable sources and versioning information maintained whenever possible. For each dataset, DataRec provides metadata such as the number of users, items, and interactions and data characteristics known to impact recommendation performance (e.g., sparsity and user/item distribution shifts). The dataset collection in DataRec is continuously updated to include more recent and widely used datasets from the recommendation systems literature. The most recent and widely used version is included when the original data source is unavailable to ensure backward compatibility.</p> <p>The following datasets are currently included in DataRec:</p> DatasetVersionDataset Page Alibaba iFashionv1page Amazon Baby2023page Amazon Beauty2023page Amazon Books2023page Amazon Clothing2023page Amazon Music2023page Amazon Office2023page Amazon Sports and Outdoors2023page Amazon Toys and Games2023page Amazon Video Games2023page Ambar2024page CiaoDVDv1page CiteULikeapage tpage Epinionsv1page Gowallacheckinspage friendshipspage LastFM2011page Mindlargepage smallpage Movielens100kpage 1mpage 20mpage Tmallv1page Yelpv1page"},{"location":"get_started/","title":"Get Started","text":""},{"location":"get_started/#installation-guidelines","title":"Installation guidelines","text":"<p>Please make sure to have the following installed on your system:</p> <ul> <li>Python <code>3.9.0</code> or later</li> </ul> <p>you first need to clone this repository: <pre><code>git clone https://github.com/sisinflab/DataRec.git\n</code></pre> You may create the virtual environment with the requirements files we included in the repository, as follows: <pre><code>$ python3.9 -m venv venv\n$ source venv/bin/activate\n$ pip install --upgrade pip\n$ pip install -r requirements.txt\n</code></pre></p>"},{"location":"tutorial/","title":"Tutorial","text":""},{"location":"tutorial/#external-tutorial","title":"External Tutorial","text":"<p>For a full tutorial on standardized data processing and multimodal feature extraction with DataRec (and Ducho), see: https://sites.google.com/view/dd4rec-tutorial/home</p>"},{"location":"assets/pages/datasets/alibaba_ifashion_v1/","title":"Alibaba iFashion","text":""},{"location":"assets/pages/datasets/alibaba_ifashion_v1/#overview","title":"Overview","text":"<p>Dataset name: Alibaba iFashion Latest version: v1 Available versions: v1 Source: https://drive.google.com/drive/folders/1xFdx5xuNXHGsUVG2VIohFTXf9S7G5veq</p> <p>Alibaba-iFashion is a dataset for fashion recommendation.</p>"},{"location":"assets/pages/datasets/alibaba_ifashion_v1/#citation","title":"Citation","text":"<pre><code>@inproceedings{DBLP:conf/kdd/ChenHXGGSLPZZ19,\n  author       = {Wen Chen and\n                  Pipei Huang and\n                  Jiaming Xu and\n                  Xin Guo and\n                  Cheng Guo and\n                  Fei Sun and\n                  Chao Li and\n                  Andreas Pfadler and\n                  Huan Zhao and\n                  Binqiang Zhao},\n  editor       = {Ankur Teredesai and\n                  Vipin Kumar and\n                  Ying Li and\n                  R{\\'{o}}mer Rosales and\n                  Evimaria Terzi and\n                  George Karypis},\n  title        = {{POG:} Personalized Outfit Generation for Fashion Recommendation at\n                  Alibaba iFashion},\n  booktitle    = {Proceedings of the 25th {ACM} {SIGKDD} International Conference on\n                  Knowledge Discovery {\\&amp;} Data Mining, {KDD} 2019, Anchorage, AK,\n                  USA, August 4-8, 2019},\n  pages        = {2662--2670},\n  publisher    = {{ACM}},\n  year         = {2019},\n  url          = {https://doi.org/10.1145/3292500.3330652},\n  doi          = {10.1145/3292500.3330652},\n  timestamp    = {Tue, 16 Aug 2022 23:04:27 +0200},\n  biburl       = {https://dblp.org/rec/conf/kdd/ChenHXGGSLPZZ19.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n</code></pre>"},{"location":"assets/pages/datasets/alibaba_ifashion_v1/#version-v1","title":"Version: v1","text":""},{"location":"assets/pages/datasets/alibaba_ifashion_v1/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum user_data_archive GdownSource 7z https://drive.google.com/uc?id=1G_1SV9H7fQMPPJOBmZpCnCkgifSsb9Ar md5:2ff9254d67fb13d04824621ca1387622 item_data_archive GdownSource zip https://drive.google.com/uc?id=17MAGl20_mf9V8j0-J6c7T3ayfZd-dIx8 md5:f501244e784ae33defb71b3478d1125c outfit_data_archive GdownSource zip https://drive.google.com/uc?id=1HFKUqBe5oMizU0lxy6sQE5Er1w9x-cC4 md5:f24078606235c122bd1d1c988766e83f"},{"location":"assets/pages/datasets/alibaba_ifashion_v1/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/alibaba_ifashion_v1/#interactions","title":"interactions","text":"<ul> <li>Type: interactions</li> <li>Format: <code>sequence_tabular_inline</code></li> <li>Required: yes</li> <li>Source: <code>user_data_archive</code></li> <li>Filename: <code>user_data.txt</code></li> </ul> <p>Schema</p> <pre><code>cols:\n- user\n- item\n- outfit\nuser_col: user\nsequence_col: item\ncol_sep: ','\nsequence_sep: ;\nstream: true\nencode_ids: true\n</code></pre>"},{"location":"assets/pages/datasets/alibaba_ifashion_v1/#item_data","title":"item_data","text":"<ul> <li>Type: content</li> <li>Source: <code>item_data_archive</code></li> <li>Filename: <code>item_data.txt</code></li> </ul>"},{"location":"assets/pages/datasets/alibaba_ifashion_v1/#outfit_data","title":"outfit_data","text":"<ul> <li>Type: content</li> <li>Source: <code>outfit_data_archive</code></li> <li>Filename: <code>outfit_data.txt</code></li> </ul>"},{"location":"assets/pages/datasets/alibaba_ifashion_v1/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-15</p> Metric Value n_users 3569112 n_items 4463302 n_interactions 620880083 space_size 3991.2435064556007 space_size_log 3.6011082249736726 shape 0.799657294084066 shape_log -0.09709609697870838 density 3.8975462600227275e-05 density_log -4.40920872146218 gini_item 0.8774026297117475 gini_user 0.4029096347976372 ratings_per_user 173.9592601745196 ratings_per_item 139.10779127202238"},{"location":"assets/pages/datasets/alibaba_ifashion_v1/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. https://drive.google.com/drive/folders/1xFdx5xuNXHGsUVG2VIohFTXf9S7G5veq</p>"},{"location":"assets/pages/datasets/amazon_baby_2023/","title":"Amazon Baby","text":""},{"location":"assets/pages/datasets/amazon_baby_2023/#overview","title":"Overview","text":"<p>Dataset name: Amazon Baby Latest version: 2023 Available versions: 2023 Source: https://amazon-reviews-2023.github.io/</p> <p>A large-scale Amazon Reviews dataset, collected by McAuley Lab</p>"},{"location":"assets/pages/datasets/amazon_baby_2023/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/abs-2403-03952,\nauthor       = {Yupeng Hou and\n                Jiacheng Li and\n                Zhankui He and\n                An Yan and\n                Xiusi Chen and\n                Julian J. McAuley},\ntitle        = {Bridging Language and Items for Retrieval and Recommendation},\njournal      = {CoRR},\nvolume       = {abs/2403.03952},\nyear         = {2024}\n}\n</code></pre>"},{"location":"assets/pages/datasets/amazon_baby_2023/#version-2023","title":"Version: 2023","text":""},{"location":"assets/pages/datasets/amazon_baby_2023/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum ratings HttpSource gz https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/benchmark/0core/rating_only/Baby_Products.csv.gz md5:644025598b2e4eb6dc69b55a7f23c8ae"},{"location":"assets/pages/datasets/amazon_baby_2023/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/amazon_baby_2023/#ratings","title":"ratings","text":"<ul> <li>Type: interactions</li> <li>Format: <code>transactions_tabular</code></li> <li>Required: yes</li> <li>Source: <code>ratings</code></li> <li>Filename: <code>Baby_Products.csv</code></li> </ul> <p>Schema</p> <pre><code>sep: ','\nuser_col: user_id\nitem_col: parent_asin\nrating_col: rating\ntimestamp_col: timestamp\nheader: 0\n</code></pre>"},{"location":"assets/pages/datasets/amazon_baby_2023/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-15</p> Metric Value n_users 3386206 n_items 217654 n_interactions 5953891 space_size 858.4994354826333 space_size_log 2.933740013921096 shape 15.557747617778677 shape_log 1.1919467219611264 density 8.078316265374725e-06 density_log -5.092679148242243 gini_item 0.8648800449385958 gini_user 0.36108985080926276 ratings_per_user 1.7582778484238704 ratings_per_item 27.354843007709484"},{"location":"assets/pages/datasets/amazon_baby_2023/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. https://amazon-reviews-2023.github.io/</p>"},{"location":"assets/pages/datasets/amazon_beauty_2023/","title":"Amazon Beauty","text":""},{"location":"assets/pages/datasets/amazon_beauty_2023/#overview","title":"Overview","text":"<p>Dataset name: Amazon Beauty Latest version: 2023 Available versions: 2023 Source: https://amazon-reviews-2023.github.io/</p> <p>A large-scale Amazon Reviews dataset, collected by McAuley Lab</p>"},{"location":"assets/pages/datasets/amazon_beauty_2023/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/abs-2403-03952,\nauthor       = {Yupeng Hou and\n                Jiacheng Li and\n                Zhankui He and\n                An Yan and\n                Xiusi Chen and\n                Julian J. McAuley},\ntitle        = {Bridging Language and Items for Retrieval and Recommendation},\njournal      = {CoRR},\nvolume       = {abs/2403.03952},\nyear         = {2024}\n}\n</code></pre>"},{"location":"assets/pages/datasets/amazon_beauty_2023/#version-2023","title":"Version: 2023","text":""},{"location":"assets/pages/datasets/amazon_beauty_2023/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum ratings HttpSource gz https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/benchmark/0core/rating_only/Beauty_and_Personal_Care.csv.gz md5:6b9dfce7fca70dd05e1bcf77c1953c40"},{"location":"assets/pages/datasets/amazon_beauty_2023/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/amazon_beauty_2023/#ratings","title":"ratings","text":"<ul> <li>Type: interactions</li> <li>Format: <code>transactions_tabular</code></li> <li>Required: yes</li> <li>Source: <code>ratings</code></li> <li>Filename: <code>Beauty_and_Personal_Care.csv</code></li> </ul> <p>Schema</p> <pre><code>sep: ','\nuser_col: user_id\nitem_col: parent_asin\nrating_col: rating\ntimestamp_col: timestamp\nheader: 0\n</code></pre>"},{"location":"assets/pages/datasets/amazon_beauty_2023/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-15</p> Metric Value n_users 11325787 n_items 1028731 n_interactions 23591312 space_size 3413.383685772374 space_size_log 3.5331851084509505 shape 11.009473808021728 shape_log 1.0417665625926829 density 2.024798812171433e-06 density_log -5.693618122624369 gini_item 0.8549725671297695 gini_user 0.42172362782257544 ratings_per_user 2.0829733068439307 ratings_per_item 22.932440064506658"},{"location":"assets/pages/datasets/amazon_beauty_2023/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. https://amazon-reviews-2023.github.io/</p>"},{"location":"assets/pages/datasets/amazon_books_2023/","title":"Amazon Books","text":""},{"location":"assets/pages/datasets/amazon_books_2023/#overview","title":"Overview","text":"<p>Dataset name: Amazon Books Latest version: 2023 Available versions: 2023 Source: https://amazon-reviews-2023.github.io/</p> <p>A large-scale Amazon Reviews dataset, collected by McAuley Lab</p>"},{"location":"assets/pages/datasets/amazon_books_2023/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/abs-2403-03952,\nauthor       = {Yupeng Hou and\n                Jiacheng Li and\n                Zhankui He and\n                An Yan and\n                Xiusi Chen and\n                Julian J. McAuley},\ntitle        = {Bridging Language and Items for Retrieval and Recommendation},\njournal      = {CoRR},\nvolume       = {abs/2403.03952},\nyear         = {2024}\n}\n</code></pre>"},{"location":"assets/pages/datasets/amazon_books_2023/#version-2023","title":"Version: 2023","text":""},{"location":"assets/pages/datasets/amazon_books_2023/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum ratings HttpSource gz https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/benchmark/0core/rating_only/Books.csv.gz md5:9d5d693dae385efa9053961675e1f14a"},{"location":"assets/pages/datasets/amazon_books_2023/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/amazon_books_2023/#ratings","title":"ratings","text":"<ul> <li>Type: interactions</li> <li>Format: <code>transactions_tabular</code></li> <li>Required: yes</li> <li>Source: <code>ratings</code></li> <li>Filename: <code>Books.csv</code></li> </ul> <p>Schema</p> <pre><code>sep: ','\nuser_col: user_id\nitem_col: parent_asin\nrating_col: rating\ntimestamp_col: timestamp\nheader: 0\n</code></pre>"},{"location":"assets/pages/datasets/amazon_books_2023/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-15</p> Metric Value n_users 10297355 n_items 4446065 n_interactions 29139329 space_size 6766.292164699586 space_size_log 3.8303507464489135 shape 2.3160603814834015 shape_log 0.3647498775956473 density 6.364701700188797e-07 density_log -6.196221945976983 gini_item 0.7336893261122213 gini_user 0.5401752191408588 ratings_per_user 2.8297877464649903 ratings_per_item 6.55395928759476"},{"location":"assets/pages/datasets/amazon_books_2023/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. https://amazon-reviews-2023.github.io/</p>"},{"location":"assets/pages/datasets/amazon_clothing_2023/","title":"Amazon Clothing","text":""},{"location":"assets/pages/datasets/amazon_clothing_2023/#overview","title":"Overview","text":"<p>Dataset name: Amazon Clothing Latest version: 2023 Available versions: 2023 Source: https://amazon-reviews-2023.github.io/</p> <p>A large-scale Amazon Reviews dataset, collected by McAuley Lab</p>"},{"location":"assets/pages/datasets/amazon_clothing_2023/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/abs-2403-03952,\nauthor       = {Yupeng Hou and\n                Jiacheng Li and\n                Zhankui He and\n                An Yan and\n                Xiusi Chen and\n                Julian J. McAuley},\ntitle        = {Bridging Language and Items for Retrieval and Recommendation},\njournal      = {CoRR},\nvolume       = {abs/2403.03952},\nyear         = {2024}\n}\n</code></pre>"},{"location":"assets/pages/datasets/amazon_clothing_2023/#version-2023","title":"Version: 2023","text":""},{"location":"assets/pages/datasets/amazon_clothing_2023/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum ratings HttpSource gz https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/benchmark/0core/rating_only/Clothing_Shoes_and_Jewelry.csv.gz md5:ce282bf3fdd269717486960be26b92e2"},{"location":"assets/pages/datasets/amazon_clothing_2023/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/amazon_clothing_2023/#ratings","title":"ratings","text":"<ul> <li>Type: interactions</li> <li>Format: <code>transactions_tabular</code></li> <li>Required: yes</li> <li>Source: <code>ratings</code></li> <li>Filename: <code>Clothing_Shoes_and_Jewelry.csv</code></li> </ul> <p>Schema</p> <pre><code>sep: ','\nuser_col: user_id\nitem_col: parent_asin\nrating_col: rating\ntimestamp_col: timestamp\nheader: 0\n</code></pre>"},{"location":"assets/pages/datasets/amazon_clothing_2023/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-15</p> Metric Value n_users 22553370 n_items 7216886 n_interactions 65179586 space_size 12757.942632173104 space_size_log 4.105780644954536 shape 3.1250833115557044 shape_log 0.49486159968541643 density 4.004518531158031e-07 density_log -6.397449692271609 gini_item 0.8263247764145355 gini_user 0.4949939818552991 ratings_per_user 2.8900153724254958 ratings_per_item 9.03153881050636"},{"location":"assets/pages/datasets/amazon_clothing_2023/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. https://amazon-reviews-2023.github.io/</p>"},{"location":"assets/pages/datasets/amazon_music_2023/","title":"Amazon Music","text":""},{"location":"assets/pages/datasets/amazon_music_2023/#overview","title":"Overview","text":"<p>Dataset name: Amazon Music Latest version: 2023 Available versions: 2023 Source: https://amazon-reviews-2023.github.io/</p> <p>A large-scale Amazon Reviews dataset, collected by McAuley Lab</p>"},{"location":"assets/pages/datasets/amazon_music_2023/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/abs-2403-03952,\nauthor       = {Yupeng Hou and\n                Jiacheng Li and\n                Zhankui He and\n                An Yan and\n                Xiusi Chen and\n                Julian J. McAuley},\ntitle        = {Bridging Language and Items for Retrieval and Recommendation},\njournal      = {CoRR},\nvolume       = {abs/2403.03952},\nyear         = {2024}\n}\n</code></pre>"},{"location":"assets/pages/datasets/amazon_music_2023/#version-2023","title":"Version: 2023","text":""},{"location":"assets/pages/datasets/amazon_music_2023/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum ratings HttpSource gz https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/benchmark/0core/rating_only/Digital_Music.csv.gz md5:592aaf8554ad1fec842edee82ba4d9e6"},{"location":"assets/pages/datasets/amazon_music_2023/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/amazon_music_2023/#ratings","title":"ratings","text":"<ul> <li>Type: interactions</li> <li>Format: <code>transactions_tabular</code></li> <li>Required: yes</li> <li>Source: <code>ratings</code></li> <li>Filename: <code>Digital_Music.csv</code></li> </ul> <p>Schema</p> <pre><code>sep: ','\nuser_col: user_id\nitem_col: parent_asin\nrating_col: rating\ntimestamp_col: timestamp\nheader: 0\n</code></pre>"},{"location":"assets/pages/datasets/amazon_music_2023/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-15</p> Metric Value n_users 100952 n_items 70511 n_interactions 128763 space_size 84.36958262312314 space_size_log 1.9261859006548787 shape 1.4317198734949157 shape_log 0.15585805337951963 density 1.8089196867576144e-05 density_log -4.742580714716903 gini_item 0.4038153653058994 gini_user 0.20280361886090198 ratings_per_user 1.2754873603296617 ratings_per_item 1.826140602175547"},{"location":"assets/pages/datasets/amazon_music_2023/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. https://amazon-reviews-2023.github.io/</p>"},{"location":"assets/pages/datasets/amazon_office_2023/","title":"Amazon Office","text":""},{"location":"assets/pages/datasets/amazon_office_2023/#overview","title":"Overview","text":"<p>Dataset name: Amazon Office Latest version: 2023 Available versions: 2023 Source: https://amazon-reviews-2023.github.io/</p> <p>A large-scale Amazon Reviews dataset, collected by McAuley Lab</p>"},{"location":"assets/pages/datasets/amazon_office_2023/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/abs-2403-03952,\nauthor       = {Yupeng Hou and\n                Jiacheng Li and\n                Zhankui He and\n                An Yan and\n                Xiusi Chen and\n                Julian J. McAuley},\ntitle        = {Bridging Language and Items for Retrieval and Recommendation},\njournal      = {CoRR},\nvolume       = {abs/2403.03952},\nyear         = {2024}\n}\n</code></pre>"},{"location":"assets/pages/datasets/amazon_office_2023/#version-2023","title":"Version: 2023","text":""},{"location":"assets/pages/datasets/amazon_office_2023/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum ratings HttpSource gz https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/benchmark/0core/rating_only/Office_Products.csv.gz md5:d4c05697d3acd22d1c23a01b64b25a16"},{"location":"assets/pages/datasets/amazon_office_2023/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/amazon_office_2023/#ratings","title":"ratings","text":"<ul> <li>Type: interactions</li> <li>Format: <code>transactions_tabular</code></li> <li>Required: yes</li> <li>Source: <code>ratings</code></li> <li>Filename: <code>Office_Products.csv</code></li> </ul> <p>Schema</p> <pre><code>sep: ','\nuser_col: user_id\nitem_col: parent_asin\nrating_col: rating\ntimestamp_col: timestamp\nheader: 0\n</code></pre>"},{"location":"assets/pages/datasets/amazon_office_2023/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-15</p> Metric Value n_users 7613158 n_items 710403 n_interactions 12689349 space_size 2325.5989083833865 space_size_log 3.3665348149082335 shape 10.716674901429188 shape_log 1.030060056255404 density 2.346225292975775e-06 density_log -5.6296302877028435 gini_item 0.8377877061123389 gini_user 0.3341419828184234 ratings_per_user 1.6667654868058694 ratings_per_item 17.862183859020867"},{"location":"assets/pages/datasets/amazon_office_2023/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. https://amazon-reviews-2023.github.io/</p>"},{"location":"assets/pages/datasets/amazon_sports_and_outdoors_2023/","title":"Amazon Sports and Outdoors","text":""},{"location":"assets/pages/datasets/amazon_sports_and_outdoors_2023/#overview","title":"Overview","text":"<p>Dataset name: Amazon Sports and Outdoors Latest version: 2023 Available versions: 2023 Source: https://amazon-reviews-2023.github.io/</p> <p>A large-scale Amazon Reviews dataset, collected by McAuley Lab</p>"},{"location":"assets/pages/datasets/amazon_sports_and_outdoors_2023/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/abs-2403-03952,\nauthor       = {Yupeng Hou and\n                Jiacheng Li and\n                Zhankui He and\n                An Yan and\n                Xiusi Chen and\n                Julian J. McAuley},\ntitle        = {Bridging Language and Items for Retrieval and Recommendation},\njournal      = {CoRR},\nvolume       = {abs/2403.03952},\nyear         = {2024}\n}\n</code></pre>"},{"location":"assets/pages/datasets/amazon_sports_and_outdoors_2023/#version-2023","title":"Version: 2023","text":""},{"location":"assets/pages/datasets/amazon_sports_and_outdoors_2023/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum ratings HttpSource gz https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/benchmark/0core/rating_only/Sports_and_Outdoors.csv.gz md5:75e1dfbb3b3014fab914832b734922e6"},{"location":"assets/pages/datasets/amazon_sports_and_outdoors_2023/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/amazon_sports_and_outdoors_2023/#ratings","title":"ratings","text":"<ul> <li>Type: interactions</li> <li>Format: <code>transactions_tabular</code></li> <li>Required: yes</li> <li>Source: <code>ratings</code></li> <li>Filename: <code>Sports_and_Outdoors.csv</code></li> </ul> <p>Schema</p> <pre><code>sep: ','\nuser_col: user_id\nitem_col: parent_asin\nrating_col: rating\ntimestamp_col: timestamp\nheader: 0\n</code></pre>"},{"location":"assets/pages/datasets/amazon_sports_and_outdoors_2023/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-15</p> Metric Value n_users 10331141 n_items 1587219 n_interactions 19349403 space_size 4049.4176478697527 space_size_log 3.607392571238953 shape 6.5089574910582595 shape_log 0.813511435244585 density 1.1800011417081474e-06 density_log -5.9281175724927655 gini_item 0.8232507097140822 gini_user 0.38244082486129133 ratings_per_user 1.8729202321408642 ratings_per_item 12.190758175147852"},{"location":"assets/pages/datasets/amazon_sports_and_outdoors_2023/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. https://amazon-reviews-2023.github.io/</p>"},{"location":"assets/pages/datasets/amazon_toys_and_games_2023/","title":"Amazon Toys and Games","text":""},{"location":"assets/pages/datasets/amazon_toys_and_games_2023/#overview","title":"Overview","text":"<p>Dataset name: Amazon Toys and Games Latest version: 2023 Available versions: 2023 Source: https://amazon-reviews-2023.github.io/</p> <p>A large-scale Amazon Reviews dataset, collected by McAuley Lab</p>"},{"location":"assets/pages/datasets/amazon_toys_and_games_2023/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/abs-2403-03952,\nauthor       = {Yupeng Hou and\n                Jiacheng Li and\n                Zhankui He and\n                An Yan and\n                Xiusi Chen and\n                Julian J. McAuley},\ntitle        = {Bridging Language and Items for Retrieval and Recommendation},\njournal      = {CoRR},\nvolume       = {abs/2403.03952},\nyear         = {2024}\n}\n</code></pre>"},{"location":"assets/pages/datasets/amazon_toys_and_games_2023/#version-2023","title":"Version: 2023","text":""},{"location":"assets/pages/datasets/amazon_toys_and_games_2023/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum ratings HttpSource gz https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/benchmark/0core/rating_only/Toys_and_Games.csv.gz md5:542250672811854e9803d90b1f52cc14"},{"location":"assets/pages/datasets/amazon_toys_and_games_2023/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/amazon_toys_and_games_2023/#ratings","title":"ratings","text":"<ul> <li>Type: interactions</li> <li>Format: <code>transactions_tabular</code></li> <li>Required: yes</li> <li>Source: <code>ratings</code></li> <li>Filename: <code>Toys_and_Games.csv</code></li> </ul> <p>Schema</p> <pre><code>sep: ','\nuser_col: user_id\nitem_col: parent_asin\nrating_col: rating\ntimestamp_col: timestamp\nheader: 0\n</code></pre>"},{"location":"assets/pages/datasets/amazon_toys_and_games_2023/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-15</p> Metric Value n_users 8116226 n_items 890667 n_interactions 16052440 space_size 2688.652945759642 space_size_log 3.429534746637998 shape 9.112525781240352 shape_log 0.9596387700376451 density 2.2206062715211785e-06 density_log -5.653528437968423 gini_item 0.8119593755054738 gini_user 0.4065159260942122 ratings_per_user 1.9778207260369536 ratings_per_item 18.02294235668325"},{"location":"assets/pages/datasets/amazon_toys_and_games_2023/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. https://amazon-reviews-2023.github.io/</p>"},{"location":"assets/pages/datasets/amazon_videogames_2023/","title":"Amazon Video Games","text":""},{"location":"assets/pages/datasets/amazon_videogames_2023/#overview","title":"Overview","text":"<p>Dataset name: Amazon Video Games Latest version: 2023 Available versions: 2023 Source: https://amazon-reviews-2023.github.io/</p> <p>A large-scale Amazon Reviews dataset, collected by McAuley Lab</p>"},{"location":"assets/pages/datasets/amazon_videogames_2023/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/corr/abs-2403-03952,\nauthor       = {Yupeng Hou and\n                Jiacheng Li and\n                Zhankui He and\n                An Yan and\n                Xiusi Chen and\n                Julian J. McAuley},\ntitle        = {Bridging Language and Items for Retrieval and Recommendation},\njournal      = {CoRR},\nvolume       = {abs/2403.03952},\nyear         = {2024}\n}\n</code></pre>"},{"location":"assets/pages/datasets/amazon_videogames_2023/#version-2023","title":"Version: 2023","text":""},{"location":"assets/pages/datasets/amazon_videogames_2023/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum ratings HttpSource gz https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/benchmark/0core/rating_only/Video_Games.csv.gz md5:60fdc3e812de871c30d65722e9a91a0a"},{"location":"assets/pages/datasets/amazon_videogames_2023/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/amazon_videogames_2023/#ratings","title":"ratings","text":"<ul> <li>Type: interactions</li> <li>Format: <code>transactions_tabular</code></li> <li>Required: yes</li> <li>Source: <code>ratings</code></li> <li>Filename: <code>Video_Games.csv</code></li> </ul> <p>Schema</p> <pre><code>sep: ','\nuser_col: user_id\nitem_col: parent_asin\nrating_col: rating\ntimestamp_col: timestamp\nheader: 0\n</code></pre>"},{"location":"assets/pages/datasets/amazon_videogames_2023/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-15</p> Metric Value n_users 2766656 n_items 137249 n_interactions 4555500 space_size 616.2148727059418 space_size_log 2.789732176058451 shape 20.157931933930303 shape_log 1.304445974412738 density 1.1996973480987134e-05 density_log -4.920928301142955 gini_item 0.8571064226609856 gini_user 0.3353671504827272 ratings_per_user 1.646572613292003 ratings_per_item 33.19149866301394"},{"location":"assets/pages/datasets/amazon_videogames_2023/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. https://amazon-reviews-2023.github.io/</p>"},{"location":"assets/pages/datasets/ambar_2024/","title":"Ambar","text":""},{"location":"assets/pages/datasets/ambar_2024/#overview","title":"Overview","text":"<p>Dataset name: Ambar Latest version: 2024 Available versions: 2024 Source: https://github.com/davidcontrerasaguilar/AMBAR</p> <p>The AMBAR dataset is a dataset in the music domain. It contains both user feedback and attributes, including sensitive features. The users have been anonymized.</p>"},{"location":"assets/pages/datasets/ambar_2024/#citation","title":"Citation","text":"<pre><code>@inproceedings{DBLP:conf/recsys/GomezCBS24,\nauthor       = {Elizabeth G{\\'{o}}mez and\n                David Contreras and\n                Ludovico Boratto and\n                Maria Salam{\\'{o}}},\ntitle        = {{AMBAR:} {A} dataset for Assessing Multiple Beyond-Accuracy Recommenders},\nbooktitle    = {RecSys},\npages        = {137--147},\npublisher    = {{ACM}},\nyear         = {2024}\n}\n</code></pre>"},{"location":"assets/pages/datasets/ambar_2024/#version-2024","title":"Version: 2024","text":""},{"location":"assets/pages/datasets/ambar_2024/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum ratings HttpSource False https://raw.githubusercontent.com/davidcontrerasaguilar/AMBAR/refs/heads/main/data/AMBAR/ratings_info.csv md5:c4ee943350b7ee42b167a3f3ee525005 artists HttpSource False https://raw.githubusercontent.com/davidcontrerasaguilar/AMBAR/refs/heads/main/data/AMBAR/artists_info.csv md5:8c15a03f15f8a926e41f0689ca66c650 tracks HttpSource False https://raw.githubusercontent.com/davidcontrerasaguilar/AMBAR/refs/heads/main/data/AMBAR/tracks_info.csv md5:872e2375da31c56e0d26ead87343fb71 users HttpSource False https://raw.githubusercontent.com/davidcontrerasaguilar/AMBAR/refs/heads/main/data/AMBAR/users_info.csv md5:d17cd7396ed88915495fc58537490d4b"},{"location":"assets/pages/datasets/ambar_2024/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/ambar_2024/#ratings","title":"ratings","text":"<ul> <li>Type: interactions</li> <li>Format: <code>transactions_tabular</code></li> <li>Required: yes</li> <li>Source: <code>ratings</code></li> <li>Filename: <code>ratings_info.csv</code></li> </ul> <p>Schema</p> <pre><code>sep: ','\nuser_col: user_id\nitem_col: track_id\nrating_col: rating\nheader: 0\n</code></pre>"},{"location":"assets/pages/datasets/ambar_2024/#artists","title":"artists","text":"<ul> <li>Type: content</li> <li>Source: <code>artists</code></li> <li>Filename: <code>artists_info.csv</code></li> <li>About: items</li> </ul>"},{"location":"assets/pages/datasets/ambar_2024/#tracks","title":"tracks","text":"<ul> <li>Type: content</li> <li>Source: <code>tracks</code></li> <li>Filename: <code>tracks_info.csv</code></li> <li>About: items</li> </ul>"},{"location":"assets/pages/datasets/ambar_2024/#users","title":"users","text":"<ul> <li>Type: content</li> <li>Source: <code>users</code></li> <li>Filename: <code>users_info.csv</code></li> <li>About: users</li> </ul>"},{"location":"assets/pages/datasets/ambar_2024/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-15</p> Metric Value n_users 31013 n_items 443921 n_interactions 3311462 space_size 117.33423188907831 space_size_log 2.069424734636089 shape 0.06986152941627001 shape_log -1.1557619109718333 density 0.00024053058441535148 density_log -3.618829693517537 gini_item 0.7527298907837248 gini_user 0.26300532146144723 ratings_per_user 106.77657756424725 ratings_per_item 7.4595750144732955"},{"location":"assets/pages/datasets/ambar_2024/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. https://github.com/davidcontrerasaguilar/AMBAR</p>"},{"location":"assets/pages/datasets/ciao_v1/","title":"CiaoDVD","text":""},{"location":"assets/pages/datasets/ciao_v1/#overview","title":"Overview","text":"<p>Dataset name: CiaoDVD Latest version: v1 Available versions: v1 Source: https://guoguibing.github.io/librec/datasets.html</p> <p>CiaoDVD is a dataset for DVDs recommendation.</p>"},{"location":"assets/pages/datasets/ciao_v1/#citation","title":"Citation","text":"<pre><code>@inproceedings{DBLP:conf/asunam/GuoZTY14,\n  author       = {Guibing Guo and\n                  Jie Zhang and\n                  Daniel Thalmann and\n                  Neil Yorke{-}Smith},\n  title        = {{ETAF:} An extended trust antecedents framework for trust prediction},\n  booktitle    = {{ASONAM}},\n  pages        = {540--547},\n  publisher    = {{IEEE} Computer Society},\n  year         = {2014}\n}\n</code></pre>"},{"location":"assets/pages/datasets/ciao_v1/#version-v1","title":"Version: v1","text":""},{"location":"assets/pages/datasets/ciao_v1/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum github_archive HttpSource zip https://guoguibing.github.io/librec/datasets/CiaoDVD.zip md5:43a39e068e3fc494a7f7f7581293e2c2"},{"location":"assets/pages/datasets/ciao_v1/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/ciao_v1/#ratings","title":"ratings","text":"<ul> <li>Type: interactions</li> <li>Format: <code>transactions_tabular</code></li> <li>Required: yes</li> <li>Source: <code>github_archive</code></li> <li>Filename: <code>movie-ratings.txt</code></li> </ul> <p>Schema</p> <pre><code>sep: ','\nuser_col: 0\nitem_col: 1\nrating_col: 4\ntimestamp_col: 5\n</code></pre>"},{"location":"assets/pages/datasets/ciao_v1/#reviews","title":"reviews","text":"<ul> <li>Type: content</li> <li>Format: <code>tabular</code></li> <li>Source: <code>github_archive</code></li> <li>Filename: <code>review-ratings.txt</code></li> <li>About: items</li> </ul>"},{"location":"assets/pages/datasets/ciao_v1/#friendships","title":"friendships","text":"<ul> <li>Type: content</li> <li>Format: <code>tabular</code></li> <li>Source: <code>github_archive</code></li> <li>Filename: <code>trusts.txt</code></li> <li>About: users</li> </ul>"},{"location":"assets/pages/datasets/ciao_v1/#readme","title":"readme","text":"<ul> <li>Type: documentation</li> <li>Format: <code>plaintext</code></li> <li>Source: <code>github_archive</code></li> <li>Filename: <code>readme.txt</code></li> </ul>"},{"location":"assets/pages/datasets/ciao_v1/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-15</p> Metric Value n_users 17615 n_items 16121 n_interactions 72665 space_size 16.851451421168445 space_size_log 1.2266373127478747 shape 1.0926741517275602 shape_log 0.0384906695387733 density 0.0002558884315873835 density_log -3.5919493476076516 gini_item 0.6557512489633792 gini_user 0.6631267979210606 ratings_per_user 4.12517740562021 ratings_per_item 4.507474722411761"},{"location":"assets/pages/datasets/ciao_v1/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. https://guoguibing.github.io/librec/datasets.html</p>"},{"location":"assets/pages/datasets/citeulike_a/","title":"CiteULike","text":""},{"location":"assets/pages/datasets/citeulike_a/#overview","title":"Overview","text":"<p>Dataset name: CiteULike Latest version: t Available versions: a, t Source: http://www.citeulike.ort/faq/data.adp</p> <p>CiteULike allows users to create their own collections of articles. There are abstracts, titles, and tags for each article.</p>"},{"location":"assets/pages/datasets/citeulike_a/#citation","title":"Citation","text":"<pre><code>@inproceedings{DBLP:conf/kdd/WangB11,\nauthor       = {Chong Wang and\n                David M. Blei},\neditor       = {Chid Apt{\\'{e}} and\n                Joydeep Ghosh and\n                Padhraic Smyth},\ntitle        = {Collaborative topic modeling for recommending scientific articles},\nbooktitle    = {Proceedings of the 17th {ACM} {SIGKDD} International Conference on\n                Knowledge Discovery and Data Mining, San Diego, CA, USA, August 21-24,\n                2011},\npages        = {448--456},\npublisher    = {{ACM}},\nyear         = {2011},\nurl          = {https://doi.org/10.1145/2020408.2020480},\ndoi          = {10.1145/2020408.2020480},\ntimestamp    = {Tue, 06 Nov 2018 16:59:35 +0100},\nbiburl       = {https://dblp.org/rec/conf/kdd/WangB11.bib},\nbibsource    = {dblp computer science bibliography, https://dblp.org}\n}\n\n@inproceedings{DBLP:conf/ijcai/WangCL13,\nauthor       = {Hao Wang and\n                Binyi Chen and\n                Wu{-}Jun Li},\neditor       = {Francesca Rossi},\ntitle        = {Collaborative Topic Regression with Social Regularization for Tag\n                Recommendation},\nbooktitle    = {{IJCAI} 2013, Proceedings of the 23rd International Joint Conference\n                on Artificial Intelligence, Beijing, China, August 3-9, 2013},\npages        = {2719--2725},\npublisher    = {{IJCAI/AAAI}},\nyear         = {2013},\nurl          = {http://www.aaai.org/ocs/index.php/IJCAI/IJCAI13/paper/view/7006},\ntimestamp    = {Tue, 23 Jan 2024 13:25:46 +0100},\nbiburl       = {https://dblp.org/rec/conf/ijcai/WangCL13.bib},\nbibsource    = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"assets/pages/datasets/citeulike_a/#version-a","title":"Version: a","text":""},{"location":"assets/pages/datasets/citeulike_a/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum ratings HttpSource False https://raw.githubusercontent.com/js05212/citeulike-a/refs/heads/master/users.dat md5:b384e22f3b6cd9d0c8e11a13066fe70e citations HttpSource False https://raw.githubusercontent.com/js05212/citeulike-a/refs/heads/master/citations.dat md5:6635e40fc71f862d7338a6990535b99c item-tag HttpSource False https://raw.githubusercontent.com/js05212/citeulike-a/refs/heads/master/item-tag.dat md5:600433234dda44e846a40aeb2b211ca3 mult HttpSource False https://raw.githubusercontent.com/js05212/citeulike-a/refs/heads/master/mult.dat md5:63cca7f5fee224c60eb79a92f1fbecd9 tags HttpSource False https://raw.githubusercontent.com/js05212/citeulike-a/refs/heads/master/tags.dat md5:406562795523497ec00aaa9121cc9b39 vocabulary HttpSource False https://raw.githubusercontent.com/js05212/citeulike-a/refs/heads/master/vocabulary.dat md5:4834ecd48c7d073180cdf30973937093 raw-data HttpSource False https://raw.githubusercontent.com/js05212/citeulike-a/refs/heads/master/raw-data.csv md5:ebef07c3e0eebd93b3adaf7ef710b58f"},{"location":"assets/pages/datasets/citeulike_a/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/citeulike_a/#ratings","title":"ratings","text":"<ul> <li>Type: interactions</li> <li>Format: <code>sequence_tabular_implicit</code></li> <li>Required: yes</li> <li>Source: <code>ratings</code></li> <li>Filename: <code>users.data</code></li> </ul> <p>Schema</p> <pre><code>col_sep: ' '\n</code></pre>"},{"location":"assets/pages/datasets/citeulike_a/#citations","title":"citations","text":"<ul> <li>Type: content</li> <li>Source: <code>citations</code></li> <li>Filename: <code>citation.dat</code></li> </ul>"},{"location":"assets/pages/datasets/citeulike_a/#item-tag","title":"item-tag","text":"<ul> <li>Type: content</li> <li>Source: <code>item-tag</code></li> <li>Filename: <code>item-tag.dat</code></li> </ul>"},{"location":"assets/pages/datasets/citeulike_a/#mult","title":"mult","text":"<ul> <li>Type: content</li> <li>Source: <code>mult</code></li> <li>Filename: <code>mult.dat</code></li> </ul>"},{"location":"assets/pages/datasets/citeulike_a/#tags","title":"tags","text":"<ul> <li>Type: content</li> <li>Source: <code>tags</code></li> <li>Filename: <code>tags.dat</code></li> </ul>"},{"location":"assets/pages/datasets/citeulike_a/#vocabulary","title":"vocabulary","text":"<ul> <li>Type: content</li> <li>Source: <code>vocabulary</code></li> <li>Filename: <code>vocabulary.dat</code></li> </ul>"},{"location":"assets/pages/datasets/citeulike_a/#raw-data","title":"raw-data","text":"<ul> <li>Type: content</li> <li>Source: <code>raw-data</code></li> <li>Filename: <code>raw-data.csv</code></li> </ul>"},{"location":"assets/pages/datasets/citeulike_a/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-15</p> Metric Value n_users 5551 n_items 16980 n_interactions 204986 space_size 9.708551900257834 space_size_log 0.9871544566198973 shape 0.32691401648998825 shape_log -0.48556645857607317 density 0.002174779785855497 density_log -2.662584712332187 gini_item 0.3696468161800518 gini_user 0.4706337006537276 ratings_per_user 36.927760763826335 ratings_per_item 12.072202591283864"},{"location":"assets/pages/datasets/citeulike_a/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. http://www.citeulike.ort/faq/data.adp</p>"},{"location":"assets/pages/datasets/citeulike_t/","title":"CiteULike","text":""},{"location":"assets/pages/datasets/citeulike_t/#overview","title":"Overview","text":"<p>Dataset name: CiteULike Latest version: t Available versions: a, t Source: http://www.citeulike.ort/faq/data.adp</p> <p>CiteULike allows users to create their own collections of articles. There are abstracts, titles, and tags for each article.</p>"},{"location":"assets/pages/datasets/citeulike_t/#citation","title":"Citation","text":"<pre><code>@inproceedings{DBLP:conf/kdd/WangB11,\nauthor       = {Chong Wang and\n                David M. Blei},\neditor       = {Chid Apt{\\'{e}} and\n                Joydeep Ghosh and\n                Padhraic Smyth},\ntitle        = {Collaborative topic modeling for recommending scientific articles},\nbooktitle    = {Proceedings of the 17th {ACM} {SIGKDD} International Conference on\n                Knowledge Discovery and Data Mining, San Diego, CA, USA, August 21-24,\n                2011},\npages        = {448--456},\npublisher    = {{ACM}},\nyear         = {2011},\nurl          = {https://doi.org/10.1145/2020408.2020480},\ndoi          = {10.1145/2020408.2020480},\ntimestamp    = {Tue, 06 Nov 2018 16:59:35 +0100},\nbiburl       = {https://dblp.org/rec/conf/kdd/WangB11.bib},\nbibsource    = {dblp computer science bibliography, https://dblp.org}\n}\n\n@inproceedings{DBLP:conf/ijcai/WangCL13,\nauthor       = {Hao Wang and\n                Binyi Chen and\n                Wu{-}Jun Li},\neditor       = {Francesca Rossi},\ntitle        = {Collaborative Topic Regression with Social Regularization for Tag\n                Recommendation},\nbooktitle    = {{IJCAI} 2013, Proceedings of the 23rd International Joint Conference\n                on Artificial Intelligence, Beijing, China, August 3-9, 2013},\npages        = {2719--2725},\npublisher    = {{IJCAI/AAAI}},\nyear         = {2013},\nurl          = {http://www.aaai.org/ocs/index.php/IJCAI/IJCAI13/paper/view/7006},\ntimestamp    = {Tue, 23 Jan 2024 13:25:46 +0100},\nbiburl       = {https://dblp.org/rec/conf/ijcai/WangCL13.bib},\nbibsource    = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"assets/pages/datasets/citeulike_t/#version-t","title":"Version: t","text":""},{"location":"assets/pages/datasets/citeulike_t/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum ratings HttpSource False https://raw.githubusercontent.com/js05212/citeulike-t/refs/heads/master/users.dat md5:4e6b896cc923445803c1b8f12eb1c3bd citations HttpSource False https://raw.githubusercontent.com/js05212/citeulike-t/refs/heads/master/citations.dat md5:e9678798a607e613f9070d0734bde515 tag-item HttpSource False https://raw.githubusercontent.com/js05212/citeulike-t/refs/heads/master/tag-item.dat md5:91f0997a6199f7129b55e0f98a3caa02 mult HttpSource False https://raw.githubusercontent.com/js05212/citeulike-t/refs/heads/master/mult.dat md5:b8fdefb63bdd4c9b56bc5b7c488b3184 tags HttpSource False https://raw.githubusercontent.com/js05212/citeulike-t/refs/heads/master/tags.dat md5:23043d13aae074ae5977d7a8aae1790c vocabulary HttpSource False https://raw.githubusercontent.com/js05212/citeulike-t/refs/heads/master/vocabulary.dat md5:83b7909eaeb1ce5d3400d48b412edc70 rawtext HttpSource False https://raw.githubusercontent.com/js05212/citeulike-t/refs/heads/master/rawtext.dat md5:129dad5e7059057036c561569b58a47b"},{"location":"assets/pages/datasets/citeulike_t/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/citeulike_t/#ratings","title":"ratings","text":"<ul> <li>Type: interactions</li> <li>Format: <code>sequence_tabular_implicit</code></li> <li>Required: yes</li> <li>Source: <code>ratings</code></li> <li>Filename: <code>users.data</code></li> </ul> <p>Schema</p> <pre><code>col_sep: ' '\n</code></pre>"},{"location":"assets/pages/datasets/citeulike_t/#citations","title":"citations","text":"<ul> <li>Type: content</li> <li>Source: <code>citations</code></li> <li>Filename: <code>citation.dat</code></li> </ul>"},{"location":"assets/pages/datasets/citeulike_t/#tag-item","title":"tag-item","text":"<ul> <li>Type: content</li> <li>Source: <code>tag-item</code></li> <li>Filename: <code>tag-item.dat</code></li> </ul>"},{"location":"assets/pages/datasets/citeulike_t/#mult","title":"mult","text":"<ul> <li>Type: content</li> <li>Source: <code>mult</code></li> <li>Filename: <code>mult.dat</code></li> </ul>"},{"location":"assets/pages/datasets/citeulike_t/#tags","title":"tags","text":"<ul> <li>Type: content</li> <li>Source: <code>tags</code></li> <li>Filename: <code>tags.dat</code></li> </ul>"},{"location":"assets/pages/datasets/citeulike_t/#vocabulary","title":"vocabulary","text":"<ul> <li>Type: content</li> <li>Source: <code>vocabulary</code></li> <li>Filename: <code>vocabulary.dat</code></li> </ul>"},{"location":"assets/pages/datasets/citeulike_t/#rawtext","title":"rawtext","text":"<ul> <li>Type: content</li> <li>Source: <code>rawtext</code></li> <li>Filename: <code>rawtext.dat</code></li> </ul>"},{"location":"assets/pages/datasets/citeulike_t/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-15</p> Metric Value n_users 7947 n_items 25584 n_interactions 134860 space_size 14.258893645721606 space_size_log 1.1540858297095264 shape 0.3106238273921201 shape_log -0.507765233385266 density 0.000663302288858182 density_log -3.1782885040784317 gini_item 0.5197283261904477 gini_user 0.6368531036879522 ratings_per_user 16.969925758147728 ratings_per_item 5.271263289555972"},{"location":"assets/pages/datasets/citeulike_t/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. http://www.citeulike.ort/faq/data.adp</p>"},{"location":"assets/pages/datasets/epinions_v1/","title":"Epinions","text":""},{"location":"assets/pages/datasets/epinions_v1/#overview","title":"Overview","text":"<p>Dataset name: Epinions Latest version: v1 Available versions: v1 Source: https://snap.stanford.edu/data/soc-Epinions1.html</p> <p>Epinions is a dataset for product recommendation research.</p>"},{"location":"assets/pages/datasets/epinions_v1/#citation","title":"Citation","text":"<pre><code>@inproceedings{DBLP:conf/semweb/RichardsonAD03,\n  author       = {Matthew Richardson and\n                  Rakesh Agrawal and\n                  Pedro M. Domingos},\n  editor       = {Dieter Fensel and\n                  Katia P. Sycara and\n                  John Mylopoulos},\n  title        = {Trust Management for the Semantic Web},\n  booktitle    = {The Semantic Web - {ISWC} 2003, Second International Semantic Web\n                  Conference, Sanibel Island, FL, USA, October 20-23, 2003, Proceedings},\n  series       = {Lecture Notes in Computer Science},\n  volume       = {2870},\n  pages        = {351--368},\n  publisher    = {Springer},\n  year         = {2003},\n  url          = {https://doi.org/10.1007/978-3-540-39718-2\\_23},\n  doi          = {10.1007/978-3-540-39718-2\\_23},\n  timestamp    = {Tue, 07 Sep 2021 13:48:16 +0200},\n  biburl       = {https://dblp.org/rec/conf/semweb/RichardsonAD03.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"assets/pages/datasets/epinions_v1/#version-v1","title":"Version: v1","text":""},{"location":"assets/pages/datasets/epinions_v1/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum trust HttpSource gz https://snap.stanford.edu/data/soc-Epinions1.txt.gz md5:8df7433d4486ba68eb25e623feacff04"},{"location":"assets/pages/datasets/epinions_v1/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/epinions_v1/#trust","title":"trust","text":"<ul> <li>Type: interactions</li> <li>Format: <code>transactions_tabular</code></li> <li>Required: yes</li> <li>Source: <code>trust</code></li> <li>Filename: <code>soc-Epinions1.txt</code></li> </ul> <p>Schema</p> <pre><code>sep: \"\\t\"\nuser_col: 0\nitem_col: 1\nskiprows: 4\n</code></pre>"},{"location":"assets/pages/datasets/epinions_v1/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-15</p> Metric Value n_users 60341 n_items 51957 n_interactions 508837 space_size 55.99229712201492 space_size_log 1.7481282850865338 shape 1.161364205015686 shape_log 0.06496843629715585 density 0.00016230134290924048 density_log -3.789677886731624 gini_item 0.7921899552346756 gini_user 0.7644161705431212 ratings_per_user 8.432690873535407 ratings_per_item 9.793425332486478"},{"location":"assets/pages/datasets/epinions_v1/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. https://snap.stanford.edu/data/soc-Epinions1.html</p>"},{"location":"assets/pages/datasets/gowalla_checkins/","title":"Gowalla","text":""},{"location":"assets/pages/datasets/gowalla_checkins/#overview","title":"Overview","text":"<p>Dataset name: Gowalla Latest version: checkins Available versions: checkins, friendships Source: https://snap.stanford.edu/data/loc-gowalla.html</p> <p>Gowalla is a location-based social networking website where users share their locations by checking-in.</p>"},{"location":"assets/pages/datasets/gowalla_checkins/#citation","title":"Citation","text":"<pre><code>@inproceedings{DBLP:conf/kdd/ChoML11,\n  author       = {Eunjoon Cho and\n                  Seth A. Myers and\n                  Jure Leskovec},\n  editor       = {Chid Apt{\\'{e}} and\n                  Joydeep Ghosh and\n                  Padhraic Smyth},\n  title        = {Friendship and mobility: user movement in location-based social networks},\n  booktitle    = {Proceedings of the 17th {ACM} {SIGKDD} International Conference on\n                  Knowledge Discovery and Data Mining, San Diego, CA, USA, August 21-24,\n                  2011},\n  pages        = {1082--1090},\n  publisher    = {{ACM}},\n  year         = {2011},\n  url          = {https://doi.org/10.1145/2020408.2020579},\n  doi          = {10.1145/2020408.2020579},\n  timestamp    = {Sun, 02 Jun 2019 21:11:54 +0200},\n  biburl       = {https://dblp.org/rec/conf/kdd/ChoML11.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"assets/pages/datasets/gowalla_checkins/#version-checkins","title":"Version: checkins","text":""},{"location":"assets/pages/datasets/gowalla_checkins/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum checkins HttpSource gz https://snap.stanford.edu/data/loc-gowalla_totalCheckins.txt.gz md5:8ebd5ed2dd376d8982987c49429cb9f9"},{"location":"assets/pages/datasets/gowalla_checkins/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/gowalla_checkins/#checkins","title":"checkins","text":"<ul> <li>Type: interactions</li> <li>Format: <code>transactions_tabular</code></li> <li>Required: yes</li> <li>Source: <code>checkins</code></li> <li>Filename: <code>loc-gowalla_totalCheckins.txt</code></li> </ul> <p>Schema</p> <pre><code>sep: \"\\t\"\nuser_col: 0\nitem_col: 4\ntimestamp_col: 1\n</code></pre>"},{"location":"assets/pages/datasets/gowalla_checkins/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-15</p> Metric Value n_users 107092 n_items 1280969 n_interactions 6442892 space_size 370.3802534531235 space_size_log 2.5686478245408155 shape 0.08360233541951445 shape_log -1.0777815904360875 density 4.696617612528927e-05 density_log -4.32821479760447 gini_item 0.6297417387377424 gini_user 0.6915022878282465 ratings_per_user 60.16221566503567 ratings_per_item 5.029701733609478"},{"location":"assets/pages/datasets/gowalla_checkins/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. https://snap.stanford.edu/data/loc-gowalla.html</p>"},{"location":"assets/pages/datasets/gowalla_friendships/","title":"Gowalla","text":""},{"location":"assets/pages/datasets/gowalla_friendships/#overview","title":"Overview","text":"<p>Dataset name: Gowalla Latest version: checkins Available versions: checkins, friendships Source: https://snap.stanford.edu/data/loc-gowalla.html</p> <p>Gowalla is a location-based social networking website where users share their locations by checking-in.</p>"},{"location":"assets/pages/datasets/gowalla_friendships/#citation","title":"Citation","text":"<pre><code>@inproceedings{DBLP:conf/kdd/ChoML11,\n  author       = {Eunjoon Cho and\n                  Seth A. Myers and\n                  Jure Leskovec},\n  editor       = {Chid Apt{\\'{e}} and\n                  Joydeep Ghosh and\n                  Padhraic Smyth},\n  title        = {Friendship and mobility: user movement in location-based social networks},\n  booktitle    = {Proceedings of the 17th {ACM} {SIGKDD} International Conference on\n                  Knowledge Discovery and Data Mining, San Diego, CA, USA, August 21-24,\n                  2011},\n  pages        = {1082--1090},\n  publisher    = {{ACM}},\n  year         = {2011},\n  url          = {https://doi.org/10.1145/2020408.2020579},\n  doi          = {10.1145/2020408.2020579},\n  timestamp    = {Sun, 02 Jun 2019 21:11:54 +0200},\n  biburl       = {https://dblp.org/rec/conf/kdd/ChoML11.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"assets/pages/datasets/gowalla_friendships/#version-friendships","title":"Version: friendships","text":""},{"location":"assets/pages/datasets/gowalla_friendships/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum friendships HttpSource gz https://snap.stanford.edu/data/loc-gowalla_edges.txt.gz md5:68bce8dc51609fe32bbd95e668aaf65e"},{"location":"assets/pages/datasets/gowalla_friendships/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/gowalla_friendships/#ratings","title":"ratings","text":"<ul> <li>Type: interactions</li> <li>Format: <code>transactions_tabular</code></li> <li>Required: yes</li> <li>Source: <code>friendships</code></li> <li>Filename: <code>loc-gowalla_edges.txt</code></li> </ul> <p>Schema</p> <pre><code>sep: \"\\t\"\nuser_col: 0\nitem_col: 1\n</code></pre>"},{"location":"assets/pages/datasets/gowalla_friendships/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-15</p> Metric Value n_users 196591 n_items 196591 n_interactions 1900654 space_size 196.591 space_size_log 2.2935636318083987 shape 1.0 shape_log 0.0 density 4.917855913452399e-05 density_log -4.308224199653671 gini_item 0.6833529689546158 gini_user 0.6833529689546158 ratings_per_user 9.668062118815206 ratings_per_item 9.668062118815206"},{"location":"assets/pages/datasets/gowalla_friendships/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. https://snap.stanford.edu/data/loc-gowalla.html</p>"},{"location":"assets/pages/datasets/lastfm_2011/","title":"LastFM","text":""},{"location":"assets/pages/datasets/lastfm_2011/#overview","title":"Overview","text":"<p>Dataset name: LastFM Latest version: 2011 Available versions: 2011 Source: https://grouplens.org/datasets/hetrec-2011/</p> <p>Last.fm dataset provided for the 2nd International Workshop on Information Heterogeneity and Fusion in Recommender Systems (HetRec 2011). The full archive contains multiple files (user-friends, tags, etc.), but this loader specifically processes the <code>user_artists.dat</code> file, which contains artists listened to by each user and a corresponding listening count (<code>weight</code>).</p>"},{"location":"assets/pages/datasets/lastfm_2011/#citation","title":"Citation","text":"<pre><code>@inproceedings{Cantador:RecSys2011,\n   author = {Cantador, Iv\\'{a}n and Brusilovsky, Peter and Kuflik, Tsvi},\n   title = {2nd Workshop on Information Heterogeneity and Fusion in Recommender Systems (HetRec 2011)},\n   booktitle = {Proceedings of the 5th ACM conference on Recommender systems},\n   series = {RecSys 2011},\n   year = {2011},\n   location = {Chicago, IL, USA},\n   publisher = {ACM},\n   address = {New York, NY, USA},\n   keywords = {information heterogeneity, information integration, recommender systems},\n}\n</code></pre>"},{"location":"assets/pages/datasets/lastfm_2011/#version-2011","title":"Version: 2011","text":""},{"location":"assets/pages/datasets/lastfm_2011/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum lastfm-2k.zip HttpSource zip https://files.grouplens.org/datasets/hetrec2011/hetrec2011-lastfm-2k.zip md5:296d61afe4e8632b173fc2dd3be20ce2"},{"location":"assets/pages/datasets/lastfm_2011/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/lastfm_2011/#ratings","title":"ratings","text":"<ul> <li>Type: interactions</li> <li>Format: <code>transactions_tabular</code></li> <li>Required: yes</li> <li>Source: <code>lastfm-2k.zip</code></li> <li>Filename: <code>user_artists.dat</code></li> </ul> <p>Schema</p> <pre><code>sep: \"\\t\"\nuser_col: userID\nitem_col: artistID\nrating_col: weight\nheader: 0\n</code></pre>"},{"location":"assets/pages/datasets/lastfm_2011/#tags","title":"tags","text":"<ul> <li>Type: content</li> <li>Source: <code>lastfm-2k.zip</code></li> <li>Filename: <code>tags.dat</code></li> <li>About: items</li> </ul>"},{"location":"assets/pages/datasets/lastfm_2011/#artists","title":"artists","text":"<ul> <li>Type: content</li> <li>Source: <code>lastfm-2k.zip</code></li> <li>Filename: <code>artists.dat</code></li> <li>About: users</li> </ul>"},{"location":"assets/pages/datasets/lastfm_2011/#user_taggedartists","title":"user_taggedartists","text":"<ul> <li>Type: content</li> <li>Source: <code>lastfm-2k.zip</code></li> <li>Filename: <code>user_taggedartists.dat</code></li> <li>About: users</li> </ul>"},{"location":"assets/pages/datasets/lastfm_2011/#user_taggedartists_timestamps","title":"user_taggedartists_timestamps","text":"<ul> <li>Type: content</li> <li>Source: <code>lastfm-2k.zip</code></li> <li>Filename: <code>user_taggedartists_timestamps.dat</code></li> <li>About: users</li> </ul>"},{"location":"assets/pages/datasets/lastfm_2011/#user_friends","title":"user_friends","text":"<ul> <li>Type: content</li> <li>Source: <code>lastfm-2k.zip</code></li> <li>Filename: <code>user_friends.dat</code></li> <li>About: users</li> </ul>"},{"location":"assets/pages/datasets/lastfm_2011/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-15</p> Metric Value n_users 1892 n_items 17632 n_interactions 92834 space_size 5.77578946984739 space_size_log 0.7616113546187324 shape 0.10730490018148821 shape_log -0.969380445105917 density 0.002782815119924182 density_log -2.555515645647218 gini_item 0.7300996009718888 gini_user 0.01859984137728208 ratings_per_user 49.06659619450317 ratings_per_item 5.265086206896552"},{"location":"assets/pages/datasets/lastfm_2011/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. https://grouplens.org/datasets/hetrec-2011/</p>"},{"location":"assets/pages/datasets/mind_large/","title":"Mind","text":""},{"location":"assets/pages/datasets/mind_large/#overview","title":"Overview","text":"<p>Dataset name: Mind Latest version: large Available versions: small, large Source: https://msnews.github.io/</p> <p>MIcrosoft News Dataset (MIND) is a large-scale dataset for news recommendation research.</p>"},{"location":"assets/pages/datasets/mind_large/#citation","title":"Citation","text":"<pre><code>@inproceedings{DBLP:conf/acl/WuQCWQLLXGWZ20,\n  author       = {Fangzhao Wu and\n                  Ying Qiao and\n                  Jiun{-}Hung Chen and\n                  Chuhan Wu and\n                  Tao Qi and\n                  Jianxun Lian and\n                  Danyang Liu and\n                  Xing Xie and\n                  Jianfeng Gao and\n                  Winnie Wu and\n                  Ming Zhou},\n  title        = {{MIND:} {A} Large-scale Dataset for News Recommendation},\n  booktitle    = {{ACL}},\n  pages        = {3597--3606},\n  publisher    = {Association for Computational Linguistics},\n  year         = {2020}\n}\n</code></pre>"},{"location":"assets/pages/datasets/mind_large/#version-large","title":"Version: large","text":""},{"location":"assets/pages/datasets/mind_large/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum train_archive ManualSource zip md5:0bfe5f08404a69b2bd76721e7b7f7d5d validation_archive ManualSource zip md5:64b9fc265c16814ba0f470542ef6cd69 test_archive ManualSource zip md5:081f0b249f9d7927cb0c78fb37db833a"},{"location":"assets/pages/datasets/mind_large/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/mind_large/#train","title":"train","text":"<ul> <li>Type: interactions</li> <li>Format: <code>sequence_tabular_inline</code></li> <li>Required: yes</li> <li>Source: <code>train_archive</code></li> <li>Filename: <code>behaviors.tsv</code></li> </ul> <p>Schema</p> <pre><code>user_col: user\nsequence_col: sequence\ntimestamp_col: time\ncols:\n- impression_id\n- user\n- time\n- sequence\n- impressions\ncol_sep: \"\\t\"\nsequence_sep: ' '\n</code></pre>"},{"location":"assets/pages/datasets/mind_large/#validation","title":"validation","text":"<ul> <li>Type: interactions</li> <li>Format: <code>sequence_tabular_inline</code></li> <li>Source: <code>validation_archive</code></li> <li>Filename: <code>behaviors.tsv</code></li> </ul> <p>Schema</p> <pre><code>user_col: user\nsequence_col: sequence\ntimestamp_col: time\ncols:\n- impression_id\n- user\n- time\n- sequence\n- impressions\ncol_sep: \"\\t\"\nsequence_sep: ' '\n</code></pre>"},{"location":"assets/pages/datasets/mind_large/#test","title":"test","text":"<ul> <li>Type: interactions</li> <li>Format: <code>sequence_tabular_inline</code></li> <li>Source: <code>test_archive</code></li> <li>Filename: <code>behaviors.tsv</code></li> </ul> <p>Schema</p> <pre><code>user_col: user\nsequence_col: sequence\ntimestamp_col: time\ncols:\n- impression_id\n- user\n- time\n- sequence\n- impressions\ncol_sep: \"\\t\"\nsequence_sep: ' '\n</code></pre>"},{"location":"assets/pages/datasets/mind_large/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-16</p> Metric Value n_users 698365 n_items 79546 n_interactions 73629852 space_size 235.69501965463758 space_size_log 2.3723504057769302 shape 8.779385512785055 shape_log 0.9434641198074959 density 0.001325418768112102 density_log -2.8776468840317686 gini_item 0.9249633473855028 gini_user 0.778868062516292 ratings_per_user 105.43176132824526 ratings_per_item 925.6260779926081"},{"location":"assets/pages/datasets/mind_large/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. https://msnews.github.io/</p>"},{"location":"assets/pages/datasets/mind_small/","title":"Mind","text":""},{"location":"assets/pages/datasets/mind_small/#overview","title":"Overview","text":"<p>Dataset name: Mind Latest version: large Available versions: small, large Source: https://msnews.github.io/</p> <p>MIcrosoft News Dataset (MIND) is a large-scale dataset for news recommendation research.</p>"},{"location":"assets/pages/datasets/mind_small/#citation","title":"Citation","text":"<pre><code>@inproceedings{DBLP:conf/acl/WuQCWQLLXGWZ20,\n  author       = {Fangzhao Wu and\n                  Ying Qiao and\n                  Jiun{-}Hung Chen and\n                  Chuhan Wu and\n                  Tao Qi and\n                  Jianxun Lian and\n                  Danyang Liu and\n                  Xing Xie and\n                  Jianfeng Gao and\n                  Winnie Wu and\n                  Ming Zhou},\n  title        = {{MIND:} {A} Large-scale Dataset for News Recommendation},\n  booktitle    = {{ACL}},\n  pages        = {3597--3606},\n  publisher    = {Association for Computational Linguistics},\n  year         = {2020}\n}\n</code></pre>"},{"location":"assets/pages/datasets/mind_small/#version-small","title":"Version: small","text":""},{"location":"assets/pages/datasets/mind_small/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum train_archive ManualSource zip md5:bd6ae77fa15949653f39829e946d327c validation_archive ManualSource zip md5:1c9e798fe440c1999547211cd5245e3e"},{"location":"assets/pages/datasets/mind_small/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/mind_small/#train","title":"train","text":"<ul> <li>Type: interactions</li> <li>Format: <code>sequence_tabular_inline</code></li> <li>Required: yes</li> <li>Source: <code>train_archive</code></li> <li>Filename: <code>behaviors.tsv</code></li> </ul> <p>Schema</p> <pre><code>user_col: user\nsequence_col: sequence\ntimestamp_col: time\ncols:\n- impression_id\n- user\n- time\n- sequence\n- impressions\ncol_sep: \"\\t\"\nsequence_sep: ' '\n</code></pre>"},{"location":"assets/pages/datasets/mind_small/#validation","title":"validation","text":"<ul> <li>Type: interactions</li> <li>Format: <code>sequence_tabular_inline</code></li> <li>Source: <code>validation_archive</code></li> <li>Filename: <code>behaviors.tsv</code></li> </ul> <p>Schema</p> <pre><code>user_col: user\nsequence_col: sequence\ntimestamp_col: time\ncols:\n- impression_id\n- user\n- time\n- sequence\n- impressions\ncol_sep: \"\\t\"\nsequence_sep: ' '\n</code></pre>"},{"location":"assets/pages/datasets/mind_small/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-16</p> Metric Value n_users 49108 n_items 33195 n_interactions 5107630 space_size 40.37499300309537 space_size_log 1.6061124600768104 shape 1.4793794246121403 shape_log 0.17007957418774253 density 0.00313324610892637 density_log -2.5040054909941873 gini_item 0.8403493747706466 gini_user 0.7792197590761297 ratings_per_user 104.00810458581087 ratings_per_item 153.8674499171562"},{"location":"assets/pages/datasets/mind_small/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. https://msnews.github.io/</p>"},{"location":"assets/pages/datasets/movielens_100k/","title":"Movielens","text":""},{"location":"assets/pages/datasets/movielens_100k/#overview","title":"Overview","text":"<p>Dataset name: Movielens Latest version: 1m Available versions: 100k, 1m, 20m Source: https://grouplens.org/datasets/movielens/</p> <p>The MovieLens datasets are a collection of movie ratings data collected by the GroupLens Research project at the University of Minnesota.</p>"},{"location":"assets/pages/datasets/movielens_100k/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/tiis/HarperK16,\nauthor       = {F. Maxwell Harper and\n                Joseph A. Konstan},\ntitle        = {The MovieLens Datasets: History and Context},\njournal      = {{ACM} Trans. Interact. Intell. Syst.},\nvolume       = {5},\nnumber       = {4},\npages        = {19:1--19:19},\nyear         = {2016}\n}\n</code></pre>"},{"location":"assets/pages/datasets/movielens_100k/#version-100k","title":"Version: 100k","text":""},{"location":"assets/pages/datasets/movielens_100k/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum ml100k_zip HttpSource zip https://files.grouplens.org/datasets/movielens/ml-100k.zip md5:0e33842e24a9c977be4e0107933c0723"},{"location":"assets/pages/datasets/movielens_100k/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/movielens_100k/#ratings","title":"ratings","text":"<ul> <li>Type: interactions</li> <li>Format: <code>transactions_tabular</code></li> <li>Required: yes</li> <li>Source: <code>ml100k_zip</code></li> <li>Filename: <code>u.data</code></li> </ul> <p>Schema</p> <pre><code>sep: \"\\t\"\nuser_col: 0\nitem_col: 1\nrating_col: 2\ntimestamp_col: 3\n</code></pre>"},{"location":"assets/pages/datasets/movielens_100k/#genre","title":"genre","text":"<ul> <li>Type: content</li> <li>Source: <code>ml100k_zip</code></li> <li>Filename: <code>u.genre</code></li> <li>About: items</li> </ul>"},{"location":"assets/pages/datasets/movielens_100k/#info","title":"info","text":"<ul> <li>Type: content</li> <li>Source: <code>ml100k_zip</code></li> <li>Filename: <code>u.info</code></li> <li>About: items</li> </ul>"},{"location":"assets/pages/datasets/movielens_100k/#item","title":"item","text":"<ul> <li>Type: content</li> <li>Source: <code>ml100k_zip</code></li> <li>Filename: <code>u.item</code></li> <li>About: items</li> </ul>"},{"location":"assets/pages/datasets/movielens_100k/#occupation","title":"occupation","text":"<ul> <li>Type: content</li> <li>Source: <code>ml100k_zip</code></li> <li>Filename: <code>u.occupation</code></li> <li>About: users</li> </ul>"},{"location":"assets/pages/datasets/movielens_100k/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-15</p> Metric Value n_users 943 n_items 1682 n_interactions 100000 space_size 1.2594149435352908 space_size_log 0.10016884209961084 shape 0.56064209274673 shape_log -0.251314298724565 density 0.06304669364224531 density_log -1.2003376841992217 gini_item 0.628999631391201 gini_user 0.47190850477200424 ratings_per_user 106.04453870625663 ratings_per_item 59.45303210463734"},{"location":"assets/pages/datasets/movielens_100k/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. https://grouplens.org/datasets/movielens/</p>"},{"location":"assets/pages/datasets/movielens_1m/","title":"Movielens","text":""},{"location":"assets/pages/datasets/movielens_1m/#overview","title":"Overview","text":"<p>Dataset name: Movielens Latest version: 1m Available versions: 100k, 1m, 20m Source: https://grouplens.org/datasets/movielens/</p> <p>The MovieLens datasets are a collection of movie ratings data collected by the GroupLens Research project at the University of Minnesota.</p>"},{"location":"assets/pages/datasets/movielens_1m/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/tiis/HarperK16,\nauthor       = {F. Maxwell Harper and\n                Joseph A. Konstan},\ntitle        = {The MovieLens Datasets: History and Context},\njournal      = {{ACM} Trans. Interact. Intell. Syst.},\nvolume       = {5},\nnumber       = {4},\npages        = {19:1--19:19},\nyear         = {2016}\n}\n</code></pre>"},{"location":"assets/pages/datasets/movielens_1m/#version-1m","title":"Version: 1m","text":""},{"location":"assets/pages/datasets/movielens_1m/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum ml1m_zip HttpSource zip https://files.grouplens.org/datasets/movielens/ml-1m.zip md5:c4d9eecfca2ab87c1945afe126590906"},{"location":"assets/pages/datasets/movielens_1m/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/movielens_1m/#ratings","title":"ratings","text":"<ul> <li>Type: interactions</li> <li>Format: <code>transactions_tabular</code></li> <li>Required: yes</li> <li>Source: <code>ml1m_zip</code></li> <li>Filename: <code>ratings.dat</code></li> </ul> <p>Schema</p> <pre><code>sep: '::'\nuser_col: 0\nitem_col: 1\nrating_col: 2\ntimestamp_col: 3\nengine: python\n</code></pre>"},{"location":"assets/pages/datasets/movielens_1m/#movies","title":"movies","text":"<ul> <li>Type: content</li> <li>Source: <code>ml1m_zip</code></li> <li>Filename: <code>movies.dat</code></li> <li>About: items</li> </ul>"},{"location":"assets/pages/datasets/movielens_1m/#users","title":"users","text":"<ul> <li>Type: content</li> <li>Source: <code>ml1m_zip</code></li> <li>Filename: <code>users.dat</code></li> <li>About: users</li> </ul>"},{"location":"assets/pages/datasets/movielens_1m/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-15</p> Metric Value n_users 6040 n_items 3706 n_interactions 1000209 space_size 4.731198579641315 space_size_log 0.6749711768020052 shape 1.6297895304910954 shape_log 0.21213152363825302 density 0.044683625622312845 density_log -1.34985159554118 gini_item 0.6335616301416965 gini_user 0.5286242435264804 ratings_per_user 165.5975165562914 ratings_per_item 269.88909875876953"},{"location":"assets/pages/datasets/movielens_1m/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. https://grouplens.org/datasets/movielens/</p>"},{"location":"assets/pages/datasets/movielens_20m/","title":"Movielens","text":""},{"location":"assets/pages/datasets/movielens_20m/#overview","title":"Overview","text":"<p>Dataset name: Movielens Latest version: 1m Available versions: 100k, 1m, 20m Source: https://grouplens.org/datasets/movielens/</p> <p>The MovieLens datasets are a collection of movie ratings data collected by the GroupLens Research project at the University of Minnesota.</p>"},{"location":"assets/pages/datasets/movielens_20m/#citation","title":"Citation","text":"<pre><code>@article{DBLP:journals/tiis/HarperK16,\nauthor       = {F. Maxwell Harper and\n                Joseph A. Konstan},\ntitle        = {The MovieLens Datasets: History and Context},\njournal      = {{ACM} Trans. Interact. Intell. Syst.},\nvolume       = {5},\nnumber       = {4},\npages        = {19:1--19:19},\nyear         = {2016}\n}\n</code></pre>"},{"location":"assets/pages/datasets/movielens_20m/#version-20m","title":"Version: 20m","text":""},{"location":"assets/pages/datasets/movielens_20m/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum ml20m_zip HttpSource zip https://files.grouplens.org/datasets/movielens/ml-20m.zip md5:cd245b17a1ae2cc31bb14903e1204af3"},{"location":"assets/pages/datasets/movielens_20m/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/movielens_20m/#ratings","title":"ratings","text":"<ul> <li>Type: interactions</li> <li>Format: <code>transactions_tabular</code></li> <li>Required: yes</li> <li>Source: <code>ml20m_zip</code></li> <li>Filename: <code>ratings.csv</code></li> </ul> <p>Schema</p> <pre><code>sep: ','\nuser_col: userId\nitem_col: movieId\nrating_col: rating\ntimestamp_col: timestamp\nheader: 0\n</code></pre>"},{"location":"assets/pages/datasets/movielens_20m/#genome_scores","title":"genome_scores","text":"<ul> <li>Type: content</li> <li>Source: <code>ml20m_zip</code></li> <li>Filename: <code>genome_scores.csv</code></li> <li>About: items</li> </ul>"},{"location":"assets/pages/datasets/movielens_20m/#genome_tags","title":"genome_tags","text":"<ul> <li>Type: content</li> <li>Source: <code>ml20m_zip</code></li> <li>Filename: <code>genome_tags.csv</code></li> <li>About: items</li> </ul>"},{"location":"assets/pages/datasets/movielens_20m/#links","title":"links","text":"<ul> <li>Type: content</li> <li>Source: <code>ml20m_zip</code></li> <li>Filename: <code>links.csv</code></li> <li>About: items</li> </ul>"},{"location":"assets/pages/datasets/movielens_20m/#movies","title":"movies","text":"<ul> <li>Type: content</li> <li>Source: <code>ml20m_zip</code></li> <li>Filename: <code>movies.csv</code></li> <li>About: users</li> </ul>"},{"location":"assets/pages/datasets/movielens_20m/#tags","title":"tags","text":"<ul> <li>Type: content</li> <li>Source: <code>ml20m_zip</code></li> <li>Filename: <code>tags.csv</code></li> <li>About: users</li> </ul>"},{"location":"assets/pages/datasets/movielens_20m/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-15</p> Metric Value n_users 138493 n_items 26744 n_interactions 20000263 space_size 60.8593196807194 space_size_log 1.784327093264329 shape 5.178469937182172 shape_log 0.7142014593596334 density 0.0053998478135544505 density_log -2.267618479929789 gini_item 0.902942497565131 gini_user 0.5807014936671218 ratings_per_user 144.4135299257002 ratings_per_item 747.8411232425965"},{"location":"assets/pages/datasets/movielens_20m/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. https://grouplens.org/datasets/movielens/</p>"},{"location":"assets/pages/datasets/tmall_v1/","title":"Tmall","text":""},{"location":"assets/pages/datasets/tmall_v1/#overview","title":"Overview","text":"<p>Dataset name: Tmall Latest version: v1 Available versions: v1 Source: https://tianchi.aliyun.com/dataset/42</p> <p>This dataset is from IJCAI 2015 Contest: https://tianchi.aliyun.com/competition/entrance/231674/information The data set contains anonymized users' shopping logs in the past 6 months before and on the \"Double 11\" day,and the label information indicating whether they are repeated buyers.  Due to privacy issue, data is sampled in a biased way, so the statistical result on this data set would deviate from the actual of Tmall.com.</p>"},{"location":"assets/pages/datasets/tmall_v1/#citation","title":"Citation","text":"<pre><code>@misc{\n    title={IJCAI-15 Repeat Buyers Prediction Dataset}\n    url={https://tianchi.aliyun.com/dataset/dataDetail?dataId=42},\n    author={Tianchi},\n    year={2018}\n    }\n</code></pre>"},{"location":"assets/pages/datasets/tmall_v1/#version-v1","title":"Version: v1","text":""},{"location":"assets/pages/datasets/tmall_v1/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum main_archive ManualSource zip md5:b8143773b800c7420b201d04bc7a9c15"},{"location":"assets/pages/datasets/tmall_v1/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/tmall_v1/#train","title":"train","text":"<ul> <li>Type: interactions</li> <li>Format: <code>sequence_tabular_inline</code></li> <li>Required: yes</li> <li>Source: <code>main_archive</code></li> <li>Filename: <code>train_format2.csv</code></li> </ul> <p>Schema</p> <pre><code>cols:\n- user_id\n- age_range\n- gender\n- merchant_id\n- label\n- activity_log\nuser_col: user_id\nsequence_col: activity_log\ncol_sep: ','\nsequence_sep: '#'\nheader: 0\n</code></pre>"},{"location":"assets/pages/datasets/tmall_v1/#test","title":"test","text":"<ul> <li>Type: interactions</li> <li>Format: <code>sequence_tabular_inline</code></li> <li>Source: <code>main_archive</code></li> <li>Filename: <code>train_format2.csv</code></li> </ul> <p>Schema</p> <pre><code>cols:\n- user_id\n- age_range\n- gender\n- merchant_id\n- label\n- activity_log\nuser_col: user_id\nsequence_col: activity_log\ncol_sep: ','\nsequence_sep: '#'\nheader: 0\n</code></pre>"},{"location":"assets/pages/datasets/tmall_v1/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-16</p> Metric Value n_users 212062 n_items 9790396 n_interactions 27384139 space_size 1440.8924167168068 space_size_log 3.1586315557130478 shape 0.02166020659429915 shape_log -1.6643374054052402 density 1.318973223933264e-05 density_log -4.879764020841857 gini_item 0.5585949565188277 gini_user 0.5412012856445343 ratings_per_user 129.13270175703332 ratings_per_item 2.797040998137358"},{"location":"assets/pages/datasets/tmall_v1/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. https://tianchi.aliyun.com/dataset/42</p>"},{"location":"assets/pages/datasets/yelp_v1/","title":"Yelp","text":""},{"location":"assets/pages/datasets/yelp_v1/#overview","title":"Overview","text":"<p>Dataset name: Yelp Latest version: v1 Available versions: v1 Source: https://business.yelp.com/data/resources/open-dataset/</p> <p>The Yelp dataset is a subset of Yelp's businesses, reviews, and user data for academic research.</p>"},{"location":"assets/pages/datasets/yelp_v1/#citation","title":"Citation","text":"<p>No citation provided.</p>"},{"location":"assets/pages/datasets/yelp_v1/#version-v1","title":"Version: v1","text":""},{"location":"assets/pages/datasets/yelp_v1/#data-sources","title":"Data Sources","text":"Name Source type Archive URL Checksum main_archive ManualSource zip md5:b0c36fe2d00a52d8de44fa3b2513c9d2 data_archive NestedSource tar md5:0bc8cc1481ccbbd140d2aba2909a928a"},{"location":"assets/pages/datasets/yelp_v1/#resources","title":"Resources","text":""},{"location":"assets/pages/datasets/yelp_v1/#review","title":"review","text":"<ul> <li>Type: interactions</li> <li>Format: <code>transactions_jsonl</code></li> <li>Required: yes</li> <li>Source: <code>data_archive</code></li> <li>Filename: <code>yelp_academic_dataset_review.json</code></li> </ul> <p>Schema</p> <pre><code>user_col: user_id\nitem_col: business_id\nrating_col: stars\ntimestamp_col: date\n</code></pre>"},{"location":"assets/pages/datasets/yelp_v1/#business","title":"business","text":"<ul> <li>Type: content</li> <li>Format: <code>json</code></li> <li>Source: <code>data_archive</code></li> <li>Filename: <code>yelp_academic_dataset_business.json</code></li> </ul>"},{"location":"assets/pages/datasets/yelp_v1/#checkin","title":"checkin","text":"<ul> <li>Type: content</li> <li>Format: <code>json</code></li> <li>Source: <code>data_archive</code></li> <li>Filename: <code>yelp_academic_dataset_checkin.json</code></li> </ul>"},{"location":"assets/pages/datasets/yelp_v1/#tip","title":"tip","text":"<ul> <li>Type: content</li> <li>Format: <code>json</code></li> <li>Source: <code>data_archive</code></li> <li>Filename: <code>yelp_academic_dataset_tip.json</code></li> </ul>"},{"location":"assets/pages/datasets/yelp_v1/#user","title":"user","text":"<ul> <li>Type: content</li> <li>Format: <code>json</code></li> <li>Source: <code>data_archive</code></li> <li>Filename: <code>yelp_academic_dataset_user.json</code></li> </ul>"},{"location":"assets/pages/datasets/yelp_v1/#terms_of_use","title":"terms_of_use","text":"<ul> <li>Type: content</li> <li>Format: <code>pdf</code></li> <li>Source: <code>data_archive</code></li> <li>Filename: <code>Dataset_User_Agreement.pdf</code></li> </ul>"},{"location":"assets/pages/datasets/yelp_v1/#dataset-characteristics","title":"Dataset Characteristics","text":"<p>Computed at: 2025-12-15</p> Metric Value n_users 1987929 n_items 150346 n_interactions 6990280 space_size 546.6966008985239 space_size_log 2.737746373661808 shape 13.222360421960012 shape_log 1.1213089912105905 density 2.3388470653961263e-05 density_log -4.630998175294749 gini_item 0.678351072470664 gini_user 0.6115362253305341 ratings_per_user 3.51636300894046 ratings_per_item 46.49461907865856"},{"location":"assets/pages/datasets/yelp_v1/#license-usage","title":"License &amp; Usage","text":"<p>Please refer to the official dataset page for licensing and usage restrictions. https://business.yelp.com/data/resources/open-dataset/</p>"},{"location":"documentation/data/","title":"Data Module Reference","text":"<p>This section provides a detailed API reference for the <code>datarec.data</code> package, which defines the core dataset abstraction, dataset builders, and supporting utilities.</p>"},{"location":"documentation/data/#on-this-page","title":"On This Page","text":"<ul> <li>Core Data Utilities</li> <li>Dataset Builders</li> <li>DataRec and Data Wrappers</li> <li>Torch Dataset Wrappers</li> </ul>"},{"location":"documentation/data/#core-data-utilities","title":"Core Data Utilities","text":"<p>Utilities shared across the data layer (encoders, helpers, characteristics helpers).</p>"},{"location":"documentation/data/#datarec.data.utils.Encoder","title":"<code>Encoder</code>","text":"<p>A simple encoder class to encode and decode IDs.</p> Source code in <code>datarec/data/utils.py</code> <pre><code>class Encoder:\n    \"\"\"\n    A simple encoder class to encode and decode IDs.\n    \"\"\"\n\n    def __init__(self):\n        self.encoding = dict()\n\n    def is_encoded(self) -&gt; bool:\n        \"\"\"\n        Checks if the encoding dictionary is not empty.\n\n        Returns:\n            (bool): True if the encoding dictionary is not empty, False otherwise.\n        \"\"\"\n        return bool(self.encoding)\n\n    def build_encoding(self, lst: list, offset: int=0) -&gt; None:\n        \"\"\"\n        Encodes a list of public IDs into integer IDs.\n\n        Args:\n            lst (list): A list of public IDs.\n            offset (int): The starting integer for the private IDs.\n        \"\"\"\n        if self.is_encoded():\n            raise ValueError('Encoding dictionary is not empty. Please, reset it before building a new encoding.')\n        self.encoding = dict(zip(lst, range(offset, offset + len(lst))))\n\n    def reset_encoding(self) -&gt; None:\n        \"\"\"\n        Resets the encoding dictionary.\n        \"\"\"\n        self.encoding = dict()\n\n    def change_offset(self, offset: int) -&gt; None:\n        \"\"\"\n        Changes the offset of the current encoding.\n\n        Args:\n            offset (int): The new starting integer for the private IDs.\n        \"\"\"\n        if not self.encoding:\n            raise ValueError('Encoding dictionary is empty. Please, build the encoding first.')\n        lst = list(self.encoding.values())\n        current_min = min(lst)\n        new_offset = offset - current_min\n        self.encoding = {idx: el + new_offset for idx, el in self.encoding.items()}\n\n    def apply_encoding(self, encoding: dict) -&gt; None:\n        \"\"\"\n        Applies an external encoding dictionary.\n\n        Args:\n            encoding (dict): A dictionary encoding IDs.\n        \"\"\"\n        if self.is_encoded():\n            raise ValueError('Encoding dictionary is not empty. Please, reset it before applying a new encoding.')\n        self.encoding = encoding\n\n    def encode(self, lst: list) -&gt; list:\n        \"\"\"\n        Encodes a list of public IDs into integer IDs using the built encoding.\n\n        Args:\n            lst (list): A list of public IDs.\n\n        Returns:\n            (list): A list of encoded integer IDs.\n        \"\"\"\n        if self.is_encoded() is False:\n            raise ValueError('Encoding dictionary is empty. Please, build the encoding first.')\n        return [self.encoding[el] for el in lst]\n\n    def decode(self, lst: list) -&gt; list:\n        \"\"\"\n        Creates a reverse mapping from private IDs back to public IDs.\n\n        Args:\n            lst (list): A list of private IDs.\n\n        Returns:\n            (list): A list of decoded public IDs.\n        \"\"\"\n        if self.is_encoded() is False:\n            raise ValueError('Encoding dictionary is empty. Please, build the encoding first.')\n        decoder = {el: idx for idx, el in self.encoding.items()}\n        if len(decoder) != len(self.encoding):\n            print('WARNING: the ID encoding could be incorrect. Please, check your data.')\n        return [decoder[el] for el in lst]\n</code></pre>"},{"location":"documentation/data/#datarec.data.utils.Encoder.is_encoded","title":"<code>is_encoded()</code>","text":"<p>Checks if the encoding dictionary is not empty.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the encoding dictionary is not empty, False otherwise.</p> Source code in <code>datarec/data/utils.py</code> <pre><code>def is_encoded(self) -&gt; bool:\n    \"\"\"\n    Checks if the encoding dictionary is not empty.\n\n    Returns:\n        (bool): True if the encoding dictionary is not empty, False otherwise.\n    \"\"\"\n    return bool(self.encoding)\n</code></pre>"},{"location":"documentation/data/#datarec.data.utils.Encoder.build_encoding","title":"<code>build_encoding(lst, offset=0)</code>","text":"<p>Encodes a list of public IDs into integer IDs.</p> <p>Parameters:</p> Name Type Description Default <code>lst</code> <code>list</code> <p>A list of public IDs.</p> required <code>offset</code> <code>int</code> <p>The starting integer for the private IDs.</p> <code>0</code> Source code in <code>datarec/data/utils.py</code> <pre><code>def build_encoding(self, lst: list, offset: int=0) -&gt; None:\n    \"\"\"\n    Encodes a list of public IDs into integer IDs.\n\n    Args:\n        lst (list): A list of public IDs.\n        offset (int): The starting integer for the private IDs.\n    \"\"\"\n    if self.is_encoded():\n        raise ValueError('Encoding dictionary is not empty. Please, reset it before building a new encoding.')\n    self.encoding = dict(zip(lst, range(offset, offset + len(lst))))\n</code></pre>"},{"location":"documentation/data/#datarec.data.utils.Encoder.reset_encoding","title":"<code>reset_encoding()</code>","text":"<p>Resets the encoding dictionary.</p> Source code in <code>datarec/data/utils.py</code> <pre><code>def reset_encoding(self) -&gt; None:\n    \"\"\"\n    Resets the encoding dictionary.\n    \"\"\"\n    self.encoding = dict()\n</code></pre>"},{"location":"documentation/data/#datarec.data.utils.Encoder.change_offset","title":"<code>change_offset(offset)</code>","text":"<p>Changes the offset of the current encoding.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>The new starting integer for the private IDs.</p> required Source code in <code>datarec/data/utils.py</code> <pre><code>def change_offset(self, offset: int) -&gt; None:\n    \"\"\"\n    Changes the offset of the current encoding.\n\n    Args:\n        offset (int): The new starting integer for the private IDs.\n    \"\"\"\n    if not self.encoding:\n        raise ValueError('Encoding dictionary is empty. Please, build the encoding first.')\n    lst = list(self.encoding.values())\n    current_min = min(lst)\n    new_offset = offset - current_min\n    self.encoding = {idx: el + new_offset for idx, el in self.encoding.items()}\n</code></pre>"},{"location":"documentation/data/#datarec.data.utils.Encoder.apply_encoding","title":"<code>apply_encoding(encoding)</code>","text":"<p>Applies an external encoding dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>encoding</code> <code>dict</code> <p>A dictionary encoding IDs.</p> required Source code in <code>datarec/data/utils.py</code> <pre><code>def apply_encoding(self, encoding: dict) -&gt; None:\n    \"\"\"\n    Applies an external encoding dictionary.\n\n    Args:\n        encoding (dict): A dictionary encoding IDs.\n    \"\"\"\n    if self.is_encoded():\n        raise ValueError('Encoding dictionary is not empty. Please, reset it before applying a new encoding.')\n    self.encoding = encoding\n</code></pre>"},{"location":"documentation/data/#datarec.data.utils.Encoder.encode","title":"<code>encode(lst)</code>","text":"<p>Encodes a list of public IDs into integer IDs using the built encoding.</p> <p>Parameters:</p> Name Type Description Default <code>lst</code> <code>list</code> <p>A list of public IDs.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of encoded integer IDs.</p> Source code in <code>datarec/data/utils.py</code> <pre><code>def encode(self, lst: list) -&gt; list:\n    \"\"\"\n    Encodes a list of public IDs into integer IDs using the built encoding.\n\n    Args:\n        lst (list): A list of public IDs.\n\n    Returns:\n        (list): A list of encoded integer IDs.\n    \"\"\"\n    if self.is_encoded() is False:\n        raise ValueError('Encoding dictionary is empty. Please, build the encoding first.')\n    return [self.encoding[el] for el in lst]\n</code></pre>"},{"location":"documentation/data/#datarec.data.utils.Encoder.decode","title":"<code>decode(lst)</code>","text":"<p>Creates a reverse mapping from private IDs back to public IDs.</p> <p>Parameters:</p> Name Type Description Default <code>lst</code> <code>list</code> <p>A list of private IDs.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of decoded public IDs.</p> Source code in <code>datarec/data/utils.py</code> <pre><code>def decode(self, lst: list) -&gt; list:\n    \"\"\"\n    Creates a reverse mapping from private IDs back to public IDs.\n\n    Args:\n        lst (list): A list of private IDs.\n\n    Returns:\n        (list): A list of decoded public IDs.\n    \"\"\"\n    if self.is_encoded() is False:\n        raise ValueError('Encoding dictionary is empty. Please, build the encoding first.')\n    decoder = {el: idx for idx, el in self.encoding.items()}\n    if len(decoder) != len(self.encoding):\n        print('WARNING: the ID encoding could be incorrect. Please, check your data.')\n    return [decoder[el] for el in lst]\n</code></pre>"},{"location":"documentation/data/#datarec.data.utils.IncrementalEncoder","title":"<code>IncrementalEncoder</code>","text":"<p>Streaming-friendly encoder that assigns integer IDs incrementally.</p> <p>Preserves reproducibility by keeping both forward (public -&gt; int) and reverse (int -&gt; public) mappings without requiring the full list upfront.</p> Source code in <code>datarec/data/utils.py</code> <pre><code>class IncrementalEncoder:\n    \"\"\"\n    Streaming-friendly encoder that assigns integer IDs incrementally.\n\n    Preserves reproducibility by keeping both forward (public -&gt; int)\n    and reverse (int -&gt; public) mappings without requiring the full list upfront.\n    \"\"\"\n\n    def __init__(self, offset: int = 0):\n        self.offset = offset\n        self._forward: dict = {}          # public -&gt; int\n        self._reverse: list = []          # index -&gt; public (index = id - offset)\n\n    def __len__(self):\n        return len(self._forward)\n\n    def encode_one(self, key):\n        \"\"\"\n        Encode a single key, creating a new id if unseen.\n        \"\"\"\n        if key in self._forward:\n            return self._forward[key]\n        idx = self.offset + len(self._forward)\n        self._forward[key] = idx\n        self._reverse.append(key)\n        return idx\n\n    def encode_many(self, iterable):\n        \"\"\"\n        Encode an iterable of keys, returning a list of int ids.\n        \"\"\"\n        return [self.encode_one(k) for k in iterable]\n\n    def decode_one(self, idx: int):\n        \"\"\"\n        Decode a single id back to the original key.\n        \"\"\"\n        pos = idx - self.offset\n        if pos &lt; 0 or pos &gt;= len(self._reverse):\n            raise KeyError(f\"Id {idx} not in encoder\")\n        return self._reverse[pos]\n\n    def decode_many(self, iterable):\n        \"\"\"\n        Decode an iterable of ids back to original keys.\n        \"\"\"\n        return [self.decode_one(i) for i in iterable]\n\n    @property\n    def forward(self):\n        return self._forward\n\n    @property\n    def reverse(self):\n        return self._reverse\n</code></pre>"},{"location":"documentation/data/#datarec.data.utils.IncrementalEncoder.encode_one","title":"<code>encode_one(key)</code>","text":"<p>Encode a single key, creating a new id if unseen.</p> Source code in <code>datarec/data/utils.py</code> <pre><code>def encode_one(self, key):\n    \"\"\"\n    Encode a single key, creating a new id if unseen.\n    \"\"\"\n    if key in self._forward:\n        return self._forward[key]\n    idx = self.offset + len(self._forward)\n    self._forward[key] = idx\n    self._reverse.append(key)\n    return idx\n</code></pre>"},{"location":"documentation/data/#datarec.data.utils.IncrementalEncoder.encode_many","title":"<code>encode_many(iterable)</code>","text":"<p>Encode an iterable of keys, returning a list of int ids.</p> Source code in <code>datarec/data/utils.py</code> <pre><code>def encode_many(self, iterable):\n    \"\"\"\n    Encode an iterable of keys, returning a list of int ids.\n    \"\"\"\n    return [self.encode_one(k) for k in iterable]\n</code></pre>"},{"location":"documentation/data/#datarec.data.utils.IncrementalEncoder.decode_one","title":"<code>decode_one(idx)</code>","text":"<p>Decode a single id back to the original key.</p> Source code in <code>datarec/data/utils.py</code> <pre><code>def decode_one(self, idx: int):\n    \"\"\"\n    Decode a single id back to the original key.\n    \"\"\"\n    pos = idx - self.offset\n    if pos &lt; 0 or pos &gt;= len(self._reverse):\n        raise KeyError(f\"Id {idx} not in encoder\")\n    return self._reverse[pos]\n</code></pre>"},{"location":"documentation/data/#datarec.data.utils.IncrementalEncoder.decode_many","title":"<code>decode_many(iterable)</code>","text":"<p>Decode an iterable of ids back to original keys.</p> Source code in <code>datarec/data/utils.py</code> <pre><code>def decode_many(self, iterable):\n    \"\"\"\n    Decode an iterable of ids back to original keys.\n    \"\"\"\n    return [self.decode_one(i) for i in iterable]\n</code></pre>"},{"location":"documentation/data/#datarec.data.utils.set_column_name","title":"<code>set_column_name(columns, value, rename=True, default_name=None)</code>","text":"<p>Identifies a column by its name or index and optionally renames it.</p> <p>This utility function provides a flexible way to handle DataFrame columns. It can find a column based on its current name (string) or its position (integer). If <code>rename</code> is True, it replaces the found column name in the list of columns with a <code>default_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list</code> <p>The list of current column names in the DataFrame.</p> required <code>value</code> <code>Union[str, int]</code> <p>The identifier for the column, either its name or its integer index.</p> required <code>rename</code> <code>bool</code> <p>If True, the identified column's name is changed to <code>default_name</code> in the returned list. Defaults to True.</p> <code>True</code> <code>default_name</code> <code>str</code> <p>The new name for the column if <code>rename</code> is True. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[list, str]</code> <p>A tuple containing: - The (potentially modified) list of column names. - The final name of the selected column (either the original or the   <code>default_name</code> if renamed).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>value</code> is not a valid column name or index, or if it is not a string or integer.</p> Source code in <code>datarec/data/utils.py</code> <pre><code>def set_column_name(columns: list, value: Union[str, int], rename=True, default_name=None) -&gt; (list, str):\n    \"\"\"\n    Identifies a column by its name or index and optionally renames it.\n\n    This utility function provides a flexible way to handle DataFrame columns. It\n    can find a column based on its current name (string) or its position (integer).\n    If `rename` is True, it replaces the found column name in the list of columns\n    with a `default_name`.\n\n    Args:\n        columns (list): The list of current column names in the DataFrame.\n        value (Union[str, int]): The identifier for the column, either its name\n            or its integer index.\n        rename (bool, optional): If True, the identified column's name is\n            changed to `default_name` in the returned list. Defaults to True.\n        default_name (str, optional): The new name for the column if `rename` is\n            True. Defaults to None.\n\n    Returns:\n        (tuple[list, str]): A tuple containing:\n            - The (potentially modified) list of column names.\n            - The final name of the selected column (either the original or the\n              `default_name` if renamed).\n\n    Raises:\n        ValueError: If the `value` is not a valid column name or index, or if\n            it is not a string or integer.\n    \"\"\"\n    columns = list(columns)\n\n    if isinstance(value, str):\n        if value not in columns:\n            raise ValueError(f'column \\'{value}\\' is not a valid column name.')\n        selected_column = value\n\n    elif isinstance(value, int):\n        if value in columns:\n            selected_column = value\n        else:\n            if value not in range(len(columns)):\n                raise ValueError(f'column int \\'{value}\\' is out of range ({len(columns)} columns).')\n            selected_column = columns[value]\n    else:\n        raise ValueError(f'column value must be either a string (column name) or an integer (column index). Got {type(value)} instead.')\n\n    if rename is True:\n        columns[columns.index(selected_column)] = default_name\n        return columns, default_name\n\n    return columns, selected_column\n</code></pre>"},{"location":"documentation/data/#datarec.data.utils.quartiles","title":"<code>quartiles(count)</code>","text":"<p>Assigns quartile indices (0-3) to items based on their frequency counts.</p> <p>The function divides the input values into four quartiles using the  median and quantiles. Each item is assigned an integer:     0: long tail (lowest quartile)     1: common     2: popular     3: most popular (highest quartile)</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>dict</code> <p>A dictionary mapping items to numeric counts or frequencies.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping each item to its quartile index (0-3).</p> Source code in <code>datarec/data/utils.py</code> <pre><code>def quartiles(count: dict):\n    \"\"\" \n    Assigns quartile indices (0-3) to items based on their frequency counts.\n\n    The function divides the input values into four quartiles using the \n    median and quantiles. Each item is assigned an integer:\n        0: long tail (lowest quartile)\n        1: common\n        2: popular\n        3: most popular (highest quartile)\n\n    Args:\n        count (dict): A dictionary mapping items to numeric counts or frequencies.\n\n    Returns:\n        (dict): A dictionary mapping each item to its quartile index (0-3).\n    \"\"\"\n    q1, q2, q3 = statistics.quantiles(count.values())\n\n    def assign(value):\n        if value &lt;= q2:\n            if value &lt;= q1:\n                return 0\n            else:\n                return 1\n        else:\n            if value &lt;= q3:\n                return 2\n            else:\n                return 3\n\n    return {k: assign(f) for k, f in count.items()}\n</code></pre>"},{"location":"documentation/data/#datarec.data.utils.popularity","title":"<code>popularity(quartiles)</code>","text":"<p>Categorizes items based on their quartile indices.</p> <p>Converts quartile indices (0-3) into descriptive popularity categories:     0 -&gt; 'long tail'     1 -&gt; 'common'     2 -&gt; 'popular'     3 -&gt; 'most popular'</p> <p>Parameters:</p> Name Type Description Default <code>quartiles</code> <code>dict</code> <p>A dictionary mapping items to quartile indices (0-3).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping each popularity category to a list of items.</p> Source code in <code>datarec/data/utils.py</code> <pre><code>def popularity(quartiles: dict):\n    \"\"\" \n    Categorizes items based on their quartile indices.\n\n    Converts quartile indices (0-3) into descriptive popularity categories:\n        0 -&gt; 'long tail'\n        1 -&gt; 'common'\n        2 -&gt; 'popular'\n        3 -&gt; 'most popular'\n\n    Args:\n        quartiles (dict): A dictionary mapping items to quartile indices (0-3).\n\n    Returns:\n        (dict): A dictionary mapping each popularity category to a list of items.\n    \"\"\"\n    categories_map = \\\n        {3: 'most popular',\n         2: 'popular',\n         1: 'common',\n         0: 'long tail'}\n\n    categories = \\\n        {'most popular': [],\n         'popular': [],\n         'common': [],\n         'long tail': []}\n\n    for k, q in quartiles.items():\n        categories[categories_map[q]].append(k)\n\n    return categories\n</code></pre>"},{"location":"documentation/data/#datarec.data.utils.verify_checksum","title":"<code>verify_checksum(file_path, checksum)</code>","text":"<p>Verifies the MD5 checksum of a file.</p> <p>This function computes the MD5 hash of the file at the given path and compares it to the expected checksum. If the file does not exist, a FileNotFoundError is raised. If the checksum does not match, a RuntimeError is raised indicating possible corruption or version mismatch.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the file to verify.</p> required <code>checksum</code> <code>str</code> <p>The expected MD5 checksum.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>RuntimeError</code> <p>If the computed checksum does not match the expected value.</p> Source code in <code>datarec/data/utils.py</code> <pre><code>def verify_checksum(file_path: str, checksum: str) -&gt; None:\n    \"\"\"\n    Verifies the MD5 checksum of a file.\n\n    This function computes the MD5 hash of the file at the given path and\n    compares it to the expected checksum. If the file does not exist, a\n    FileNotFoundError is raised. If the checksum does not match, a RuntimeError\n    is raised indicating possible corruption or version mismatch.\n\n    Args:\n        file_path (str): The path to the file to verify.\n        checksum (str): The expected MD5 checksum.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n        RuntimeError: If the computed checksum does not match the expected value.\n    \"\"\"\n\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"File '{file_path}' not found.\")\n\n    md5 = hashlib.md5()\n    with open(file_path, \"rb\") as f:\n        for chunck in iter(lambda: f.read(65536), b\"\"):\n            md5.update(chunck)\n\n    digest = md5.hexdigest()\n    if not digest == checksum:\n        raise RuntimeError(f\"Checksum mismatch for '{file_path}': expected {checksum}, but got {digest}. \"\n                           f\"The file may be corrupted or a new version has been downloaded.\")\n\n    print(f'Checksum verified.')\n</code></pre>"},{"location":"documentation/data/#dataset-builders","title":"Dataset Builders","text":"<p>Dataset builder used by the registry to prepare and load resources.</p>"},{"location":"documentation/data/#datarec.data.datarec_builder.Dataset","title":"<code>Dataset</code>","text":"Source code in <code>datarec/data/datarec_builder.py</code> <pre><code>class Dataset:\n\n    def __init__(self, dataset_name:str, version:str, folder=None):\n\n        self.dataset_name = dataset_name\n        self.version = version\n        self.output_folder = self.find_output_folder(folder=folder)\n        self.config = load_dataset_config(dataset_name=self.dataset_name,\n                                          dataset_version=self.version)\n\n        # sets sources and resources data structures\n        self.sources = set_sources(self.config, folder=self.output_folder)\n        self.resources = set_resources(self.config)\n\n        # links resources with sources and dataset info\n        for resource in self.resources.values():\n            resource.link_source(self.sources)\n            resource.assign_dataset_info(self.dataset_name, self.version)\n            resource.output_folder = self.output_folder\n\n        # initialize prepared resources dictionary to keep track of prepared resources\n        self.prepared_resources = {}\n\n\n    def find_resource_by_constraints(\n        self,\n        *,\n        only_required: bool = True,\n        resource_types: Union[Collection[str], str, None] = None,\n        resource_names: Union[Collection[str], str, None] = None):\n\n        collection = self.resources\n        if len(collection) == 0:\n            raise ValueError('No prepared resource found. Use the method .prepare to prepare the dataset resources.')\n\n        if isinstance(resource_types, str):\n            resource_types = [resource_types]\n\n        if isinstance(resource_names, str):\n            resource_names = [resource_names]\n\n        selected: dict[str, Any] = {}\n        for res_name, res in collection.items():\n            res_type = res.type\n            # filter by required attribute\n            if only_required and not res.required:\n                continue\n            # filter by resource type\n            if resource_types is not None:\n                if res_type not in resource_types:\n                    continue\n            # filter by resource name\n            if resource_names is not None:\n                if res_name not in resource_names:\n                    continue\n\n            selected |= {res_name: res}\n\n        return selected\n\n\n    def prepare(\n        self,\n        *,\n        only_required: bool = True,\n        use_cache: bool = True,\n        resource_types: Union[Collection[str], str, None] = None,\n        resource_names: Union[Collection[str], str, None] = None):\n        \"\"\"\n        Prepare (download and decompress) all selected resources.\n        Args:\n            only_required (bool): Prepare only resources marked as required when True.\n            use_cache (bool): Use cached resources when True.\n            resource_types (Collection[str] | str | None): Resource types to include; all when None.\n            resource_names (Collection[str] | str | None): Specific resource names to include; all when None.\n        Returns:\n            dict[str, Any]: Map from resource name to the prepared artifact (e.g. local path).\n        \"\"\"\n\n        selected = self.find_resource_by_constraints(\n            only_required=only_required,\n            resource_names=resource_names,\n            resource_types=resource_types)\n\n        if len(selected) == 0:\n            print('No resource found with the given requirements.')\n\n        for res_name, res in selected.items():\n            if res_name in self.prepared_resources.keys():\n                print(f\"Resource {res_name} was already prepared.\")\n                continue\n            res.prepare(use_cache=use_cache)\n            print(f\"Resource {res_name} ready\")\n            self.prepared_resources |= {res_name: res}\n\n    def prepare_interactions(\n        self,\n        *,\n        only_required: bool = True,\n        use_cache: bool = True,\n        resource_names: Union[Collection[str], str, None] = None):\n        \"\"\"\n        Prepare (download and decompress) ratings resources.\n        Args:\n            only_required (bool): Prepare only resources marked as required when True.\n            use_cache (bool): Use cached resources when True.\n            resource_names (Collection[str] | str | None): Specific resource names to include; all when None.\n        Returns:\n            dict[str, Any]: Map from resource name to the prepared artifact (e.g. local path).\n        \"\"\"\n        self.prepare(\n                    resource_types='interactions',\n                    only_required=only_required,\n                    use_cache=use_cache,\n                    resource_names=resource_names)\n\n\n    def load(\n        self, *,\n        use_cache: bool = False,\n        to_cache: bool = False,\n        resource_name: Union[str, None] = None,\n        resource_type: Union[str, None] = 'interactions',\n        only_required: bool = False) -&gt; DataRec:\n        \"\"\"\n        Load the dataset into a DataRec object.\n        Args:\n            use_cache (bool): Load from cache when True.\n            to_cache (bool): Save to cache when True.\n            resource_name (str | None): Specific resource name to load; all when None.\n            resource_type (str | None): Resource type to load; all when None.\n            only_required (bool): When True, consider only resources marked as required.\n        Returns:\n            DataRec: The loaded dataset.\n        \"\"\"\n        resource = self.find_resource_by_constraints(\n            only_required=only_required,\n            resource_names=resource_name,\n            resource_types=resource_type)\n        if len(resource) &gt; 1:\n            raise ValueError(f'More than one resource match the name: {resource_name} and type: {resource_type}.\\n \\\n                             Resources found: {resource.keys}. Please, select one by using the resource_name attribute.')\n        if len(resource) == 0:\n            raise ValueError(f'No resource found that matches the name: {resource_name} and type: {resource_type}')\n\n        if len(resource) == 1:\n            res_name, res = next(iter(resource.items()))\n            if res.type == 'interactions':\n                if res.prepared == True:\n                    return res.load(use_cache=use_cache, to_cache=to_cache)\n                else:\n                    raise ValueError(f'Resource \\'{res_name}\\' must be prepared before loading it. Try calling .prepare method before.')\n            else:\n                raise ValueError(f'DataRec does not support load method for resources with \\'{res.type}\\' type.\\\n                                 This version of DataRec supports only \\'ratings\\' type.')\n\n        raise ValueError('Something went wrong while loading the resource')\n\n    def prepare_and_load(self) -&gt; DataRec:\n        \"\"\"\n        A convenience method that runs the full prepare and load pipeline.\n\n        Returns:\n            (DataRec): The fully prepared and loaded dataset.\n        \"\"\"\n        self.prepare()\n        return self.load()\n\n\n    def prepare_content(self, ctype:str='all'):\n        \"\"\"\n        Prepares content resources of the specified type.\n        Args:\n            ctype (str): The type of content to prepare. Use 'all' to prepare all content types.\n        Raises:\n            AssertionError: If the specified content type is not found.\n        Returns:\n            (None): None\n        \"\"\"\n        resources = find_resource_by_type(self.resources, 'content')\n        ctypes = set(resources.keys())\n        assert ctype in ctypes or ctype == 'all', f\"Content type {ctype} not found. Available content types: {ctypes}\"\n        if ctype == 'all':\n            for res in resources.values():\n                res_name = res.resource_name\n                if res_name in self.prepared_resources:\n                    continue\n                res_path = res.prepare()\n                self.prepared_resources = {res_name: res_path}\n        else:\n            res = resources[ctype]\n            res_path= res.prepare()\n            self.prepared_resources = {res.resource_name: res_path}\n\n    def download(self)-&gt;List[str]:\n        \"\"\"\n        Downloads the raw dataset files.\n        Returns:\n            (str): The path to the downloaded files.\n        \"\"\"\n        resources = []\n        for source in self.sources.values():\n            resources.append(source.download())\n        return resources\n\n    def find_output_folder(self, folder=None) -&gt; str:\n        \"\"\"\n        Find the output folder for the given dataset and version.\n        Args:\n            folder (str): Explicit output folder path.\n        Returns:\n            (str): The output folder path.\n        \"\"\"\n        if folder:\n            return os.path.abspath(os.path.join(folder, RAW_DATA_FOLDER))\n        return os.path.join(dataset_raw_directory(self.dataset_name, self.version))\n\n    def free_cache(self, \n                   *,\n                   resource_types: Union[Collection[str], str, None] = None,\n                   resource_names: Union[Collection[str], str, None] = None):\n\n        selected = self.find_resource_by_constraints(\n            resource_names=resource_names,\n            resource_types=resource_types)\n        for res in selected.values():\n            res.free_cache()\n</code></pre>"},{"location":"documentation/data/#datarec.data.datarec_builder.Dataset.prepare","title":"<code>prepare(*, only_required=True, use_cache=True, resource_types=None, resource_names=None)</code>","text":"<p>Prepare (download and decompress) all selected resources. Args:     only_required (bool): Prepare only resources marked as required when True.     use_cache (bool): Use cached resources when True.     resource_types (Collection[str] | str | None): Resource types to include; all when None.     resource_names (Collection[str] | str | None): Specific resource names to include; all when None. Returns:     dict[str, Any]: Map from resource name to the prepared artifact (e.g. local path).</p> Source code in <code>datarec/data/datarec_builder.py</code> <pre><code>def prepare(\n    self,\n    *,\n    only_required: bool = True,\n    use_cache: bool = True,\n    resource_types: Union[Collection[str], str, None] = None,\n    resource_names: Union[Collection[str], str, None] = None):\n    \"\"\"\n    Prepare (download and decompress) all selected resources.\n    Args:\n        only_required (bool): Prepare only resources marked as required when True.\n        use_cache (bool): Use cached resources when True.\n        resource_types (Collection[str] | str | None): Resource types to include; all when None.\n        resource_names (Collection[str] | str | None): Specific resource names to include; all when None.\n    Returns:\n        dict[str, Any]: Map from resource name to the prepared artifact (e.g. local path).\n    \"\"\"\n\n    selected = self.find_resource_by_constraints(\n        only_required=only_required,\n        resource_names=resource_names,\n        resource_types=resource_types)\n\n    if len(selected) == 0:\n        print('No resource found with the given requirements.')\n\n    for res_name, res in selected.items():\n        if res_name in self.prepared_resources.keys():\n            print(f\"Resource {res_name} was already prepared.\")\n            continue\n        res.prepare(use_cache=use_cache)\n        print(f\"Resource {res_name} ready\")\n        self.prepared_resources |= {res_name: res}\n</code></pre>"},{"location":"documentation/data/#datarec.data.datarec_builder.Dataset.prepare_interactions","title":"<code>prepare_interactions(*, only_required=True, use_cache=True, resource_names=None)</code>","text":"<p>Prepare (download and decompress) ratings resources. Args:     only_required (bool): Prepare only resources marked as required when True.     use_cache (bool): Use cached resources when True.     resource_names (Collection[str] | str | None): Specific resource names to include; all when None. Returns:     dict[str, Any]: Map from resource name to the prepared artifact (e.g. local path).</p> Source code in <code>datarec/data/datarec_builder.py</code> <pre><code>def prepare_interactions(\n    self,\n    *,\n    only_required: bool = True,\n    use_cache: bool = True,\n    resource_names: Union[Collection[str], str, None] = None):\n    \"\"\"\n    Prepare (download and decompress) ratings resources.\n    Args:\n        only_required (bool): Prepare only resources marked as required when True.\n        use_cache (bool): Use cached resources when True.\n        resource_names (Collection[str] | str | None): Specific resource names to include; all when None.\n    Returns:\n        dict[str, Any]: Map from resource name to the prepared artifact (e.g. local path).\n    \"\"\"\n    self.prepare(\n                resource_types='interactions',\n                only_required=only_required,\n                use_cache=use_cache,\n                resource_names=resource_names)\n</code></pre>"},{"location":"documentation/data/#datarec.data.datarec_builder.Dataset.load","title":"<code>load(*, use_cache=False, to_cache=False, resource_name=None, resource_type='interactions', only_required=False)</code>","text":"<p>Load the dataset into a DataRec object. Args:     use_cache (bool): Load from cache when True.     to_cache (bool): Save to cache when True.     resource_name (str | None): Specific resource name to load; all when None.     resource_type (str | None): Resource type to load; all when None.     only_required (bool): When True, consider only resources marked as required. Returns:     DataRec: The loaded dataset.</p> Source code in <code>datarec/data/datarec_builder.py</code> <pre><code>def load(\n    self, *,\n    use_cache: bool = False,\n    to_cache: bool = False,\n    resource_name: Union[str, None] = None,\n    resource_type: Union[str, None] = 'interactions',\n    only_required: bool = False) -&gt; DataRec:\n    \"\"\"\n    Load the dataset into a DataRec object.\n    Args:\n        use_cache (bool): Load from cache when True.\n        to_cache (bool): Save to cache when True.\n        resource_name (str | None): Specific resource name to load; all when None.\n        resource_type (str | None): Resource type to load; all when None.\n        only_required (bool): When True, consider only resources marked as required.\n    Returns:\n        DataRec: The loaded dataset.\n    \"\"\"\n    resource = self.find_resource_by_constraints(\n        only_required=only_required,\n        resource_names=resource_name,\n        resource_types=resource_type)\n    if len(resource) &gt; 1:\n        raise ValueError(f'More than one resource match the name: {resource_name} and type: {resource_type}.\\n \\\n                         Resources found: {resource.keys}. Please, select one by using the resource_name attribute.')\n    if len(resource) == 0:\n        raise ValueError(f'No resource found that matches the name: {resource_name} and type: {resource_type}')\n\n    if len(resource) == 1:\n        res_name, res = next(iter(resource.items()))\n        if res.type == 'interactions':\n            if res.prepared == True:\n                return res.load(use_cache=use_cache, to_cache=to_cache)\n            else:\n                raise ValueError(f'Resource \\'{res_name}\\' must be prepared before loading it. Try calling .prepare method before.')\n        else:\n            raise ValueError(f'DataRec does not support load method for resources with \\'{res.type}\\' type.\\\n                             This version of DataRec supports only \\'ratings\\' type.')\n\n    raise ValueError('Something went wrong while loading the resource')\n</code></pre>"},{"location":"documentation/data/#datarec.data.datarec_builder.Dataset.prepare_and_load","title":"<code>prepare_and_load()</code>","text":"<p>A convenience method that runs the full prepare and load pipeline.</p> <p>Returns:</p> Type Description <code>DataRec</code> <p>The fully prepared and loaded dataset.</p> Source code in <code>datarec/data/datarec_builder.py</code> <pre><code>def prepare_and_load(self) -&gt; DataRec:\n    \"\"\"\n    A convenience method that runs the full prepare and load pipeline.\n\n    Returns:\n        (DataRec): The fully prepared and loaded dataset.\n    \"\"\"\n    self.prepare()\n    return self.load()\n</code></pre>"},{"location":"documentation/data/#datarec.data.datarec_builder.Dataset.prepare_content","title":"<code>prepare_content(ctype='all')</code>","text":"<p>Prepares content resources of the specified type. Args:     ctype (str): The type of content to prepare. Use 'all' to prepare all content types. Raises:     AssertionError: If the specified content type is not found. Returns:     (None): None</p> Source code in <code>datarec/data/datarec_builder.py</code> <pre><code>def prepare_content(self, ctype:str='all'):\n    \"\"\"\n    Prepares content resources of the specified type.\n    Args:\n        ctype (str): The type of content to prepare. Use 'all' to prepare all content types.\n    Raises:\n        AssertionError: If the specified content type is not found.\n    Returns:\n        (None): None\n    \"\"\"\n    resources = find_resource_by_type(self.resources, 'content')\n    ctypes = set(resources.keys())\n    assert ctype in ctypes or ctype == 'all', f\"Content type {ctype} not found. Available content types: {ctypes}\"\n    if ctype == 'all':\n        for res in resources.values():\n            res_name = res.resource_name\n            if res_name in self.prepared_resources:\n                continue\n            res_path = res.prepare()\n            self.prepared_resources = {res_name: res_path}\n    else:\n        res = resources[ctype]\n        res_path= res.prepare()\n        self.prepared_resources = {res.resource_name: res_path}\n</code></pre>"},{"location":"documentation/data/#datarec.data.datarec_builder.Dataset.download","title":"<code>download()</code>","text":"<p>Downloads the raw dataset files. Returns:     (str): The path to the downloaded files.</p> Source code in <code>datarec/data/datarec_builder.py</code> <pre><code>def download(self)-&gt;List[str]:\n    \"\"\"\n    Downloads the raw dataset files.\n    Returns:\n        (str): The path to the downloaded files.\n    \"\"\"\n    resources = []\n    for source in self.sources.values():\n        resources.append(source.download())\n    return resources\n</code></pre>"},{"location":"documentation/data/#datarec.data.datarec_builder.Dataset.find_output_folder","title":"<code>find_output_folder(folder=None)</code>","text":"<p>Find the output folder for the given dataset and version. Args:     folder (str): Explicit output folder path. Returns:     (str): The output folder path.</p> Source code in <code>datarec/data/datarec_builder.py</code> <pre><code>def find_output_folder(self, folder=None) -&gt; str:\n    \"\"\"\n    Find the output folder for the given dataset and version.\n    Args:\n        folder (str): Explicit output folder path.\n    Returns:\n        (str): The output folder path.\n    \"\"\"\n    if folder:\n        return os.path.abspath(os.path.join(folder, RAW_DATA_FOLDER))\n    return os.path.join(dataset_raw_directory(self.dataset_name, self.version))\n</code></pre>"},{"location":"documentation/data/#datarec-and-data-wrappers","title":"DataRec and Data Wrappers","text":"<p>Core dataset container and helpers.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec","title":"<code>DataRec</code>","text":"<p>Core data structure for recommendation datasets in the DataRec framework.</p> <p>This class wraps a Pandas DataFrame and standardizes common columns (user, item, rating, timestamp) to provide a consistent interface for recommendation tasks. It supports data preprocessing,  user/item remapping (public vs private IDs), frequency analysis,  sparsity/density metrics, Gini coefficients, and conversion into  PyTorch datasets for training.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>class DataRec:\n\n    \"\"\"\n    Core data structure for recommendation datasets in the DataRec framework.\n\n    This class wraps a Pandas DataFrame and standardizes common columns\n    (user, item, rating, timestamp) to provide a consistent interface\n    for recommendation tasks. It supports data preprocessing, \n    user/item remapping (public vs private IDs), frequency analysis, \n    sparsity/density metrics, Gini coefficients, and conversion into \n    PyTorch datasets for training.\n    \"\"\"\n\n    def __init__(\n            self,\n            rawdata: RawData = None,\n            copy: bool = False,\n            dataset_name: str = 'datarec',\n            version_name: str = 'no_version_provided',\n            pipeline: Optional[Pipeline] = None,\n            registry_dataset: bool = False,\n            *args,\n            **kwargs\n    ):\n        \"\"\"\n        Initializes the DataRec object.\n\n        Args:\n            rawdata (RawData): The input dataset wrapped in a RawData object.\n                If None, the DataRec is initialized empty.\n            copy (bool): Whether to copy the input DataFrame to avoid \n                modifying the original RawData.\n            dataset_name (str): A name to identify the dataset.\n            version_name (str): A version identifier \n                for the dataset.\n            pipeline (Pipeline): A pipeline object to track preprocessing steps.\n            registry_dataset (bool): Whether the DataRec derives from a registered dataset.\n\n        \"\"\"\n        self.path = None\n        self._data = None\n        self.dataset_name = dataset_name\n        self.version_name = version_name\n\n        rawdata_step = None\n        if rawdata is not None:\n            if copy:\n                self._data: pd.DataFrame = rawdata.data.copy()\n            else:\n                self._data: pd.DataFrame = rawdata.data\n\n            if rawdata.pipeline_step is not None:\n                rawdata_step = rawdata.pipeline_step\n\n        # PIPELINE INITIALIZATION\n        if pipeline:\n            self.pipeline = pipeline\n        else:\n            if registry_dataset:\n                self.pipeline = Pipeline()\n                self.pipeline.add_step(\"load\", \"registry_dataset\", {\"dataset_name\": self.dataset_name, \"version\": self.version_name})\n            elif rawdata_step is not None:\n                self.pipeline = Pipeline()\n                self.pipeline.steps.append(rawdata_step)\n            else:\n                warnings.warn(\"No pipeline provided. Initializing empty pipeline.\")\n                self.pipeline = Pipeline()\n\n        # ------------------------------------\n        # --------- STANDARD COLUMNS ---------\n        # if a column is None it means that the DataRec does not have that information\n        self.__assigned_columns = []\n\n        self._user_col = None\n        self._item_col = None\n        self._rating_col = None\n        self._timestamp_col = None\n\n        if rawdata:\n            self.set_columns(rawdata)\n\n        # map users and items with a 0-indexed mapping\n        self.user_id_encoder = Encoder()\n        self.item_id_encoder = Encoder()\n\n        # Apply external encoders (e.g., from streaming readers) if provided\n        if rawdata is not None:\n            if getattr(rawdata, \"user_encoder\", None):\n                self.user_id_encoder.apply_encoding(rawdata.user_encoder)\n            if getattr(rawdata, \"item_encoder\", None):\n                self.item_id_encoder.apply_encoding(rawdata.item_encoder)\n\n        # ------------------------------\n        # --------- PROPERTIES ---------\n        self._sorted_users = None\n        self._sorted_items = None\n\n        self._n_users = None\n        self._n_items = None\n        self._transactions = None\n\n        self.characteristics = CharacteristicAccessor(self)\n\n\n    def __str__(self):\n        \"\"\"\n        Returns 'self.data' as a string variable.\n\n        Returns:\n            (str): 'self.data' as a string variable.\n        \"\"\"\n        return self.data.__str__()\n\n    def __repr__(self):\n        \"\"\"\n        Returns the official string representation of the internal DataFrame.\n        \"\"\"\n        return self.data.__repr__()\n\n    def _repr_html_(self):\n        \"\"\"\n        Returns an HTML representation of the internal DataFrame for rich displays.\n        \"\"\"\n        return self.data._repr_html_()\n\n    def __len__(self):\n        \"\"\"\n        Returns the total number of samples in the dataset.\n\n        Returns:\n            (int): number of samples in the dataset.\n        \"\"\"\n        return len(self.data)\n\n    def set_columns(self, rawdata):\n        \"\"\"\n        Assign dataset column names from a RawData object and reorder the data accordingly.\n\n        Args:\n            rawdata (RawData): A RawData object containing the column names for \n                user, item, rating, and timestamp.\n        \"\"\"\n        if rawdata.user is not None:\n            self.user_col = rawdata.user\n            self.__assigned_columns.append(self.user_col)\n        if rawdata.item is not None:\n            self.item_col = rawdata.item\n            self.__assigned_columns.append(self.item_col)\n        if rawdata.rating is not None:\n            self.rating_col = rawdata.rating\n            self.__assigned_columns.append(self.rating_col)\n        if rawdata.timestamp is not None:\n            self.timestamp_col = rawdata.timestamp\n            self.__assigned_columns.append(self.timestamp_col)\n\n        # re-order columns\n        self._data = self.data[self.__assigned_columns]\n\n    def reset(self):\n\n        \"\"\"\n        Reset cached statistics and assigned columns of the DataRec object.\n\n        This method clears all precomputed dataset statistics (e.g., sorted users, \n        density, Gini indices, shape, ratings per user/item) and empties the list \n        of assigned columns. It is automatically called when the underlying data is changed.\n        \"\"\"\n\n        self.__assigned_columns = []\n\n    @property\n    def data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        The underlying pandas DataFrame holding the interaction data.\n        \"\"\"\n        return self._data\n\n    @data.setter\n    def data(self, value: RawData):\n        \"\"\"\n        Sets the internal DataFrame from a RawData object and resets stats.\n        \"\"\"\n        if (value is not None and\n                not isinstance(value, RawData)):\n            raise ValueError(f'Data must be RawData or None if empty. Found {type(value)}')\n        value = value if value is not None else pd.DataFrame()\n\n        self._data = value.data\n        self.reset()\n        self.set_columns(value)\n\n    @property\n    def user_col(self):\n        \"\"\"\n        The name of the user ID column.\n        \"\"\"\n        return self._user_col\n\n    @user_col.setter\n    def user_col(self, value: Union[str, int]):\n        \"\"\"\n        Sets and renames the user column to the internal standard name.\n        \"\"\"\n        self.set_user_col(value, rename=True)\n\n    def set_user_col(self, value: Union[str, int] = DATAREC_USER_COL, rename=True):\n        \"\"\"\n        Identifies and optionally renames the user column.\n\n        Args:\n            value (Union[str, int]): The current name or index of the user column.\n            rename (bool): If True, renames the column to the standard internal name.\n        \"\"\"\n        self.data.columns, self._user_col = set_column_name(columns=list(self.data.columns),\n                                                            value=value,\n                                                            default_name=DATAREC_USER_COL,\n                                                            rename=rename)\n\n    @property\n    def item_col(self):\n        \"\"\"\n        The name of the item ID column.\n        \"\"\"\n        return self._item_col\n\n    @item_col.setter\n    def item_col(self, value: Union[str, int]):\n        \"\"\"\n        Sets and renames the item column to the internal standard name.\n        \"\"\"\n        self.set_item_col(value, rename=True)\n\n    def set_item_col(self, value: Union[str, int] = DATAREC_ITEM_COL, rename=True):\n        \"\"\"\n        Identifies and optionally renames the item column.\n\n        Args:\n            value (Union[str, int]): The current name or index of the item column.\n            rename (bool): If True, renames the column to the standard internal name.\n        \"\"\"\n        self.data.columns, self._item_col = set_column_name(columns=list(self.data.columns),\n                                                            value=value,\n                                                            default_name=DATAREC_ITEM_COL,\n                                                            rename=rename)\n\n    @property\n    def rating_col(self):\n        \"\"\"\n        The name of the rating column.\n        \"\"\"\n        return self._rating_col\n\n    @rating_col.setter\n    def rating_col(self, value: Union[str, int]):\n        \"\"\"\n        Sets and renames the rating column to the internal standard name.\n        \"\"\"\n        self.set_rating_col(value, rename=True)\n\n    def set_rating_col(self, value: Union[str, int] = DATAREC_RATING_COL, rename=True):\n        \"\"\"\n        Identifies and optionally renames the rating column.\n\n        Args:\n            value (Union[str, int]): The current name or index of the rating column.\n            rename (bool): If True, renames the column to the standard internal name.\n        \"\"\"\n        self.data.columns, self._rating_col = set_column_name(columns=list(self.data.columns),\n                                                              value=value,\n                                                              default_name=DATAREC_RATING_COL,\n                                                              rename=rename)\n\n    @property\n    def timestamp_col(self):\n        \"\"\"\n        The name of the timestamp column.\n        \"\"\"\n        return self._timestamp_col\n\n    @timestamp_col.setter\n    def timestamp_col(self, value: Union[str, int]):\n        \"\"\"\n        Sets and renames the timestamp column to the internal standard name.\n        \"\"\"\n        self.set_timestamp_col(value, rename=True)\n\n    def set_timestamp_col(self, value: Union[str, int] = DATAREC_TIMESTAMP_COL, rename=True):\n        \"\"\"\n        Identifies and optionally renames the timestamp column.\n\n        Args:\n            value (Union[str, int]): The current name or index of the timestamp column.\n            rename (bool): If True, renames the column to the standard internal name.\n        \"\"\"\n        self.data.columns, self._timestamp_col = set_column_name(columns=list(self.data.columns),\n                                                                 value=value,\n                                                                 default_name=DATAREC_TIMESTAMP_COL,\n                                                                 rename=rename)\n\n    @property\n    def users(self):\n        \"\"\"\n        Returns a list of unique user IDs in the dataset.\n        \"\"\"\n        return self.data[self.user_col].unique().tolist()\n\n    @property\n    def items(self):\n        \"\"\"\n        Returns a list of unique item IDs in the dataset.\n        \"\"\"\n        return self.data[self.item_col].unique().tolist()\n\n    @property\n    def n_users(self):\n        \"\"\"\n        Returns the number of unique users.\n        \"\"\"\n        return int(self.data[self.user_col].nunique())\n\n    @property\n    def n_items(self):\n        \"\"\"\n        Returns the number of unique items.\n        \"\"\"\n        return int(self.data[self.item_col].nunique())\n\n    @property\n    def columns(self):\n        \"\"\"\n        Returns the list of column names of the internal DataFrame.\n        \"\"\"\n        return self.data.columns\n\n    @columns.setter\n    def columns(self, columns):\n        \"\"\"\n        Sets the column names of the internal DataFrame.\n        \"\"\"\n        self.data.columns = columns\n\n    @property\n    def sorted_items(self):\n        \"\"\"\n        Returns a dictionary of items sorted by their interaction count.\n        \"\"\"\n        if self._sorted_items is None:\n            count_items = self.data.groupby(self.item_col).count().sort_values(by=[self.user_col])\n            self._sorted_items = dict(zip(count_items.index, count_items[self.user_col]))\n        return self._sorted_items\n\n    @property\n    def sorted_users(self):\n        \"\"\"\n        Returns a dictionary of users sorted by their interaction count.\n        \"\"\"\n        if self._sorted_users is None:\n            count_users = self.data.groupby(self.user_col).count().sort_values(by=[self.item_col])\n            self._sorted_users = dict(zip(count_users.index, count_users[self.item_col]))\n        return self._sorted_users\n\n    @property\n    def transactions(self):\n        \"\"\"\n        Returns the total number of interactions (rows) in the dataset.\n        \"\"\"\n        if self._transactions is None:\n            self._transactions = len(self.data)\n        return self._transactions\n\n    # --- ID ENCODING/DECODING FUNCTIONS ---\n\n    def is_encoded(self, on:str) -&gt; bool:\n        \"\"\"\n        Checks if user or item IDs are encoded to private integer IDs.\n\n        Args:\n            on (str): 'users' to check user encoding, 'items' for item encoding.\n        Returns:\n            (bool): True if the specified IDs are encoded, False otherwise.\n        \"\"\"\n        if on == 'users':\n            return self.user_id_encoder.is_encoded()\n        elif on == 'items':\n            return self.item_id_encoder.is_encoded()\n        else:\n            raise ValueError(\"Parameter 'on' must be either 'users' or 'items'.\")\n\n\n    def encode(self, users=True, items=True) -&gt; None:\n        \"\"\"\n        Converts user and item IDs to encoded integer IDs.\n        Args:\n            users (bool): If True, encodes user IDs.\n            items (bool): If True, encodes item IDs.\n        \"\"\"\n        if users:\n            if not self.user_id_encoder.is_encoded():\n                raise ValueError(\"User encoder is empty. Build or apply an encoding before calling encode().\")\n            self.data[self.user_col] = self.user_id_encoder.encode(self.data[self.user_col].tolist())\n        if items:\n            if not self.item_id_encoder.is_encoded():\n                raise ValueError(\"Item encoder is empty. Build or apply an encoding before calling encode().\")\n            self.data[self.item_col] = self.item_id_encoder.encode(self.data[self.item_col].tolist())\n\n    def decode(self, users=True, items=True) -&gt; None:\n        \"\"\"\n        Converts user and item IDs back to original IDs.\n        Args:\n            users (bool): If True, encodes user IDs.\n            items (bool): If True, encodes item IDs.\n        \"\"\"\n        if users:\n            self.data[self.user_col] = self.user_id_encoder.decode(self.data[self.user_col].tolist())\n        if items:\n            self.data[self.item_col] = self.item_id_encoder.decode(self.data[self.item_col].tolist())\n\n    def reset_encoding(self, on='all') -&gt; None:\n        \"\"\"\n        Resets the encoding for users or items.\n\n        Args:\n            on (str): 'users' to reset user encoding, 'items' for item encoding.\n        \"\"\"\n        if on == 'users':\n            self.decode(users=True, items=False)\n            self.user_id_encoder.reset_encoding()\n        elif on == 'items':\n            self.decode(users=False, items=True)\n            self.item_id_encoder.reset_encoding()\n        elif on == 'all':\n            self.decode(users=True, items=True)\n            self.user_id_encoder.reset_encoding()\n            self.item_id_encoder.reset_encoding()\n        else:\n            raise ValueError(\"Parameter 'on' must be either 'users' or 'items'.\")\n\n\n    def build_encoding(self, on='users', offset=0) -&gt; None:\n        \"\"\"\n        Builds the encoding for users or items.\n\n        Args:\n            on (str): 'users' to build user encoding, 'items' for item encoding, 'all' for both.\n            offset (int): The starting integer for the private IDs.\n        \"\"\"\n        if on == 'users':\n            self.user_id_encoder.build_encoding(self.users, offset=offset)\n        elif on == 'items':\n            self.item_id_encoder.build_encoding(self.items, offset=offset)\n        elif on == 'all':\n            self.user_id_encoder.build_encoding(self.users, offset=offset)\n            self.item_id_encoder.build_encoding(self.items, offset=offset)\n        else:\n            raise ValueError(\"Parameter 'on' must be either 'users' or 'items'.\")\n\n\n    # -- CHARACTERISTICS --\n\n    def characteristic(self, name: str, **kwargs: Any) -&gt; Any:\n        \"\"\"\n        Retrieves a calculated dataset characteristic by name.\n\n        Args:\n            name (str): The name of the characteristic to retrieve.\n            **kwargs: Additional arguments to pass to the characteristic function.\n\n        Returns:\n            The value of the requested characteristic.\n        \"\"\"\n        return getattr(self.characteristics, name)(**kwargs)\n\n    def space_size(self, **kwargs):\n        return self.characteristic(\"space_size\", **kwargs)\n\n    def space_size_log(self, **kwargs):\n        return self.characteristic(\"space_size_log\", **kwargs)\n\n    def shape(self, **kwargs):\n        return self.characteristic(\"shape\", **kwargs)\n\n    def shape_log(self, **kwargs):\n        return self.characteristic(\"shape_log\", **kwargs)\n\n    def density(self, **kwargs):\n        return self.characteristic(\"density\", **kwargs)\n\n    def density_log(self, **kwargs):\n        return self.characteristic(\"density_log\", **kwargs)\n\n    def gini_item(self, **kwargs):\n        return self.characteristic(\"gini_item\", **kwargs)\n\n    def gini_user(self, **kwargs):\n        return self.characteristic(\"gini_user\", **kwargs)\n\n    def ratings_per_user(self, **kwargs):\n        return self.characteristic(\"ratings_per_user\", **kwargs)\n\n    def ratings_per_item(self, **kwargs):\n        return self.characteristic(\"ratings_per_item\", **kwargs)\n\n    def users_frequency(self):\n        \"\"\"\n        Computes the absolute frequency of each user in the dataset.\n\n        Returns:\n            (dict): A dictionary mapping user IDs to the number of interactions, \n                sorted in descending order of frequency.\n        \"\"\"\n        fr = dict(Counter(self.data[self.user_col]))\n        return dict(sorted(fr.items(), key=lambda item: item[1], reverse=True))\n\n    def users_relative_frequency(self):\n        \"\"\"\n        Computes the relative frequency of each user in the dataset.\n\n        Returns:\n            (dict): A dictionary mapping user IDs to their relative frequency \n                (fraction of total transactions).\n        \"\"\"\n        return {u: (f / self.transactions) for u, f in self.users_frequency().items()}\n\n    def items_frequency(self):\n        \"\"\"\n        Computes the absolute frequency of each item in the dataset.\n\n        Returns:\n            (dict): A dictionary mapping item IDs to the number of interactions, \n                sorted in descending order of frequency.\n        \"\"\"\n        fr = dict(Counter(self.data[self.item_col]))\n        return dict(sorted(fr.items(), key=lambda item: item[1], reverse=True))\n\n    def items_relative_frequency(self):\n        \"\"\"\n        Computes the relative frequency of each item in the dataset.\n\n        Returns:\n            (dict): A dictionary mapping item IDs to their relative frequency \n                (fraction of total transactions).\n        \"\"\"\n        return {u: (f / self.transactions) for u, f in self.items_frequency().items()}\n\n    def users_quartiles(self):\n        \"\"\"\n        Assigns quartile indices to users based on their frequency.\n\n        Returns:\n            (dict): A dictionary mapping each user ID to a quartile index (0-3),\n                where 0 = lowest, 3 = highest frequency.\n        \"\"\" \n        return quartiles(self.users_frequency())\n\n    def items_quartiles(self):\n        \"\"\"\n        Assigns quartile indices to items based on their frequency.\n\n        Returns:\n            (dict): A dictionary mapping each item ID to a quartile index (0-3),\n                where 0 = lowest, 3 = highest frequency.\n        \"\"\"\n        return quartiles(self.items_frequency())\n\n    def users_popularity(self):\n        \"\"\"\n        Categorizes users into descriptive popularity groups based on quartiles.\n\n        Returns:\n            (dict): A dictionary mapping popularity categories ('long tail', \n                'common', 'popular', 'most popular') to lists of user IDs.\n        \"\"\"\n        return popularity(self.users_quartiles())\n\n    def items_popularity(self):\n        \"\"\"\n        Categorizes items into descriptive popularity groups based on quartiles.\n\n        Returns:\n            (dict): A dictionary mapping popularity categories ('long tail', \n                'common', 'popular', 'most popular') to lists of item IDs.\n        \"\"\"\n        return popularity(self.items_quartiles())\n\n    def list_characteristics(self) -&gt; list[str]:\n        \"\"\"Return the names of all characteristics that can be computed on this dataset.\"\"\"\n        return sorted(CHARACTERISTICS.keys())\n\n    def describe_characteristics(self) -&gt; dict[str, str]:\n        \"\"\"Return a mapping name -&gt; short docstring for each available characteristic.\"\"\"\n        return {\n            name: (func.__doc__ or \"\").strip()\n            for name, func in CHARACTERISTICS.items()\n        }\n\n    def copy(self):\n        \"\"\"\n        Create a deep copy of the current DataRec object.\n\n        This method duplicates the DataRec instance, including its data,\n        metadata (user, item, rating, timestamp columns), pipeline, and internal\n        state such as privacy settings and implicit flags.\n\n        Returns:\n            (DataRec): A new DataRec object that is a deep copy of the current instance.\n        \"\"\"\n        pipeline = self.pipeline.copy()\n\n        new_dr = DataRec(rawdata=self.to_rawdata(),\n                         pipeline=pipeline,\n                         copy=True)\n\n        new_dr._user_col = self.user_col\n        new_dr._item_col = self.item_col\n        new_dr._rating_col = self.rating_col\n        new_dr._timestamp_col = self.timestamp_col\n        return new_dr\n\n    def to_rawdata(self):\n        \"\"\"\n        Convert the current DataRec object into a RawData object.\n\n        This method creates a RawData instance containing the same data and\n        metadata (user, item, rating, timestamp columns) as the DataRec object.\n\n        Returns:\n            (RawData): A new RawData object containing the DataRec's data and column information.\n        \"\"\"\n        raw = RawData(self.data)\n        raw.user = self.user_col\n        raw.item = self.item_col\n        raw.rating = self.rating_col\n        raw.timestamp = self.timestamp_col\n        return raw\n\n    def save_pipeline(self, filepath: str) -&gt; None:\n        \"\"\"\n        Save the current processing pipeline to a YAML file.\n\n        Args:\n            filepath (str): The path (including filename) where the pipeline \n                YAML file will be saved.\n        \"\"\"\n        print(f'Saving pipeline to {filepath}')\n\n        self.pipeline.to_yaml(filepath)\n\n        print(f'Pipeline correctly saved to {filepath}')\n\n    def to_torch_dataset(self, task: str = \"pointwise\", autoprepare: bool = True, **kwargs: Any) -&gt; Any:\n        \"\"\"\n        Converts the current DataRec object into a PyTorch-compatible dataset.\n\n        This method prepares the dataset (e.g., remaps user/item IDs to a dense index space)\n        and returns a `torch.utils.data.Dataset` object suitable for training with PyTorch.\n\n        Args:\n            task (str): The recommendation task type. Must be one of:\n                - \"pointwise\": returns PointwiseTorchDataset\n                - \"pairwise\": returns PairwiseTorchDataset\n                - \"ranking\": returns RankingTorchDataset\n            autoprepare (bool): If True, automatically applies user/item remapping\n                and switches the dataset to private IDs. If False, assumes the dataset\n                is already properly prepared.\n            **kwargs: Additional arguments passed to the specific torch dataset class.\n\n        Returns:\n            (torch.utils.data.Dataset): A PyTorch dataset instance corresponding to the selected task.\n\n        Raises:\n            ImportError: If PyTorch is not installed.\n            ValueError: If an unknown task name is provided.\n        \"\"\"\n\n        try:\n            import torch\n        except ModuleNotFoundError:\n            raise ImportError(\n                \"Torch is required for this feature. Please install it with `pip install datarec[torch]` or\"\n                \" `pip install -r requirements/requirements-torch.txt`.\"\n            )\n\n        if autoprepare:\n            self.map_users_and_items()\n            self.encode()\n        else:\n            warnings.warn(\n                \"Autoprepare is set to False. \"\n                \"Ensure that the dataset is prepared correctly before using it with PyTorch.\"\n            )\n\n        if task == \"pointwise\":\n            from datarec.data.torch_dataset import PointwiseTorchDataset\n            return PointwiseTorchDataset(self, **kwargs)\n        elif task == \"pairwise\":\n            from datarec.data.torch_dataset import PairwiseTorchDataset\n            return PairwiseTorchDataset(self, **kwargs)\n        elif task == \"ranking\":\n            from datarec.data.torch_dataset import RankingTorchDataset\n            return RankingTorchDataset(self, **kwargs)\n        else:\n            raise ValueError(f\"Unknown task: {task}\")\n\n    def to_graphrec(self):\n        \"\"\"\n        Converts the current DataRec object into a GraphRec object.\n\n        This method creates a GraphRec instance representing the bipartite graph\n        of user-item interactions contained in the DataRec object.\n\n        Returns:\n            (GraphRec): A new GraphRec object representing the interaction graph.\n        \"\"\"\n        from datarec.data.graph import GraphRec\n        return GraphRec(self)\n\n    def to_pickle(self, filepath: str = '') -&gt; None:\n        \"\"\"\n        Save the current DataRec object to a pickle file.\n\n        Args:\n            filepath (str): The path (including filename) where the pickle file will be saved.\n        \"\"\"\n        import pickle\n\n        if filepath == '':\n            filepath = paths.pickle_version_filepath(self.dataset_name, self.version_name)\n\n        print(f'Saving DataRec to {filepath}')\n\n        with open(filepath, 'wb') as f:\n            pickle.dump(self, f)\n\n        print(f'DataRec correctly saved to {filepath}')\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.data","title":"<code>data</code>  <code>property</code> <code>writable</code>","text":"<p>The underlying pandas DataFrame holding the interaction data.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.user_col","title":"<code>user_col</code>  <code>property</code> <code>writable</code>","text":"<p>The name of the user ID column.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.item_col","title":"<code>item_col</code>  <code>property</code> <code>writable</code>","text":"<p>The name of the item ID column.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.rating_col","title":"<code>rating_col</code>  <code>property</code> <code>writable</code>","text":"<p>The name of the rating column.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.timestamp_col","title":"<code>timestamp_col</code>  <code>property</code> <code>writable</code>","text":"<p>The name of the timestamp column.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.users","title":"<code>users</code>  <code>property</code>","text":"<p>Returns a list of unique user IDs in the dataset.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.items","title":"<code>items</code>  <code>property</code>","text":"<p>Returns a list of unique item IDs in the dataset.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.n_users","title":"<code>n_users</code>  <code>property</code>","text":"<p>Returns the number of unique users.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.n_items","title":"<code>n_items</code>  <code>property</code>","text":"<p>Returns the number of unique items.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.columns","title":"<code>columns</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the list of column names of the internal DataFrame.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.sorted_items","title":"<code>sorted_items</code>  <code>property</code>","text":"<p>Returns a dictionary of items sorted by their interaction count.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.sorted_users","title":"<code>sorted_users</code>  <code>property</code>","text":"<p>Returns a dictionary of users sorted by their interaction count.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.transactions","title":"<code>transactions</code>  <code>property</code>","text":"<p>Returns the total number of interactions (rows) in the dataset.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.__init__","title":"<code>__init__(rawdata=None, copy=False, dataset_name='datarec', version_name='no_version_provided', pipeline=None, registry_dataset=False, *args, **kwargs)</code>","text":"<p>Initializes the DataRec object.</p> <p>Parameters:</p> Name Type Description Default <code>rawdata</code> <code>RawData</code> <p>The input dataset wrapped in a RawData object. If None, the DataRec is initialized empty.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>Whether to copy the input DataFrame to avoid  modifying the original RawData.</p> <code>False</code> <code>dataset_name</code> <code>str</code> <p>A name to identify the dataset.</p> <code>'datarec'</code> <code>version_name</code> <code>str</code> <p>A version identifier  for the dataset.</p> <code>'no_version_provided'</code> <code>pipeline</code> <code>Pipeline</code> <p>A pipeline object to track preprocessing steps.</p> <code>None</code> <code>registry_dataset</code> <code>bool</code> <p>Whether the DataRec derives from a registered dataset.</p> <code>False</code> Source code in <code>datarec/data/dataset.py</code> <pre><code>def __init__(\n        self,\n        rawdata: RawData = None,\n        copy: bool = False,\n        dataset_name: str = 'datarec',\n        version_name: str = 'no_version_provided',\n        pipeline: Optional[Pipeline] = None,\n        registry_dataset: bool = False,\n        *args,\n        **kwargs\n):\n    \"\"\"\n    Initializes the DataRec object.\n\n    Args:\n        rawdata (RawData): The input dataset wrapped in a RawData object.\n            If None, the DataRec is initialized empty.\n        copy (bool): Whether to copy the input DataFrame to avoid \n            modifying the original RawData.\n        dataset_name (str): A name to identify the dataset.\n        version_name (str): A version identifier \n            for the dataset.\n        pipeline (Pipeline): A pipeline object to track preprocessing steps.\n        registry_dataset (bool): Whether the DataRec derives from a registered dataset.\n\n    \"\"\"\n    self.path = None\n    self._data = None\n    self.dataset_name = dataset_name\n    self.version_name = version_name\n\n    rawdata_step = None\n    if rawdata is not None:\n        if copy:\n            self._data: pd.DataFrame = rawdata.data.copy()\n        else:\n            self._data: pd.DataFrame = rawdata.data\n\n        if rawdata.pipeline_step is not None:\n            rawdata_step = rawdata.pipeline_step\n\n    # PIPELINE INITIALIZATION\n    if pipeline:\n        self.pipeline = pipeline\n    else:\n        if registry_dataset:\n            self.pipeline = Pipeline()\n            self.pipeline.add_step(\"load\", \"registry_dataset\", {\"dataset_name\": self.dataset_name, \"version\": self.version_name})\n        elif rawdata_step is not None:\n            self.pipeline = Pipeline()\n            self.pipeline.steps.append(rawdata_step)\n        else:\n            warnings.warn(\"No pipeline provided. Initializing empty pipeline.\")\n            self.pipeline = Pipeline()\n\n    # ------------------------------------\n    # --------- STANDARD COLUMNS ---------\n    # if a column is None it means that the DataRec does not have that information\n    self.__assigned_columns = []\n\n    self._user_col = None\n    self._item_col = None\n    self._rating_col = None\n    self._timestamp_col = None\n\n    if rawdata:\n        self.set_columns(rawdata)\n\n    # map users and items with a 0-indexed mapping\n    self.user_id_encoder = Encoder()\n    self.item_id_encoder = Encoder()\n\n    # Apply external encoders (e.g., from streaming readers) if provided\n    if rawdata is not None:\n        if getattr(rawdata, \"user_encoder\", None):\n            self.user_id_encoder.apply_encoding(rawdata.user_encoder)\n        if getattr(rawdata, \"item_encoder\", None):\n            self.item_id_encoder.apply_encoding(rawdata.item_encoder)\n\n    # ------------------------------\n    # --------- PROPERTIES ---------\n    self._sorted_users = None\n    self._sorted_items = None\n\n    self._n_users = None\n    self._n_items = None\n    self._transactions = None\n\n    self.characteristics = CharacteristicAccessor(self)\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.__str__","title":"<code>__str__()</code>","text":"<p>Returns 'self.data' as a string variable.</p> <p>Returns:</p> Type Description <code>str</code> <p>'self.data' as a string variable.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def __str__(self):\n    \"\"\"\n    Returns 'self.data' as a string variable.\n\n    Returns:\n        (str): 'self.data' as a string variable.\n    \"\"\"\n    return self.data.__str__()\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns the official string representation of the internal DataFrame.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def __repr__(self):\n    \"\"\"\n    Returns the official string representation of the internal DataFrame.\n    \"\"\"\n    return self.data.__repr__()\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.__len__","title":"<code>__len__()</code>","text":"<p>Returns the total number of samples in the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>number of samples in the dataset.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Returns the total number of samples in the dataset.\n\n    Returns:\n        (int): number of samples in the dataset.\n    \"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.set_columns","title":"<code>set_columns(rawdata)</code>","text":"<p>Assign dataset column names from a RawData object and reorder the data accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>rawdata</code> <code>RawData</code> <p>A RawData object containing the column names for  user, item, rating, and timestamp.</p> required Source code in <code>datarec/data/dataset.py</code> <pre><code>def set_columns(self, rawdata):\n    \"\"\"\n    Assign dataset column names from a RawData object and reorder the data accordingly.\n\n    Args:\n        rawdata (RawData): A RawData object containing the column names for \n            user, item, rating, and timestamp.\n    \"\"\"\n    if rawdata.user is not None:\n        self.user_col = rawdata.user\n        self.__assigned_columns.append(self.user_col)\n    if rawdata.item is not None:\n        self.item_col = rawdata.item\n        self.__assigned_columns.append(self.item_col)\n    if rawdata.rating is not None:\n        self.rating_col = rawdata.rating\n        self.__assigned_columns.append(self.rating_col)\n    if rawdata.timestamp is not None:\n        self.timestamp_col = rawdata.timestamp\n        self.__assigned_columns.append(self.timestamp_col)\n\n    # re-order columns\n    self._data = self.data[self.__assigned_columns]\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.reset","title":"<code>reset()</code>","text":"<p>Reset cached statistics and assigned columns of the DataRec object.</p> <p>This method clears all precomputed dataset statistics (e.g., sorted users,  density, Gini indices, shape, ratings per user/item) and empties the list  of assigned columns. It is automatically called when the underlying data is changed.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def reset(self):\n\n    \"\"\"\n    Reset cached statistics and assigned columns of the DataRec object.\n\n    This method clears all precomputed dataset statistics (e.g., sorted users, \n    density, Gini indices, shape, ratings per user/item) and empties the list \n    of assigned columns. It is automatically called when the underlying data is changed.\n    \"\"\"\n\n    self.__assigned_columns = []\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.set_user_col","title":"<code>set_user_col(value=DATAREC_USER_COL, rename=True)</code>","text":"<p>Identifies and optionally renames the user column.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, int]</code> <p>The current name or index of the user column.</p> <code>DATAREC_USER_COL</code> <code>rename</code> <code>bool</code> <p>If True, renames the column to the standard internal name.</p> <code>True</code> Source code in <code>datarec/data/dataset.py</code> <pre><code>def set_user_col(self, value: Union[str, int] = DATAREC_USER_COL, rename=True):\n    \"\"\"\n    Identifies and optionally renames the user column.\n\n    Args:\n        value (Union[str, int]): The current name or index of the user column.\n        rename (bool): If True, renames the column to the standard internal name.\n    \"\"\"\n    self.data.columns, self._user_col = set_column_name(columns=list(self.data.columns),\n                                                        value=value,\n                                                        default_name=DATAREC_USER_COL,\n                                                        rename=rename)\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.set_item_col","title":"<code>set_item_col(value=DATAREC_ITEM_COL, rename=True)</code>","text":"<p>Identifies and optionally renames the item column.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, int]</code> <p>The current name or index of the item column.</p> <code>DATAREC_ITEM_COL</code> <code>rename</code> <code>bool</code> <p>If True, renames the column to the standard internal name.</p> <code>True</code> Source code in <code>datarec/data/dataset.py</code> <pre><code>def set_item_col(self, value: Union[str, int] = DATAREC_ITEM_COL, rename=True):\n    \"\"\"\n    Identifies and optionally renames the item column.\n\n    Args:\n        value (Union[str, int]): The current name or index of the item column.\n        rename (bool): If True, renames the column to the standard internal name.\n    \"\"\"\n    self.data.columns, self._item_col = set_column_name(columns=list(self.data.columns),\n                                                        value=value,\n                                                        default_name=DATAREC_ITEM_COL,\n                                                        rename=rename)\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.set_rating_col","title":"<code>set_rating_col(value=DATAREC_RATING_COL, rename=True)</code>","text":"<p>Identifies and optionally renames the rating column.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, int]</code> <p>The current name or index of the rating column.</p> <code>DATAREC_RATING_COL</code> <code>rename</code> <code>bool</code> <p>If True, renames the column to the standard internal name.</p> <code>True</code> Source code in <code>datarec/data/dataset.py</code> <pre><code>def set_rating_col(self, value: Union[str, int] = DATAREC_RATING_COL, rename=True):\n    \"\"\"\n    Identifies and optionally renames the rating column.\n\n    Args:\n        value (Union[str, int]): The current name or index of the rating column.\n        rename (bool): If True, renames the column to the standard internal name.\n    \"\"\"\n    self.data.columns, self._rating_col = set_column_name(columns=list(self.data.columns),\n                                                          value=value,\n                                                          default_name=DATAREC_RATING_COL,\n                                                          rename=rename)\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.set_timestamp_col","title":"<code>set_timestamp_col(value=DATAREC_TIMESTAMP_COL, rename=True)</code>","text":"<p>Identifies and optionally renames the timestamp column.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, int]</code> <p>The current name or index of the timestamp column.</p> <code>DATAREC_TIMESTAMP_COL</code> <code>rename</code> <code>bool</code> <p>If True, renames the column to the standard internal name.</p> <code>True</code> Source code in <code>datarec/data/dataset.py</code> <pre><code>def set_timestamp_col(self, value: Union[str, int] = DATAREC_TIMESTAMP_COL, rename=True):\n    \"\"\"\n    Identifies and optionally renames the timestamp column.\n\n    Args:\n        value (Union[str, int]): The current name or index of the timestamp column.\n        rename (bool): If True, renames the column to the standard internal name.\n    \"\"\"\n    self.data.columns, self._timestamp_col = set_column_name(columns=list(self.data.columns),\n                                                             value=value,\n                                                             default_name=DATAREC_TIMESTAMP_COL,\n                                                             rename=rename)\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.is_encoded","title":"<code>is_encoded(on)</code>","text":"<p>Checks if user or item IDs are encoded to private integer IDs.</p> <p>Parameters:</p> Name Type Description Default <code>on</code> <code>str</code> <p>'users' to check user encoding, 'items' for item encoding.</p> required <p>Returns:     (bool): True if the specified IDs are encoded, False otherwise.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def is_encoded(self, on:str) -&gt; bool:\n    \"\"\"\n    Checks if user or item IDs are encoded to private integer IDs.\n\n    Args:\n        on (str): 'users' to check user encoding, 'items' for item encoding.\n    Returns:\n        (bool): True if the specified IDs are encoded, False otherwise.\n    \"\"\"\n    if on == 'users':\n        return self.user_id_encoder.is_encoded()\n    elif on == 'items':\n        return self.item_id_encoder.is_encoded()\n    else:\n        raise ValueError(\"Parameter 'on' must be either 'users' or 'items'.\")\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.encode","title":"<code>encode(users=True, items=True)</code>","text":"<p>Converts user and item IDs to encoded integer IDs. Args:     users (bool): If True, encodes user IDs.     items (bool): If True, encodes item IDs.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def encode(self, users=True, items=True) -&gt; None:\n    \"\"\"\n    Converts user and item IDs to encoded integer IDs.\n    Args:\n        users (bool): If True, encodes user IDs.\n        items (bool): If True, encodes item IDs.\n    \"\"\"\n    if users:\n        if not self.user_id_encoder.is_encoded():\n            raise ValueError(\"User encoder is empty. Build or apply an encoding before calling encode().\")\n        self.data[self.user_col] = self.user_id_encoder.encode(self.data[self.user_col].tolist())\n    if items:\n        if not self.item_id_encoder.is_encoded():\n            raise ValueError(\"Item encoder is empty. Build or apply an encoding before calling encode().\")\n        self.data[self.item_col] = self.item_id_encoder.encode(self.data[self.item_col].tolist())\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.decode","title":"<code>decode(users=True, items=True)</code>","text":"<p>Converts user and item IDs back to original IDs. Args:     users (bool): If True, encodes user IDs.     items (bool): If True, encodes item IDs.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def decode(self, users=True, items=True) -&gt; None:\n    \"\"\"\n    Converts user and item IDs back to original IDs.\n    Args:\n        users (bool): If True, encodes user IDs.\n        items (bool): If True, encodes item IDs.\n    \"\"\"\n    if users:\n        self.data[self.user_col] = self.user_id_encoder.decode(self.data[self.user_col].tolist())\n    if items:\n        self.data[self.item_col] = self.item_id_encoder.decode(self.data[self.item_col].tolist())\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.reset_encoding","title":"<code>reset_encoding(on='all')</code>","text":"<p>Resets the encoding for users or items.</p> <p>Parameters:</p> Name Type Description Default <code>on</code> <code>str</code> <p>'users' to reset user encoding, 'items' for item encoding.</p> <code>'all'</code> Source code in <code>datarec/data/dataset.py</code> <pre><code>def reset_encoding(self, on='all') -&gt; None:\n    \"\"\"\n    Resets the encoding for users or items.\n\n    Args:\n        on (str): 'users' to reset user encoding, 'items' for item encoding.\n    \"\"\"\n    if on == 'users':\n        self.decode(users=True, items=False)\n        self.user_id_encoder.reset_encoding()\n    elif on == 'items':\n        self.decode(users=False, items=True)\n        self.item_id_encoder.reset_encoding()\n    elif on == 'all':\n        self.decode(users=True, items=True)\n        self.user_id_encoder.reset_encoding()\n        self.item_id_encoder.reset_encoding()\n    else:\n        raise ValueError(\"Parameter 'on' must be either 'users' or 'items'.\")\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.build_encoding","title":"<code>build_encoding(on='users', offset=0)</code>","text":"<p>Builds the encoding for users or items.</p> <p>Parameters:</p> Name Type Description Default <code>on</code> <code>str</code> <p>'users' to build user encoding, 'items' for item encoding, 'all' for both.</p> <code>'users'</code> <code>offset</code> <code>int</code> <p>The starting integer for the private IDs.</p> <code>0</code> Source code in <code>datarec/data/dataset.py</code> <pre><code>def build_encoding(self, on='users', offset=0) -&gt; None:\n    \"\"\"\n    Builds the encoding for users or items.\n\n    Args:\n        on (str): 'users' to build user encoding, 'items' for item encoding, 'all' for both.\n        offset (int): The starting integer for the private IDs.\n    \"\"\"\n    if on == 'users':\n        self.user_id_encoder.build_encoding(self.users, offset=offset)\n    elif on == 'items':\n        self.item_id_encoder.build_encoding(self.items, offset=offset)\n    elif on == 'all':\n        self.user_id_encoder.build_encoding(self.users, offset=offset)\n        self.item_id_encoder.build_encoding(self.items, offset=offset)\n    else:\n        raise ValueError(\"Parameter 'on' must be either 'users' or 'items'.\")\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.characteristic","title":"<code>characteristic(name, **kwargs)</code>","text":"<p>Retrieves a calculated dataset characteristic by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the characteristic to retrieve.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the characteristic function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The value of the requested characteristic.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def characteristic(self, name: str, **kwargs: Any) -&gt; Any:\n    \"\"\"\n    Retrieves a calculated dataset characteristic by name.\n\n    Args:\n        name (str): The name of the characteristic to retrieve.\n        **kwargs: Additional arguments to pass to the characteristic function.\n\n    Returns:\n        The value of the requested characteristic.\n    \"\"\"\n    return getattr(self.characteristics, name)(**kwargs)\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.users_frequency","title":"<code>users_frequency()</code>","text":"<p>Computes the absolute frequency of each user in the dataset.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping user IDs to the number of interactions,  sorted in descending order of frequency.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def users_frequency(self):\n    \"\"\"\n    Computes the absolute frequency of each user in the dataset.\n\n    Returns:\n        (dict): A dictionary mapping user IDs to the number of interactions, \n            sorted in descending order of frequency.\n    \"\"\"\n    fr = dict(Counter(self.data[self.user_col]))\n    return dict(sorted(fr.items(), key=lambda item: item[1], reverse=True))\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.users_relative_frequency","title":"<code>users_relative_frequency()</code>","text":"<p>Computes the relative frequency of each user in the dataset.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping user IDs to their relative frequency  (fraction of total transactions).</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def users_relative_frequency(self):\n    \"\"\"\n    Computes the relative frequency of each user in the dataset.\n\n    Returns:\n        (dict): A dictionary mapping user IDs to their relative frequency \n            (fraction of total transactions).\n    \"\"\"\n    return {u: (f / self.transactions) for u, f in self.users_frequency().items()}\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.items_frequency","title":"<code>items_frequency()</code>","text":"<p>Computes the absolute frequency of each item in the dataset.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping item IDs to the number of interactions,  sorted in descending order of frequency.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def items_frequency(self):\n    \"\"\"\n    Computes the absolute frequency of each item in the dataset.\n\n    Returns:\n        (dict): A dictionary mapping item IDs to the number of interactions, \n            sorted in descending order of frequency.\n    \"\"\"\n    fr = dict(Counter(self.data[self.item_col]))\n    return dict(sorted(fr.items(), key=lambda item: item[1], reverse=True))\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.items_relative_frequency","title":"<code>items_relative_frequency()</code>","text":"<p>Computes the relative frequency of each item in the dataset.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping item IDs to their relative frequency  (fraction of total transactions).</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def items_relative_frequency(self):\n    \"\"\"\n    Computes the relative frequency of each item in the dataset.\n\n    Returns:\n        (dict): A dictionary mapping item IDs to their relative frequency \n            (fraction of total transactions).\n    \"\"\"\n    return {u: (f / self.transactions) for u, f in self.items_frequency().items()}\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.users_quartiles","title":"<code>users_quartiles()</code>","text":"<p>Assigns quartile indices to users based on their frequency.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping each user ID to a quartile index (0-3), where 0 = lowest, 3 = highest frequency.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def users_quartiles(self):\n    \"\"\"\n    Assigns quartile indices to users based on their frequency.\n\n    Returns:\n        (dict): A dictionary mapping each user ID to a quartile index (0-3),\n            where 0 = lowest, 3 = highest frequency.\n    \"\"\" \n    return quartiles(self.users_frequency())\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.items_quartiles","title":"<code>items_quartiles()</code>","text":"<p>Assigns quartile indices to items based on their frequency.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping each item ID to a quartile index (0-3), where 0 = lowest, 3 = highest frequency.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def items_quartiles(self):\n    \"\"\"\n    Assigns quartile indices to items based on their frequency.\n\n    Returns:\n        (dict): A dictionary mapping each item ID to a quartile index (0-3),\n            where 0 = lowest, 3 = highest frequency.\n    \"\"\"\n    return quartiles(self.items_frequency())\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.users_popularity","title":"<code>users_popularity()</code>","text":"<p>Categorizes users into descriptive popularity groups based on quartiles.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping popularity categories ('long tail',  'common', 'popular', 'most popular') to lists of user IDs.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def users_popularity(self):\n    \"\"\"\n    Categorizes users into descriptive popularity groups based on quartiles.\n\n    Returns:\n        (dict): A dictionary mapping popularity categories ('long tail', \n            'common', 'popular', 'most popular') to lists of user IDs.\n    \"\"\"\n    return popularity(self.users_quartiles())\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.items_popularity","title":"<code>items_popularity()</code>","text":"<p>Categorizes items into descriptive popularity groups based on quartiles.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping popularity categories ('long tail',  'common', 'popular', 'most popular') to lists of item IDs.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def items_popularity(self):\n    \"\"\"\n    Categorizes items into descriptive popularity groups based on quartiles.\n\n    Returns:\n        (dict): A dictionary mapping popularity categories ('long tail', \n            'common', 'popular', 'most popular') to lists of item IDs.\n    \"\"\"\n    return popularity(self.items_quartiles())\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.list_characteristics","title":"<code>list_characteristics()</code>","text":"<p>Return the names of all characteristics that can be computed on this dataset.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def list_characteristics(self) -&gt; list[str]:\n    \"\"\"Return the names of all characteristics that can be computed on this dataset.\"\"\"\n    return sorted(CHARACTERISTICS.keys())\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.describe_characteristics","title":"<code>describe_characteristics()</code>","text":"<p>Return a mapping name -&gt; short docstring for each available characteristic.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def describe_characteristics(self) -&gt; dict[str, str]:\n    \"\"\"Return a mapping name -&gt; short docstring for each available characteristic.\"\"\"\n    return {\n        name: (func.__doc__ or \"\").strip()\n        for name, func in CHARACTERISTICS.items()\n    }\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.copy","title":"<code>copy()</code>","text":"<p>Create a deep copy of the current DataRec object.</p> <p>This method duplicates the DataRec instance, including its data, metadata (user, item, rating, timestamp columns), pipeline, and internal state such as privacy settings and implicit flags.</p> <p>Returns:</p> Type Description <code>DataRec</code> <p>A new DataRec object that is a deep copy of the current instance.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def copy(self):\n    \"\"\"\n    Create a deep copy of the current DataRec object.\n\n    This method duplicates the DataRec instance, including its data,\n    metadata (user, item, rating, timestamp columns), pipeline, and internal\n    state such as privacy settings and implicit flags.\n\n    Returns:\n        (DataRec): A new DataRec object that is a deep copy of the current instance.\n    \"\"\"\n    pipeline = self.pipeline.copy()\n\n    new_dr = DataRec(rawdata=self.to_rawdata(),\n                     pipeline=pipeline,\n                     copy=True)\n\n    new_dr._user_col = self.user_col\n    new_dr._item_col = self.item_col\n    new_dr._rating_col = self.rating_col\n    new_dr._timestamp_col = self.timestamp_col\n    return new_dr\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.to_rawdata","title":"<code>to_rawdata()</code>","text":"<p>Convert the current DataRec object into a RawData object.</p> <p>This method creates a RawData instance containing the same data and metadata (user, item, rating, timestamp columns) as the DataRec object.</p> <p>Returns:</p> Type Description <code>RawData</code> <p>A new RawData object containing the DataRec's data and column information.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def to_rawdata(self):\n    \"\"\"\n    Convert the current DataRec object into a RawData object.\n\n    This method creates a RawData instance containing the same data and\n    metadata (user, item, rating, timestamp columns) as the DataRec object.\n\n    Returns:\n        (RawData): A new RawData object containing the DataRec's data and column information.\n    \"\"\"\n    raw = RawData(self.data)\n    raw.user = self.user_col\n    raw.item = self.item_col\n    raw.rating = self.rating_col\n    raw.timestamp = self.timestamp_col\n    return raw\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.save_pipeline","title":"<code>save_pipeline(filepath)</code>","text":"<p>Save the current processing pipeline to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path (including filename) where the pipeline  YAML file will be saved.</p> required Source code in <code>datarec/data/dataset.py</code> <pre><code>def save_pipeline(self, filepath: str) -&gt; None:\n    \"\"\"\n    Save the current processing pipeline to a YAML file.\n\n    Args:\n        filepath (str): The path (including filename) where the pipeline \n            YAML file will be saved.\n    \"\"\"\n    print(f'Saving pipeline to {filepath}')\n\n    self.pipeline.to_yaml(filepath)\n\n    print(f'Pipeline correctly saved to {filepath}')\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.to_torch_dataset","title":"<code>to_torch_dataset(task='pointwise', autoprepare=True, **kwargs)</code>","text":"<p>Converts the current DataRec object into a PyTorch-compatible dataset.</p> <p>This method prepares the dataset (e.g., remaps user/item IDs to a dense index space) and returns a <code>torch.utils.data.Dataset</code> object suitable for training with PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>The recommendation task type. Must be one of: - \"pointwise\": returns PointwiseTorchDataset - \"pairwise\": returns PairwiseTorchDataset - \"ranking\": returns RankingTorchDataset</p> <code>'pointwise'</code> <code>autoprepare</code> <code>bool</code> <p>If True, automatically applies user/item remapping and switches the dataset to private IDs. If False, assumes the dataset is already properly prepared.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to the specific torch dataset class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>A PyTorch dataset instance corresponding to the selected task.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If PyTorch is not installed.</p> <code>ValueError</code> <p>If an unknown task name is provided.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def to_torch_dataset(self, task: str = \"pointwise\", autoprepare: bool = True, **kwargs: Any) -&gt; Any:\n    \"\"\"\n    Converts the current DataRec object into a PyTorch-compatible dataset.\n\n    This method prepares the dataset (e.g., remaps user/item IDs to a dense index space)\n    and returns a `torch.utils.data.Dataset` object suitable for training with PyTorch.\n\n    Args:\n        task (str): The recommendation task type. Must be one of:\n            - \"pointwise\": returns PointwiseTorchDataset\n            - \"pairwise\": returns PairwiseTorchDataset\n            - \"ranking\": returns RankingTorchDataset\n        autoprepare (bool): If True, automatically applies user/item remapping\n            and switches the dataset to private IDs. If False, assumes the dataset\n            is already properly prepared.\n        **kwargs: Additional arguments passed to the specific torch dataset class.\n\n    Returns:\n        (torch.utils.data.Dataset): A PyTorch dataset instance corresponding to the selected task.\n\n    Raises:\n        ImportError: If PyTorch is not installed.\n        ValueError: If an unknown task name is provided.\n    \"\"\"\n\n    try:\n        import torch\n    except ModuleNotFoundError:\n        raise ImportError(\n            \"Torch is required for this feature. Please install it with `pip install datarec[torch]` or\"\n            \" `pip install -r requirements/requirements-torch.txt`.\"\n        )\n\n    if autoprepare:\n        self.map_users_and_items()\n        self.encode()\n    else:\n        warnings.warn(\n            \"Autoprepare is set to False. \"\n            \"Ensure that the dataset is prepared correctly before using it with PyTorch.\"\n        )\n\n    if task == \"pointwise\":\n        from datarec.data.torch_dataset import PointwiseTorchDataset\n        return PointwiseTorchDataset(self, **kwargs)\n    elif task == \"pairwise\":\n        from datarec.data.torch_dataset import PairwiseTorchDataset\n        return PairwiseTorchDataset(self, **kwargs)\n    elif task == \"ranking\":\n        from datarec.data.torch_dataset import RankingTorchDataset\n        return RankingTorchDataset(self, **kwargs)\n    else:\n        raise ValueError(f\"Unknown task: {task}\")\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.to_graphrec","title":"<code>to_graphrec()</code>","text":"<p>Converts the current DataRec object into a GraphRec object.</p> <p>This method creates a GraphRec instance representing the bipartite graph of user-item interactions contained in the DataRec object.</p> <p>Returns:</p> Type Description <code>GraphRec</code> <p>A new GraphRec object representing the interaction graph.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def to_graphrec(self):\n    \"\"\"\n    Converts the current DataRec object into a GraphRec object.\n\n    This method creates a GraphRec instance representing the bipartite graph\n    of user-item interactions contained in the DataRec object.\n\n    Returns:\n        (GraphRec): A new GraphRec object representing the interaction graph.\n    \"\"\"\n    from datarec.data.graph import GraphRec\n    return GraphRec(self)\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.to_pickle","title":"<code>to_pickle(filepath='')</code>","text":"<p>Save the current DataRec object to a pickle file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path (including filename) where the pickle file will be saved.</p> <code>''</code> Source code in <code>datarec/data/dataset.py</code> <pre><code>def to_pickle(self, filepath: str = '') -&gt; None:\n    \"\"\"\n    Save the current DataRec object to a pickle file.\n\n    Args:\n        filepath (str): The path (including filename) where the pickle file will be saved.\n    \"\"\"\n    import pickle\n\n    if filepath == '':\n        filepath = paths.pickle_version_filepath(self.dataset_name, self.version_name)\n\n    print(f'Saving DataRec to {filepath}')\n\n    with open(filepath, 'wb') as f:\n        pickle.dump(self, f)\n\n    print(f'DataRec correctly saved to {filepath}')\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.CharacteristicAccessor","title":"<code>CharacteristicAccessor</code>","text":"<p>Accessor for dataset characteristics. Allows dynamic retrieval of dataset characteristics as attributes.</p> <p>Parameters:</p> Name Type Description Default <code>dr</code> <code>DataRec</code> <p>The DataRec object to access characteristics from.</p> required Source code in <code>datarec/data/dataset.py</code> <pre><code>class CharacteristicAccessor:\n    \"\"\"\n    Accessor for dataset characteristics.\n    Allows dynamic retrieval of dataset characteristics as attributes.\n\n    Args:\n        dr (DataRec): The DataRec object to access characteristics from.\n    \"\"\"\n    def __init__(self, dr: DataRec):\n        self._datarec = dr\n\n    def __getattr__(self, name):\n        try:\n            func = CHARACTERISTICS[name]\n        except KeyError:\n            raise AttributeError(name) from None\n\n        def bound(**kwargs):\n            return func(self._datarec, **kwargs)\n\n        bound.__doc__ = func.__doc__\n        return bound\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.from_pickle","title":"<code>from_pickle(dataset_name='', version_name='', filepath='')</code>","text":"<p>Load a DataRec object from a pickle file.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>The name of the dataset.</p> <code>''</code> <code>version_name</code> <code>str</code> <p>The version identifier of the dataset.</p> <code>''</code> <code>filepath</code> <code>str</code> <p>The path to the pickle file.</p> <code>''</code> <p>Returns:     (DataRec): The loaded DataRec object.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def from_pickle(dataset_name:str = '', version_name:str = '', filepath: str = '') -&gt; DataRec:\n    \"\"\"\n    Load a DataRec object from a pickle file.\n\n    Args:\n        dataset_name (str): The name of the dataset.\n        version_name (str): The version identifier of the dataset.\n        filepath (str): The path to the pickle file.\n    Returns:\n        (DataRec): The loaded DataRec object.\n    \"\"\"\n    import pickle\n\n    if filepath == '':\n        if dataset_name == '' and version_name == '':\n            raise ValueError(\"Either dataset_name and version_name or filepath must be provided.\")\n        filepath = paths.pickle_version_filepath(dataset_name, version_name)\n\n    print(f'Loading DataRec from {filepath}')\n\n    with open(filepath, 'rb') as f:\n        dr = pickle.load(f)\n\n    print(f'DataRec correctly loaded from {filepath}')\n\n    return dr\n</code></pre>"},{"location":"documentation/data/#datarec.data.source.Source","title":"<code>Source</code>  <code>dataclass</code>","text":"Source code in <code>datarec/data/source.py</code> <pre><code>@dataclass\nclass Source:\n    checksum: str = None\n    checksum_algorithm: str = \"md5\"\n    prepared: bool = False\n    source_name: Optional[str] = None\n    filename: Optional[str] = None\n    archive: Optional[str] = None\n    inner_paths: Optional[Dict[str, str]] = None\n    downloadable: bool = False\n    output_folder: Optional[str] = None\n\n    def path(self, output_folder=None) -&gt; str:\n        if output_folder is None:\n            if self.output_folder is None:\n                raise ValueError(\"Must specify an output folder\")\n            output_folder = self.output_folder\n        return os.path.join(output_folder, self.filename)\n\n    def is_locally_available(self, output_folder=None) -&gt; bool:\n        \"\"\"\n        Check if the source file exists in the output folder.\n        \"\"\"\n        if output_folder is None:\n            if self.output_folder is None:\n                raise ValueError(\"Must specify an output folder\")\n        return os.path.exists(self.path())\n\n    def verify_checksum(self, output_folder=None) -&gt; None:\n        print(f'{self.filename}: verifying checksum')\n        if output_folder is None:\n            output_folder = self.output_folder\n        verify_checksum(self.path(output_folder), self.checksum)\n\n    def download(self) -&gt; str:\n        pass\n\n    def prepare(self) -&gt; None:\n        \"\"\"\n        Prepares the source by downloading it if not available locally\n        and verifying its checksum.\n        Returns:\n            (None)\n        \"\"\"\n        if self.prepared:\n            return\n        # check if source file exists, if not download it\n        if not self.is_locally_available():\n            self.download()\n        # verify source checksum\n        self.verify_checksum()\n        self.prepared = True\n\n    def resource_paths(self) -&gt; Dict[str, str]:\n        \"\"\"\n        Returns a dictionary containing the paths of the resources inside the source.\n        Returns:\n            (dict): A dictionary containing the paths of the resources inside the source.\n        \"\"\"\n        resources_path = {res: os.path.join(self.output_folder, inner_path) for res, inner_path in self.inner_paths.items()}\n        return resources_path\n\n    def resources_available(self) -&gt; bool:\n        \"\"\"\n        Check if all resources inside the source are available locally.\n        Returns:\n            (bool): True if all resources are available, False otherwise.\n        \"\"\"\n        resource_paths = self.resource_paths()\n        for resource_path in resource_paths.values():\n            if not os.path.exists(resource_path):\n                return False\n        return True\n\n\n    def get_resources(self, force=False) -&gt; Dict[str, str]:\n        \"\"\"\n        Returns a dictionary containing the paths of the resources inside the source.\n        Args:\n            force (bool): If True, forces re-extraction of resources even if they are already available.\n        Returns:\n            (dict): A dictionary containing the paths of the resources inside the source.\n        \"\"\"\n        if self.resources_available() and not force:\n            return self.resource_paths()\n\n        if self.archive:\n            decompress_file(self.path(), self.output_folder, self.archive)\n        resources = {}\n        for resource, inner_path in self.inner_paths.items():\n            resource_path = os.path.join(self.output_folder, inner_path)\n            if os.path.exists(resource_path):\n                resources[resource] = resource_path\n        return resources\n</code></pre>"},{"location":"documentation/data/#datarec.data.source.Source.is_locally_available","title":"<code>is_locally_available(output_folder=None)</code>","text":"<p>Check if the source file exists in the output folder.</p> Source code in <code>datarec/data/source.py</code> <pre><code>def is_locally_available(self, output_folder=None) -&gt; bool:\n    \"\"\"\n    Check if the source file exists in the output folder.\n    \"\"\"\n    if output_folder is None:\n        if self.output_folder is None:\n            raise ValueError(\"Must specify an output folder\")\n    return os.path.exists(self.path())\n</code></pre>"},{"location":"documentation/data/#datarec.data.source.Source.prepare","title":"<code>prepare()</code>","text":"<p>Prepares the source by downloading it if not available locally and verifying its checksum. Returns:     (None)</p> Source code in <code>datarec/data/source.py</code> <pre><code>def prepare(self) -&gt; None:\n    \"\"\"\n    Prepares the source by downloading it if not available locally\n    and verifying its checksum.\n    Returns:\n        (None)\n    \"\"\"\n    if self.prepared:\n        return\n    # check if source file exists, if not download it\n    if not self.is_locally_available():\n        self.download()\n    # verify source checksum\n    self.verify_checksum()\n    self.prepared = True\n</code></pre>"},{"location":"documentation/data/#datarec.data.source.Source.resource_paths","title":"<code>resource_paths()</code>","text":"<p>Returns a dictionary containing the paths of the resources inside the source. Returns:     (dict): A dictionary containing the paths of the resources inside the source.</p> Source code in <code>datarec/data/source.py</code> <pre><code>def resource_paths(self) -&gt; Dict[str, str]:\n    \"\"\"\n    Returns a dictionary containing the paths of the resources inside the source.\n    Returns:\n        (dict): A dictionary containing the paths of the resources inside the source.\n    \"\"\"\n    resources_path = {res: os.path.join(self.output_folder, inner_path) for res, inner_path in self.inner_paths.items()}\n    return resources_path\n</code></pre>"},{"location":"documentation/data/#datarec.data.source.Source.resources_available","title":"<code>resources_available()</code>","text":"<p>Check if all resources inside the source are available locally. Returns:     (bool): True if all resources are available, False otherwise.</p> Source code in <code>datarec/data/source.py</code> <pre><code>def resources_available(self) -&gt; bool:\n    \"\"\"\n    Check if all resources inside the source are available locally.\n    Returns:\n        (bool): True if all resources are available, False otherwise.\n    \"\"\"\n    resource_paths = self.resource_paths()\n    for resource_path in resource_paths.values():\n        if not os.path.exists(resource_path):\n            return False\n    return True\n</code></pre>"},{"location":"documentation/data/#datarec.data.source.Source.get_resources","title":"<code>get_resources(force=False)</code>","text":"<p>Returns a dictionary containing the paths of the resources inside the source. Args:     force (bool): If True, forces re-extraction of resources even if they are already available. Returns:     (dict): A dictionary containing the paths of the resources inside the source.</p> Source code in <code>datarec/data/source.py</code> <pre><code>def get_resources(self, force=False) -&gt; Dict[str, str]:\n    \"\"\"\n    Returns a dictionary containing the paths of the resources inside the source.\n    Args:\n        force (bool): If True, forces re-extraction of resources even if they are already available.\n    Returns:\n        (dict): A dictionary containing the paths of the resources inside the source.\n    \"\"\"\n    if self.resources_available() and not force:\n        return self.resource_paths()\n\n    if self.archive:\n        decompress_file(self.path(), self.output_folder, self.archive)\n    resources = {}\n    for resource, inner_path in self.inner_paths.items():\n        resource_path = os.path.join(self.output_folder, inner_path)\n        if os.path.exists(resource_path):\n            resources[resource] = resource_path\n    return resources\n</code></pre>"},{"location":"documentation/data/#datarec.data.source.NestedSource","title":"<code>NestedSource</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Source</code></p> Source code in <code>datarec/data/source.py</code> <pre><code>@dataclass\nclass NestedSource(Source):\n    parent_source_name: str = ''\n    parent_source: Optional[Source] = None\n\n    # def path(self, output_folder=None) -&gt; str:\n    #     output_folder = self.parent_source.output_folder\n    #     if output_folder is None:\n    #         raise ValueError(\"Must specify an output folder\")\n    #     if self.parent_source is None:\n    #         raise RuntimeError('Parent source {self.parent_source_name} is not available. You need to link the parent source before (see self.link_parent_source).')\n\n    #     inner_paths = self.parent_source.inner_paths\n    #     if inner_paths is None:\n    #         raise RuntimeError(f'Inner paths not found in parent source \\'{self.parent_source_name}\\'.')\n\n    #     inner_path = inner_paths.get(self.source_name, None)\n    #     if inner_path is None:\n    #         raise RuntimeError(f\"NestedSource {self.source_name} not found in parent source \\'{self.parent_source_name}\\' inner paths\")\n\n    #     return os.path.join(output_folder, inner_path)\n\n    def link_parent_source(self, sources: dict[str, Source]):\n        \"\"\"\n        Links the resource to its source.\n        Args:\n            sources (dict): A dictionary containing dataset sources objects.\n        Returns:\n            (None): None\n        \"\"\"\n        if self.parent_source_name is None:\n            raise RuntimeError(f\"No source provided for resource {self.filename}\")\n        if self.parent_source_name not in sources:\n            raise RuntimeError(f\"Source {self.parent_source_name} not found\")\n        self.parent_source = sources[self.parent_source_name]\n\n        output_folder = self.parent_source.output_folder\n        if output_folder is None:\n            raise ValueError(\"Must specify an output folder\")\n\n        inner_paths = self.parent_source.inner_paths\n        if inner_paths is None:\n            raise RuntimeError(f'Inner paths not found in parent source \\'{self.parent_source_name}\\'.')\n\n        inner_path = inner_paths.get(self.source_name, None)\n        if inner_path is None:\n            raise RuntimeError(f\"NestedSource {self.source_name} not found in parent source \\'{self.parent_source_name}\\' inner paths\")\n\n        inner_parent_path = os.path.dirname(inner_path)\n        self.output_folder = os.path.join(self.parent_source.output_folder, inner_parent_path)\n\n\n    def prepare(self) -&gt; None:\n        \"\"\"\n        Prepares the source by downloading it if not available locally\n        and verifying its checksum.\n        Returns:\n            (None)\n        \"\"\"\n        if self.prepared:\n            return\n        # check if source file exists, if not download it\n        if not self.is_locally_available():\n            if self.parent_source_name is None:\n                raise RuntimeError(f\"Parent source name not defined for nested source\")\n            if self.parent_source is None:\n                raise RuntimeError(f\"Parent source not set for nested source\")\n\n            # prepare parent source\n            if not self.parent_source.prepared:\n                self.parent_source.prepare()\n\n            # check that this resource is available in parent source\n            resources = self.parent_source.get_resources()\n            if self.source_name not in resources:\n                raise RuntimeError(f\"Resource {self.filename} not found in parent source\")\n\n            # verify checksum\n            self.verify_checksum()\n\n            self.prepared = True\n        return\n</code></pre>"},{"location":"documentation/data/#datarec.data.source.NestedSource.link_parent_source","title":"<code>link_parent_source(sources)</code>","text":"<p>Links the resource to its source. Args:     sources (dict): A dictionary containing dataset sources objects. Returns:     (None): None</p> Source code in <code>datarec/data/source.py</code> <pre><code>def link_parent_source(self, sources: dict[str, Source]):\n    \"\"\"\n    Links the resource to its source.\n    Args:\n        sources (dict): A dictionary containing dataset sources objects.\n    Returns:\n        (None): None\n    \"\"\"\n    if self.parent_source_name is None:\n        raise RuntimeError(f\"No source provided for resource {self.filename}\")\n    if self.parent_source_name not in sources:\n        raise RuntimeError(f\"Source {self.parent_source_name} not found\")\n    self.parent_source = sources[self.parent_source_name]\n\n    output_folder = self.parent_source.output_folder\n    if output_folder is None:\n        raise ValueError(\"Must specify an output folder\")\n\n    inner_paths = self.parent_source.inner_paths\n    if inner_paths is None:\n        raise RuntimeError(f'Inner paths not found in parent source \\'{self.parent_source_name}\\'.')\n\n    inner_path = inner_paths.get(self.source_name, None)\n    if inner_path is None:\n        raise RuntimeError(f\"NestedSource {self.source_name} not found in parent source \\'{self.parent_source_name}\\' inner paths\")\n\n    inner_parent_path = os.path.dirname(inner_path)\n    self.output_folder = os.path.join(self.parent_source.output_folder, inner_parent_path)\n</code></pre>"},{"location":"documentation/data/#datarec.data.source.NestedSource.prepare","title":"<code>prepare()</code>","text":"<p>Prepares the source by downloading it if not available locally and verifying its checksum. Returns:     (None)</p> Source code in <code>datarec/data/source.py</code> <pre><code>def prepare(self) -&gt; None:\n    \"\"\"\n    Prepares the source by downloading it if not available locally\n    and verifying its checksum.\n    Returns:\n        (None)\n    \"\"\"\n    if self.prepared:\n        return\n    # check if source file exists, if not download it\n    if not self.is_locally_available():\n        if self.parent_source_name is None:\n            raise RuntimeError(f\"Parent source name not defined for nested source\")\n        if self.parent_source is None:\n            raise RuntimeError(f\"Parent source not set for nested source\")\n\n        # prepare parent source\n        if not self.parent_source.prepared:\n            self.parent_source.prepare()\n\n        # check that this resource is available in parent source\n        resources = self.parent_source.get_resources()\n        if self.source_name not in resources:\n            raise RuntimeError(f\"Resource {self.filename} not found in parent source\")\n\n        # verify checksum\n        self.verify_checksum()\n\n        self.prepared = True\n    return\n</code></pre>"},{"location":"documentation/data/#datarec.data.source.set_source","title":"<code>set_source(source_name, source_conf)</code>","text":"<p>Given a resource configuration, return a new resource object Args:     resource_name (str): name of the resource     raw_resource (dict): resource configuration Returns:     (Source): a dataset source object</p> Source code in <code>datarec/data/source.py</code> <pre><code>def set_source(source_name:str, source_conf:dict) -&gt; Source:\n    \"\"\"\n    Given a resource configuration, return a new resource object\n    Args:\n        resource_name (str): name of the resource\n        raw_resource (dict): resource configuration\n    Returns:\n        (Source): a dataset source object\n    \"\"\"\n    source_type = SOURCE_TYPES[source_conf['source_type']]\n    source = source_type(source_name=source_name, **source_conf['args'])\n    return source\n</code></pre>"},{"location":"documentation/data/#datarec.data.source.set_sources","title":"<code>set_sources(config, folder=None)</code>","text":"<p>Given a dataset configuration, return a new dataset configuration Args:     config (dict): dataset configuration     folder (str): source output folder Returns:     (dict): a dictionary containing dataset sources objects</p> Source code in <code>datarec/data/source.py</code> <pre><code>def set_sources(config:dict, folder:Optional[str]=None) -&gt; dict[str, Source]:\n    \"\"\"\n    Given a dataset configuration, return a new dataset configuration\n    Args:\n        config (dict): dataset configuration\n        folder (str): source output folder\n    Returns:\n        (dict): a dictionary containing dataset sources objects\n    \"\"\"\n    sources = dict()\n    for source_name, raw_source in config['sources'].items():\n        source = set_source(source_name, raw_source)\n        if folder is not None:\n            source.output_folder = folder\n        sources[source_name] = source\n\n    # link parent source to nested sources, if any\n    for source in sources.values():\n        if isinstance(source, NestedSource):\n            source.link_parent_source(sources)\n    return sources\n</code></pre>"},{"location":"documentation/data/#datarec.data.resource.Resource","title":"<code>Resource</code>  <code>dataclass</code>","text":"Source code in <code>datarec/data/resource.py</code> <pre><code>@dataclass\nclass Resource:\n    resource_name: Optional[str] = None\n    source_name: Optional[str] = None\n    source: Optional[Source] = None\n    filename: Optional[str] = None\n    type: Optional[str] = None\n    format: Optional[str] = None\n    required: Optional[bool] = False\n    about: Optional[str] = None\n    dataset_name: Optional[str] = None\n    version: Optional[str] = None\n    output_folder: Optional[str] = None\n    prepared = False\n\n    def link_source(self, sources: dict[str, Source]):\n        \"\"\"\n        Links the resource to its source.\n        Args:\n            sources (dict): A dictionary containing dataset sources objects.\n        Returns:\n            (None): None\n        \"\"\"\n        if self.source_name is None:\n            raise RuntimeError(f\"No source provided for resource {self.filename}\")\n        if self.source_name not in sources:\n            raise RuntimeError(f\"Source {self.source_name} not found\")\n        self.source = sources[self.source_name]\n\n    def is_locally_available(self) -&gt; Union[str, bool]:\n        \"\"\"\n        Checks if the resource file is available locally.\n        Returns:\n            (str): The local path of the resource file if available, otherwise False.\n        \"\"\"\n        # Check that a source is linked to resource\n        if self.source is None:\n            raise RuntimeError(f\"No source provided for resource {self.filename}\")\n        resource_path = self.path()\n        # Check if resource file already exists\n        if os.path.exists(resource_path):\n            return True\n        return False\n\n    def prepare(self, *args, **kwargs):\n        \"\"\"\n        Ensures the resource file is downloaded and available locally.\n        \"\"\"\n        if self.source is None:\n            raise RuntimeError(f\"No source provided for resource {self.filename}\")\n\n        self.source.prepare()\n        resources = self.source.get_resources()\n\n        if self.resource_name not in resources:\n            raise RuntimeError(f\"Resource {self.resource_name} not found in source\")\n        self.prepared = True\n\n    def path(self):\n        \"\"\"\n        Returns the local path of the resource file.\n        Raises an error if the output folder is not specified.\n        Returns:\n            (str): The local path of the resource file.\n        \"\"\"\n        output_folder = self.source.output_folder\n        if output_folder is None:\n            raise ValueError(\"Must specify an output folder\")\n        inner_path = self.source.inner_paths.get(self.resource_name, None)\n        if inner_path is None:\n            raise RuntimeError(f\"Resource {self.resource_name} not found in source inner paths\")\n        return os.path.join(output_folder, inner_path)\n\n    def assign_dataset_info(self, dataset_name:str, version:str):\n        \"\"\"\n        Assigns dataset name and version to the resource.\n        Args:\n            dataset_name (str): The name of the dataset.\n            version (str): The version of the dataset.\n        Returns:\n            (None): None\n        \"\"\"\n        self.dataset_name = dataset_name\n        self.version = version\n\n    def free_cache(self):\n        \"\"\"\n        Frees the cached version of the resource if it exists.\n        Returns:\n            (None): None\n        \"\"\"\n        pass\n</code></pre>"},{"location":"documentation/data/#datarec.data.resource.Resource.link_source","title":"<code>link_source(sources)</code>","text":"<p>Links the resource to its source. Args:     sources (dict): A dictionary containing dataset sources objects. Returns:     (None): None</p> Source code in <code>datarec/data/resource.py</code> <pre><code>def link_source(self, sources: dict[str, Source]):\n    \"\"\"\n    Links the resource to its source.\n    Args:\n        sources (dict): A dictionary containing dataset sources objects.\n    Returns:\n        (None): None\n    \"\"\"\n    if self.source_name is None:\n        raise RuntimeError(f\"No source provided for resource {self.filename}\")\n    if self.source_name not in sources:\n        raise RuntimeError(f\"Source {self.source_name} not found\")\n    self.source = sources[self.source_name]\n</code></pre>"},{"location":"documentation/data/#datarec.data.resource.Resource.is_locally_available","title":"<code>is_locally_available()</code>","text":"<p>Checks if the resource file is available locally. Returns:     (str): The local path of the resource file if available, otherwise False.</p> Source code in <code>datarec/data/resource.py</code> <pre><code>def is_locally_available(self) -&gt; Union[str, bool]:\n    \"\"\"\n    Checks if the resource file is available locally.\n    Returns:\n        (str): The local path of the resource file if available, otherwise False.\n    \"\"\"\n    # Check that a source is linked to resource\n    if self.source is None:\n        raise RuntimeError(f\"No source provided for resource {self.filename}\")\n    resource_path = self.path()\n    # Check if resource file already exists\n    if os.path.exists(resource_path):\n        return True\n    return False\n</code></pre>"},{"location":"documentation/data/#datarec.data.resource.Resource.prepare","title":"<code>prepare(*args, **kwargs)</code>","text":"<p>Ensures the resource file is downloaded and available locally.</p> Source code in <code>datarec/data/resource.py</code> <pre><code>def prepare(self, *args, **kwargs):\n    \"\"\"\n    Ensures the resource file is downloaded and available locally.\n    \"\"\"\n    if self.source is None:\n        raise RuntimeError(f\"No source provided for resource {self.filename}\")\n\n    self.source.prepare()\n    resources = self.source.get_resources()\n\n    if self.resource_name not in resources:\n        raise RuntimeError(f\"Resource {self.resource_name} not found in source\")\n    self.prepared = True\n</code></pre>"},{"location":"documentation/data/#datarec.data.resource.Resource.path","title":"<code>path()</code>","text":"<p>Returns the local path of the resource file. Raises an error if the output folder is not specified. Returns:     (str): The local path of the resource file.</p> Source code in <code>datarec/data/resource.py</code> <pre><code>def path(self):\n    \"\"\"\n    Returns the local path of the resource file.\n    Raises an error if the output folder is not specified.\n    Returns:\n        (str): The local path of the resource file.\n    \"\"\"\n    output_folder = self.source.output_folder\n    if output_folder is None:\n        raise ValueError(\"Must specify an output folder\")\n    inner_path = self.source.inner_paths.get(self.resource_name, None)\n    if inner_path is None:\n        raise RuntimeError(f\"Resource {self.resource_name} not found in source inner paths\")\n    return os.path.join(output_folder, inner_path)\n</code></pre>"},{"location":"documentation/data/#datarec.data.resource.Resource.assign_dataset_info","title":"<code>assign_dataset_info(dataset_name, version)</code>","text":"<p>Assigns dataset name and version to the resource. Args:     dataset_name (str): The name of the dataset.     version (str): The version of the dataset. Returns:     (None): None</p> Source code in <code>datarec/data/resource.py</code> <pre><code>def assign_dataset_info(self, dataset_name:str, version:str):\n    \"\"\"\n    Assigns dataset name and version to the resource.\n    Args:\n        dataset_name (str): The name of the dataset.\n        version (str): The version of the dataset.\n    Returns:\n        (None): None\n    \"\"\"\n    self.dataset_name = dataset_name\n    self.version = version\n</code></pre>"},{"location":"documentation/data/#datarec.data.resource.Resource.free_cache","title":"<code>free_cache()</code>","text":"<p>Frees the cached version of the resource if it exists. Returns:     (None): None</p> Source code in <code>datarec/data/resource.py</code> <pre><code>def free_cache(self):\n    \"\"\"\n    Frees the cached version of the resource if it exists.\n    Returns:\n        (None): None\n    \"\"\"\n    pass\n</code></pre>"},{"location":"documentation/data/#datarec.data.resource.Interactions","title":"<code>Interactions</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Resource</code></p> Source code in <code>datarec/data/resource.py</code> <pre><code>@dataclass\nclass Interactions(Resource):\n    schema: Optional[dict] = None\n    _cache_ready: Optional[bool] = False\n\n    def cache_path(self) -&gt; str:\n        \"\"\"\n        Returns the path of the cached version of the resource.\n        Returns:\n            (str): The path of the cached resource file.\n        \"\"\"\n        if self.dataset_name is None or self.version is None:\n            raise ValueError(\"Dataset name and version must be set to get cache path\")\n        return pickle_version_filepath(self.dataset_name, self.version)\n\n    def _has_cache(self) -&gt; bool:\n        \"\"\"\n        Checks if a cached version of the resource exists.\n        Returns:\n            (bool): True if the cached file exists, otherwise False.\n        \"\"\"\n        if self._cache_ready:\n            return True\n\n        if self.dataset_name is None or self.version is None:\n            raise ValueError(\"Dataset name and version must be set to check for cache\")\n        self._cache_ready = os.path.exists(self.cache_path())\n        return self._cache_ready\n\n    def prepare(self, use_cache=True, *args, **kwargs):\n        \"\"\"\n        Prepares the resource by ensuring it is downloaded and available locally.\n        If a cached version exists, it skips downloading.\n        \"\"\"\n        if use_cache is True and self._has_cache():\n            self.prepared = True\n            print(f\"Resource '{self.resource_name}' found in cache. Skipping download.\")\n            return\n        super().prepare(*args, **kwargs)\n\n    def load(self, use_cache=True, to_cache=True) -&gt; DataRec:\n        \"\"\"\n        Loads the ratings resource into a DataRec dataset.\n        Returns:\n            DataRec: The loaded dataset.\n        \"\"\"\n        if use_cache and self._has_cache():\n            print(f\"Loading resource '{self.resource_name}' from cache at {self.cache_path()}.\")\n            return from_pickle(filepath=self.cache_path())\n\n        if self.format == 'transactions_tabular':\n            schema = self.schema\n            if schema is None:\n                raise ValueError(\"Schema must be provided for transactions_tabular format\")\n\n            dataset = read_transactions_tabular(self.path(), \n                                                sep=schema['sep'],\n                                                user_col=schema['user_col'],\n                                                item_col=schema['item_col'],\n                                                rating_col=schema.get('rating_col', None),\n                                                timestamp_col=schema.get('timestamp_col', None),\n                                                header=schema.get('header', None),\n                                                skiprows=schema.get('skiprows', 0),\n                                                cols=schema.get('cols', None),\n                                                engine=schema.get('engine', 'c'),\n                                                fallback_engine=schema.get('fallback_engine', 'python'),\n                                                stream=schema.get('stream', False),\n                                                encode_ids=schema.get('encode_ids', False),\n                                                chunksize=schema.get('chunksize', 100_000))\n\n        elif self.format == 'transactions_json':\n            schema = self.schema\n            if schema is None:\n                raise ValueError(\"Schema must be provided for transactions json format\")\n\n            dataset = read_transactions_json(self.path(),\n                                             user_col=schema['user_col'],\n                                             item_col=schema['item_col'],\n                                             rating_col=schema.get('rating_col', None),\n                                             timestamp_col=schema.get('timestamp_col', None),\n                                             stream=schema.get('stream', False),\n                                             encode_ids=schema.get('encode_ids', False),\n                                             chunksize=schema.get('chunksize', 100_000))\n\n        elif self.format == 'transactions_jsonl':\n            schema = self.schema\n            if schema is None:\n                raise ValueError(\"Schema must be provided for transactions jsonl format\")\n\n            dataset = read_transactions_jsonl(self.path(),\n                                              user_col=schema['user_col'],\n                                              item_col=schema['item_col'],\n                                              rating_col=schema.get('rating_col', None),\n                                              timestamp_col=schema.get('timestamp_col', None),\n                                              stream=schema.get('stream', False),\n                                              encode_ids=schema.get('encode_ids', False),\n                                              chunksize=schema.get('chunksize', 100_000))\n\n        elif self.format == 'sequence_tabular_inline':\n            schema = self.schema\n            if schema is None:\n                raise ValueError(\"Schema must be provided for sequence tabular inline format\")\n\n            sequence_col = schema.get('sequence_col', None)\n            if sequence_col is None:\n                raise ValueError(\"sequence_col must be provided in schema for sequence tabular inline format\")\n\n            dataset = read_sequence_tabular_inline(\n                self.path(),\n                user_col=schema['user_col'],\n                sequence_col=sequence_col,\n                sequence_sep=schema.get('sequence_sep', ' '),\n                timestamp_col=schema.get('timestamp_col', None),\n                meta_cols=schema.get('meta_cols', None),\n                col_sep=schema.get('col_sep', ','),\n                header=schema.get('header', 0),\n                cols=schema.get('cols', None),\n                engine=schema.get('engine', 'c'),\n                fallback_engine=schema.get('fallback_engine', 'python'),\n                stream=schema.get('stream', schema.get('stream_encode', False)),\n                encode_ids=schema.get('encode_ids', False),\n                chunksize=schema.get('chunksize', 100_000),\n            )\n\n        elif self.format == 'sequence_tabular_wide':\n            schema = self.schema\n            if schema is None:\n                raise ValueError(\"Schema must be provided for sequence tabular wide format\")\n\n            dataset = read_sequence_tabular_wide(self.path(),\n                                                 user_col=schema.get('user_col', 'user'),\n                                                 item_col=schema.get('item_col', 'item'),\n                                                 col_sep=schema.get('col_sep', ' '),\n                                                 header=schema.get('header', None),\n                                                 encode_ids=schema.get('encode_ids', False))\n\n        elif self.format == 'sequence_tabular_implicit':\n            schema = self.schema\n            if schema is None:\n                raise ValueError(\"Schema must be provided for sequence tabular implicit format\")\n\n            dataset = read_sequence_tabular_implicit(self.path(),\n                                                     user_col=schema.get('user_col', 'sequence_id'),\n                                                     item_col=schema.get('item_col', 'item'),\n                                                     col_sep=schema.get('col_sep', ' '),\n                                                     header=schema.get('header', None),\n                                                     drop_length_col=schema.get('drop_length_col', True),\n                                                     encode_ids=schema.get('encode_ids', False))\n\n        elif self.format == 'sequence_json':\n            schema = self.schema\n            if schema is None:\n                raise ValueError(\"Schema must be provided for sequence json format\")\n\n            dataset = read_sequences_json(self.path(),\n                                          user_col=schema['user_col'],\n                                          item_col=schema['item_col'],\n                                          rating_col=schema.get('rating_col', None),\n                                          timestamp_col=schema.get('timestamp_col', None))\n\n        elif self.format == 'sequence_json_array':\n            schema = self.schema\n            if schema is None:\n                raise ValueError(\"Schema must be provided for sequence json array format\")\n\n            dataset = read_sequences_json_array(self.path(),\n                                                user_col=schema['user_col'],\n                                                item_col=schema['item_col'],\n                                                rating_col=schema.get('rating_col', None),\n                                                timestamp_col=schema.get('timestamp_col', None),\n                                                sequence_key=schema.get('sequence_key', 'sequence'))\n\n        else:\n            raise NotImplementedError(f\"Format {self.format} not supported for resource loading.\")\n\n        if self.dataset_name is None:\n            print(\"Warning: dataset_name is not set for the resource. Using 'unknown_dataset'.\")\n            self.dataset_name = \"unknown_dataset\"\n        if self.version is None:\n            print(\"Warning: version is not set for the resource. Using 'unknown_version'.\")\n            self.version = \"unknown_version\"\n\n        dr = DataRec(rawdata=dataset, dataset_name=self.dataset_name, version_name=self.version, registry_dataset=True)\n\n        # cache the dataset in pickle format\n        if to_cache:\n            dr.to_pickle()\n\n        return dr\n\n    def free_cache(self):\n        \"\"\"\n        Frees the cached version of the resource if it exists.\n        Returns:\n            (None): None\n        \"\"\"\n        if self._has_cache():\n            os.remove(self.cache_path())\n            self._cache_ready = False\n            print(f\"Cache for resource '{self.resource_name}' has been removed.\")\n</code></pre>"},{"location":"documentation/data/#datarec.data.resource.Interactions.cache_path","title":"<code>cache_path()</code>","text":"<p>Returns the path of the cached version of the resource. Returns:     (str): The path of the cached resource file.</p> Source code in <code>datarec/data/resource.py</code> <pre><code>def cache_path(self) -&gt; str:\n    \"\"\"\n    Returns the path of the cached version of the resource.\n    Returns:\n        (str): The path of the cached resource file.\n    \"\"\"\n    if self.dataset_name is None or self.version is None:\n        raise ValueError(\"Dataset name and version must be set to get cache path\")\n    return pickle_version_filepath(self.dataset_name, self.version)\n</code></pre>"},{"location":"documentation/data/#datarec.data.resource.Interactions.prepare","title":"<code>prepare(use_cache=True, *args, **kwargs)</code>","text":"<p>Prepares the resource by ensuring it is downloaded and available locally. If a cached version exists, it skips downloading.</p> Source code in <code>datarec/data/resource.py</code> <pre><code>def prepare(self, use_cache=True, *args, **kwargs):\n    \"\"\"\n    Prepares the resource by ensuring it is downloaded and available locally.\n    If a cached version exists, it skips downloading.\n    \"\"\"\n    if use_cache is True and self._has_cache():\n        self.prepared = True\n        print(f\"Resource '{self.resource_name}' found in cache. Skipping download.\")\n        return\n    super().prepare(*args, **kwargs)\n</code></pre>"},{"location":"documentation/data/#datarec.data.resource.Interactions.load","title":"<code>load(use_cache=True, to_cache=True)</code>","text":"<p>Loads the ratings resource into a DataRec dataset. Returns:     DataRec: The loaded dataset.</p> Source code in <code>datarec/data/resource.py</code> <pre><code>def load(self, use_cache=True, to_cache=True) -&gt; DataRec:\n    \"\"\"\n    Loads the ratings resource into a DataRec dataset.\n    Returns:\n        DataRec: The loaded dataset.\n    \"\"\"\n    if use_cache and self._has_cache():\n        print(f\"Loading resource '{self.resource_name}' from cache at {self.cache_path()}.\")\n        return from_pickle(filepath=self.cache_path())\n\n    if self.format == 'transactions_tabular':\n        schema = self.schema\n        if schema is None:\n            raise ValueError(\"Schema must be provided for transactions_tabular format\")\n\n        dataset = read_transactions_tabular(self.path(), \n                                            sep=schema['sep'],\n                                            user_col=schema['user_col'],\n                                            item_col=schema['item_col'],\n                                            rating_col=schema.get('rating_col', None),\n                                            timestamp_col=schema.get('timestamp_col', None),\n                                            header=schema.get('header', None),\n                                            skiprows=schema.get('skiprows', 0),\n                                            cols=schema.get('cols', None),\n                                            engine=schema.get('engine', 'c'),\n                                            fallback_engine=schema.get('fallback_engine', 'python'),\n                                            stream=schema.get('stream', False),\n                                            encode_ids=schema.get('encode_ids', False),\n                                            chunksize=schema.get('chunksize', 100_000))\n\n    elif self.format == 'transactions_json':\n        schema = self.schema\n        if schema is None:\n            raise ValueError(\"Schema must be provided for transactions json format\")\n\n        dataset = read_transactions_json(self.path(),\n                                         user_col=schema['user_col'],\n                                         item_col=schema['item_col'],\n                                         rating_col=schema.get('rating_col', None),\n                                         timestamp_col=schema.get('timestamp_col', None),\n                                         stream=schema.get('stream', False),\n                                         encode_ids=schema.get('encode_ids', False),\n                                         chunksize=schema.get('chunksize', 100_000))\n\n    elif self.format == 'transactions_jsonl':\n        schema = self.schema\n        if schema is None:\n            raise ValueError(\"Schema must be provided for transactions jsonl format\")\n\n        dataset = read_transactions_jsonl(self.path(),\n                                          user_col=schema['user_col'],\n                                          item_col=schema['item_col'],\n                                          rating_col=schema.get('rating_col', None),\n                                          timestamp_col=schema.get('timestamp_col', None),\n                                          stream=schema.get('stream', False),\n                                          encode_ids=schema.get('encode_ids', False),\n                                          chunksize=schema.get('chunksize', 100_000))\n\n    elif self.format == 'sequence_tabular_inline':\n        schema = self.schema\n        if schema is None:\n            raise ValueError(\"Schema must be provided for sequence tabular inline format\")\n\n        sequence_col = schema.get('sequence_col', None)\n        if sequence_col is None:\n            raise ValueError(\"sequence_col must be provided in schema for sequence tabular inline format\")\n\n        dataset = read_sequence_tabular_inline(\n            self.path(),\n            user_col=schema['user_col'],\n            sequence_col=sequence_col,\n            sequence_sep=schema.get('sequence_sep', ' '),\n            timestamp_col=schema.get('timestamp_col', None),\n            meta_cols=schema.get('meta_cols', None),\n            col_sep=schema.get('col_sep', ','),\n            header=schema.get('header', 0),\n            cols=schema.get('cols', None),\n            engine=schema.get('engine', 'c'),\n            fallback_engine=schema.get('fallback_engine', 'python'),\n            stream=schema.get('stream', schema.get('stream_encode', False)),\n            encode_ids=schema.get('encode_ids', False),\n            chunksize=schema.get('chunksize', 100_000),\n        )\n\n    elif self.format == 'sequence_tabular_wide':\n        schema = self.schema\n        if schema is None:\n            raise ValueError(\"Schema must be provided for sequence tabular wide format\")\n\n        dataset = read_sequence_tabular_wide(self.path(),\n                                             user_col=schema.get('user_col', 'user'),\n                                             item_col=schema.get('item_col', 'item'),\n                                             col_sep=schema.get('col_sep', ' '),\n                                             header=schema.get('header', None),\n                                             encode_ids=schema.get('encode_ids', False))\n\n    elif self.format == 'sequence_tabular_implicit':\n        schema = self.schema\n        if schema is None:\n            raise ValueError(\"Schema must be provided for sequence tabular implicit format\")\n\n        dataset = read_sequence_tabular_implicit(self.path(),\n                                                 user_col=schema.get('user_col', 'sequence_id'),\n                                                 item_col=schema.get('item_col', 'item'),\n                                                 col_sep=schema.get('col_sep', ' '),\n                                                 header=schema.get('header', None),\n                                                 drop_length_col=schema.get('drop_length_col', True),\n                                                 encode_ids=schema.get('encode_ids', False))\n\n    elif self.format == 'sequence_json':\n        schema = self.schema\n        if schema is None:\n            raise ValueError(\"Schema must be provided for sequence json format\")\n\n        dataset = read_sequences_json(self.path(),\n                                      user_col=schema['user_col'],\n                                      item_col=schema['item_col'],\n                                      rating_col=schema.get('rating_col', None),\n                                      timestamp_col=schema.get('timestamp_col', None))\n\n    elif self.format == 'sequence_json_array':\n        schema = self.schema\n        if schema is None:\n            raise ValueError(\"Schema must be provided for sequence json array format\")\n\n        dataset = read_sequences_json_array(self.path(),\n                                            user_col=schema['user_col'],\n                                            item_col=schema['item_col'],\n                                            rating_col=schema.get('rating_col', None),\n                                            timestamp_col=schema.get('timestamp_col', None),\n                                            sequence_key=schema.get('sequence_key', 'sequence'))\n\n    else:\n        raise NotImplementedError(f\"Format {self.format} not supported for resource loading.\")\n\n    if self.dataset_name is None:\n        print(\"Warning: dataset_name is not set for the resource. Using 'unknown_dataset'.\")\n        self.dataset_name = \"unknown_dataset\"\n    if self.version is None:\n        print(\"Warning: version is not set for the resource. Using 'unknown_version'.\")\n        self.version = \"unknown_version\"\n\n    dr = DataRec(rawdata=dataset, dataset_name=self.dataset_name, version_name=self.version, registry_dataset=True)\n\n    # cache the dataset in pickle format\n    if to_cache:\n        dr.to_pickle()\n\n    return dr\n</code></pre>"},{"location":"documentation/data/#datarec.data.resource.Interactions.free_cache","title":"<code>free_cache()</code>","text":"<p>Frees the cached version of the resource if it exists. Returns:     (None): None</p> Source code in <code>datarec/data/resource.py</code> <pre><code>def free_cache(self):\n    \"\"\"\n    Frees the cached version of the resource if it exists.\n    Returns:\n        (None): None\n    \"\"\"\n    if self._has_cache():\n        os.remove(self.cache_path())\n        self._cache_ready = False\n        print(f\"Cache for resource '{self.resource_name}' has been removed.\")\n</code></pre>"},{"location":"documentation/data/#datarec.data.resource.load_dataset_config","title":"<code>load_dataset_config(dataset_name, dataset_version='')</code>","text":"<p>Given the dataset name returns the path of the dataset configuration file in the dataset registry Args:     dataset_name (str): name of the dataset     dataset_version (str): version of the dataset Returns:     (dict): dataset configuration</p> Source code in <code>datarec/data/resource.py</code> <pre><code>def load_dataset_config(dataset_name:str, dataset_version:str='')-&gt;dict:\n    \"\"\"\n    Given the dataset name returns the path of the dataset configuration file in the dataset registry\n    Args:\n        dataset_name (str): name of the dataset\n        dataset_version (str): version of the dataset\n    Returns:\n        (dict): dataset configuration\n    \"\"\"\n    if dataset_version == '':\n        config_path = registry_dataset_filepath(dataset_name)\n        assert os.path.exists(config_path), f\"Config file {config_path} does not exist\"\n        with open(config_path, \"r\") as f:\n            config = yaml.safe_load(f)\n        return config\n    else:\n        config_path = registry_version_filepath(dataset_name, dataset_version)\n        with open(config_path, \"r\") as f:\n            config = yaml.safe_load(f)\n        assert dataset_version == config['version'], \\\n            \"Dataset version must be the same as the one in the config file.\"\n        return config\n</code></pre>"},{"location":"documentation/data/#datarec.data.resource.set_resource","title":"<code>set_resource(resource_name, resource_conf)</code>","text":"<p>Given a resource configuration, return a new resource object Args:     resource_name (str): name of the resource     resource_conf (dict): resource configuration Returns:     (Resource): a dataset resource object</p> Source code in <code>datarec/data/resource.py</code> <pre><code>def set_resource(resource_name: str, resource_conf: dict) -&gt; Resource:\n    \"\"\"\n    Given a resource configuration, return a new resource object\n    Args:\n        resource_name (str): name of the resource\n        resource_conf (dict): resource configuration\n    Returns:\n        (Resource): a dataset resource object\"\"\"\n    resource_typename = resource_conf.get('type', 'Resource')\n    resource_type = RESOURCE_TYPES.get(resource_typename.lower(), None)\n    if resource_type is None:\n        raise ValueError(f'Resource type {resource_typename.lower()} not allowed. Available resource types: {RESOURCE_TYPES.keys()}.')\n    resource = resource_type(**resource_conf)\n    resource.resource_name = resource_name\n    return resource\n</code></pre>"},{"location":"documentation/data/#datarec.data.resource.set_resources","title":"<code>set_resources(config)</code>","text":"<p>Given a dataset configuration, return a new dataset configuration Args:     config (dict): dataset configuration Returns:     (dict): a dictionary containing dataset sources objects</p> Source code in <code>datarec/data/resource.py</code> <pre><code>def set_resources(config:dict) -&gt; dict[str, Resource]:\n    \"\"\"\n    Given a dataset configuration, return a new dataset configuration\n    Args:\n        config (dict): dataset configuration\n    Returns:\n        (dict): a dictionary containing dataset sources objects\n    \"\"\"\n    resources = dict()\n    for resource_name, raw_resource in config['resources'].items():\n        resources[resource_name] = set_resource(resource_name, raw_resource)\n    return resources\n</code></pre>"},{"location":"documentation/data/#datarec.data.resource.load_class","title":"<code>load_class(class_import)</code>","text":"<p>Given a class import name, return a class object</p> Source code in <code>datarec/data/resource.py</code> <pre><code>def load_class(class_import: str):\n    \"\"\"\n    Given a class import name, return a class object\n    \"\"\"\n    module_name, class_name = class_import.rsplit(\".\", 1)\n    module = importlib.import_module(module_name)\n    return getattr(module, class_name)\n</code></pre>"},{"location":"documentation/data/#datarec.data.resource.load_versions","title":"<code>load_versions(dataset_name)</code>","text":"<p>Given a dataset name, return a dictionary containing dataset versions and relative classes</p> Source code in <code>datarec/data/resource.py</code> <pre><code>def load_versions(dataset_name:str)-&gt;dict:\n    \"\"\"\n    Given a dataset name, return a dictionary containing dataset versions and relative classes\n    \"\"\"\n    conf = load_dataset_config(dataset_name)\n    return {n: load_class(m) for n, m in conf['versions'].items()}\n</code></pre>"},{"location":"documentation/data/#datarec.data.resource.find_ratings_resource","title":"<code>find_ratings_resource(resources)</code>","text":"<p>Given a dictionary of resources, return the ratings resource Args:     resources (dict): a dictionary containing dataset resources objects Returns:     (Resource): the ratings resource object</p> Source code in <code>datarec/data/resource.py</code> <pre><code>def find_ratings_resource(resources:dict)-&gt;Interactions:\n    \"\"\"\n    Given a dictionary of resources, return the ratings resource\n    Args:\n        resources (dict): a dictionary containing dataset resources objects\n    Returns:\n        (Resource): the ratings resource object\n    \"\"\"\n    for res in resources.values():\n        if res.type == 'ratings':\n            return res\n    raise RuntimeError(f\"Resource type 'ratings' not found in resources\")\n</code></pre>"},{"location":"documentation/data/#datarec.data.resource.find_resource_by_type","title":"<code>find_resource_by_type(resources, rtype)</code>","text":"<p>Given a dictionary of resources, return the ratings resource Args:     resources (dict): a dictionary containing dataset resources objects     rtype (str): resource type to find Returns:     (Resource): the ratings resource object</p> Source code in <code>datarec/data/resource.py</code> <pre><code>def find_resource_by_type(resources:dict, rtype:str)-&gt;Dict[str, Resource]:\n    \"\"\"\n    Given a dictionary of resources, return the ratings resource\n    Args:\n        resources (dict): a dictionary containing dataset resources objects\n        rtype (str): resource type to find\n    Returns:\n        (Resource): the ratings resource object\n    \"\"\"\n    found = {}\n    for res in resources.values():\n        if res.type == rtype:\n            found[res.resource_name] = res\n    if len(found) == 0:\n        raise RuntimeError(f\"Resource type '{rtype}' not found in resources\")\n    return found\n</code></pre>"},{"location":"documentation/data/#torch-dataset-wrappers","title":"Torch Dataset Wrappers","text":"<p>PyTorch-compatible dataset wrappers.</p>"},{"location":"documentation/data/#datarec.data.torch_dataset.BaseTorchDataset","title":"<code>BaseTorchDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Base class for Torch datasets wrapping a DataRec dataset.</p> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>class BaseTorchDataset(Dataset):\n    \"\"\"\n    Base class for Torch datasets wrapping a DataRec dataset.\n    \"\"\"\n    def __init__(self, datarec, copy_data=False):\n        \"\"\"\n        Initializes the BaseTorchDataset object.    \n\n        Args:\n            datarec (DataRec): An instance of a DataRec dataset.\n            copy_data (bool): Whether to copy the dataset or use it by reference.\n        \"\"\"\n        self.df = datarec.data.copy() if copy_data else datarec.data\n        self.user_col = datarec.user_col\n        self.item_col = datarec.item_col\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.BaseTorchDataset.__init__","title":"<code>__init__(datarec, copy_data=False)</code>","text":"<p>Initializes the BaseTorchDataset object.    </p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>An instance of a DataRec dataset.</p> required <code>copy_data</code> <code>bool</code> <p>Whether to copy the dataset or use it by reference.</p> <code>False</code> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>def __init__(self, datarec, copy_data=False):\n    \"\"\"\n    Initializes the BaseTorchDataset object.    \n\n    Args:\n        datarec (DataRec): An instance of a DataRec dataset.\n        copy_data (bool): Whether to copy the dataset or use it by reference.\n    \"\"\"\n    self.df = datarec.data.copy() if copy_data else datarec.data\n    self.user_col = datarec.user_col\n    self.item_col = datarec.item_col\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.PointwiseTorchDataset","title":"<code>PointwiseTorchDataset</code>","text":"<p>               Bases: <code>BaseTorchDataset</code></p> <p>Torch dataset for pointwise recommendation tasks.</p> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>class PointwiseTorchDataset(BaseTorchDataset):\n    \"\"\"\n    Torch dataset for pointwise recommendation tasks.\n    \"\"\"\n    def __init__(self, datarec, copy_data=False):\n        \"\"\"\n        Initializes the PointwiseTorchDataset object.\n\n        Args:\n            datarec (DataRec): An instance of a DataRec dataset.\n            copy_data (bool): Whether to copy the dataset or use it by reference.\n        \"\"\"\n        super().__init__(datarec, copy_data)\n        self.rating_col = datarec.rating_col\n\n    def __len__(self):\n        \"\"\"\n        Returns the total number of samples in the dataset.\n\n        This is required by PyTorch's DataLoader to iterate over the dataset.\n\n        Returns:\n            (int): Number of samples in the dataset.\n        \"\"\"\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Returns a sample with user, item, and rating.\n\n        Args:\n            idx (int): Sample index to be returned.\n\n        Returns:\n            (dict): Sample with user, item, and rating.\n        \"\"\"\n        row = self.df.iloc[idx]\n        return {\n            \"user\": row[self.user_col],\n            \"item\": row[self.item_col],\n            \"rating\": row.get(self.rating_col, 1.0)\n        }\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.PointwiseTorchDataset.__init__","title":"<code>__init__(datarec, copy_data=False)</code>","text":"<p>Initializes the PointwiseTorchDataset object.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>An instance of a DataRec dataset.</p> required <code>copy_data</code> <code>bool</code> <p>Whether to copy the dataset or use it by reference.</p> <code>False</code> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>def __init__(self, datarec, copy_data=False):\n    \"\"\"\n    Initializes the PointwiseTorchDataset object.\n\n    Args:\n        datarec (DataRec): An instance of a DataRec dataset.\n        copy_data (bool): Whether to copy the dataset or use it by reference.\n    \"\"\"\n    super().__init__(datarec, copy_data)\n    self.rating_col = datarec.rating_col\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.PointwiseTorchDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the total number of samples in the dataset.</p> <p>This is required by PyTorch's DataLoader to iterate over the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of samples in the dataset.</p> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Returns the total number of samples in the dataset.\n\n    This is required by PyTorch's DataLoader to iterate over the dataset.\n\n    Returns:\n        (int): Number of samples in the dataset.\n    \"\"\"\n    return len(self.df)\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.PointwiseTorchDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns a sample with user, item, and rating.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Sample index to be returned.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Sample with user, item, and rating.</p> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Returns a sample with user, item, and rating.\n\n    Args:\n        idx (int): Sample index to be returned.\n\n    Returns:\n        (dict): Sample with user, item, and rating.\n    \"\"\"\n    row = self.df.iloc[idx]\n    return {\n        \"user\": row[self.user_col],\n        \"item\": row[self.item_col],\n        \"rating\": row.get(self.rating_col, 1.0)\n    }\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.PairwiseTorchDataset","title":"<code>PairwiseTorchDataset</code>","text":"<p>               Bases: <code>BaseTorchDataset</code></p> <p>Torch dataset for pairwise recommendation tasks with negative sampling.</p> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>class PairwiseTorchDataset(BaseTorchDataset):\n    \"\"\"\n    Torch dataset for pairwise recommendation tasks with negative sampling.\n    \"\"\"\n    def __init__(self, datarec, num_negatives=1, item_pool=None, copy_data=False):\n        \"\"\" \n        Initializes the PairwiseTorchDataset object.\n\n        Args:\n            datarec (DataRec): An instance of a DataRec dataset.\n            num_negatives (int): Number of negative samples to generate per interaction.\n            item_pool (array-like): Pool of items to sample from. Defaults to all items in the dataset.\n            copy_data (bool): Whether to copy the dataset or use it by reference.\n        \"\"\"\n        super().__init__(datarec, copy_data)\n        self.num_negatives = num_negatives\n        self.item_pool = item_pool or self.df[self.item_col].unique()\n        self.user_pos_items = self.df.groupby(self.user_col)[self.item_col].apply(set).to_dict()\n\n    def sample_negatives(self, user: Any) -&gt; List[Any]:\n        \"\"\"\n        Samples negative items for a given user, avoiding known positive items.\n\n        This method is designed to be overridden to implement custom negative\n        sampling strategies (e.g., popularity-based, adversarial, or\n        distribution-aware sampling). The default implementation draws\n        uniformly from the item pool, excluding items the user has already interacted with.\n\n        Args:\n            user: The user ID for which to sample negatives.\n\n        Returns:\n            (List): List of sampled negative item IDs.\n        \"\"\"\n        neg_items = []\n        user_positives = self.user_pos_items.get(user, set())\n        while len(neg_items) &lt; self.num_negatives:\n            candidate = np.random.choice(self.item_pool)\n            if candidate not in user_positives:\n                neg_items.append(candidate)\n        return neg_items\n\n    def __len__(self):\n        \"\"\"\n        Returns the total number of samples in the dataset.\n\n        This is required by PyTorch's DataLoader to iterate over the dataset.\n\n        Returns:\n            (int): number of samples in the dataset.\n        \"\"\"\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Returns a sample with user, positive item, and negative items.\n\n        Args:\n            idx (int): Sample index to be returned.\n\n        Returns:\n            (dict): Sample with user, positive item, and negative items.\n        \"\"\"\n        row = self.df.iloc[idx]\n        user = row[self.user_col]\n        pos_item = row[self.item_col]\n        neg_items = self.sample_negatives(user)\n        return {\n            \"user\": user,\n            \"pos_item\": pos_item,\n            \"neg_items\": neg_items\n        }\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.PairwiseTorchDataset.__init__","title":"<code>__init__(datarec, num_negatives=1, item_pool=None, copy_data=False)</code>","text":"<p>Initializes the PairwiseTorchDataset object.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>An instance of a DataRec dataset.</p> required <code>num_negatives</code> <code>int</code> <p>Number of negative samples to generate per interaction.</p> <code>1</code> <code>item_pool</code> <code>array - like</code> <p>Pool of items to sample from. Defaults to all items in the dataset.</p> <code>None</code> <code>copy_data</code> <code>bool</code> <p>Whether to copy the dataset or use it by reference.</p> <code>False</code> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>def __init__(self, datarec, num_negatives=1, item_pool=None, copy_data=False):\n    \"\"\" \n    Initializes the PairwiseTorchDataset object.\n\n    Args:\n        datarec (DataRec): An instance of a DataRec dataset.\n        num_negatives (int): Number of negative samples to generate per interaction.\n        item_pool (array-like): Pool of items to sample from. Defaults to all items in the dataset.\n        copy_data (bool): Whether to copy the dataset or use it by reference.\n    \"\"\"\n    super().__init__(datarec, copy_data)\n    self.num_negatives = num_negatives\n    self.item_pool = item_pool or self.df[self.item_col].unique()\n    self.user_pos_items = self.df.groupby(self.user_col)[self.item_col].apply(set).to_dict()\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.PairwiseTorchDataset.sample_negatives","title":"<code>sample_negatives(user)</code>","text":"<p>Samples negative items for a given user, avoiding known positive items.</p> <p>This method is designed to be overridden to implement custom negative sampling strategies (e.g., popularity-based, adversarial, or distribution-aware sampling). The default implementation draws uniformly from the item pool, excluding items the user has already interacted with.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>Any</code> <p>The user ID for which to sample negatives.</p> required <p>Returns:</p> Type Description <code>List</code> <p>List of sampled negative item IDs.</p> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>def sample_negatives(self, user: Any) -&gt; List[Any]:\n    \"\"\"\n    Samples negative items for a given user, avoiding known positive items.\n\n    This method is designed to be overridden to implement custom negative\n    sampling strategies (e.g., popularity-based, adversarial, or\n    distribution-aware sampling). The default implementation draws\n    uniformly from the item pool, excluding items the user has already interacted with.\n\n    Args:\n        user: The user ID for which to sample negatives.\n\n    Returns:\n        (List): List of sampled negative item IDs.\n    \"\"\"\n    neg_items = []\n    user_positives = self.user_pos_items.get(user, set())\n    while len(neg_items) &lt; self.num_negatives:\n        candidate = np.random.choice(self.item_pool)\n        if candidate not in user_positives:\n            neg_items.append(candidate)\n    return neg_items\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.PairwiseTorchDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the total number of samples in the dataset.</p> <p>This is required by PyTorch's DataLoader to iterate over the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>number of samples in the dataset.</p> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Returns the total number of samples in the dataset.\n\n    This is required by PyTorch's DataLoader to iterate over the dataset.\n\n    Returns:\n        (int): number of samples in the dataset.\n    \"\"\"\n    return len(self.df)\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.PairwiseTorchDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns a sample with user, positive item, and negative items.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Sample index to be returned.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Sample with user, positive item, and negative items.</p> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Returns a sample with user, positive item, and negative items.\n\n    Args:\n        idx (int): Sample index to be returned.\n\n    Returns:\n        (dict): Sample with user, positive item, and negative items.\n    \"\"\"\n    row = self.df.iloc[idx]\n    user = row[self.user_col]\n    pos_item = row[self.item_col]\n    neg_items = self.sample_negatives(user)\n    return {\n        \"user\": user,\n        \"pos_item\": pos_item,\n        \"neg_items\": neg_items\n    }\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.RankingTorchDataset","title":"<code>RankingTorchDataset</code>","text":"<p>               Bases: <code>BaseTorchDataset</code></p> <p>Torch dataset for full softmax-style ranking tasks.</p> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>class RankingTorchDataset(BaseTorchDataset):\n    \"\"\"\n    Torch dataset for full softmax-style ranking tasks.\n    \"\"\"\n    def __init__(self, datarec, copy_data=False):\n        \"\"\"\n        Initializes the RankingTorchDataset object.\n\n        Args:\n            datarec (DataRec): An instance of a DataRec dataset.\n            copy_data (bool): Whether to copy the dataset or use it by reference.\n        \"\"\"\n        super().__init__(datarec, copy_data)\n        # Could prepare user-&gt;items mapping here for evaluation\n\n    def __len__(self):\n        \"\"\"\n        Returns the total number of samples in the dataset.\n\n        This is required by PyTorch's DataLoader to iterate over the dataset.\n\n        Returns:\n            (int): Number of samples in the dataset.\n        \"\"\"\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Returns a sample with user and item.\n\n        Args:\n            idx (int): Sample index to be returned.\n\n        Returns:\n            (dict): Sample with user and item data.\n        \"\"\"\n        row = self.df.iloc[idx]\n        return {\n            \"user\": row[self.user_col],\n            \"item\": row[self.item_col]\n            # No target \u2014 implicit ranking\n        }\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.RankingTorchDataset.__init__","title":"<code>__init__(datarec, copy_data=False)</code>","text":"<p>Initializes the RankingTorchDataset object.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>An instance of a DataRec dataset.</p> required <code>copy_data</code> <code>bool</code> <p>Whether to copy the dataset or use it by reference.</p> <code>False</code> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>def __init__(self, datarec, copy_data=False):\n    \"\"\"\n    Initializes the RankingTorchDataset object.\n\n    Args:\n        datarec (DataRec): An instance of a DataRec dataset.\n        copy_data (bool): Whether to copy the dataset or use it by reference.\n    \"\"\"\n    super().__init__(datarec, copy_data)\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.RankingTorchDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the total number of samples in the dataset.</p> <p>This is required by PyTorch's DataLoader to iterate over the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of samples in the dataset.</p> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Returns the total number of samples in the dataset.\n\n    This is required by PyTorch's DataLoader to iterate over the dataset.\n\n    Returns:\n        (int): Number of samples in the dataset.\n    \"\"\"\n    return len(self.df)\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.RankingTorchDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns a sample with user and item.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Sample index to be returned.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Sample with user and item data.</p> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Returns a sample with user and item.\n\n    Args:\n        idx (int): Sample index to be returned.\n\n    Returns:\n        (dict): Sample with user and item data.\n    \"\"\"\n    row = self.df.iloc[idx]\n    return {\n        \"user\": row[self.user_col],\n        \"item\": row[self.item_col]\n        # No target \u2014 implicit ranking\n    }\n</code></pre>"},{"location":"documentation/datasets/","title":"Datasets Reference","text":"<p>This section provides a detailed API reference for all modules related to built\u2011in datasets in the <code>datarec</code> library. Dataset entry points live in <code>datarec.datasets</code> and connect to registry metadata and versions.</p>"},{"location":"documentation/datasets/#on-this-page","title":"On This Page","text":"<ul> <li>Dataset Entry Points</li> <li>Registry Utilities</li> <li>Minimal usage</li> </ul>"},{"location":"documentation/datasets/#minimal-usage","title":"Minimal usage","text":"<pre><code>from datarec.datasets import Movielens\n\ndata = Movielens(version=\"1m\").prepare_and_load()\n</code></pre>"},{"location":"documentation/datasets/#dataset-entry-points","title":"Dataset Entry Points","text":""},{"location":"documentation/datasets/#datarec.datasets.list_datasets","title":"<code>list_datasets()</code>","text":"<p>Return the built-in dataset names registered in DataRec.</p> Source code in <code>datarec/datasets/__init__.py</code> <pre><code>def list_datasets() -&gt; list[str]:\n    \"\"\"Return the built-in dataset names registered in DataRec.\"\"\"\n    return sorted(available_datasets())\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.list_dataset_versions","title":"<code>list_dataset_versions(name)</code>","text":"<p>Return all available versions for a registered dataset.</p> Source code in <code>datarec/datasets/__init__.py</code> <pre><code>def list_dataset_versions(name: str) -&gt; list[str]:\n    \"\"\"Return all available versions for a registered dataset.\"\"\"\n    conf = load_dataset_config(name)\n    return list(conf.get(\"versions\", []))\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.latest_dataset_version","title":"<code>latest_dataset_version(name)</code>","text":"<p>Return the latest version for a registered dataset.</p> Source code in <code>datarec/datasets/__init__.py</code> <pre><code>def latest_dataset_version(name: str) -&gt; str:\n    \"\"\"Return the latest version for a registered dataset.\"\"\"\n    return load_dataset_config(name)[\"latest_version\"]\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.load_dataset","title":"<code>load_dataset(name, version='latest', **kwargs)</code>","text":"<p>Instantiate a dataset by registry name, validating available versions.</p> Source code in <code>datarec/datasets/__init__.py</code> <pre><code>def load_dataset(name: str, version: str = \"latest\", **kwargs) -&gt; Dataset:\n    \"\"\"\n    Instantiate a dataset by registry name, validating available versions.\n    \"\"\"\n    if name not in available_datasets():\n        raise ValueError(f\"Dataset '{name}' is not registered. Available: {', '.join(list_datasets())}\")\n\n    if version == \"latest\":\n        version = latest_dataset_version(name)\n\n    if version not in list_dataset_versions(name):\n        raise ValueError(\n            f\"Unsupported version '{version}' for dataset '{name}'. \"\n            f\"Supported versions: {', '.join(list_dataset_versions(name))}\"\n        )\n\n    for cls in DatasetEntryPoint.__subclasses__():\n        if cls.dataset_name == name:\n            return cls(version=version, **kwargs)\n\n    raise ValueError(f\"No entrypoint registered for dataset '{name}'.\")\n</code></pre>"},{"location":"documentation/datasets/#registry-utilities","title":"Registry Utilities","text":""},{"location":"documentation/datasets/#datarec.registry.utils.available_datasets","title":"<code>available_datasets()</code>","text":"<p>Return a list of available built-in datasets Returns:     List[str]: list of built-in datasets</p> Source code in <code>datarec/registry/utils.py</code> <pre><code>def available_datasets()-&gt;List[str]:\n    \"\"\"\n    Return a list of available built-in datasets\n    Returns:\n        List[str]: list of built-in datasets\n    \"\"\"\n    return sorted([d.replace('.yml', '') for d in os.listdir(REGISTRY_DATASETS_FOLDER)])\n</code></pre>"},{"location":"documentation/datasets/#datarec.registry.utils.print_available_datasets","title":"<code>print_available_datasets()</code>","text":"<p>Prints the list of available built-in datasets Returns:     None</p> Source code in <code>datarec/registry/utils.py</code> <pre><code>def print_available_datasets()-&gt;None:\n    \"\"\"\n    Prints the list of available built-in datasets\n    Returns:\n        None\n    \"\"\"\n    print(\"\"\"\nDataRec built-in datasets:\n- \"\"\"+'\\n - '.join(available_datasets()))\n</code></pre>"},{"location":"documentation/datasets/#datarec.registry.utils.compute_dataset_characteristics","title":"<code>compute_dataset_characteristics(dataset_name, version, *, output_dir=REGISTRY_METRICS_FOLDER, use_cache=True, overwrite=False)</code>","text":"<p>Compute and persist characteristics for a specific dataset/version.</p> <p>Returns:</p> Type Description <code>str</code> <p>Path to the written YAML file.</p> Source code in <code>datarec/registry/utils.py</code> <pre><code>def compute_dataset_characteristics(dataset_name: str,\n                                    version: str,\n                                    *,\n                                    output_dir: str = REGISTRY_METRICS_FOLDER,\n                                    use_cache: bool = True,\n                                    overwrite: bool = False) -&gt; str:\n    \"\"\"\n    Compute and persist characteristics for a specific dataset/version.\n\n    Returns:\n        Path to the written YAML file.\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    out_path = registry_metrics_filepath(dataset_name, version)\n\n    if os.path.exists(out_path) and not overwrite:\n        print(f\"Skip {dataset_name} {version}: file exists ({out_path})\")\n        return out_path\n\n    dset = Dataset(dataset_name=dataset_name, version=version)\n    dset.prepare(use_cache=use_cache)\n    dr = dset.load(use_cache=use_cache, to_cache=use_cache, only_required=True)\n\n    characteristics = {}\n    for name, func in CHARACTERISTICS.items():\n        try:\n            value = func(dr)\n            # Make YAML-friendly: unwrap numpy scalars if possible\n            if hasattr(value, \"item\"):\n                try:\n                    value = value.item()\n                except Exception:\n                    pass\n            characteristics[name] = value\n        except Exception as exc:\n            characteristics[name] = None\n            characteristics[f\"{name}_error\"] = str(exc)\n\n    payload = {\n        \"dataset\": dataset_name,\n        \"version\": version,\n        \"computed_at\": datetime.now(timezone.utc).isoformat(),\n        \"characteristics\": characteristics,\n    }\n\n    with open(out_path, \"w\") as f:\n        yaml.safe_dump(payload, f, sort_keys=False)\n\n    print(f\"Wrote {out_path}\")\n    return out_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.registry.utils.get_metrics_filepath","title":"<code>get_metrics_filepath(dataset_name, version)</code>","text":"<p>Return the expected registry metrics filepath for a dataset/version.</p> Source code in <code>datarec/registry/utils.py</code> <pre><code>def get_metrics_filepath(dataset_name: str, version: str) -&gt; str:\n    \"\"\"\n    Return the expected registry metrics filepath for a dataset/version.\n    \"\"\"\n    return registry_metrics_filepath(dataset_name, version)\n</code></pre>"},{"location":"documentation/datasets/#datarec.registry.utils.compute_all_characteristics","title":"<code>compute_all_characteristics(output_dir=REGISTRY_METRICS_FOLDER, use_cache=True, overwrite=False)</code>","text":"<p>Compute characteristics for every dataset/version and write YAML files.</p> Source code in <code>datarec/registry/utils.py</code> <pre><code>def compute_all_characteristics(output_dir: str = REGISTRY_METRICS_FOLDER,\n                                use_cache: bool = True,\n                                overwrite: bool = False) -&gt; None:\n    \"\"\"\n    Compute characteristics for every dataset/version and write YAML files.\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n\n    for ds_name in available_datasets():\n        conf = load_dataset_config(ds_name)\n        versions = conf.get(\"versions\", [])\n        if isinstance(versions, dict):  # support both list and dict configs\n            versions = list(versions.keys())\n\n        for version in versions:\n            try:\n                compute_dataset_characteristics(\n                    dataset_name=ds_name,\n                    version=version,\n                    output_dir=output_dir,\n                    use_cache=use_cache,\n                    overwrite=overwrite,\n                )\n            except Exception as exc:  # noqa: BLE001\n                print(f\"Failed {ds_name} {version}: {exc}\")\n</code></pre>"},{"location":"documentation/io/","title":"Input/Output Reference","text":"<p>This section provides a detailed API reference for all modules related to data input/output and framework interoperability in the <code>datarec</code> library. Readers return <code>RawData</code>, writers accept <code>RawData</code> or <code>DataRec</code>, and framework exporters convert datasets to external formats.</p>"},{"location":"documentation/io/#on-this-page","title":"On This Page","text":"<ul> <li>Core I/O Modules</li> <li>Framework Interoperability</li> </ul> <p>Minimal usage:</p> <pre><code>from datarec.io.readers.transactions.tabular import read_transactions_tabular\nfrom datarec.io.writers.transactions.tabular import write_transactions_tabular\n\nraw = read_transactions_tabular(\n    \"data/interactions.csv\",\n    sep=\",\",\n    user_col=\"user\",\n    item_col=\"item\",\n    rating_col=\"rating\",\n)\nwrite_transactions_tabular(raw, \"out/interactions.tsv\", sep=\"\\t\")\n</code></pre>"},{"location":"documentation/io/#core-io-modules","title":"Core I/O Modules","text":"<p>These modules handle the fundamental tasks of reading, writing, and representing raw data.</p>"},{"location":"documentation/io/#datarec.io.rawdata.RawData","title":"<code>RawData</code>","text":"<p>Container for raw datasets in DataRec.</p> <p>Wraps a <code>pandas.DataFrame</code> and stores metadata about user, item, rating, and timestamp columns. Provides lightweight methods for slicing, copying, and merging data.</p> Source code in <code>datarec/io/rawdata.py</code> <pre><code>class RawData:\n    \"\"\"\n    Container for raw datasets in DataRec.\n\n    Wraps a `pandas.DataFrame` and stores metadata about user, item, rating, and timestamp columns.\n    Provides lightweight methods for slicing, copying, and merging data.\n    \"\"\"\n    def __init__(\n            self,\n            data=None,\n            header=False,\n            user=None,\n            item=None,\n            rating=None,\n            timestamp=None,\n            user_encoder=None,\n            item_encoder=None):\n        \"\"\"\n        Initialize a RawData object.\n\n        Args:\n            data (pd.DataFrame): DataFrame of the dataset. Defaults to None.\n            header (bool): Whether the file has a header. Defaults to False.\n            user (str): Column name for user IDs.\n            item (str): Column name for item IDs.\n            rating (str): Column name for ratings.\n            timestamp (str): Column name for timestamps.\n            user_encoder (dict | None): Optional user encoding mapping.\n            item_encoder (dict | None): Optional item encoding mapping.\n        \"\"\"\n        self.data = data\n        self.header = header\n        if data is None:\n            self.data = pd.DataFrame\n            self.header = header\n        self.path = None\n\n        self.user = user\n        self.item = item\n        self.rating = rating\n        self.timestamp = timestamp\n        # Aliases for consistency with DataRec naming\n        self.user_col = user\n        self.item_col = item\n        self.rating_col = rating\n        self.timestamp_col = timestamp\n        # Aliases for consistency with DataRec naming\n        self.user_col = user\n        self.item_col = item\n        self.rating_col = rating\n        self.timestamp_col = timestamp\n        # Aliases for consistency with DataRec naming\n        self.user_col = user\n        self.item_col = item\n        self.rating_col = rating\n        self.timestamp_col = timestamp\n        # Optional encoders to support streaming/incremental loading\n        self.user_encoder = user_encoder\n        self.item_encoder = item_encoder\n\n        self.pipeline_step: Optional[\"PipelineStep\"] = None  # To track the pipeline step that produced this RawData\n\n    def append(self, new_data) -&gt; None:\n        \"\"\"\n        Append new rows to the dataset.\n\n        Args:\n            new_data (pd.DataFrame): DataFrame to append.\n\n        Returns:\n            None\n        \"\"\"\n        self.data.append(new_data)\n\n    def copy(self, deep=True) -&gt; \"RawData\":\n        \"\"\"\n        Make a copy of the dataset.\n\n        Args:\n            deep (bool): If True, return a deep copy of the dataset.\n\n        Returns:\n            (RawData): A copy of the dataset.\n\n        \"\"\"\n        return RawData(\n            self.data.copy(deep=deep),\n            header=self.header,\n            user=self.user,\n            item=self.item,\n            rating=self.rating,\n            timestamp=self.timestamp,\n            user_encoder=self.user_encoder,\n            item_encoder=self.item_encoder,\n        )\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation of the dataset.\n        \"\"\"\n        return repr(self.data)\n\n    def __len__(self):\n        \"\"\"\n        Return the length of the dataset.\n        \"\"\"\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Return the item at the given index.\n        Args:\n            idx: index of the item to return.\n\n        Returns:\n            (RawData): the sample at the given index.\n\n        \"\"\"\n        return self.data[idx]\n\n    def __add__(self, other):\n        \"\"\"\n        Concatenate two RawData objects.\n        Args:\n            other (RawData): the other RawData to concatenate.\n\n        Returns:\n            (RawData): the concatenated RawData object.\n\n        \"\"\"\n        self.__check_rawdata_compatibility__(other)\n        new_data = pd.concat([self.data, other.data])\n        new_rawdata = RawData(new_data, user=self.user, item=self.item, rating=self.rating,\n                              timestamp=self.timestamp, header=self.header)\n        return new_rawdata\n\n    def __iter__(self):\n        \"\"\"\n        Iterate over dataset rows.\n\n        Returns:\n            (pd.Series): Each row in the dataset.\n\n        \"\"\"\n        return iter(self.data)\n\n    def __check_rawdata_compatibility__(self, rawdata):\n        \"\"\"\n        Check compatibility between RawData objects.\n        Args:\n            rawdata (RawData): RawData object to check.\n\n        Returns:\n            (bool): True if compatibility is verified.\n\n        \"\"\"\n        return __check_rawdata_compatibility__(self, rawdata)\n</code></pre>"},{"location":"documentation/io/#datarec.io.rawdata.RawData.__init__","title":"<code>__init__(data=None, header=False, user=None, item=None, rating=None, timestamp=None, user_encoder=None, item_encoder=None)</code>","text":"<p>Initialize a RawData object.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame of the dataset. Defaults to None.</p> <code>None</code> <code>header</code> <code>bool</code> <p>Whether the file has a header. Defaults to False.</p> <code>False</code> <code>user</code> <code>str</code> <p>Column name for user IDs.</p> <code>None</code> <code>item</code> <code>str</code> <p>Column name for item IDs.</p> <code>None</code> <code>rating</code> <code>str</code> <p>Column name for ratings.</p> <code>None</code> <code>timestamp</code> <code>str</code> <p>Column name for timestamps.</p> <code>None</code> <code>user_encoder</code> <code>dict | None</code> <p>Optional user encoding mapping.</p> <code>None</code> <code>item_encoder</code> <code>dict | None</code> <p>Optional item encoding mapping.</p> <code>None</code> Source code in <code>datarec/io/rawdata.py</code> <pre><code>def __init__(\n        self,\n        data=None,\n        header=False,\n        user=None,\n        item=None,\n        rating=None,\n        timestamp=None,\n        user_encoder=None,\n        item_encoder=None):\n    \"\"\"\n    Initialize a RawData object.\n\n    Args:\n        data (pd.DataFrame): DataFrame of the dataset. Defaults to None.\n        header (bool): Whether the file has a header. Defaults to False.\n        user (str): Column name for user IDs.\n        item (str): Column name for item IDs.\n        rating (str): Column name for ratings.\n        timestamp (str): Column name for timestamps.\n        user_encoder (dict | None): Optional user encoding mapping.\n        item_encoder (dict | None): Optional item encoding mapping.\n    \"\"\"\n    self.data = data\n    self.header = header\n    if data is None:\n        self.data = pd.DataFrame\n        self.header = header\n    self.path = None\n\n    self.user = user\n    self.item = item\n    self.rating = rating\n    self.timestamp = timestamp\n    # Aliases for consistency with DataRec naming\n    self.user_col = user\n    self.item_col = item\n    self.rating_col = rating\n    self.timestamp_col = timestamp\n    # Aliases for consistency with DataRec naming\n    self.user_col = user\n    self.item_col = item\n    self.rating_col = rating\n    self.timestamp_col = timestamp\n    # Aliases for consistency with DataRec naming\n    self.user_col = user\n    self.item_col = item\n    self.rating_col = rating\n    self.timestamp_col = timestamp\n    # Optional encoders to support streaming/incremental loading\n    self.user_encoder = user_encoder\n    self.item_encoder = item_encoder\n\n    self.pipeline_step: Optional[\"PipelineStep\"] = None  # To track the pipeline step that produced this RawData\n</code></pre>"},{"location":"documentation/io/#datarec.io.rawdata.RawData.append","title":"<code>append(new_data)</code>","text":"<p>Append new rows to the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>new_data</code> <code>DataFrame</code> <p>DataFrame to append.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>datarec/io/rawdata.py</code> <pre><code>def append(self, new_data) -&gt; None:\n    \"\"\"\n    Append new rows to the dataset.\n\n    Args:\n        new_data (pd.DataFrame): DataFrame to append.\n\n    Returns:\n        None\n    \"\"\"\n    self.data.append(new_data)\n</code></pre>"},{"location":"documentation/io/#datarec.io.rawdata.RawData.copy","title":"<code>copy(deep=True)</code>","text":"<p>Make a copy of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>If True, return a deep copy of the dataset.</p> <code>True</code> <p>Returns:</p> Type Description <code>RawData</code> <p>A copy of the dataset.</p> Source code in <code>datarec/io/rawdata.py</code> <pre><code>def copy(self, deep=True) -&gt; \"RawData\":\n    \"\"\"\n    Make a copy of the dataset.\n\n    Args:\n        deep (bool): If True, return a deep copy of the dataset.\n\n    Returns:\n        (RawData): A copy of the dataset.\n\n    \"\"\"\n    return RawData(\n        self.data.copy(deep=deep),\n        header=self.header,\n        user=self.user,\n        item=self.item,\n        rating=self.rating,\n        timestamp=self.timestamp,\n        user_encoder=self.user_encoder,\n        item_encoder=self.item_encoder,\n    )\n</code></pre>"},{"location":"documentation/io/#datarec.io.rawdata.RawData.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the dataset.</p> Source code in <code>datarec/io/rawdata.py</code> <pre><code>def __repr__(self):\n    \"\"\"\n    Return a string representation of the dataset.\n    \"\"\"\n    return repr(self.data)\n</code></pre>"},{"location":"documentation/io/#datarec.io.rawdata.RawData.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the dataset.</p> Source code in <code>datarec/io/rawdata.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Return the length of the dataset.\n    \"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"documentation/io/#datarec.io.rawdata.RawData.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Return the item at the given index. Args:     idx: index of the item to return.</p> <p>Returns:</p> Type Description <code>RawData</code> <p>the sample at the given index.</p> Source code in <code>datarec/io/rawdata.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Return the item at the given index.\n    Args:\n        idx: index of the item to return.\n\n    Returns:\n        (RawData): the sample at the given index.\n\n    \"\"\"\n    return self.data[idx]\n</code></pre>"},{"location":"documentation/io/#datarec.io.rawdata.RawData.__add__","title":"<code>__add__(other)</code>","text":"<p>Concatenate two RawData objects. Args:     other (RawData): the other RawData to concatenate.</p> <p>Returns:</p> Type Description <code>RawData</code> <p>the concatenated RawData object.</p> Source code in <code>datarec/io/rawdata.py</code> <pre><code>def __add__(self, other):\n    \"\"\"\n    Concatenate two RawData objects.\n    Args:\n        other (RawData): the other RawData to concatenate.\n\n    Returns:\n        (RawData): the concatenated RawData object.\n\n    \"\"\"\n    self.__check_rawdata_compatibility__(other)\n    new_data = pd.concat([self.data, other.data])\n    new_rawdata = RawData(new_data, user=self.user, item=self.item, rating=self.rating,\n                          timestamp=self.timestamp, header=self.header)\n    return new_rawdata\n</code></pre>"},{"location":"documentation/io/#datarec.io.rawdata.RawData.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over dataset rows.</p> <p>Returns:</p> Type Description <code>Series</code> <p>Each row in the dataset.</p> Source code in <code>datarec/io/rawdata.py</code> <pre><code>def __iter__(self):\n    \"\"\"\n    Iterate over dataset rows.\n\n    Returns:\n        (pd.Series): Each row in the dataset.\n\n    \"\"\"\n    return iter(self.data)\n</code></pre>"},{"location":"documentation/io/#datarec.io.rawdata.RawData.__check_rawdata_compatibility__","title":"<code>__check_rawdata_compatibility__(rawdata)</code>","text":"<p>Check compatibility between RawData objects. Args:     rawdata (RawData): RawData object to check.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if compatibility is verified.</p> Source code in <code>datarec/io/rawdata.py</code> <pre><code>def __check_rawdata_compatibility__(self, rawdata):\n    \"\"\"\n    Check compatibility between RawData objects.\n    Args:\n        rawdata (RawData): RawData object to check.\n\n    Returns:\n        (bool): True if compatibility is verified.\n\n    \"\"\"\n    return __check_rawdata_compatibility__(self, rawdata)\n</code></pre>"},{"location":"documentation/io/#datarec.io.rawdata.__check_rawdata_compatibility__","title":"<code>__check_rawdata_compatibility__(rawdata1, rawdata2)</code>","text":"<p>Check compatibility between two RawData objects. Args:     rawdata1 (RawData): First RawData object to check.     rawdata2 (RawData): Second RawData object to check.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if compatibility is verified.</p> Source code in <code>datarec/io/rawdata.py</code> <pre><code>def __check_rawdata_compatibility__(rawdata1: RawData, rawdata2: RawData):\n    \"\"\"\n    Check compatibility between two RawData objects.\n    Args:\n        rawdata1 (RawData): First RawData object to check.\n        rawdata2 (RawData): Second RawData object to check.\n\n    Returns:\n        (bool): True if compatibility is verified.\n\n    \"\"\"\n    if rawdata1.user != rawdata2.user:\n        raise ValueError('User columns are not compatible')\n    if rawdata1.item != rawdata2.item:\n        raise ValueError('Item columns are not compatible')\n    if rawdata1.rating != rawdata2.rating:\n        raise ValueError('Rating columns are not compatible')\n    if rawdata1.timestamp != rawdata2.timestamp:\n        raise ValueError('Timestamp columns are not compatible')\n    if rawdata1.header != rawdata2.header:\n        raise ValueError('Header is not compatible')\n    return True\n</code></pre>"},{"location":"documentation/io/#datarec.io.readers.read_sequences_json","title":"<code>read_sequences_json(filepath, *, user_col='user', item_col='item', rating_col=None, timestamp_col=None)</code>","text":"<p>Reads a JSON file representing sequential interaction data in the form:</p> <p>{   \"user_id\": [     { \"item\": ..., \"rating\": ..., \"timestamp\": ... },     ...   ],   ... }</p> <p>Converts it into a transactional RawData format with one row per interaction.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the JSON file.</p> required <code>user_col</code> <code>str</code> <p>Name assigned to the user column in the output.</p> <code>'user'</code> <code>item_col</code> <code>str</code> <p>Key containing the item field inside each event.</p> <code>'item'</code> <code>rating_col</code> <code>Optional[str]</code> <p>Key containing the rating field inside each event.</p> <code>None</code> <code>timestamp_col</code> <code>Optional[str]</code> <p>Key containing the timestamp field inside each event.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>RawData</code> <code>RawData</code> <p>A RawData object containing all interactions exploded row-by-row.</p> Source code in <code>datarec/io/readers/sequences/json.py</code> <pre><code>@annotate_rawdata_output\ndef read_sequences_json(\n    filepath: str,\n    *,\n    user_col: str = \"user\",\n    item_col: str = \"item\",\n    rating_col: Optional[str] = None,\n    timestamp_col: Optional[str] = None,\n) -&gt; RawData:\n    \"\"\"\n    Reads a JSON file representing sequential interaction data in the form:\n\n    {\n      \"user_id\": [\n        { \"item\": ..., \"rating\": ..., \"timestamp\": ... },\n        ...\n      ],\n      ...\n    }\n\n    Converts it into a transactional RawData format with one row per interaction.\n\n    Args:\n        filepath: Path to the JSON file.\n        user_col: Name assigned to the user column in the output.\n        item_col: Key containing the item field inside each event.\n        rating_col: Key containing the rating field inside each event.\n        timestamp_col: Key containing the timestamp field inside each event.\n\n    Returns:\n        RawData: A RawData object containing all interactions exploded row-by-row.\n    \"\"\"\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"File not found: {filepath}\")\n\n    # Load the entire JSON structure\n    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n        payload: Dict[str, Any] = json.load(f)\n\n    rows = []\n\n    # Iterate over each user and their list of events\n    for user_id, events in payload.items():\n        if not isinstance(events, list):\n            raise ValueError(f\"Expected a list of events for user '{user_id}', got {type(events)}.\")\n\n        for event in events:\n            # Extract required fields\n            row = {user_col: user_id}\n\n            # Mandatory\n            if item_col not in event:\n                raise ValueError(f\"Missing item field '{item_col}' in event for user {user_id}.\")\n            row[item_col] = event[item_col]\n\n            # Optional\n            if rating_col is not None:\n                row[rating_col] = event.get(rating_col)\n            if timestamp_col is not None:\n                row[timestamp_col] = event.get(timestamp_col)\n\n            rows.append(row)\n\n    # Build DataFrame\n    data = pd.DataFrame(rows)\n\n    # Reset index for consistency\n    data = data.reset_index(drop=True)\n\n    # Final RawData object\n    raw = RawData(\n        data,\n        user=user_col,\n        item=item_col,\n        rating=rating_col,\n        timestamp=timestamp_col,\n    )\n\n    return raw\n</code></pre>"},{"location":"documentation/io/#datarec.io.readers.read_sequences_json_array","title":"<code>read_sequences_json_array(filepath, *, user_col='user', item_col='item', rating_col=None, timestamp_col=None, sequence_key='sequence')</code>","text":"<p>Reads a JSON file representing sequential interaction data in the form of an ARRAY of user-sequence objects, e.g.:</p> <pre><code>[\n  {\n    \"user\": 0,\n    \"sequence\": [\n      { \"item\": 1, \"rating\": 1, \"timestamp\": \"001\" },\n      { \"item\": 2, \"rating\": 1, \"timestamp\": \"022\" }\n    ]\n  },\n  {\n    \"user\": 1,\n    \"sequence\": [\n      { \"item\": 1, \"rating\": 4, \"timestamp\": \"011\" }\n    ]\n  }\n]\n</code></pre> <p>and converts it into a transactional RawData format with one row per interaction.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the JSON file.</p> required <code>user_col</code> <code>str</code> <p>Name assigned to the user column in the output.       Also used as the key to read the user identifier in       each top-level object.</p> <code>'user'</code> <code>item_col</code> <code>str</code> <p>Key containing the item field inside each event.</p> <code>'item'</code> <code>rating_col</code> <code>Optional[str]</code> <p>Key containing the rating field inside each event.</p> <code>None</code> <code>timestamp_col</code> <code>Optional[str]</code> <p>Key containing the timestamp field inside each event.</p> <code>None</code> <code>sequence_key</code> <code>str</code> <p>Key containing the list of events for each user.</p> <code>'sequence'</code> <p>Returns:</p> Name Type Description <code>RawData</code> <code>RawData</code> <p>A RawData object containing all interactions exploded      row-by-row.</p> Source code in <code>datarec/io/readers/sequences/json.py</code> <pre><code>@annotate_rawdata_output\ndef read_sequences_json_array(\n    filepath: str,\n    *,\n    user_col: str = \"user\",\n    item_col: str = \"item\",\n    rating_col: Optional[str] = None,\n    timestamp_col: Optional[str] = None,\n    sequence_key: str = \"sequence\",\n) -&gt; RawData:\n    \"\"\"\n    Reads a JSON file representing sequential interaction data in the form\n    of an ARRAY of user-sequence objects, e.g.:\n\n        [\n          {\n            \"user\": 0,\n            \"sequence\": [\n              { \"item\": 1, \"rating\": 1, \"timestamp\": \"001\" },\n              { \"item\": 2, \"rating\": 1, \"timestamp\": \"022\" }\n            ]\n          },\n          {\n            \"user\": 1,\n            \"sequence\": [\n              { \"item\": 1, \"rating\": 4, \"timestamp\": \"011\" }\n            ]\n          }\n        ]\n\n    and converts it into a transactional RawData format with one row per\n    interaction.\n\n    Args:\n        filepath: Path to the JSON file.\n        user_col: Name assigned to the user column in the output.\n                  Also used as the key to read the user identifier in\n                  each top-level object.\n        item_col: Key containing the item field inside each event.\n        rating_col: Key containing the rating field inside each event.\n        timestamp_col: Key containing the timestamp field inside each event.\n        sequence_key: Key containing the list of events for each user.\n\n    Returns:\n        RawData: A RawData object containing all interactions exploded\n                 row-by-row.\n    \"\"\"\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"File not found: {filepath}\")\n\n    # Load the entire JSON structure\n    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n        payload: Any = json.load(f)\n\n    if not isinstance(payload, list):\n        raise ValueError(\n            f\"Expected a JSON array at top level, got {type(payload)} instead.\"\n        )\n\n    rows: List[Dict[str, Any]] = []\n\n    # Iterate over each user-level object\n    for obj in payload:\n        if not isinstance(obj, dict):\n            raise ValueError(\n                f\"Expected each element of the array to be an object, got {type(obj)}.\"\n            )\n\n        if user_col not in obj:\n            raise ValueError(\n                f\"Missing user field '{user_col}' in top-level object: {obj}.\"\n            )\n        user_id = obj[user_col]\n\n        if sequence_key not in obj:\n            raise ValueError(\n                f\"Missing sequence field '{sequence_key}' in top-level object for user {user_id}.\"\n            )\n\n        events = obj[sequence_key]\n        if not isinstance(events, list):\n            raise ValueError(\n                f\"Expected '{sequence_key}' to be a list for user {user_id}, \"\n                f\"got {type(events)}.\"\n            )\n\n        for event in events:\n            if not isinstance(event, dict):\n                raise ValueError(\n                    f\"Expected each event in '{sequence_key}' to be an object, got {type(event)}.\"\n                )\n\n            row: Dict[str, Any] = {user_col: user_id}\n\n            # Mandatory item field\n            if item_col not in event:\n                raise ValueError(\n                    f\"Missing item field '{item_col}' in event for user {user_id}: {event}.\"\n                )\n            row[item_col] = event[item_col]\n\n            # Optional fields\n            if rating_col is not None:\n                row[rating_col] = event.get(rating_col)\n            if timestamp_col is not None:\n                row[timestamp_col] = event.get(timestamp_col)\n\n            rows.append(row)\n\n    # Build DataFrame\n    data = pd.DataFrame(rows).reset_index(drop=True)\n\n    # Final RawData object\n    raw = RawData(\n        data,\n        user=user_col,\n        item=item_col,\n        rating=rating_col,\n        timestamp=timestamp_col,\n    )\n\n    return raw\n</code></pre>"},{"location":"documentation/io/#datarec.io.readers.read_sequence_tabular_inline","title":"<code>read_sequence_tabular_inline(filepath, *, user_col='user', sequence_col='sequence', sequence_sep=' ', timestamp_col='timestamp', meta_cols=None, col_sep=',', header=None, cols=None, engine='c', fallback_engine='python', stream=False, encode_ids=False, chunksize=100000)</code>","text":"<p>Reads a file where interaction sequences are stored in a single string column.</p> <p>Example: user_id,sequence -&gt; 70,\"495 1631 2317\"</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the CSV file.</p> required <code>user_col</code> <code>str</code> <p>Column name containing the user ID.</p> <code>'user'</code> <code>sequence_col</code> <code>str</code> <p>Column containing the serialized interaction sequence.</p> <code>'sequence'</code> <code>sequence_sep</code> <code>str</code> <p>Separator used inside the sequence string.</p> <code>' '</code> <code>timestamp_col</code> <code>Optional[str]</code> <p>Column name for timestamp (if present).</p> <code>'timestamp'</code> <code>meta_cols</code> <code>Optional[List[str]]</code> <p>Additional metadata columns to keep.</p> <code>None</code> <code>col_sep</code> <code>str</code> <p>Column separator used in the CSV file.</p> <code>','</code> <code>header</code> <code>Union[int, List[int], str, None]</code> <p>Row number for the header.</p> <code>None</code> <code>cols</code> <code>Optional[List[str]]</code> <p>Explicit column names if the file has no header.</p> <code>None</code> <code>engine</code> <code>str</code> <p>Pandas CSV engine to use (\"c\" or \"python\"). Defaults to \"c\" with automatic     fallback to <code>fallback_engine</code> on failure.</p> <code>'c'</code> <code>fallback_engine</code> <code>str</code> <p>Engine to try if the primary one fails. Defaults to \"python\".</p> <code>'python'</code> <code>stream</code> <code>bool</code> <p>If True, process the file in chunks to reduce peak memory.</p> <code>False</code> <code>encode_ids</code> <code>bool</code> <p>If True, encode user/item to int ids using IncrementalEncoder (streaming or full).</p> <code>False</code> <code>chunksize</code> <code>int</code> <p>Number of rows per chunk when streaming.</p> <code>100000</code> <p>Returns:</p> Name Type Description <code>RawData</code> <code>RawData</code> <p>Transactional data.</p> Source code in <code>datarec/io/readers/sequences/tabular.py</code> <pre><code>@annotate_rawdata_output\ndef read_sequence_tabular_inline(\n    filepath: str,\n    *,\n    user_col: str = \"user\",\n    sequence_col: str = \"sequence\",\n    sequence_sep: str = \" \",\n    timestamp_col: Optional[str] = \"timestamp\",\n    meta_cols: Optional[List[str]] = None,\n    col_sep: str = \",\",\n    header: Union[int, List[int], str, None] = None,\n    cols: Optional[List[str]] = None,\n    engine: str = \"c\",\n    fallback_engine: str = \"python\",\n    stream: bool = False,\n    encode_ids: bool = False,\n    chunksize: int = 100_000,\n) -&gt; RawData:\n    \"\"\"\n    Reads a file where interaction sequences are stored in a single string column.\n\n    Example: user_id,sequence -&gt; 70,\"495 1631 2317\"\n\n    Args:\n        filepath: Path to the CSV file.\n        user_col: Column name containing the user ID.\n        sequence_col: Column containing the serialized interaction sequence.\n        sequence_sep: Separator used inside the sequence string.\n        timestamp_col: Column name for timestamp (if present).\n        meta_cols: Additional metadata columns to keep.\n        col_sep: Column separator used in the CSV file.\n        header: Row number for the header.\n        cols: Explicit column names if the file has no header.\n        engine: Pandas CSV engine to use (\"c\" or \"python\"). Defaults to \"c\" with automatic\n                fallback to `fallback_engine` on failure.\n        fallback_engine: Engine to try if the primary one fails. Defaults to \"python\".\n        stream: If True, process the file in chunks to reduce peak memory.\n        encode_ids: If True, encode user/item to int ids using IncrementalEncoder (streaming or full).\n        chunksize: Number of rows per chunk when streaming.\n\n    Returns:\n        RawData: Transactional data.\n    \"\"\"\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"File not found: {filepath}\")\n\n    if meta_cols is None:\n        meta_cols = []\n\n    read_kwargs = dict(\n        sep=col_sep,\n        header=header,\n        engine=engine,\n        dtype=str,  # avoid type inference surprises\n        encoding=\"utf-8\",\n        encoding_errors=\"ignore\",\n        quoting=csv.QUOTE_NONE,\n    )\n    if cols is not None:\n        read_kwargs[\"names\"] = cols\n\n    if stream:\n        return _read_sequence_tabular_inline_stream(\n            filepath=filepath,\n            user_col=user_col,\n            sequence_col=sequence_col,\n            sequence_sep=sequence_sep,\n            timestamp_col=timestamp_col,\n            meta_cols=meta_cols,\n            read_kwargs=read_kwargs,\n            fallback_engine=fallback_engine,\n            encode_ids=encode_ids,\n            chunksize=chunksize,\n        )\n\n    return _read_sequence_tabular_inline_full(\n        filepath=filepath,\n        user_col=user_col,\n        sequence_col=sequence_col,\n        sequence_sep=sequence_sep,\n        timestamp_col=timestamp_col,\n        meta_cols=meta_cols,\n        read_kwargs=read_kwargs,\n        fallback_engine=fallback_engine,\n        encode_ids=encode_ids,\n    )\n</code></pre>"},{"location":"documentation/io/#datarec.io.readers.read_sequence_tabular_wide","title":"<code>read_sequence_tabular_wide(filepath, *, user_col='user', item_col='item', col_sep='\\t', header=None, encode_ids=False)</code>","text":"<p>Reads a file containing variable-length interaction sequences (ragged).</p> <p>Example: u0 i0      i1      i3</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the text/CSV file.</p> required <code>user_col</code> <code>str</code> <p>Name to assign to the user column.</p> <code>'user'</code> <code>item_col</code> <code>str</code> <p>Name to assign to the item column.</p> <code>'item'</code> <code>col_sep</code> <code>str</code> <p>Delimiter used in the file.</p> <code>'\\t'</code> <code>header</code> <code>Optional[int]</code> <p>Row number (0-indexed) to use as the header (to be skipped).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>RawData</code> <code>RawData</code> <p>Transactional DataFrame.</p> Source code in <code>datarec/io/readers/sequences/tabular.py</code> <pre><code>@annotate_rawdata_output\ndef read_sequence_tabular_wide(\n    filepath: str,\n    *,\n    user_col: str = \"user\",\n    item_col: str = \"item\",\n    col_sep: str = \"\\t\",\n    header: Optional[int] = None,\n    encode_ids: bool = False,\n) -&gt; RawData:\n    \"\"\"\n    Reads a file containing variable-length interaction sequences (ragged).\n\n    Example: u0\\ti0\\ti1\\ti3\n\n    Args:\n        filepath: Path to the text/CSV file.\n        user_col: Name to assign to the user column.\n        item_col: Name to assign to the item column.\n        col_sep: Delimiter used in the file.\n        header: Row number (0-indexed) to use as the header (to be skipped).\n\n    Returns:\n        RawData: Transactional DataFrame.\n    \"\"\"\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"File not found: {filepath}\")\n\n    if header is not None and header &lt; 0:\n        raise ValueError(f\"header must be &gt;= 0 or None, got {header}.\")\n\n    if header is not None:\n        _skiprows = header + 1 \n    else:\n        _skiprows = 0\n\n    # Fast Loading (Raw Text)\n    df_raw = pd.read_csv(\n        filepath,\n        sep=\"\\0\",\n        header=None,\n        names=[\"_raw_string\"],\n        skiprows=_skiprows,\n        engine=\"c\",\n        quoting=csv.QUOTE_NONE,\n    )\n\n    if df_raw.empty:\n        raise ValueError(\"The provided file is empty or all rows were skipped.\")\n\n    # Vectorized processing\n    df_raw[\"_split\"] = df_raw[\"_raw_string\"].str.split(col_sep)\n    df_raw[user_col] = df_raw[\"_split\"].str[0].str.strip()\n    df_raw[\"_items_list\"] = df_raw[\"_split\"].str[1:]\n\n    df_long = df_raw.explode(\"_items_list\")\n    df_long = df_long.rename(columns={\"_items_list\": item_col})\n    df_long = df_long[[user_col, item_col]]\n\n    df_long = df_long.dropna(subset=[item_col])\n\n    df_long[item_col] = df_long[item_col].astype(str).str.strip()\n    df_long = df_long[df_long[item_col] != \"\"]\n\n    df_long = df_long.reset_index(drop=True)\n\n    user_encoder = None\n    item_encoder = None\n    if encode_ids:\n        u_enc = IncrementalEncoder(offset=0)\n        i_enc = IncrementalEncoder(offset=0)\n        df_long[user_col] = u_enc.encode_many(df_long[user_col].tolist())\n        df_long[item_col] = i_enc.encode_many(df_long[item_col].tolist())\n        user_encoder = u_enc.forward\n        item_encoder = i_enc.forward\n\n    return RawData(df_long, user=user_col, item=item_col, user_encoder=user_encoder, item_encoder=item_encoder)\n</code></pre>"},{"location":"documentation/io/#datarec.io.readers.read_sequence_tabular_implicit","title":"<code>read_sequence_tabular_implicit(filepath, *, user_col='sequence_id', item_col='item', col_sep=' ', header=None, drop_length_col=True, encode_ids=False)</code>","text":"<p>Reads a tabular file where each row represents a sequence with an implicit identifier (row-based), optionally starting with a declared sequence length.</p> Example <p>3  10  20  30 2  11  42</p> <p>Each row is interpreted as a distinct sequence (pseudo-user). The first column may represent the declared sequence length and is ignored by default.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the text/CSV file.</p> required <code>user_col</code> <code>str</code> <p>Name assigned to the implicit sequence identifier column.</p> <code>'sequence_id'</code> <code>item_col</code> <code>str</code> <p>Name assigned to the item column.</p> <code>'item'</code> <code>col_sep</code> <code>str</code> <p>Delimiter used in the file.</p> <code>' '</code> <code>header</code> <code>Optional[int]</code> <p>Optional row index to skip as header.</p> <code>None</code> <code>drop_length_col</code> <code>bool</code> <p>If True, the first column is treated as sequence              length and discarded.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>RawData</code> <code>RawData</code> <p>Transactional DataFrame where each sequence is treated      as a pseudo-user.</p> Source code in <code>datarec/io/readers/sequences/tabular.py</code> <pre><code>@annotate_rawdata_output\ndef read_sequence_tabular_implicit(\n    filepath: str,\n    *,\n    user_col: str = \"sequence_id\",\n    item_col: str = \"item\",\n    col_sep: str = \" \",\n    header: Optional[int] = None,\n    drop_length_col: bool = True,\n    encode_ids: bool = False,\n) -&gt; RawData:\n    \"\"\"\n    Reads a tabular file where each row represents a sequence with an\n    implicit identifier (row-based), optionally starting with a declared\n    sequence length.\n\n    Example:\n        3  10  20  30\n        2  11  42\n\n    Each row is interpreted as a distinct sequence (pseudo-user).\n    The first column may represent the declared sequence length and is\n    ignored by default.\n\n    Args:\n        filepath: Path to the text/CSV file.\n        user_col: Name assigned to the implicit sequence identifier column.\n        item_col: Name assigned to the item column.\n        col_sep: Delimiter used in the file.\n        header: Optional row index to skip as header.\n        drop_length_col: If True, the first column is treated as sequence\n                         length and discarded.\n\n    Returns:\n        RawData: Transactional DataFrame where each sequence is treated\n                 as a pseudo-user.\n    \"\"\"\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"File not found: {filepath}\")\n\n    if header is not None and header &lt; 0:\n        raise ValueError(f\"header must be &gt;= 0 or None, got {header}.\")\n\n    skiprows = header + 1 if header is not None else 0\n\n    # Load each line as a raw string (ragged-safe, fast)\n    df_raw = pd.read_csv(\n        filepath,\n        sep=\"\\0\",\n        header=None,\n        names=[\"_raw_string\"],\n        skiprows=skiprows,\n        engine=\"c\",\n        quoting=csv.QUOTE_NONE,\n    )\n\n    if df_raw.empty:\n        raise ValueError(\"The provided file is empty or all rows were skipped.\")\n\n    # Split line into tokens\n    df_raw[\"_split\"] = df_raw[\"_raw_string\"].str.strip().str.split(col_sep)\n\n    # Generate implicit sequence id (row-based)\n    df_raw[user_col] = df_raw.index.astype(str)\n\n    # Drop declared length if requested\n    if drop_length_col:\n        df_raw[\"_items_list\"] = df_raw[\"_split\"].str[1:]\n    else:\n        df_raw[\"_items_list\"] = df_raw[\"_split\"]\n\n    # Explode sequences\n    df_long = df_raw.explode(\"_items_list\")\n\n    df_long = df_long.rename(columns={\"_items_list\": item_col})\n    df_long = df_long[[user_col, item_col]]\n\n    # Cleanup\n    df_long = df_long.dropna(subset=[item_col])\n    df_long[item_col] = df_long[item_col].astype(str).str.strip()\n    df_long = df_long[df_long[item_col] != \"\"]\n\n    df_long = df_long.reset_index(drop=True)\n\n    user_encoder = None\n    item_encoder = None\n    if encode_ids:\n        u_enc = IncrementalEncoder(offset=0)\n        i_enc = IncrementalEncoder(offset=0)\n        df_long[user_col] = u_enc.encode_many(df_long[user_col].tolist())\n        df_long[item_col] = i_enc.encode_many(df_long[item_col].tolist())\n        user_encoder = u_enc.forward\n        item_encoder = i_enc.forward\n\n    return RawData(df_long, user=user_col, item=item_col, user_encoder=user_encoder, item_encoder=item_encoder)\n</code></pre>"},{"location":"documentation/io/#datarec.io.readers.read_transactions_tabular","title":"<code>read_transactions_tabular(filepath, *, sep='\\t', user_col, item_col, rating_col=None, timestamp_col=None, header=None, skiprows=0, cols=None, engine='c', fallback_engine='python', stream=False, encode_ids=False, chunksize=100000)</code>","text":"<p>Reads a tabular data file (CSV, TSV, etc.) into a RawData object.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the tabular data file.</p> required <code>sep</code> <code>str</code> <p>Delimiter to use (default: tab).</p> <code>'\\t'</code> <code>user_col</code> <code>Union[str, int]</code> <p>Column name or index for the user field (Required).</p> required <code>item_col</code> <code>Union[str, int]</code> <p>Column name or index for the item field (Required).</p> required <code>rating_col</code> <code>Optional[Union[str, int]]</code> <p>Column name or index for the rating field.</p> <code>None</code> <code>timestamp_col</code> <code>Optional[Union[str, int]]</code> <p>Column name or index for the timestamp field.</p> <code>None</code> <code>header</code> <code>Union[int, List[int], str, None]</code> <p>Row number(s) to use as the column names. Defaults to 'infer'.</p> <code>None</code> <code>skiprows</code> <code>Union[int, List[int]]</code> <p>Line numbers to skip at the start of the file.</p> <code>0</code> <code>cols</code> <code>Optional[List[str]]</code> <p>Explicit column names if the file has no header. Passed as <code>names</code>   to <code>pandas.read_csv</code>.</p> <code>None</code> <code>engine</code> <code>Optional[str]</code> <p>Pandas CSV engine.</p> <code>'c'</code> <code>fallback_engine</code> <code>Optional[str]</code> <p>Engine to try if the primary fails.</p> <code>'python'</code> <code>stream</code> <code>bool</code> <p>If True, read in chunks to reduce memory.</p> <code>False</code> <code>encode_ids</code> <code>bool</code> <p>If True, encode user/item to int ids using IncrementalEncoder.</p> <code>False</code> <code>chunksize</code> <code>int</code> <p>Rows per chunk when streaming.</p> <code>100000</code> <p>Returns:</p> Name Type Description <code>RawData</code> <code>RawData</code> <p>The loaded data.</p> Source code in <code>datarec/io/readers/transactions/tabular.py</code> <pre><code>@annotate_rawdata_output\ndef read_transactions_tabular(\n    filepath: str,\n    *,\n    sep: str = \"\\t\",\n    user_col: Union[str, int],\n    item_col: Union[str, int],\n    rating_col: Optional[Union[str, int]] = None,\n    timestamp_col: Optional[Union[str, int]] = None,\n    header: Union[int, List[int], str, None] = None,\n    skiprows: Union[int, List[int]] = 0,\n    cols: Optional[List[str]] = None,\n    engine: Optional[str] = 'c',\n    fallback_engine: Optional[str] = 'python',\n    stream: bool = False,\n    encode_ids: bool = False,\n    chunksize: int = 100_000) -&gt; RawData:\n    \"\"\"\n    Reads a tabular data file (CSV, TSV, etc.) into a RawData object.\n\n    Args:\n        filepath: Path to the tabular data file.\n        sep: Delimiter to use (default: tab).\n        user_col: Column name or index for the user field (Required).\n        item_col: Column name or index for the item field (Required).\n        rating_col: Column name or index for the rating field.\n        timestamp_col: Column name or index for the timestamp field.\n        header: Row number(s) to use as the column names. Defaults to 'infer'.\n        skiprows: Line numbers to skip at the start of the file.\n        cols: Explicit column names if the file has no header. Passed as `names`\n              to `pandas.read_csv`.\n        engine: Pandas CSV engine.\n        fallback_engine: Engine to try if the primary fails.\n        stream: If True, read in chunks to reduce memory.\n        encode_ids: If True, encode user/item to int ids using IncrementalEncoder.\n        chunksize: Rows per chunk when streaming.\n\n    Returns:\n        RawData: The loaded data.\n    \"\"\"\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"File not found: {filepath}\")\n\n    read_kwargs = dict(sep=sep, header=header, skiprows=skiprows, engine=engine)\n    if cols is not None:\n        read_kwargs[\"names\"] = cols\n\n    if stream:\n        return _read_transactions_tabular_stream(\n            filepath=filepath,\n            user_col=user_col,\n            item_col=item_col,\n            rating_col=rating_col,\n            timestamp_col=timestamp_col,\n            read_kwargs=read_kwargs,\n            fallback_engine=fallback_engine,\n            encode_ids=encode_ids,\n            chunksize=chunksize,\n        )\n\n    try:\n        data = pd.read_csv(filepath, **read_kwargs)\n    except Exception as exc:\n        if fallback_engine and fallback_engine != engine:\n            read_kwargs[\"engine\"] = fallback_engine\n            data = pd.read_csv(filepath, **read_kwargs)\n        else:\n            raise exc\n\n    # Helper to resolve column name from string or index\n    def _resolve_col(col: Optional[Union[str, int]]) -&gt; Optional[object]:\n        \"\"\"\n        Resolve a column specifier to an existing column label in `data.columns`.\n\n        Rules:\n        - str: treated as a column name (label)\n        - int: if it matches an existing column label, use it as label;\n                otherwise treat it as positional index.\n        \"\"\"\n        if col is None:\n            return None\n\n        cols_index = data.columns\n\n        if isinstance(col, int):\n            # Prefer label semantics if the label exists (works also with numpy integer labels)\n            if col in cols_index:\n                # Return the exact label object stored in the Index (could be np.int64(0), etc.)\n                return int(cols_index[cols_index.get_loc(col)])\n\n            # Otherwise, interpret as positional index\n            if col &lt; 0 or col &gt;= len(cols_index):\n                raise ValueError(\n                    f\"Column index {col} is out of bounds for {len(cols_index)} columns.\"\n                )\n            return cols_index[col]\n\n        # str case: strict label match\n        if col not in cols_index:\n            raise ValueError(\n                f\"Column '{col}' not found in dataset columns: {list(cols_index)}\"\n            )\n        return col\n\n    user_col_name = _resolve_col(user_col)\n    item_col_name = _resolve_col(item_col)\n    rating_col_name = _resolve_col(rating_col)\n    timestamp_col_name = _resolve_col(timestamp_col)\n\n    std_columns = [user_col_name, item_col_name, rating_col_name, timestamp_col_name]\n    assigned_columns = [c for c in std_columns if c is not None]\n\n    if not assigned_columns or user_col_name is None or item_col_name is None:\n        raise ValueError(\"User and Item columns are required.\")\n\n    # Subset data to the relevant columns\n    data = data[assigned_columns]\n\n    user_encoder = None\n    item_encoder = None\n    if encode_ids:\n        u_enc = IncrementalEncoder(offset=0)\n        i_enc = IncrementalEncoder(offset=0)\n        data[user_col_name] = u_enc.encode_many(data[user_col_name].tolist())\n        data[item_col_name] = i_enc.encode_many(data[item_col_name].tolist())\n        user_encoder = u_enc.forward\n        item_encoder = i_enc.forward\n\n    rawdata = RawData(\n        data,\n        user=user_col_name,\n        item=item_col_name,\n        rating=rating_col_name,\n        timestamp=timestamp_col_name,\n        user_encoder=user_encoder,\n        item_encoder=item_encoder,\n    )\n\n    return rawdata\n</code></pre>"},{"location":"documentation/io/#datarec.io.readers.read_transactions_json","title":"<code>read_transactions_json(filepath, *, user_col, item_col, rating_col=None, timestamp_col=None, lines=True, stream=False, encode_ids=False, chunksize=100000)</code>","text":"<p>Reads a JSON (or JSON Lines) file and returns it as a RawData object.</p> <p>Arg names standardized to match read_tabular (user_col instead of user_field).</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the JSON file.</p> required <code>user_col</code> <code>str</code> <p>JSON key corresponding to the user field (Required).</p> required <code>item_col</code> <code>str</code> <p>JSON key corresponding to the item field (Required).</p> required <code>rating_col</code> <code>Optional[str]</code> <p>JSON key corresponding to the rating field.</p> <code>None</code> <code>timestamp_col</code> <code>Optional[str]</code> <p>JSON key corresponding to the timestamp field.</p> <code>None</code> <code>lines</code> <code>bool</code> <p>If True, reads the file as a JSON object per line (JSONL).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>RawData</code> <code>RawData</code> <p>The loaded data.</p> Source code in <code>datarec/io/readers/transactions/json.py</code> <pre><code>@annotate_rawdata_output\ndef read_transactions_json(\n    filepath: str, \n    *,\n    user_col: str, \n    item_col: str, \n    rating_col: Optional[str] = None, \n    timestamp_col: Optional[str] = None, \n    lines: bool = True,\n    stream: bool = False,\n    encode_ids: bool = False,\n    chunksize: int = 100_000) -&gt; RawData:\n    \"\"\"\n    Reads a JSON (or JSON Lines) file and returns it as a RawData object.\n\n    Arg names standardized to match read_tabular (user_col instead of user_field).\n\n    Args:\n        filepath: Path to the JSON file.\n        user_col: JSON key corresponding to the user field (Required).\n        item_col: JSON key corresponding to the item field (Required).\n        rating_col: JSON key corresponding to the rating field.\n        timestamp_col: JSON key corresponding to the timestamp field.\n        lines: If True, reads the file as a JSON object per line (JSONL).\n\n    Returns:\n        RawData: The loaded data.\n    \"\"\"\n    return read_transactions_json_base(\n        filepath,\n        user_col=user_col,\n        item_col=item_col,\n        rating_col=rating_col,\n        timestamp_col=timestamp_col,\n        lines=False,\n        stream=stream,\n        encode_ids=encode_ids,\n        chunksize=chunksize,\n    )\n</code></pre>"},{"location":"documentation/io/#datarec.io.readers.read_transactions_jsonl","title":"<code>read_transactions_jsonl(filepath, *, user_col, item_col, rating_col=None, timestamp_col=None, stream=False, encode_ids=False, chunksize=100000)</code>","text":"<p>Reads a JSON Lines file and returns it as a RawData object.</p> <p>Arg names standardized to match read_tabular (user_col instead of user_field).</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the JSON file.</p> required <code>user_col</code> <code>str</code> <p>JSON key corresponding to the user field (Required).</p> required <code>item_col</code> <code>str</code> <p>JSON key corresponding to the item field (Required).</p> required <code>rating_col</code> <code>Optional[str]</code> <p>JSON key corresponding to the rating field.</p> <code>None</code> <code>timestamp_col</code> <code>Optional[str]</code> <p>JSON key corresponding to the timestamp field.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>RawData</code> <code>RawData</code> <p>The loaded data.</p> Source code in <code>datarec/io/readers/transactions/jsonl.py</code> <pre><code>@annotate_rawdata_output\ndef read_transactions_jsonl(\n    filepath: str, \n    *,\n    user_col: str, \n    item_col: str, \n    rating_col: Optional[str] = None, \n    timestamp_col: Optional[str] = None,\n    stream: bool = False,\n    encode_ids: bool = False,\n    chunksize: int = 100_000) -&gt; RawData:\n    \"\"\"\n    Reads a JSON Lines file and returns it as a RawData object.\n\n    Arg names standardized to match read_tabular (user_col instead of user_field).\n\n    Args:\n        filepath: Path to the JSON file.\n        user_col: JSON key corresponding to the user field (Required).\n        item_col: JSON key corresponding to the item field (Required).\n        rating_col: JSON key corresponding to the rating field.\n        timestamp_col: JSON key corresponding to the timestamp field.\n\n    Returns:\n        RawData: The loaded data.\n    \"\"\"\n    return read_transactions_json_base(\n        filepath,\n        user_col=user_col,\n        item_col=item_col,\n        rating_col=rating_col,\n        timestamp_col=timestamp_col,\n        lines=True,\n        stream=stream,\n        encode_ids=encode_ids,\n        chunksize=chunksize,\n    )\n</code></pre>"},{"location":"documentation/io/#datarec.io.writers.write_sequences_json","title":"<code>write_sequences_json(data, filepath, *, item_col='item', rating_col='rating', timestamp_col='timestamp', include_rating=False, include_timestamp=False, ensure_ascii=False, indent=2, verbose=True)</code>","text":"<p>Writes sequential interaction data to a JSON mapping in the form:</p> <pre><code>{\n  \"&lt;user_id&gt;\": [\n    { \"&lt;item_col&gt;\": ..., \"&lt;rating_col&gt;\": ..., \"&lt;timestamp_col&gt;\": ... },\n    ...\n  ],\n  ...\n}\n</code></pre> <p>Notes: - The input is expected to be transactional RawData (one row per interaction). - User identifiers become JSON object keys (strings). Therefore, the output does   NOT contain a user field name (no <code>user_col</code> parameter).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[RawData, DataRec]</code> <p>RawData or DataRec instance.</p> required <code>filepath</code> <code>str</code> <p>Output path.</p> required <code>item_col</code> <code>str</code> <p>Output key for the item field inside each event.</p> <code>'item'</code> <code>rating_col</code> <code>str</code> <p>Output key for the rating field inside each event (only if include_rating=True).</p> <code>'rating'</code> <code>timestamp_col</code> <code>str</code> <p>Output key for the timestamp field inside each event (only if include_timestamp=True).</p> <code>'timestamp'</code> <code>include_rating</code> <code>bool</code> <p>Whether to include rating in each event.</p> <code>False</code> <code>include_timestamp</code> <code>bool</code> <p>Whether to include timestamp in each event.</p> <code>False</code> <code>ensure_ascii</code> <code>bool</code> <p>Whether to escape non-ascii characters.</p> <code>False</code> <code>indent</code> <code>Optional[int]</code> <p>Pretty-print indentation level.</p> <code>2</code> <code>verbose</code> <code>bool</code> <p>Whether to print a confirmation message.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>datarec/io/writers/sequences/json.py</code> <pre><code>def write_sequences_json(\n    data: Union[\"RawData\", \"DataRec\"],\n    filepath: str,\n    *,\n    item_col: str = \"item\",\n    rating_col: str = \"rating\",\n    timestamp_col: str = \"timestamp\",\n    include_rating: bool = False,\n    include_timestamp: bool = False,\n    ensure_ascii: bool = False,\n    indent: Optional[int] = 2,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"\n    Writes sequential interaction data to a JSON mapping in the form:\n\n        {\n          \"&lt;user_id&gt;\": [\n            { \"&lt;item_col&gt;\": ..., \"&lt;rating_col&gt;\": ..., \"&lt;timestamp_col&gt;\": ... },\n            ...\n          ],\n          ...\n        }\n\n    Notes:\n    - The input is expected to be transactional RawData (one row per interaction).\n    - User identifiers become JSON object keys (strings). Therefore, the output does\n      NOT contain a user field name (no `user_col` parameter).\n\n    Args:\n        data: RawData or DataRec instance.\n        filepath: Output path.\n        item_col: Output key for the item field inside each event.\n        rating_col: Output key for the rating field inside each event (only if include_rating=True).\n        timestamp_col: Output key for the timestamp field inside each event (only if include_timestamp=True).\n        include_rating: Whether to include rating in each event.\n        include_timestamp: Whether to include timestamp in each event.\n        ensure_ascii: Whether to escape non-ascii characters.\n        indent: Pretty-print indentation level.\n        verbose: Whether to print a confirmation message.\n\n    Returns:\n        None\n    \"\"\"\n    raw = as_rawdata(data)\n\n    if raw.user is None:\n        raise ValueError(\"RawData.user is not defined.\")\n    if raw.item is None:\n        raise ValueError(\"RawData.item is not defined.\")\n    if include_rating and raw.rating is None:\n        raise ValueError(\"include_rating=True but RawData.rating is not defined.\")\n    if include_timestamp and raw.timestamp is None:\n        raise ValueError(\"include_timestamp=True but RawData.timestamp is not defined.\")\n\n    cols: List[object] = [raw.user, raw.item]\n    if include_rating:\n        cols.append(raw.rating)  # type: ignore[arg-type]\n    if include_timestamp:\n        cols.append(raw.timestamp)  # type: ignore[arg-type]\n\n    df = raw.data[cols].dropna(subset=[raw.user, raw.item])\n\n    payload: Dict[str, List[Dict[str, Any]]] = {}\n\n    # Preserve order of appearance\n    for uid, g in df.groupby(raw.user, sort=False):\n        # Build event dicts in a vectorized-ish way\n        g2 = g.rename(columns={raw.item: item_col})\n\n        keep_cols = [item_col]\n        if include_rating:\n            g2 = g2.rename(columns={raw.rating: rating_col})  # type: ignore[arg-type]\n            keep_cols.append(rating_col)\n        if include_timestamp:\n            g2 = g2.rename(columns={raw.timestamp: timestamp_col})  # type: ignore[arg-type]\n            keep_cols.append(timestamp_col)\n\n        records = g2[keep_cols].to_dict(orient=\"records\")\n\n        # Make JSON-safe\n        safe_events: List[Dict[str, Any]] = []\n        for rec in records:\n            safe_events.append({k: _json_safe(v) for k, v in rec.items()})\n\n        payload[str(_json_safe(uid))] = safe_events\n\n    out_dir = os.path.dirname(os.path.abspath(filepath))\n    if out_dir and not os.path.exists(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n\n    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n        json.dump(payload, f, ensure_ascii=ensure_ascii, indent=indent)\n\n    if verbose:\n        print(f\"Sequences JSON mapping written to '{filepath}'\")\n</code></pre>"},{"location":"documentation/io/#datarec.io.writers.write_sequences_json_array","title":"<code>write_sequences_json_array(data, filepath, *, user_col='user', item_col='item', rating_col='rating', timestamp_col='timestamp', sequence_key='sequence', include_rating=False, include_timestamp=False, ensure_ascii=False, indent=2, verbose=True)</code>","text":"<p>Writes sequential interaction data to a JSON array format:</p> <pre><code>[\n  {\n    \"&lt;user_col&gt;\": &lt;user_id&gt;,\n    \"&lt;sequence_key&gt;\": [\n      { \"&lt;item_col&gt;\": ..., \"&lt;rating_col&gt;\": ..., \"&lt;timestamp_col&gt;\": ... },\n      ...\n    ]\n  },\n  ...\n]\n</code></pre> <p>Notes: - The input is expected to be transactional RawData (one row per interaction).   This writer groups interactions by user and produces a per-user sequence list. - Unlike the mapping format, user ids remain values (not JSON keys).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[RawData, DataRec]</code> <p>RawData or DataRec instance.</p> required <code>filepath</code> <code>str</code> <p>Output path.</p> required <code>user_col</code> <code>str</code> <p>Output key for the user field in each top-level object.</p> <code>'user'</code> <code>item_col</code> <code>str</code> <p>Output key for the item field inside each event.</p> <code>'item'</code> <code>rating_col</code> <code>str</code> <p>Output key for the rating field inside each event (only if include_rating=True).</p> <code>'rating'</code> <code>timestamp_col</code> <code>str</code> <p>Output key for the timestamp field inside each event (only if include_timestamp=True).</p> <code>'timestamp'</code> <code>sequence_key</code> <code>str</code> <p>Output key containing the list of events per user.</p> <code>'sequence'</code> <code>include_rating</code> <code>bool</code> <p>Whether to include rating in each event.</p> <code>False</code> <code>include_timestamp</code> <code>bool</code> <p>Whether to include timestamp in each event.</p> <code>False</code> <code>ensure_ascii</code> <code>bool</code> <p>Whether to escape non-ascii characters.</p> <code>False</code> <code>indent</code> <code>Optional[int]</code> <p>Pretty-print indentation level.</p> <code>2</code> <code>verbose</code> <code>bool</code> <p>Whether to print a confirmation message.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>datarec/io/writers/sequences/json.py</code> <pre><code>def write_sequences_json_array(\n    data: Union[\"RawData\", \"DataRec\"],\n    filepath: str,\n    *,\n    user_col: str = \"user\",\n    item_col: str = \"item\",\n    rating_col: str = \"rating\",\n    timestamp_col: str = \"timestamp\",\n    sequence_key: str = \"sequence\",\n    include_rating: bool = False,\n    include_timestamp: bool = False,\n    ensure_ascii: bool = False,\n    indent: Optional[int] = 2,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"\n    Writes sequential interaction data to a JSON array format:\n\n        [\n          {\n            \"&lt;user_col&gt;\": &lt;user_id&gt;,\n            \"&lt;sequence_key&gt;\": [\n              { \"&lt;item_col&gt;\": ..., \"&lt;rating_col&gt;\": ..., \"&lt;timestamp_col&gt;\": ... },\n              ...\n            ]\n          },\n          ...\n        ]\n\n    Notes:\n    - The input is expected to be transactional RawData (one row per interaction).\n      This writer groups interactions by user and produces a per-user sequence list.\n    - Unlike the mapping format, user ids remain values (not JSON keys).\n\n    Args:\n        data: RawData or DataRec instance.\n        filepath: Output path.\n        user_col: Output key for the user field in each top-level object.\n        item_col: Output key for the item field inside each event.\n        rating_col: Output key for the rating field inside each event (only if include_rating=True).\n        timestamp_col: Output key for the timestamp field inside each event (only if include_timestamp=True).\n        sequence_key: Output key containing the list of events per user.\n        include_rating: Whether to include rating in each event.\n        include_timestamp: Whether to include timestamp in each event.\n        ensure_ascii: Whether to escape non-ascii characters.\n        indent: Pretty-print indentation level.\n        verbose: Whether to print a confirmation message.\n\n    Returns:\n        None\n    \"\"\"\n    raw = as_rawdata(data)\n\n    if raw.user is None:\n        raise ValueError(\"RawData.user is not defined.\")\n    if raw.item is None:\n        raise ValueError(\"RawData.item is not defined.\")\n    if include_rating and raw.rating is None:\n        raise ValueError(\"include_rating=True but RawData.rating is not defined.\")\n    if include_timestamp and raw.timestamp is None:\n        raise ValueError(\"include_timestamp=True but RawData.timestamp is not defined.\")\n\n    cols: List[object] = [raw.user, raw.item]\n    if include_rating:\n        cols.append(raw.rating)  # type: ignore[arg-type]\n    if include_timestamp:\n        cols.append(raw.timestamp)  # type: ignore[arg-type]\n\n    df = raw.data[cols].dropna(subset=[raw.user, raw.item])\n\n    payload: List[Dict[str, Any]] = []\n\n    for uid, g in df.groupby(raw.user, sort=False):\n        g2 = g.rename(columns={raw.item: item_col})\n\n        keep_cols = [item_col]\n        if include_rating:\n            g2 = g2.rename(columns={raw.rating: rating_col})  # type: ignore[arg-type]\n            keep_cols.append(rating_col)\n        if include_timestamp:\n            g2 = g2.rename(columns={raw.timestamp: timestamp_col})  # type: ignore[arg-type]\n            keep_cols.append(timestamp_col)\n\n        records = g2[keep_cols].to_dict(orient=\"records\")\n        safe_events = [{k: _json_safe(v) for k, v in rec.items()} for rec in records]\n\n        payload.append(\n            {\n                user_col: _json_safe(uid),\n                sequence_key: safe_events,\n            }\n        )\n\n    out_dir = os.path.dirname(os.path.abspath(filepath))\n    if out_dir and not os.path.exists(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n\n    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n        json.dump(payload, f, ensure_ascii=ensure_ascii, indent=indent)\n\n    if verbose:\n        print(f\"Sequences JSON array written to '{filepath}'\")\n</code></pre>"},{"location":"documentation/io/#datarec.io.writers.write_sequence_tabular_inline","title":"<code>write_sequence_tabular_inline(data, filepath, *, user_col='user', sequence_col='sequence', sequence_sep=' ', include_timestamp=False, timestamp_col='timestamp', meta_cols=None, col_sep=',', header=True, index=False, decimal='.', engine=None, verbose=True)</code>","text":"<p>Writes sequential interaction data to a tabular file where each row contains a single user and a serialized sequence in one column (inline format).</p> <p>Output format (one sequence per row):     user_col, sequence_col, [timestamp_col], [meta_cols...]</p> <p>Notes: - The input is expected to be transactional RawData (one row per (user, item)),   as produced by DataRec readers. This writer groups interactions by user and   serializes the per-user item list using <code>sequence_sep</code>. - If include_timestamp=True, a per-user timestamp is derived by aggregating   RawData timestamps using a deterministic strategy (min timestamp). - Metadata columns (meta_cols) are aggregated per user using a deterministic   strategy (first value).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[RawData, DataRec]</code> <p>RawData or DataRec instance.</p> required <code>filepath</code> <code>str</code> <p>Output path.</p> required <code>user_col</code> <code>str</code> <p>Output column name for user IDs.</p> <code>'user'</code> <code>sequence_col</code> <code>str</code> <p>Output column name for the serialized sequence.</p> <code>'sequence'</code> <code>sequence_sep</code> <code>str</code> <p>Separator used to serialize items in the sequence string.</p> <code>' '</code> <code>include_timestamp</code> <code>bool</code> <p>Whether to include a per-user timestamp column.</p> <code>False</code> <code>timestamp_col</code> <code>str</code> <p>Output column name for timestamp (used only if include_timestamp=True).</p> <code>'timestamp'</code> <code>meta_cols</code> <code>Optional[List[str]]</code> <p>Additional metadata columns to include (if present in RawData).</p> <code>None</code> <code>col_sep</code> <code>str</code> <p>Column delimiter for the output file.</p> <code>','</code> <code>header</code> <code>bool</code> <p>Whether to write column names.</p> <code>True</code> <code>index</code> <code>bool</code> <p>Whether to write the DataFrame index.</p> <code>False</code> <code>decimal</code> <code>str</code> <p>Decimal separator passed to pandas.</p> <code>'.'</code> <code>engine</code> <code>Optional[str]</code> <p>Optional pandas CSV engine hint.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print a confirmation message.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>datarec/io/writers/sequences/tabular.py</code> <pre><code>def write_sequence_tabular_inline(\n    data: Union[\"RawData\", \"DataRec\"],\n    filepath: str,\n    *,\n    user_col: str = \"user\",\n    sequence_col: str = \"sequence\",\n    sequence_sep: str = \" \",\n    include_timestamp: bool = False,\n    timestamp_col: str = \"timestamp\",\n    meta_cols: Optional[List[str]] = None,\n    col_sep: str = \",\",\n    header: bool = True,\n    index: bool = False,\n    decimal: str = \".\",\n    engine: Optional[str] = None,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"\n    Writes sequential interaction data to a tabular file where each row contains\n    a single user and a serialized sequence in one column (inline format).\n\n    Output format (one sequence per row):\n        user_col, sequence_col, [timestamp_col], [meta_cols...]\n\n    Notes:\n    - The input is expected to be transactional RawData (one row per (user, item)),\n      as produced by DataRec readers. This writer groups interactions by user and\n      serializes the per-user item list using `sequence_sep`.\n    - If include_timestamp=True, a per-user timestamp is derived by aggregating\n      RawData timestamps using a deterministic strategy (min timestamp).\n    - Metadata columns (meta_cols) are aggregated per user using a deterministic\n      strategy (first value).\n\n    Args:\n        data: RawData or DataRec instance.\n        filepath: Output path.\n        user_col: Output column name for user IDs.\n        sequence_col: Output column name for the serialized sequence.\n        sequence_sep: Separator used to serialize items in the sequence string.\n        include_timestamp: Whether to include a per-user timestamp column.\n        timestamp_col: Output column name for timestamp (used only if include_timestamp=True).\n        meta_cols: Additional metadata columns to include (if present in RawData).\n        col_sep: Column delimiter for the output file.\n        header: Whether to write column names.\n        index: Whether to write the DataFrame index.\n        decimal: Decimal separator passed to pandas.\n        engine: Optional pandas CSV engine hint.\n        verbose: Whether to print a confirmation message.\n\n    Returns:\n        None\n    \"\"\"\n    raw = as_rawdata(data)\n\n    if raw.user is None:\n        raise ValueError(\"RawData.user is not defined.\")\n    if raw.item is None:\n        raise ValueError(\"RawData.item is not defined.\")\n\n    if meta_cols is None:\n        meta_cols = []\n\n    df = raw.data\n\n    # Validate timestamp intent vs availability\n    if include_timestamp:\n        if raw.timestamp is None:\n            raise ValueError(\"include_timestamp=True but RawData.timestamp is not defined.\")\n        if raw.timestamp not in df.columns:\n            raise ValueError(f\"Timestamp column '{raw.timestamp}' not found in RawData.data.\")\n\n    # Determine which meta columns are available\n    available_meta = [mc for mc in meta_cols if mc in df.columns]\n\n    # Select needed columns (no copy)\n    cols_needed: List[object] = [raw.user, raw.item]\n    if include_timestamp:\n        cols_needed.append(raw.timestamp)  # type: ignore[arg-type]\n    cols_needed.extend(available_meta)\n\n    df = df[cols_needed].dropna(subset=[raw.user, raw.item])\n\n    # Aggregation strategy:\n    # - items: list in input order\n    # - timestamp: min (deterministic)\n    # - meta: first (deterministic)\n    agg: Dict[object, Any] = {raw.item: list}\n\n    if include_timestamp:\n        agg[raw.timestamp] = \"min\"  # type: ignore[index]\n\n    for mc in available_meta:\n        agg[mc] = \"first\"\n\n    grouped = df.groupby(raw.user, sort=False, dropna=False).agg(agg).reset_index()\n\n    # Serialize sequence\n    grouped[sequence_col] = grouped[raw.item].apply(\n        lambda items: sequence_sep.join(str(x).strip() for x in items if str(x).strip() != \"\")\n    )\n\n    # Drop users with empty sequences\n    grouped = grouped[grouped[sequence_col] != \"\"]\n\n    # Rename user (and timestamp if requested) to output names\n    rename_map: Dict[object, str] = {raw.user: user_col}\n    if include_timestamp:\n        rename_map[raw.timestamp] = timestamp_col  # type: ignore[index]\n\n    grouped = grouped.rename(columns=rename_map)\n\n    # Drop the aggregated list column (raw.item), keep only the serialized sequence\n    grouped = grouped.drop(columns=[raw.item])\n\n    # Final column order\n    out_cols: List[str] = [user_col, sequence_col]\n    if include_timestamp:\n        out_cols.append(timestamp_col)\n\n    # Preserve user-requested ordering for meta_cols (only those available)\n    for mc in meta_cols:\n        if mc in grouped.columns and mc not in out_cols:\n            out_cols.append(mc)\n\n    grouped = grouped[out_cols]\n\n    # Ensure output directory exists\n    out_dir = os.path.dirname(os.path.abspath(filepath))\n    if out_dir and not os.path.exists(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n\n    to_csv_kwargs = dict(sep=col_sep, header=header, index=index, decimal=decimal)\n    if engine is not None:\n        to_csv_kwargs[\"engine\"] = engine\n\n    grouped.to_csv(filepath, **to_csv_kwargs)\n\n    if verbose:\n        print(f\"Inline sequence dataset written to '{filepath}'\")\n</code></pre>"},{"location":"documentation/io/#datarec.io.writers.write_sequence_tabular_wide","title":"<code>write_sequence_tabular_wide(data, filepath, *, user_col='user', item_col_prefix='item', col_sep='\\t', header=False, index=False, decimal='.', verbose=True)</code>","text":"<p>Writes sequential interaction data to a tabular-wide format, where each row corresponds to a user and items are spread across multiple columns.</p> Output example <p>user  item_1  item_2  item_3 <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[RawData, DataRec]</code> <p>RawData or DataRec instance.</p> required <code>filepath</code> <code>str</code> <p>Output path.</p> required <code>user_col</code> <code>str</code> <p>Output column name for user IDs.</p> <code>'user'</code> <code>item_col_prefix</code> <code>str</code> <p>Prefix for item columns (e.g., item_1, item_2, ...).</p> <code>'item'</code> <code>col_sep</code> <code>str</code> <p>Column delimiter.</p> <code>'\\t'</code> <code>header</code> <code>bool</code> <p>Whether to write column names.</p> <code>False</code> <code>index</code> <code>bool</code> <p>Whether to write the DataFrame index.</p> <code>False</code> <code>decimal</code> <code>str</code> <p>Decimal separator.</p> <code>'.'</code> <code>verbose</code> <code>bool</code> <p>Whether to print a confirmation message.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>datarec/io/writers/sequences/tabular.py</code> <pre><code>def write_sequence_tabular_wide(\n    data: Union[\"RawData\", \"DataRec\"],\n    filepath: str,\n    *,\n    user_col: str = \"user\",\n    item_col_prefix: str = \"item\",\n    col_sep: str = \"\\t\",\n    header: bool = False,\n    index: bool = False,\n    decimal: str = \".\",\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"\n    Writes sequential interaction data to a tabular-wide format, where each row\n    corresponds to a user and items are spread across multiple columns.\n\n    Output example:\n        user &lt;sep&gt; item_1 &lt;sep&gt; item_2 &lt;sep&gt; item_3\n\n    Args:\n        data: RawData or DataRec instance.\n        filepath: Output path.\n        user_col: Output column name for user IDs.\n        item_col_prefix: Prefix for item columns (e.g., item_1, item_2, ...).\n        col_sep: Column delimiter.\n        header: Whether to write column names.\n        index: Whether to write the DataFrame index.\n        decimal: Decimal separator.\n        verbose: Whether to print a confirmation message.\n\n    Returns:\n        None\n    \"\"\"\n    raw = as_rawdata(data)\n\n    if raw.user is None or raw.item is None:\n        raise ValueError(\"RawData must define both user and item columns.\")\n\n    df = raw.data[[raw.user, raw.item]].dropna()\n\n    # Preserve order of appearance\n    grouped = (\n        df.groupby(raw.user, sort=False)[raw.item]\n        .apply(list)\n        .reset_index()\n    )\n\n    # Build wide DataFrame\n    max_len = grouped[raw.item].map(len).max()\n\n    wide_items = pd.DataFrame(\n        grouped[raw.item].tolist(),\n        columns=[f\"{item_col_prefix}_{i+1}\" for i in range(max_len)],\n    )\n\n    out_df = pd.concat(\n        [\n            grouped[[raw.user]].rename(columns={raw.user: user_col}),\n            wide_items,\n        ],\n        axis=1,\n    )\n\n    out_dir = os.path.dirname(os.path.abspath(filepath))\n    if out_dir and not os.path.exists(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n\n    out_df.to_csv(\n        filepath,\n        sep=col_sep,\n        header=header,\n        index=index,\n        decimal=decimal,\n    )\n\n    if verbose:\n        print(f\"Wide sequence dataset written to '{filepath}'\")\n</code></pre>"},{"location":"documentation/io/#datarec.io.writers.write_sequence_tabular_implicit","title":"<code>write_sequence_tabular_implicit(data, filepath, *, include_length_col=True, col_sep=' ', header=False, verbose=True)</code>","text":"<p>Writes sequential interaction data to a tabular-implicit format, where each row represents a sequence and no explicit user identifier is written.</p> <p>Output example (include_length_col=True, col_sep=\" \"):     3 10 20 30     2 11 42</p> <p>Notes: - Each unique user in RawData is treated as a sequence instance. - The user identifier is NOT written to file. - If include_length_col=True, the first token is the sequence length.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[RawData, DataRec]</code> <p>RawData or DataRec instance.</p> required <code>filepath</code> <code>str</code> <p>Output path.</p> required <code>include_length_col</code> <code>bool</code> <p>Whether to prepend the sequence length token.</p> <code>True</code> <code>col_sep</code> <code>str</code> <p>Token separator used within each row (must match the reader's <code>col_sep</code>).</p> <code>' '</code> <code>header</code> <code>bool</code> <p>Whether to write a header row (generally False for this format).</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to print a confirmation message.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>datarec/io/writers/sequences/tabular.py</code> <pre><code>def write_sequence_tabular_implicit(\n    data: Union[\"RawData\", \"DataRec\"],\n    filepath: str,\n    *,\n    include_length_col: bool = True,\n    col_sep: str = \" \",\n    header: bool = False,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"\n    Writes sequential interaction data to a tabular-implicit format, where each row\n    represents a sequence and no explicit user identifier is written.\n\n    Output example (include_length_col=True, col_sep=\" \"):\n        3 10 20 30\n        2 11 42\n\n    Notes:\n    - Each unique user in RawData is treated as a sequence instance.\n    - The user identifier is NOT written to file.\n    - If include_length_col=True, the first token is the sequence length.\n\n    Args:\n        data: RawData or DataRec instance.\n        filepath: Output path.\n        include_length_col: Whether to prepend the sequence length token.\n        col_sep: Token separator used within each row (must match the reader's `col_sep`).\n        header: Whether to write a header row (generally False for this format).\n        verbose: Whether to print a confirmation message.\n\n    Returns:\n        None\n    \"\"\"\n    raw = as_rawdata(data)\n\n    if raw.user is None or raw.item is None:\n        raise ValueError(\"RawData must define both user and item columns.\")\n\n    df = raw.data[[raw.user, raw.item]].dropna(subset=[raw.user, raw.item])\n\n    grouped = (\n        df.groupby(raw.user, sort=False)[raw.item]\n        .apply(list)\n        .reset_index(drop=True)\n    )\n\n    rows = []\n    for items in grouped:\n        items_str = [str(x).strip() for x in items if str(x).strip() != \"\"]\n        if not items_str:\n            continue\n        if include_length_col:\n            row_tokens = [str(len(items_str))] + items_str\n        else:\n            row_tokens = items_str\n        rows.append(col_sep.join(row_tokens))\n\n    if not rows:\n        raise ValueError(\"No valid sequences to write.\")\n\n    out_dir = os.path.dirname(os.path.abspath(filepath))\n    if out_dir and not os.path.exists(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n\n    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n        if header:\n            # This format typically has no header; if requested, write a minimal one.\n            f.write(\"sequence\\n\")\n        for row in rows:\n            f.write(row + \"\\n\")\n\n    if verbose:\n        print(f\"Implicit sequence dataset written to '{filepath}'\")\n</code></pre>"},{"location":"documentation/io/#datarec.io.writers.write_transactions_tabular","title":"<code>write_transactions_tabular(data, filepath, *, sep='\\t', header=True, decimal='.', include_user=True, include_item=True, include_rating=False, include_timestamp=False, user_col=None, item_col=None, rating_col=None, timestamp_col=None, index=False, engine=None, verbose=True)</code>","text":"<p>Writes transactional interaction data to a tabular file (CSV/TSV/etc.).</p> one interaction per row <p>user, item, [rating], [timestamp]</p> <p>This writer accepts either: - RawData - DataRec (converted via <code>.to_rawdata()</code>)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[RawData, DataRec]</code> <p>RawData or DataRec instance.</p> required <code>filepath</code> <code>str</code> <p>Output path.</p> required <code>sep</code> <code>str</code> <p>Column delimiter (e.g., '\\t', ',', ';').</p> <code>'\\t'</code> <code>header</code> <code>bool</code> <p>Whether to write column names.</p> <code>True</code> <code>decimal</code> <code>str</code> <p>Decimal separator passed to pandas.</p> <code>'.'</code> <code>include_user</code> <code>bool</code> <p>Whether to include the user column.</p> <code>True</code> <code>include_item</code> <code>bool</code> <p>Whether to include the item column.</p> <code>True</code> <code>include_rating</code> <code>bool</code> <p>Whether to include the rating column (if available).</p> <code>False</code> <code>include_timestamp</code> <code>bool</code> <p>Whether to include the timestamp column (if available).</p> <code>False</code> <code>user_col</code> <code>Optional[str]</code> <p>Output column name for user (optional rename).</p> <code>None</code> <code>item_col</code> <code>Optional[str]</code> <p>Output column name for item (optional rename).</p> <code>None</code> <code>rating_col</code> <code>Optional[str]</code> <p>Output column name for rating (optional rename).</p> <code>None</code> <code>timestamp_col</code> <code>Optional[str]</code> <p>Output column name for timestamp (optional rename).</p> <code>None</code> <code>index</code> <code>bool</code> <p>Whether to write the DataFrame index.</p> <code>False</code> <code>engine</code> <code>Optional[str]</code> <p>Optional pandas CSV engine hint.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print a confirmation message.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>datarec/io/writers/transactions/tabular.py</code> <pre><code>def write_transactions_tabular(\n    data: Union[\"RawData\", \"DataRec\"],\n    filepath: str,\n    *,\n    sep: str = \"\\t\",\n    header: bool = True,\n    decimal: str = \".\",\n    include_user: bool = True,\n    include_item: bool = True,\n    include_rating: bool = False,\n    include_timestamp: bool = False,\n    user_col: Optional[str] = None,\n    item_col: Optional[str] = None,\n    rating_col: Optional[str] = None,\n    timestamp_col: Optional[str] = None,\n    index: bool = False,\n    engine: Optional[str] = None,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"\n    Writes transactional interaction data to a tabular file (CSV/TSV/etc.).\n\n    Output format: one interaction per row\n        user, item, [rating], [timestamp]\n\n    This writer accepts either:\n    - RawData\n    - DataRec (converted via `.to_rawdata()`)\n\n    Args:\n        data: RawData or DataRec instance.\n        filepath: Output path.\n        sep: Column delimiter (e.g., '\\\\t', ',', ';').\n        header: Whether to write column names.\n        decimal: Decimal separator passed to pandas.\n        include_user: Whether to include the user column.\n        include_item: Whether to include the item column.\n        include_rating: Whether to include the rating column (if available).\n        include_timestamp: Whether to include the timestamp column (if available).\n        user_col: Output column name for user (optional rename).\n        item_col: Output column name for item (optional rename).\n        rating_col: Output column name for rating (optional rename).\n        timestamp_col: Output column name for timestamp (optional rename).\n        index: Whether to write the DataFrame index.\n        engine: Optional pandas CSV engine hint.\n        verbose: Whether to print a confirmation message.\n\n    Returns:\n        None\n    \"\"\"\n    raw = as_rawdata(data)\n\n    # Transactional exports should always contain user + item (coherent across formats)\n    if not include_user:\n        raise ValueError(\"Transactional export requires include_user=True.\")\n    if not include_item:\n        raise ValueError(\"Transactional export requires include_item=True.\")\n\n    if raw.user is None:\n        raise ValueError(\"User column is not defined in RawData.\")\n    if raw.item is None:\n        raise ValueError(\"Item column is not defined in RawData.\")\n\n    cols: List[object] = [raw.user, raw.item]\n    rename_map: Dict[object, str] = {}\n\n    if user_col is not None:\n        rename_map[raw.user] = user_col\n    if item_col is not None:\n        rename_map[raw.item] = item_col\n\n    if include_rating:\n        if raw.rating is None:\n            raise ValueError(\"Rating column requested but not defined in RawData.\")\n        cols.append(raw.rating)\n        if rating_col is not None:\n            rename_map[raw.rating] = rating_col\n\n    if include_timestamp:\n        if raw.timestamp is None:\n            raise ValueError(\"Timestamp column requested but not defined in RawData.\")\n        cols.append(raw.timestamp)\n        if timestamp_col is not None:\n            rename_map[raw.timestamp] = timestamp_col\n\n    df = raw.data[cols]\n    if rename_map:\n        df = df.rename(columns=rename_map)\n\n    out_dir = os.path.dirname(os.path.abspath(filepath))\n    if out_dir and not os.path.exists(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n\n    to_csv_kwargs = dict(sep=sep, header=header, index=index, decimal=decimal)\n    if engine is not None:\n        to_csv_kwargs[\"engine\"] = engine\n\n    df.to_csv(filepath, **to_csv_kwargs)\n\n    if verbose:\n        print(f\"Tabular dataset written to '{filepath}'\")\n</code></pre>"},{"location":"documentation/io/#datarec.io.writers.write_transactions_json","title":"<code>write_transactions_json(data, filepath, *, user_col='user', item_col='item', rating_col='rating', timestamp_col='timestamp', include_user=True, include_item=True, include_rating=False, include_timestamp=False, indent=2, ensure_ascii=False, verbose=True)</code>","text":"<p>Writes transactional interaction data as a single JSON array.</p> Source code in <code>datarec/io/writers/transactions/json.py</code> <pre><code>def write_transactions_json(\n    data: Union[\"RawData\", \"DataRec\"],\n    filepath: str,\n    *,\n    user_col: str = \"user\",\n    item_col: str = \"item\",\n    rating_col: str = \"rating\",\n    timestamp_col: str = \"timestamp\",\n    include_user: bool = True,\n    include_item: bool = True,\n    include_rating: bool = False,\n    include_timestamp: bool = False,\n    indent: Optional[int] = 2,\n    ensure_ascii: bool = False,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"\n    Writes transactional interaction data as a single JSON array.\n    \"\"\"\n    return write_transactions_json_base(\n        data,\n        filepath,\n        user_col=user_col,\n        item_col=item_col,\n        rating_col=rating_col,\n        timestamp_col=timestamp_col,\n        include_user=include_user,\n        include_item=include_item,\n        include_rating=include_rating,\n        include_timestamp=include_timestamp,\n        lines=False,\n        indent=indent,\n        ensure_ascii=ensure_ascii,\n        verbose=verbose,\n    )\n</code></pre>"},{"location":"documentation/io/#datarec.io.writers.write_transactions_jsonl","title":"<code>write_transactions_jsonl(data, filepath, *, user_col='user', item_col='item', rating_col='rating', timestamp_col='timestamp', include_user=True, include_item=True, include_rating=False, include_timestamp=False, ensure_ascii=False, verbose=True)</code>","text":"<p>Writes transactional interaction data as JSON Lines (one JSON object per line).</p> Source code in <code>datarec/io/writers/transactions/jsonl.py</code> <pre><code>def write_transactions_jsonl(\n    data: Union[\"RawData\", \"DataRec\"],\n    filepath: str,\n    *,\n    user_col: str = \"user\",\n    item_col: str = \"item\",\n    rating_col: str = \"rating\",\n    timestamp_col: str = \"timestamp\",\n    include_user: bool = True,\n    include_item: bool = True,\n    include_rating: bool = False,\n    include_timestamp: bool = False,\n    ensure_ascii: bool = False,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"\n    Writes transactional interaction data as JSON Lines (one JSON object per line).\n    \"\"\"\n    return write_transactions_json_base(\n        data,\n        filepath,\n        user_col=user_col,\n        item_col=item_col,\n        rating_col=rating_col,\n        timestamp_col=timestamp_col,\n        include_user=include_user,\n        include_item=include_item,\n        include_rating=include_rating,\n        include_timestamp=include_timestamp,\n        lines=True,\n        indent=None,\n        ensure_ascii=ensure_ascii,\n        verbose=verbose,\n    )\n</code></pre>"},{"location":"documentation/io/#datarec.io.paths.dataset_directory","title":"<code>dataset_directory(dataset_name, must_exist=False)</code>","text":"<p>Given the dataset name returns the dataset directory Args:     dataset_name (str): name of the dataset     must_exist (bool): flag for forcing to check if the folder exists</p> <p>Returns:</p> Type Description <code>str</code> <p>the path of the directory containing the dataset data</p> Source code in <code>datarec/io/paths.py</code> <pre><code>def dataset_directory(dataset_name: str, must_exist=False) -&gt; str:\n    \"\"\"\n    Given the dataset name returns the dataset directory\n    Args:\n        dataset_name (str): name of the dataset\n        must_exist (bool): flag for forcing to check if the folder exists\n\n    Returns:\n        (str): the path of the directory containing the dataset data\n    \"\"\"\n    dataset_dir = os.path.join(cache_dir(), dataset_name)\n    if must_exist and not os.path.exists(dataset_dir):\n        raise FileNotFoundError(f'Directory at {dataset_dir} not found. Please, check that dataset directory exists')\n    return os.path.abspath(dataset_dir)\n</code></pre>"},{"location":"documentation/io/#datarec.io.paths.dataset_version_directory","title":"<code>dataset_version_directory(dataset_name, dataset_version, must_exist=False)</code>","text":"<p>Given the dataset name and its version returns the dataset directory Args:     dataset_name (str): name of the dataset     version_name (str): version of the dataset     must_exist (bool): flag for forcing to check if the folder exists</p> <p>Returns:</p> Type Description <code>str</code> <p>the path of the directory containing the dataset data</p> Source code in <code>datarec/io/paths.py</code> <pre><code>def dataset_version_directory(dataset_name: str, dataset_version: str, must_exist=False) -&gt; str:\n    \"\"\"\n    Given the dataset name and its version returns the dataset directory\n    Args:\n        dataset_name (str): name of the dataset\n        version_name (str): version of the dataset\n        must_exist (bool): flag for forcing to check if the folder exists\n\n    Returns:\n        (str): the path of the directory containing the dataset data\n    \"\"\"\n    dataset_dir = os.path.join(dataset_directory(dataset_name), dataset_version)\n    if must_exist and not os.path.exists(dataset_dir):\n        raise FileNotFoundError(f'Directory at {dataset_dir} not found. Please, check that dataset directory exists')\n    return os.path.abspath(dataset_dir)\n</code></pre>"},{"location":"documentation/io/#datarec.io.paths.dataset_raw_directory","title":"<code>dataset_raw_directory(dataset_name, dataset_version=None)</code>","text":"<p>Given the dataset name returns the directory containing the raw data of the dataset Args:     dataset_name (str): name of the dataset     dataset_version (str): version of the dataset Returns:     (str): the path of the directory containing the raw data of the dataset</p> Source code in <code>datarec/io/paths.py</code> <pre><code>def dataset_raw_directory(dataset_name: str, dataset_version: str=None) -&gt; str:\n    \"\"\"\n    Given the dataset name returns the directory containing the raw data of the dataset\n    Args:\n        dataset_name (str): name of the dataset\n        dataset_version (str): version of the dataset\n    Returns:\n        (str): the path of the directory containing the raw data of the dataset\n    \"\"\"\n    if dataset_version:\n        return os.path.join(dataset_version_directory(dataset_name, dataset_version), RAW_DATA_FOLDER)\n    return os.path.join(dataset_directory(dataset_name), RAW_DATA_FOLDER)\n</code></pre>"},{"location":"documentation/io/#datarec.io.paths.dataset_processed_directory","title":"<code>dataset_processed_directory(dataset_name)</code>","text":"<p>Given the dataset name returns the directory containing the processed data of the dataset Args:     dataset_name (str): name of the dataset</p> <p>Returns:</p> Type Description <code>str</code> <p>the path of the directory containing the processed data of the dataset</p> Source code in <code>datarec/io/paths.py</code> <pre><code>def dataset_processed_directory(dataset_name: str) -&gt; str:\n    \"\"\"\n    Given the dataset name returns the directory containing the processed data of the dataset\n    Args:\n        dataset_name (str): name of the dataset\n\n    Returns:\n        (str): the path of the directory containing the processed data of the dataset\n    \"\"\"\n    return os.path.join(dataset_directory(dataset_name), PROCESSED_DATA_FOLDER)\n</code></pre>"},{"location":"documentation/io/#datarec.io.paths.dataset_filepath","title":"<code>dataset_filepath(dataset_name)</code>","text":"<p>Given the dataset name returns the path of the dataset data Args:     dataset_name (str): name of the dataset</p> <p>Returns:</p> Type Description <code>str</code> <p>the path of the dataset data</p> Source code in <code>datarec/io/paths.py</code> <pre><code>def dataset_filepath(dataset_name: str) -&gt; str:\n    \"\"\"\n    Given the dataset name returns the path of the dataset data\n    Args:\n        dataset_name (str): name of the dataset\n\n    Returns:\n        (str): the path of the dataset data\n    \"\"\"\n    return os.path.join(dataset_directory(dataset_name), DATASET_NAME)\n</code></pre>"},{"location":"documentation/io/#datarec.io.paths.registry_dataset_filepath","title":"<code>registry_dataset_filepath(dataset_name)</code>","text":"<p>Given the dataset name returns the path of the dataset configuration file in the dataset registry Args:     dataset_name (str): name of the dataset Returns:     (str): the path of the dataset configuration file</p> Source code in <code>datarec/io/paths.py</code> <pre><code>def registry_dataset_filepath(dataset_name: str) -&gt; str:\n    \"\"\"\n    Given the dataset name returns the path of the dataset configuration file in the dataset registry\n    Args:\n        dataset_name (str): name of the dataset\n    Returns:\n        (str): the path of the dataset configuration file\n    \"\"\"\n    return os.path.join(REGISTRY_DATASETS_FOLDER, dataset_name) + '.yml'\n</code></pre>"},{"location":"documentation/io/#datarec.io.paths.registry_version_filepath","title":"<code>registry_version_filepath(dataset_name, dataset_version)</code>","text":"<p>Given the dataset name returns the path of the dataset configuration file in the dataset registry Args:     dataset_name (str): name of the dataset     dataset_version (str): version of the dataset Returns:     (str): the path of the dataset configuration file</p> Source code in <code>datarec/io/paths.py</code> <pre><code>def registry_version_filepath(dataset_name: str, dataset_version: str) -&gt; str:\n    \"\"\"\n    Given the dataset name returns the path of the dataset configuration file in the dataset registry\n    Args:\n        dataset_name (str): name of the dataset\n        dataset_version (str): version of the dataset\n    Returns:\n        (str): the path of the dataset configuration file\n    \"\"\"\n    return os.path.join(REGISTRY_VERSIONS_FOLDER, dataset_name+'_'+dataset_version) + '.yml'\n</code></pre>"},{"location":"documentation/io/#datarec.io.paths.registry_metrics_filepath","title":"<code>registry_metrics_filepath(dataset_name, dataset_version)</code>","text":"<p>Given dataset name and version, return the path of the precomputed metrics file in the registry metrics folder.</p> Source code in <code>datarec/io/paths.py</code> <pre><code>def registry_metrics_filepath(dataset_name: str, dataset_version: str) -&gt; str:\n    \"\"\"\n    Given dataset name and version, return the path of the precomputed metrics file\n    in the registry metrics folder.\n    \"\"\"\n    return os.path.join(REGISTRY_METRICS_FOLDER, f\"{dataset_name}_{dataset_version}.yml\")\n</code></pre>"},{"location":"documentation/io/#datarec.io.paths.pickle_version_filepath","title":"<code>pickle_version_filepath(dataset_name, dataset_version)</code>","text":"<p>Given the dataset name and version returns the path of the pickled version of the dataset Args:     dataset_name (str): name of the dataset     dataset_version (str): version of the dataset Returns:     (str): the path of the pickled version of the dataset</p> Source code in <code>datarec/io/paths.py</code> <pre><code>def pickle_version_filepath(dataset_name: str, dataset_version: str) -&gt; str:\n    \"\"\"\n    Given the dataset name and version returns the path of the pickled version of the dataset\n    Args:\n        dataset_name (str): name of the dataset\n        dataset_version (str): version of the dataset\n    Returns:\n        (str): the path of the pickled version of the dataset\n    \"\"\"\n    return os.path.join(dataset_version_directory(dataset_name=dataset_name, dataset_version=dataset_version), dataset_name+'_'+dataset_version) + '.pkl'\n</code></pre>"},{"location":"documentation/io/#framework-interoperability","title":"Framework Interoperability","text":"<p>This section covers the tools used to export <code>DataRec</code> datasets into formats compatible with other popular recommender systems libraries.</p>"},{"location":"documentation/io/#datarec.io.frameworks.exporter.FrameworkExporter","title":"<code>FrameworkExporter</code>","text":"<p>Exporter for converting RawData datasets to external recommender system frameworks.</p> <p>Provides methods to format a <code>RawData</code> object according to the expected schema of supported libraries (e.g., Cornac, RecBole).</p> Source code in <code>datarec/io/frameworks/exporter.py</code> <pre><code>class FrameworkExporter:\n    \"\"\"\n    Exporter for converting RawData datasets to external recommender system frameworks.\n\n    Provides methods to format a `RawData` object according to\n    the expected schema of supported libraries (e.g., Cornac, RecBole).\n\n    \"\"\"\n\n    def __init__(self, output_path, user=True, item=True, rating=True, timestamp=False):\n        \"\"\"\n        Initialize a FrameworkExporter object.\n        Args:\n            output_path (str): Path where to save the output file.\n            user (bool): Whether to write the user information. If True, the user information will be written in the file.\n            item (bool): Whether to write the item information. If True, the item information will be written in the file.\n            rating (bool): Whether to write the rating information. If True, the rating information will be written in the file.\n            timestamp (bool): Whether to write the timestamp information. If True, the timestamp information will be written in the file.\n        \"\"\"\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.path = output_path\n        self.user: bool = user\n        self.item: bool = item\n        self.rating: bool = rating\n        self.timestamp: bool = timestamp\n\n    def to_clayrs(self, data: RawData):\n        \"\"\"\n        Export to ClayRS format.\n        Args:\n            data (RawData): RawData object to convert to ClayRS format.\n        \"\"\"\n        write_transactions_tabular(data=data, filepath=self.path, sep=',', header=False, \n                                   include_user=self.user, include_item=self.item, include_rating=self.rating, include_timestamp=self.timestamp)\n\n        ClayRS(timestamp=self.timestamp, path=self.path).info()\n\n    def to_cornac(self, data: RawData):\n        \"\"\"\n        Export to Cornac format.\n        Args:\n            data (RawData): RawData object to convert to Cornac format.\n        \"\"\"\n        write_transactions_tabular(data=data, filepath=self.path, sep=',', header=False, \n                                   include_user=self.user, include_item=self.item, include_rating=self.rating, include_timestamp=self.timestamp)\n        Cornac(timestamp=self.timestamp, path=self.path).info()\n\n    def to_daisyrec(self, data: RawData):\n        \"\"\"\n        Export to DaisyRec format.\n        Args:\n            data (RawData): RawData object to convert to DaisyRec format.\n        \"\"\"\n        write_transactions_tabular(data=data, filepath=self.path, sep=',', header=False, \n                                   include_user=self.user, include_item=self.item, include_rating=self.rating, include_timestamp=self.timestamp)\n\n        DaisyRec(timestamp=self.timestamp, path=self.path).info()\n\n    def to_lenskit(self, data: RawData):\n        \"\"\"\n        Export to LensKit format.\n        Args:\n            data (RawData): RawData object to convert to LensKit format.\n        \"\"\"\n        data.data.rename(columns={data.user: \"user\", data.item: \"item\", data.rating: \"rating\"}, inplace=True)\n        data.user = \"user\"\n        data.item = \"item\"\n        data.rating = \"rating\"\n\n        if self.timestamp:\n            data.data.rename(columns={data.timestamp: \"timestamp\"}, inplace=True)\n            data.timestamp = \"timestamp\"\n            data.rating = \"rating\"\n\n        write_transactions_tabular(data=data, filepath=self.path, sep=',', header=False, \n                                   include_user=self.user, include_item=self.item, include_rating=self.rating, include_timestamp=self.timestamp)\n\n        LensKit(timestamp=self.timestamp, path=self.path).info()\n\n    def to_recbole(self, data: RawData):\n        \"\"\"\n        Export to RecBole format.\n        Args:\n            data (RawData): RawData object to convert to RecBole format.\n        \"\"\"\n\n        data.data.rename(columns={data.user: \"user: token\", data.item: \"item: token\",\n                                  data.rating: \"rating: float\"}, inplace=True)\n        data.user = \"user: token\"\n        data.item = \"item: token\"\n        data.rating = \"rating: float\"\n\n        if self.timestamp:\n            data.data.rename(columns={data.timestamp: \"timestamp\"}, inplace=True)\n            data.timestamp = \"timestamp:float\"\n\n        frmk = RecBole(timestamp=self.timestamp, path=self.path)\n        frmk.info()\n        write_transactions_tabular(data=data, filepath=frmk.path, sep=',', header=False, \n                                   include_user=self.user, include_item=self.item, include_rating=self.rating, include_timestamp=self.timestamp)\n\n    def to_rechorus(self, train_data: RawData, test_data: RawData, val_data: RawData):\n        \"\"\"\n        Export to Rechus format.\n        Args:\n            train_data (RawData): Training data as RawData object to convert to Rechus format.\n            test_data (RawData): Test data as RawData object to convert to Rechus format.\n            val_data (RawData): Validation data as RawData object to convert to Rechus format.\n        \"\"\"\n        # user_id\titem_id\ttime\n        if self.rating:\n            print('Ratings will be interpreted as implicit interactions.')\n            self.rating = False\n\n        frmk = ReChorus(timestamp=self.timestamp, path=self.path)\n\n        for data, name in zip([train_data, test_data, val_data], ['train.csv', 'dev.csv', 'test.csv']):\n            data.data.rename(columns={data.user: \"user_id\", data.item: \"item_id\"}, inplace=True)\n            data.user = \"user_id\"\n            data.item = \"item_id\"\n\n            if self.timestamp:\n                data.data.rename(columns={data.timestamp: \"time\"}, inplace=True)\n                data.timestamp = \"time\"\n\n            path = os.path.join(frmk.directory, name)\n            write_transactions_tabular(data=data, filepath=self.path, sep=',', header=False, \n                                   include_user=self.user, include_item=self.item, include_rating=self.rating, include_timestamp=self.timestamp)\n\n        frmk.info()\n\n    def to_recpack(self, data: RawData):\n        \"\"\"\n        Export to RecPack format.\n        Args:\n            data (RawData): RawData object to convert to RecPack format.\n        \"\"\"\n\n        if self.rating:\n            print('Ratings will be interpreted as implicit interactions.')\n            self.rating = False\n\n        frmk = RecPack(timestamp=self.timestamp, path=self.path)\n\n        data.data.rename(columns={data.user: \"userId\", data.item: \"itemId\"}, inplace=True)\n        data.user = \"userId\"\n        data.item = \"itemId\"\n        if self.timestamp:\n            data.data.rename(columns={data.timestamp: \"timestamp\"}, inplace=True)\n            data.timestamp = \"timestamp\"\n\n        write_transactions_tabular(data=data, filepath=self.path, sep=',', header=False, \n                                   include_user=self.user, include_item=self.item, include_rating=self.rating, include_timestamp=self.timestamp)\n\n\n        frmk.info()\n\n    def to_recommenders(self, data: RawData):\n        \"\"\"\n        Export to Recommenders format.\n        Args:\n            data (RawData): RawData object to convert to Recommenders format.\n        \"\"\"\n\n        frmk = Recommenders(timestamp=self.timestamp, path=self.path)\n\n        data.data.rename(columns={data.user: \"user\", data.item: \"item\", data.rating: \"rating\"}, inplace=True)\n        data.user = \"item\"\n        data.item = \"rating\"\n        data.rating = 'rating'\n        if self.timestamp:\n            data.data.rename(columns={data.timestamp: \"timestamp\"}, inplace=True)\n            data.timestamp = \"timestamp\"\n\n        write_transactions_tabular(data=data, filepath=self.path, sep=',', header=False, \n                                   include_user=self.user, include_item=self.item, include_rating=self.rating, include_timestamp=self.timestamp)\n\n\n        frmk.info()\n\n    def to_elliot(self, train_data: DataRec, test_data: DataRec, val_data: DataRec):\n        \"\"\"\n        Export to Elliot format.\n        Args:\n            train_data (DataRec): Training data as DataRec object to convert to Elliot format.\n            test_data (DataRec): Test data as DataRec object to convert to Elliot format.\n            val_data (DataRec): Validation data as DataRec object to convert to Elliot format.\n        \"\"\"\n\n        frmk = Elliot(timestamp=self.timestamp, path=self.path)\n\n        for data, name in zip([train_data.to_rawdata(), test_data.to_rawdata(), val_data.to_rawdata()],\n                              [frmk.train_path, frmk.test_path, frmk.val_path]):\n            columns_order = [data.user, data.item, data.rating]\n            if self.timestamp:\n                columns_order.append(data.timestamp)\n\n            write_transactions_tabular(data=data, filepath=name, sep='\\t', header=False,\n                          include_user=self.user, include_item=self.item, include_rating=self.rating, include_timestamp=self.timestamp)\n\n        frmk.info()\n        train_data.pipeline.add_step(\"export\", \"Elliot\", self.params)\n        test_data.pipeline.add_step(\"export\", \"Elliot\", self.params)\n        val_data.pipeline.add_step(\"export\", \"Elliot\", self.params)\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.exporter.FrameworkExporter.__init__","title":"<code>__init__(output_path, user=True, item=True, rating=True, timestamp=False)</code>","text":"<p>Initialize a FrameworkExporter object. Args:     output_path (str): Path where to save the output file.     user (bool): Whether to write the user information. If True, the user information will be written in the file.     item (bool): Whether to write the item information. If True, the item information will be written in the file.     rating (bool): Whether to write the rating information. If True, the rating information will be written in the file.     timestamp (bool): Whether to write the timestamp information. If True, the timestamp information will be written in the file.</p> Source code in <code>datarec/io/frameworks/exporter.py</code> <pre><code>def __init__(self, output_path, user=True, item=True, rating=True, timestamp=False):\n    \"\"\"\n    Initialize a FrameworkExporter object.\n    Args:\n        output_path (str): Path where to save the output file.\n        user (bool): Whether to write the user information. If True, the user information will be written in the file.\n        item (bool): Whether to write the item information. If True, the item information will be written in the file.\n        rating (bool): Whether to write the rating information. If True, the rating information will be written in the file.\n        timestamp (bool): Whether to write the timestamp information. If True, the timestamp information will be written in the file.\n    \"\"\"\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.path = output_path\n    self.user: bool = user\n    self.item: bool = item\n    self.rating: bool = rating\n    self.timestamp: bool = timestamp\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.exporter.FrameworkExporter.to_clayrs","title":"<code>to_clayrs(data)</code>","text":"<p>Export to ClayRS format. Args:     data (RawData): RawData object to convert to ClayRS format.</p> Source code in <code>datarec/io/frameworks/exporter.py</code> <pre><code>def to_clayrs(self, data: RawData):\n    \"\"\"\n    Export to ClayRS format.\n    Args:\n        data (RawData): RawData object to convert to ClayRS format.\n    \"\"\"\n    write_transactions_tabular(data=data, filepath=self.path, sep=',', header=False, \n                               include_user=self.user, include_item=self.item, include_rating=self.rating, include_timestamp=self.timestamp)\n\n    ClayRS(timestamp=self.timestamp, path=self.path).info()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.exporter.FrameworkExporter.to_cornac","title":"<code>to_cornac(data)</code>","text":"<p>Export to Cornac format. Args:     data (RawData): RawData object to convert to Cornac format.</p> Source code in <code>datarec/io/frameworks/exporter.py</code> <pre><code>def to_cornac(self, data: RawData):\n    \"\"\"\n    Export to Cornac format.\n    Args:\n        data (RawData): RawData object to convert to Cornac format.\n    \"\"\"\n    write_transactions_tabular(data=data, filepath=self.path, sep=',', header=False, \n                               include_user=self.user, include_item=self.item, include_rating=self.rating, include_timestamp=self.timestamp)\n    Cornac(timestamp=self.timestamp, path=self.path).info()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.exporter.FrameworkExporter.to_daisyrec","title":"<code>to_daisyrec(data)</code>","text":"<p>Export to DaisyRec format. Args:     data (RawData): RawData object to convert to DaisyRec format.</p> Source code in <code>datarec/io/frameworks/exporter.py</code> <pre><code>def to_daisyrec(self, data: RawData):\n    \"\"\"\n    Export to DaisyRec format.\n    Args:\n        data (RawData): RawData object to convert to DaisyRec format.\n    \"\"\"\n    write_transactions_tabular(data=data, filepath=self.path, sep=',', header=False, \n                               include_user=self.user, include_item=self.item, include_rating=self.rating, include_timestamp=self.timestamp)\n\n    DaisyRec(timestamp=self.timestamp, path=self.path).info()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.exporter.FrameworkExporter.to_lenskit","title":"<code>to_lenskit(data)</code>","text":"<p>Export to LensKit format. Args:     data (RawData): RawData object to convert to LensKit format.</p> Source code in <code>datarec/io/frameworks/exporter.py</code> <pre><code>def to_lenskit(self, data: RawData):\n    \"\"\"\n    Export to LensKit format.\n    Args:\n        data (RawData): RawData object to convert to LensKit format.\n    \"\"\"\n    data.data.rename(columns={data.user: \"user\", data.item: \"item\", data.rating: \"rating\"}, inplace=True)\n    data.user = \"user\"\n    data.item = \"item\"\n    data.rating = \"rating\"\n\n    if self.timestamp:\n        data.data.rename(columns={data.timestamp: \"timestamp\"}, inplace=True)\n        data.timestamp = \"timestamp\"\n        data.rating = \"rating\"\n\n    write_transactions_tabular(data=data, filepath=self.path, sep=',', header=False, \n                               include_user=self.user, include_item=self.item, include_rating=self.rating, include_timestamp=self.timestamp)\n\n    LensKit(timestamp=self.timestamp, path=self.path).info()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.exporter.FrameworkExporter.to_recbole","title":"<code>to_recbole(data)</code>","text":"<p>Export to RecBole format. Args:     data (RawData): RawData object to convert to RecBole format.</p> Source code in <code>datarec/io/frameworks/exporter.py</code> <pre><code>def to_recbole(self, data: RawData):\n    \"\"\"\n    Export to RecBole format.\n    Args:\n        data (RawData): RawData object to convert to RecBole format.\n    \"\"\"\n\n    data.data.rename(columns={data.user: \"user: token\", data.item: \"item: token\",\n                              data.rating: \"rating: float\"}, inplace=True)\n    data.user = \"user: token\"\n    data.item = \"item: token\"\n    data.rating = \"rating: float\"\n\n    if self.timestamp:\n        data.data.rename(columns={data.timestamp: \"timestamp\"}, inplace=True)\n        data.timestamp = \"timestamp:float\"\n\n    frmk = RecBole(timestamp=self.timestamp, path=self.path)\n    frmk.info()\n    write_transactions_tabular(data=data, filepath=frmk.path, sep=',', header=False, \n                               include_user=self.user, include_item=self.item, include_rating=self.rating, include_timestamp=self.timestamp)\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.exporter.FrameworkExporter.to_rechorus","title":"<code>to_rechorus(train_data, test_data, val_data)</code>","text":"<p>Export to Rechus format. Args:     train_data (RawData): Training data as RawData object to convert to Rechus format.     test_data (RawData): Test data as RawData object to convert to Rechus format.     val_data (RawData): Validation data as RawData object to convert to Rechus format.</p> Source code in <code>datarec/io/frameworks/exporter.py</code> <pre><code>def to_rechorus(self, train_data: RawData, test_data: RawData, val_data: RawData):\n    \"\"\"\n    Export to Rechus format.\n    Args:\n        train_data (RawData): Training data as RawData object to convert to Rechus format.\n        test_data (RawData): Test data as RawData object to convert to Rechus format.\n        val_data (RawData): Validation data as RawData object to convert to Rechus format.\n    \"\"\"\n    # user_id\titem_id\ttime\n    if self.rating:\n        print('Ratings will be interpreted as implicit interactions.')\n        self.rating = False\n\n    frmk = ReChorus(timestamp=self.timestamp, path=self.path)\n\n    for data, name in zip([train_data, test_data, val_data], ['train.csv', 'dev.csv', 'test.csv']):\n        data.data.rename(columns={data.user: \"user_id\", data.item: \"item_id\"}, inplace=True)\n        data.user = \"user_id\"\n        data.item = \"item_id\"\n\n        if self.timestamp:\n            data.data.rename(columns={data.timestamp: \"time\"}, inplace=True)\n            data.timestamp = \"time\"\n\n        path = os.path.join(frmk.directory, name)\n        write_transactions_tabular(data=data, filepath=self.path, sep=',', header=False, \n                               include_user=self.user, include_item=self.item, include_rating=self.rating, include_timestamp=self.timestamp)\n\n    frmk.info()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.exporter.FrameworkExporter.to_recpack","title":"<code>to_recpack(data)</code>","text":"<p>Export to RecPack format. Args:     data (RawData): RawData object to convert to RecPack format.</p> Source code in <code>datarec/io/frameworks/exporter.py</code> <pre><code>def to_recpack(self, data: RawData):\n    \"\"\"\n    Export to RecPack format.\n    Args:\n        data (RawData): RawData object to convert to RecPack format.\n    \"\"\"\n\n    if self.rating:\n        print('Ratings will be interpreted as implicit interactions.')\n        self.rating = False\n\n    frmk = RecPack(timestamp=self.timestamp, path=self.path)\n\n    data.data.rename(columns={data.user: \"userId\", data.item: \"itemId\"}, inplace=True)\n    data.user = \"userId\"\n    data.item = \"itemId\"\n    if self.timestamp:\n        data.data.rename(columns={data.timestamp: \"timestamp\"}, inplace=True)\n        data.timestamp = \"timestamp\"\n\n    write_transactions_tabular(data=data, filepath=self.path, sep=',', header=False, \n                               include_user=self.user, include_item=self.item, include_rating=self.rating, include_timestamp=self.timestamp)\n\n\n    frmk.info()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.exporter.FrameworkExporter.to_recommenders","title":"<code>to_recommenders(data)</code>","text":"<p>Export to Recommenders format. Args:     data (RawData): RawData object to convert to Recommenders format.</p> Source code in <code>datarec/io/frameworks/exporter.py</code> <pre><code>def to_recommenders(self, data: RawData):\n    \"\"\"\n    Export to Recommenders format.\n    Args:\n        data (RawData): RawData object to convert to Recommenders format.\n    \"\"\"\n\n    frmk = Recommenders(timestamp=self.timestamp, path=self.path)\n\n    data.data.rename(columns={data.user: \"user\", data.item: \"item\", data.rating: \"rating\"}, inplace=True)\n    data.user = \"item\"\n    data.item = \"rating\"\n    data.rating = 'rating'\n    if self.timestamp:\n        data.data.rename(columns={data.timestamp: \"timestamp\"}, inplace=True)\n        data.timestamp = \"timestamp\"\n\n    write_transactions_tabular(data=data, filepath=self.path, sep=',', header=False, \n                               include_user=self.user, include_item=self.item, include_rating=self.rating, include_timestamp=self.timestamp)\n\n\n    frmk.info()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.exporter.FrameworkExporter.to_elliot","title":"<code>to_elliot(train_data, test_data, val_data)</code>","text":"<p>Export to Elliot format. Args:     train_data (DataRec): Training data as DataRec object to convert to Elliot format.     test_data (DataRec): Test data as DataRec object to convert to Elliot format.     val_data (DataRec): Validation data as DataRec object to convert to Elliot format.</p> Source code in <code>datarec/io/frameworks/exporter.py</code> <pre><code>def to_elliot(self, train_data: DataRec, test_data: DataRec, val_data: DataRec):\n    \"\"\"\n    Export to Elliot format.\n    Args:\n        train_data (DataRec): Training data as DataRec object to convert to Elliot format.\n        test_data (DataRec): Test data as DataRec object to convert to Elliot format.\n        val_data (DataRec): Validation data as DataRec object to convert to Elliot format.\n    \"\"\"\n\n    frmk = Elliot(timestamp=self.timestamp, path=self.path)\n\n    for data, name in zip([train_data.to_rawdata(), test_data.to_rawdata(), val_data.to_rawdata()],\n                          [frmk.train_path, frmk.test_path, frmk.val_path]):\n        columns_order = [data.user, data.item, data.rating]\n        if self.timestamp:\n            columns_order.append(data.timestamp)\n\n        write_transactions_tabular(data=data, filepath=name, sep='\\t', header=False,\n                      include_user=self.user, include_item=self.item, include_rating=self.rating, include_timestamp=self.timestamp)\n\n    frmk.info()\n    train_data.pipeline.add_step(\"export\", \"Elliot\", self.params)\n    test_data.pipeline.add_step(\"export\", \"Elliot\", self.params)\n    val_data.pipeline.add_step(\"export\", \"Elliot\", self.params)\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.manager.Framework","title":"<code>Framework</code>","text":"<p>Base class for all framework exporters.</p> Source code in <code>datarec/io/frameworks/manager.py</code> <pre><code>class Framework:\n    \"\"\"\n    Base class for all framework exporters.\n    \"\"\"\n    FRAMEWORK_NAME = None\n\n    PAPER = None\n\n    DOI = None\n\n    CITATION = None\n\n    CODE = None\n\n    REPOSITORY = None\n\n    DOC = None\n\n    def info_code(self):\n        \"\"\"\n        Print example code for integrating this framework with DataRec.\n        \"\"\"\n        print(f\"How to use {self.FRAMEWORK_NAME} with DataRec:\\n\" + self.CODE)\n\n    def info(self):\n        \"\"\"\n        Print citation information for the framework including: paper name, DOI and bibtex citation.\n        Print additional information such as: example code for integrating this framework with DataRec,\n        repository URL and framework documentation URL.\n        Returns:\n\n        \"\"\"\n        if self.FRAMEWORK_NAME is None:\n            raise AttributeError\n\n        print(f\"If you are going to use {self.FRAMEWORK_NAME} don't forget to cite the paper!\")\n\n        if self.PAPER:\n            print(f'Paper: \\'{self.PAPER}\\'')\n        if self.DOI:\n            print(f'DOI: {self.DOI}')\n        if self.CITATION:\n            print(f'Bib text from dblp.org:\\n {self.CITATION}')\n\n        if self.CODE:\n            print(\n                '\\n================================================ CODE EXAMPLE ================================================\\n')\n            self.info_code()\n            print(\n                '==============================================================================================================\\n')\n\n        if self.REPOSITORY:\n            print(f'For more information check {self.FRAMEWORK_NAME} repository: \\'{self.REPOSITORY}\\'')\n\n        if self.DOC:\n            print(f'More documentation on how to use {self.FRAMEWORK_NAME} at \\'{self.DOC}\\'')\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.manager.Framework.info_code","title":"<code>info_code()</code>","text":"<p>Print example code for integrating this framework with DataRec.</p> Source code in <code>datarec/io/frameworks/manager.py</code> <pre><code>def info_code(self):\n    \"\"\"\n    Print example code for integrating this framework with DataRec.\n    \"\"\"\n    print(f\"How to use {self.FRAMEWORK_NAME} with DataRec:\\n\" + self.CODE)\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.manager.Framework.info","title":"<code>info()</code>","text":"<p>Print citation information for the framework including: paper name, DOI and bibtex citation. Print additional information such as: example code for integrating this framework with DataRec, repository URL and framework documentation URL. Returns:</p> Source code in <code>datarec/io/frameworks/manager.py</code> <pre><code>def info(self):\n    \"\"\"\n    Print citation information for the framework including: paper name, DOI and bibtex citation.\n    Print additional information such as: example code for integrating this framework with DataRec,\n    repository URL and framework documentation URL.\n    Returns:\n\n    \"\"\"\n    if self.FRAMEWORK_NAME is None:\n        raise AttributeError\n\n    print(f\"If you are going to use {self.FRAMEWORK_NAME} don't forget to cite the paper!\")\n\n    if self.PAPER:\n        print(f'Paper: \\'{self.PAPER}\\'')\n    if self.DOI:\n        print(f'DOI: {self.DOI}')\n    if self.CITATION:\n        print(f'Bib text from dblp.org:\\n {self.CITATION}')\n\n    if self.CODE:\n        print(\n            '\\n================================================ CODE EXAMPLE ================================================\\n')\n        self.info_code()\n        print(\n            '==============================================================================================================\\n')\n\n    if self.REPOSITORY:\n        print(f'For more information check {self.FRAMEWORK_NAME} repository: \\'{self.REPOSITORY}\\'')\n\n    if self.DOC:\n        print(f'More documentation on how to use {self.FRAMEWORK_NAME} at \\'{self.DOC}\\'')\n</code></pre>"},{"location":"documentation/io/#clayrs","title":"ClayRS","text":""},{"location":"documentation/io/#datarec.io.frameworks.clayrs.clayrs.ClayRS","title":"<code>ClayRS</code>","text":"<p>               Bases: <code>Framework</code></p> <p>ClayRS framework adapter.</p> <p>Provide metadata, citation, and usage examples for ClayRS framework.</p> Source code in <code>datarec/io/frameworks/clayrs/clayrs.py</code> <pre><code>class ClayRS(Framework):\n    \"\"\"\n    ClayRS framework adapter.\n\n    Provide metadata, citation, and usage examples for ClayRS framework.\n    \"\"\"\n\n    def __init__(self, timestamp, path):\n        \"\"\"\n        Initialize ClayRS adapter.\n        Args:\n            timestamp (bool): Whether timestamps are included.\n            path (str): Path where the ClayRS-compatible dataset is stored.\n        \"\"\"\n        self.timestamp = timestamp\n        self.path = path\n\n    FRAMEWORK_NAME = 'ClayRS'\n\n    REPOSITORY = 'https://github.com/swapUniba/ClayRS/tree/master'\n\n    PAPER = \"\"\"ClayRS: An end-to-end framework for reproducible knowledge-aware recommender systems\"\"\"\n\n    DOI = \"https://doi.org/10.1016/j.is.2023.102273\"\n\n    CITATION = \"\"\"\n            @article{DBLP:journals/is/LopsPMSS23,\n              author       = {Pasquale Lops and\n                              Marco Polignano and\n                              Cataldo Musto and\n                              Antonio Silletti and\n                              Giovanni Semeraro},\n              title        = {ClayRS: An end-to-end framework for reproducible knowledge-aware recommender\n                              systems},\n              journal      = {Inf. Syst.},\n              volume       = {119},\n              pages        = {102273},\n              year         = {2023},\n              url          = {https://doi.org/10.1016/j.is.2023.102273},\n              doi          = {10.1016/J.IS.2023.102273},\n              timestamp    = {Mon, 05 Feb 2024 20:19:36 +0100},\n              biburl       = {https://dblp.org/rec/journals/is/LopsPMSS23.bib},\n              bibsource    = {dblp computer science bibliography, https://dblp.org}\n            }\"\"\"\n\n    CODE = \"\"\"\n    from clayrs import content_analyzer \n\n    ratings = content_analyzer.Ratings(content_analyzer.CSVFile(YOUR_PATH_HERE), timestamp_column=3)\n    \"\"\"\n\n    DOC = 'https://swapuniba.github.io/ClayRS/'\n\n    def info_code(self):\n        \"\"\"\n        Provide the code to use in ClayRS to run experiments.\n        \"\"\"\n        if self.timestamp:\n            self.CODE = \"\"\"\n    from clayrs import content_analyzer \n\n    ratings = content_analyzer.Ratings(content_analyzer.CSVFile('{path}'), timestamp_column=3)\n    \"\"\".format(path=self.path)\n        else:\n            self.CODE = \"\"\"\n    from clayrs import content_analyzer \n\n    ratings = content_analyzer.Ratings(content_analyzer.CSVFile('{path}'))\n    \"\"\".format(path=self.path)\n        super().info_code()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.clayrs.clayrs.ClayRS.__init__","title":"<code>__init__(timestamp, path)</code>","text":"<p>Initialize ClayRS adapter. Args:     timestamp (bool): Whether timestamps are included.     path (str): Path where the ClayRS-compatible dataset is stored.</p> Source code in <code>datarec/io/frameworks/clayrs/clayrs.py</code> <pre><code>def __init__(self, timestamp, path):\n    \"\"\"\n    Initialize ClayRS adapter.\n    Args:\n        timestamp (bool): Whether timestamps are included.\n        path (str): Path where the ClayRS-compatible dataset is stored.\n    \"\"\"\n    self.timestamp = timestamp\n    self.path = path\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.clayrs.clayrs.ClayRS.info_code","title":"<code>info_code()</code>","text":"<p>Provide the code to use in ClayRS to run experiments.</p> Source code in <code>datarec/io/frameworks/clayrs/clayrs.py</code> <pre><code>def info_code(self):\n    \"\"\"\n    Provide the code to use in ClayRS to run experiments.\n    \"\"\"\n    if self.timestamp:\n        self.CODE = \"\"\"\nfrom clayrs import content_analyzer \n\nratings = content_analyzer.Ratings(content_analyzer.CSVFile('{path}'), timestamp_column=3)\n\"\"\".format(path=self.path)\n    else:\n        self.CODE = \"\"\"\nfrom clayrs import content_analyzer \n\nratings = content_analyzer.Ratings(content_analyzer.CSVFile('{path}'))\n\"\"\".format(path=self.path)\n    super().info_code()\n</code></pre>"},{"location":"documentation/io/#cornac","title":"Cornac","text":""},{"location":"documentation/io/#datarec.io.frameworks.cornac.cornac.Cornac","title":"<code>Cornac</code>","text":"<p>               Bases: <code>Framework</code></p> <p>Cornac framework adapter.</p> <p>Provide metadata, citation, and usage examples for Cornac framework.</p> Source code in <code>datarec/io/frameworks/cornac/cornac.py</code> <pre><code>class Cornac(Framework):\n    \"\"\"\n    Cornac framework adapter.\n\n    Provide metadata, citation, and usage examples for Cornac framework.\n    \"\"\"\n\n    def __init__(self, timestamp, path):\n        \"\"\"\n        Initialize Cornac adapter.\n        Args:\n            timestamp (bool): Whether timestamps are included.\n            path (str): Path where the Cornac-compatible dataset is stored.\n        \"\"\"\n        self.timestamp = timestamp\n        self.path = path\n\n    FRAMEWORK_NAME = 'Cornac'\n\n    REPOSITORY = 'https://github.com/PreferredAI/cornac/tree/master'\n\n    PAPER = \"\"\"Cornac: A Comparative Framework for Multimodal Recommender Systems\"\"\"\n\n    DOI = None\n\n    CITATION = \"\"\"\n            @article{DBLP:journals/jmlr/SalahTL20,\n              author       = {Aghiles Salah and\n                              Quoc{-}Tuan Truong and\n                              Hady W. Lauw},\n              title        = {Cornac: {A} Comparative Framework for Multimodal Recommender Systems},\n              journal      = {J. Mach. Learn. Res.},\n              volume       = {21},\n              pages        = {95:1--95:5},\n              year         = {2020},\n              url          = {http://jmlr.org/papers/v21/19-805.html},\n              timestamp    = {Wed, 18 Nov 2020 15:58:12 +0100},\n              biburl       = {https://dblp.org/rec/journals/jmlr/SalahTL20.bib},\n              bibsource    = {dblp computer science bibliography, https://dblp.org}\n            }\"\"\"\n\n    CODE = \"\"\"\n        from cornac.data import Reader\n\n        reader = Reader()\n        train_data = reader.read(fpath='{path}', fmt=\"{frmt}\")\n    \"\"\"\n\n    DOC = 'https://cornac.preferred.ai/'\n\n    def info_code(self):\n        \"\"\"\n        Provide the code to use in Cornac to run experiments.\n        \"\"\"\n        if self.timestamp:\n            self.CODE = \"\"\"\n        from cornac.data import Reader\n\n        reader = Reader()\n        train_data = reader.read(fpath='{path}', fmt=\"{frmt}\")\n    \"\"\".format(path=self.path, frmt='UIRT')\n        else:\n            self.CODE = \"\"\"\n                from cornac.data import Reader\n\n                reader = Reader()\n                train_data = reader.read(fpath='{path}', fmt=\"{frmt}\")\n            \"\"\".format(path=self.path, frmt='UIR')\n        super().info_code()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.cornac.cornac.Cornac.__init__","title":"<code>__init__(timestamp, path)</code>","text":"<p>Initialize Cornac adapter. Args:     timestamp (bool): Whether timestamps are included.     path (str): Path where the Cornac-compatible dataset is stored.</p> Source code in <code>datarec/io/frameworks/cornac/cornac.py</code> <pre><code>def __init__(self, timestamp, path):\n    \"\"\"\n    Initialize Cornac adapter.\n    Args:\n        timestamp (bool): Whether timestamps are included.\n        path (str): Path where the Cornac-compatible dataset is stored.\n    \"\"\"\n    self.timestamp = timestamp\n    self.path = path\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.cornac.cornac.Cornac.info_code","title":"<code>info_code()</code>","text":"<p>Provide the code to use in Cornac to run experiments.</p> Source code in <code>datarec/io/frameworks/cornac/cornac.py</code> <pre><code>def info_code(self):\n    \"\"\"\n    Provide the code to use in Cornac to run experiments.\n    \"\"\"\n    if self.timestamp:\n        self.CODE = \"\"\"\n    from cornac.data import Reader\n\n    reader = Reader()\n    train_data = reader.read(fpath='{path}', fmt=\"{frmt}\")\n\"\"\".format(path=self.path, frmt='UIRT')\n    else:\n        self.CODE = \"\"\"\n            from cornac.data import Reader\n\n            reader = Reader()\n            train_data = reader.read(fpath='{path}', fmt=\"{frmt}\")\n        \"\"\".format(path=self.path, frmt='UIR')\n    super().info_code()\n</code></pre>"},{"location":"documentation/io/#daisyrec","title":"DaisyRec","text":""},{"location":"documentation/io/#datarec.io.frameworks.daisyrec.daisyrec.DaisyRec","title":"<code>DaisyRec</code>","text":"<p>               Bases: <code>Framework</code></p> <p>DaisyRec framework adapter.</p> <p>Provide metadata, citation, and usage examples for DaisyRec framework.</p> Source code in <code>datarec/io/frameworks/daisyrec/daisyrec.py</code> <pre><code>class DaisyRec(Framework):\n    \"\"\"\n    DaisyRec framework adapter.\n\n    Provide metadata, citation, and usage examples for DaisyRec framework.\n    \"\"\"\n\n    def __init__(self, timestamp, path):\n        \"\"\"\n        Initialize DaisyRec adapter.\n        Args:\n            timestamp (bool): Whether timestamps are included.\n            path (str): Path where the DaisyRec-compatible dataset is stored.\n        \"\"\"\n        self.timestamp = timestamp\n        self.path = path\n\n    FRAMEWORK_NAME = 'DaisyRec'\n\n    REPOSITORY = 'https://github.com/recsys-benchmark/DaisyRec-v2.0'\n\n    PAPER = \"\"\"DaisyRec 2.0: Benchmarking Recommendation for Rigorous Evaluation\"\"\"\n\n    DOI = \"https://doi.org/10.1109/TPAMI.2022.3231891\"\n\n    CITATION = \"\"\"\n            @inproceedings{DBLP:conf/recsys/SunY00Q0G20,\n              author       = {Zhu Sun and\n                              Di Yu and\n                              Hui Fang and\n                              Jie Yang and\n                              Xinghua Qu and\n                              Jie Zhang and\n                              Cong Geng},\n              editor       = {Rodrygo L. T. Santos and\n                              Leandro Balby Marinho and\n                              Elizabeth M. Daly and\n                              Li Chen and\n                              Kim Falk and\n                              Noam Koenigstein and\n                              Edleno Silva de Moura},\n              title        = {Are We Evaluating Rigorously? Benchmarking Recommendation for Reproducible\n                              Evaluation and Fair Comparison},\n              booktitle    = {RecSys 2020: Fourteenth {ACM} Conference on Recommender Systems, Virtual\n                              Event, Brazil, September 22-26, 2020},\n              pages        = {23--32},\n              publisher    = {{ACM}},\n              year         = {2020},\n              url          = {https://doi.org/10.1145/3383313.3412489},\n              doi          = {10.1145/3383313.3412489},\n              timestamp    = {Tue, 21 Mar 2023 20:57:01 +0100},\n              biburl       = {https://dblp.org/rec/conf/recsys/SunY00Q0G20.bib},\n              bibsource    = {dblp computer science bibliography, https://dblp.org}\n            }\n\n            @article{DBLP:journals/pami/SunFYQLYOZ23,\n              author       = {Zhu Sun and\n                              Hui Fang and\n                              Jie Yang and\n                              Xinghua Qu and\n                              Hongyang Liu and\n                              Di Yu and\n                              Yew{-}Soon Ong and\n                              Jie Zhang},\n              title        = {DaisyRec 2.0: Benchmarking Recommendation for Rigorous Evaluation},\n              journal      = {{IEEE} Trans. Pattern Anal. Mach. Intell.},\n              volume       = {45},\n              number       = {7},\n              pages        = {8206--8226},\n              year         = {2023},\n              url          = {https://doi.org/10.1109/TPAMI.2022.3231891},\n              doi          = {10.1109/TPAMI.2022.3231891},\n              timestamp    = {Fri, 07 Jul 2023 23:32:20 +0200},\n              biburl       = {https://dblp.org/rec/journals/pami/SunFYQLYOZ23.bib},\n              bibsource    = {dblp computer science bibliography, https://dblp.org}\n            }\"\"\"\n\n    CODE = \"\"\"\n\n    \"\"\"\n\n    DOC = 'https://daisyrec.readthedocs.io/en/latest/'\n\n    def info_code(self):\n        \"\"\"\n        Provide the code to use in DaisyRec to run experiments.\n        \"\"\"\n        if self.timestamp:\n            self.CODE = f\"\"\"\n            In DaisyRec you need to replace the file at \n            \\'daisy/utils/loader.py\\'\n            with the file at\n            \\'datarec/io/frameworks/daisyrec/loader.py\\'\n            Then you need to open the file, go to line 36 and change \\'YOUR_PATH_HERE\\' with\n            \\'{self.path}\\'\n            \"\"\"\n        else:\n            self.CODE = f\"\"\"\n            In DaisyRec you need to replace the file at \n            \\'daisy/utils/loader.py\\'\n            with the file at\n            \\'datarec/io/frameworks/daisyrec/loader.py\\'\n            Then you need to open the file, go to line 36 and change \\'YOUR_PATH_HERE\\' with\n            \\'{self.path}\\'\n            Morover, from the attribute \\'names\\' you have to remove the timestamp.\n            \"\"\"\n        super().info_code()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.daisyrec.daisyrec.DaisyRec.__init__","title":"<code>__init__(timestamp, path)</code>","text":"<p>Initialize DaisyRec adapter. Args:     timestamp (bool): Whether timestamps are included.     path (str): Path where the DaisyRec-compatible dataset is stored.</p> Source code in <code>datarec/io/frameworks/daisyrec/daisyrec.py</code> <pre><code>def __init__(self, timestamp, path):\n    \"\"\"\n    Initialize DaisyRec adapter.\n    Args:\n        timestamp (bool): Whether timestamps are included.\n        path (str): Path where the DaisyRec-compatible dataset is stored.\n    \"\"\"\n    self.timestamp = timestamp\n    self.path = path\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.daisyrec.daisyrec.DaisyRec.info_code","title":"<code>info_code()</code>","text":"<p>Provide the code to use in DaisyRec to run experiments.</p> Source code in <code>datarec/io/frameworks/daisyrec/daisyrec.py</code> <pre><code>def info_code(self):\n    \"\"\"\n    Provide the code to use in DaisyRec to run experiments.\n    \"\"\"\n    if self.timestamp:\n        self.CODE = f\"\"\"\n        In DaisyRec you need to replace the file at \n        \\'daisy/utils/loader.py\\'\n        with the file at\n        \\'datarec/io/frameworks/daisyrec/loader.py\\'\n        Then you need to open the file, go to line 36 and change \\'YOUR_PATH_HERE\\' with\n        \\'{self.path}\\'\n        \"\"\"\n    else:\n        self.CODE = f\"\"\"\n        In DaisyRec you need to replace the file at \n        \\'daisy/utils/loader.py\\'\n        with the file at\n        \\'datarec/io/frameworks/daisyrec/loader.py\\'\n        Then you need to open the file, go to line 36 and change \\'YOUR_PATH_HERE\\' with\n        \\'{self.path}\\'\n        Morover, from the attribute \\'names\\' you have to remove the timestamp.\n        \"\"\"\n    super().info_code()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.daisyrec.loader.load_rate","title":"<code>load_rate(src='ml-100k', prepro='origin', binary=True, pos_threshold=None, level='ui')</code>","text":"<p>Load certain raw data. Args:     src (str): Name of dataset.     prepro (str): Way to pre-process raw data input, expect 'origin', f'{N}core', f'{N}filter', N is integer value.     binary (boolean): Whether to transform rating to binary label as CTR or not as Regression.     pos_threshold (float): If not None, treat rating larger than this threshold as positive sample.     level (str): which level to do with f'{N}core' or f'{N}filter' operation (it only works when prepro contains 'core' or 'filter').</p> <p>Returns:</p> Type Description <code>Dataframe</code> <p>Rating information with columns: user, item, rating, (options: timestamp).</p> <code>int</code> <p>The number of users in the dataset.</p> <code>int</code> <p>The number of items in the dataset.</p> Source code in <code>datarec/io/frameworks/daisyrec/loader.py</code> <pre><code>def load_rate(src='ml-100k', prepro='origin', binary=True, pos_threshold=None, level='ui'):\n    \"\"\"\n    Load certain raw data.\n    Args:\n        src (str): Name of dataset.\n        prepro (str): Way to pre-process raw data input, expect 'origin', f'{N}core', f'{N}filter', N is integer value.\n        binary (boolean): Whether to transform rating to binary label as CTR or not as Regression.\n        pos_threshold (float): If not None, treat rating larger than this threshold as positive sample.\n        level (str): which level to do with f'{N}core' or f'{N}filter' operation (it only works when prepro contains 'core' or 'filter').\n\n    Returns:\n        (pd.Dataframe): Rating information with columns: user, item, rating, (options: timestamp).\n        (int): The number of users in the dataset.\n        (int): The number of items in the dataset.\n\n    \"\"\"\n    df = pd.DataFrame()\n    # which dataset will use\n    if src == 'ml-100k':\n        df = pd.read_csv(f'./data/{src}/u.data', sep='\\t', header=None,\n                         names=['user', 'item', 'rating', 'timestamp'], engine='python')\n    elif src == 'datarec':\n        df = pd.read_csv('YOUR_PATH_HERE', sep='\\t', header=None,\n                         names=['user', 'item', 'rating', 'timestamp'], engine='python')\n    elif src == 'ml-1m':\n        df = pd.read_csv(f'./data/{src}/ratings.dat', sep='::', header=None,\n                         names=['user', 'item', 'rating', 'timestamp'], engine='python')\n        # only consider rating &gt;=4 for data density\n        df = df.query('rating &gt;= 4').reset_index(drop=True).copy()\n\n    elif src == 'ml-10m':\n        df = pd.read_csv(f'./data/{src}/ratings.dat', sep='::', header=None,\n                         names=['user', 'item', 'rating', 'timestamp'], engine='python')\n        df = df.query('rating &gt;= 4').reset_index(drop=True).copy()\n\n    elif src == 'ml-20m':\n        df = pd.read_csv(f'./data/{src}/ratings.csv')\n        df.rename(columns={'userId': 'user', 'movieId': 'item'}, inplace=True)\n        df = df.query('rating &gt;= 4').reset_index(drop=True)\n\n    elif src == 'netflix':\n        cnt = 0\n        tmp_file = open(f'./data/{src}/training_data.csv', 'w')\n        tmp_file.write('user,item,rating,timestamp' + '\\n')\n        for f in os.listdir(f'./data/{src}/training_set/'):\n            cnt += 1\n            if cnt % 5000 == 0:\n                print(f'Finish Process {cnt} file......')\n            txt_file = open(f'./data/{src}/training_set/{f}', 'r')\n            contents = txt_file.readlines()\n            item = contents[0].strip().split(':')[0]\n            for val in contents[1:]:\n                user, rating, timestamp = val.strip().split(',')\n                tmp_file.write(','.join([user, item, rating, timestamp]) + '\\n')\n            txt_file.close()\n\n        tmp_file.close()\n\n        df = pd.read_csv(f'./data/{src}/training_data.csv')\n        df['rating'] = df.rating.astype(float)\n        df['timestamp'] = pd.to_datetime(df['timestamp'])\n\n    elif src == 'lastfm':\n        # user_artists.dat\n        df = pd.read_csv(f'./data/{src}/user_artists.dat', sep='\\t')\n        df.rename(columns={'userID': 'user', 'artistID': 'item', 'weight': 'rating'}, inplace=True)\n        # treat weight as interaction, as 1\n        df['rating'] = 1.0\n        # fake timestamp column\n        df['timestamp'] = 1\n\n    elif src == 'book-x':\n        df = pd.read_csv(f'./data/{src}/BX-Book-Ratings.csv', delimiter=\";\", encoding=\"latin1\")\n        df.rename(columns={'User-ID': 'user', 'ISBN': 'item', 'Book-Rating': 'rating'}, inplace=True)\n        # fake timestamp column\n        df['timestamp'] = 1\n\n    elif src == 'pinterest':\n        # TODO this dataset has wrong source URL, we will figure out in future\n        pass\n\n    elif src == 'amazon-cloth':\n        df = pd.read_csv(f'./data/{src}/ratings_Clothing_Shoes_and_Jewelry.csv',\n                         names=['user', 'item', 'rating', 'timestamp'])\n\n    elif src == 'amazon-electronic':\n        df = pd.read_csv(f'./data/{src}/ratings_Electronics.csv',\n                         names=['user', 'item', 'rating', 'timestamp'])\n\n    elif src == 'amazon-book':\n        df = pd.read_csv(f'./data/{src}/ratings_Books.csv',\n                         names=['user', 'item', 'rating', 'timestamp'], low_memory=False)\n        df = df[df['timestamp'].str.isnumeric()].copy()\n        df['timestamp'] = df['timestamp'].astype(int)\n\n    elif src == 'amazon-music':\n        df = pd.read_csv(f'./data/{src}/ratings_Digital_Music.csv',\n                         names=['user', 'item', 'rating', 'timestamp'])\n\n    elif src == 'epinions':\n        d = sio.loadmat(f'./data/{src}/rating_with_timestamp.mat')\n        prime = []\n        for val in d['rating_with_timestamp']:\n            user, item, rating, timestamp = val[0], val[1], val[3], val[5]\n            prime.append([user, item, rating, timestamp])\n        df = pd.DataFrame(prime, columns=['user', 'item', 'rating', 'timestamp'])\n        del prime\n        gc.collect()\n\n    elif src == 'yelp':\n        json_file_path = f'./data/{src}/yelp_academic_dataset_review.json'\n        prime = []\n        for line in open(json_file_path, 'r', encoding='UTF-8'):\n            val = json.loads(line)\n            prime.append([val['user_id'], val['business_id'], val['stars'], val['date']])\n        df = pd.DataFrame(prime, columns=['user', 'item', 'rating', 'timestamp'])\n        df['timestamp'] = pd.to_datetime(df.timestamp)\n        del prime\n        gc.collect()\n\n    elif src == 'citeulike':\n        user = 0\n        dt = []\n        for line in open(f'./data/{src}/users.dat', 'r'):\n            val = line.split()\n            for item in val:\n                dt.append([user, item])\n            user += 1\n        df = pd.DataFrame(dt, columns=['user', 'item'])\n        # fake timestamp column\n        df['timestamp'] = 1\n\n    else:\n        raise ValueError('Invalid Dataset Error')\n\n    # set rating &gt;= threshold as positive samples\n    if pos_threshold is not None:\n        df = df.query(f'rating &gt;= {pos_threshold}').reset_index(drop=True)\n\n    # reset rating to interaction, here just treat all rating as 1\n    if binary:\n        df['rating'] = 1.0\n\n    # which type of pre-dataset will use\n    if prepro == 'origin':\n        pass\n\n    elif prepro.endswith('filter'):\n        pattern = re.compile(r'\\d+')\n        filter_num = int(pattern.findall(prepro)[0])\n\n        tmp1 = df.groupby(['user'], as_index=False)['item'].count()\n        tmp1.rename(columns={'item': 'cnt_item'}, inplace=True)\n        tmp2 = df.groupby(['item'], as_index=False)['user'].count()\n        tmp2.rename(columns={'user': 'cnt_user'}, inplace=True)\n        df = df.merge(tmp1, on=['user']).merge(tmp2, on=['item'])\n        if level == 'ui':\n            df = df.query(f'cnt_item &gt;= {filter_num} and cnt_user &gt;= {filter_num}').reset_index(drop=True).copy()\n        elif level == 'u':\n            df = df.query(f'cnt_item &gt;= {filter_num}').reset_index(drop=True).copy()\n        elif level == 'i':\n            df = df.query(f'cnt_user &gt;= {filter_num}').reset_index(drop=True).copy()\n        else:\n            raise ValueError(f'Invalid level value: {level}')\n\n        df.drop(['cnt_item', 'cnt_user'], axis=1, inplace=True)\n        del tmp1, tmp2\n        gc.collect()\n\n    elif prepro.endswith('core'):\n        pattern = re.compile(r'\\d+')\n        core_num = int(pattern.findall(prepro)[0])\n\n        def filter_user(df):\n            tmp = df.groupby(['user'], as_index=False)['item'].count()\n            tmp.rename(columns={'item': 'cnt_item'}, inplace=True)\n            df = df.merge(tmp, on=['user'])\n            df = df.query(f'cnt_item &gt;= {core_num}').reset_index(drop=True).copy()\n            df.drop(['cnt_item'], axis=1, inplace=True)\n\n            return df\n\n        def filter_item(df):\n            tmp = df.groupby(['item'], as_index=False)['user'].count()\n            tmp.rename(columns={'user': 'cnt_user'}, inplace=True)\n            df = df.merge(tmp, on=['item'])\n            df = df.query(f'cnt_user &gt;= {core_num}').reset_index(drop=True).copy()\n            df.drop(['cnt_user'], axis=1, inplace=True)\n\n            return df\n\n        if level == 'ui':\n            while 1:\n                df = filter_user(df)\n                df = filter_item(df)\n                chk_u = df.groupby('user')['item'].count()\n                chk_i = df.groupby('item')['user'].count()\n                if len(chk_i[chk_i &lt; core_num]) &lt;= 0 and len(chk_u[chk_u &lt; core_num]) &lt;= 0:\n                    break\n        elif level == 'u':\n            df = filter_user(df)\n        elif level == 'i':\n            df = filter_item(df)\n        else:\n            raise ValueError(f'Invalid level value: {level}')\n\n        gc.collect()\n\n    else:\n        raise ValueError('Invalid dataset preprocess type, origin/Ncore/Nfilter (N is int number) expected')\n\n    # encoding user_id and item_id\n    df['user'] = pd.Categorical(df['user']).codes\n    df['item'] = pd.Categorical(df['item']).codes\n\n    user_num = df['user'].nunique()\n    item_num = df['item'].nunique()\n\n    print(f'Finish loading [{src}]-[{prepro}] dataset')\n\n    return df, user_num, item_num\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.daisyrec.loader.get_ur","title":"<code>get_ur(df)</code>","text":"<p>Get user-rating pairs. Args:     df (pd.DataFrame): Rating dataframe.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary which stores user-items interactions.</p> Source code in <code>datarec/io/frameworks/daisyrec/loader.py</code> <pre><code>def get_ur(df):\n    \"\"\"\n    Get user-rating pairs.\n    Args:\n        df (pd.DataFrame): Rating dataframe.\n\n    Returns:\n        (dict): Dictionary which stores user-items interactions.\n\n    \"\"\"\n    ur = defaultdict(set)\n    for _, row in df.iterrows():\n        ur[int(row['user'])].add(int(row['item']))\n\n    return ur\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.daisyrec.loader.get_ir","title":"<code>get_ir(df)</code>","text":"<p>Get item-rating pairs. Args:     df (pd.DataFrame): Rating dataframe.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary which stores item-items interactions.</p> Source code in <code>datarec/io/frameworks/daisyrec/loader.py</code> <pre><code>def get_ir(df):\n    \"\"\"\n    Get item-rating pairs.\n    Args:\n        df (pd.DataFrame): Rating dataframe.\n\n    Returns:\n        (dict): Dictionary which stores item-items interactions.\n\n    \"\"\"\n    ir = defaultdict(set)\n    for _, row in df.iterrows():\n        ir[int(row['item'])].add(int(row['user']))\n\n    return ir\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.daisyrec.loader.build_feat_idx_dict","title":"<code>build_feat_idx_dict(df, cat_cols=['user', 'item'], num_cols=[])</code>","text":"<p>Encode feature mapping for FM. Args:     df (pd.DataFrame): Feature dataframe.     cat_cols (list): List of categorical column names.     num_cols (list): List of numerical column names.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with index-feature column mapping information.</p> <code>int</code> <p>The number of features.</p> Source code in <code>datarec/io/frameworks/daisyrec/loader.py</code> <pre><code>def build_feat_idx_dict(df: pd.DataFrame,\n                        cat_cols: list = ['user', 'item'],\n                        num_cols: list = []):\n    \"\"\"\n    Encode feature mapping for FM.\n    Args:\n        df (pd.DataFrame): Feature dataframe.\n        cat_cols (list): List of categorical column names.\n        num_cols (list): List of numerical column names.\n\n    Returns:\n        (dict): Dictionary with index-feature column mapping information.\n        (int): The number of features.\n\n    \"\"\"\n    feat_idx_dict = {}\n    idx = 0\n    for col in cat_cols:\n        feat_idx_dict[col] = idx\n        idx = idx + df[col].max() + 1\n    for col in num_cols:\n        feat_idx_dict[col] = idx\n        idx += 1\n    print('Finish build feature index dictionary......')\n\n    cnt = 0\n    for col in cat_cols:\n        for _ in df[col].unique():\n            cnt += 1\n    for _ in num_cols:\n        cnt += 1\n    print(f'Number of features: {cnt}')\n\n    return feat_idx_dict, cnt\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.daisyrec.loader.convert_npy_mat","title":"<code>convert_npy_mat(user_num, item_num, df)</code>","text":"<p>Convert pd.Dataframe to numpy matrix. Args:     user_num(int): Number of users.     item_num (int): Number of items.     df (pd.DataFrame): Rating dataframe.</p> <p>Returns:</p> Type Description <code>array</code> <p>Rating matrix.</p> Source code in <code>datarec/io/frameworks/daisyrec/loader.py</code> <pre><code>def convert_npy_mat(user_num, item_num, df):\n    \"\"\"\n    Convert pd.Dataframe to numpy matrix.\n    Args:\n        user_num(int): Number of users.\n        item_num (int): Number of items.\n        df (pd.DataFrame): Rating dataframe.\n\n    Returns:\n        (np.array): Rating matrix.\n    \"\"\"\n    mat = np.zeros((user_num, item_num))\n    for _, row in df.iterrows():\n        u, i, r = row['user'], row['item'], row['rating']\n        mat[int(u), int(i)] = float(r)\n    return mat\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.daisyrec.loader.build_candidates_set","title":"<code>build_candidates_set(test_ur, train_ur, item_pool, candidates_num=1000)</code>","text":"<p>Build candidate  items for ranking. Args:     test_ur (dict): Ground truth that represents the relationship of user and item in the test set.     train_ur (dict): The relationship of user and item in the train set.     item_pool (list or set): Set of all items.     candidates_num (int): Number of candidates.:</p> <p>Returns:</p> Name Type Description <code>test_ucands</code> <code>dict</code> <p>Dictionary storing candidates for each user in test set.</p> Source code in <code>datarec/io/frameworks/daisyrec/loader.py</code> <pre><code>def build_candidates_set(test_ur, train_ur, item_pool, candidates_num=1000):\n    \"\"\"\n    Build candidate  items for ranking.\n    Args:\n        test_ur (dict): Ground truth that represents the relationship of user and item in the test set.\n        train_ur (dict): The relationship of user and item in the train set.\n        item_pool (list or set): Set of all items.\n        candidates_num (int): Number of candidates.:\n\n    Returns:\n        test_ucands (dict): Dictionary storing candidates for each user in test set.\n\n    \"\"\"\n    test_ucands = defaultdict(list)\n    for k, v in test_ur.items():\n        sample_num = candidates_num - len(v) if len(v) &lt; candidates_num else 0\n        sub_item_pool = item_pool - v - train_ur[k]  # remove GT &amp; interacted\n        sample_num = min(len(sub_item_pool), sample_num)\n        if sample_num == 0:\n            samples = random.sample(v, candidates_num)\n            test_ucands[k] = list(set(samples))\n        else:\n            samples = random.sample(sub_item_pool, sample_num)\n            test_ucands[k] = list(v | set(samples))\n\n    return test_ucands\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.daisyrec.loader.get_adj_mat","title":"<code>get_adj_mat(n_users, n_items)</code>","text":"<p>Get adjacency matrix. Args:     n_users (int): Number of users.     n_items (int): Number of items.</p> <p>Returns:</p> Name Type Description <code>adj_mat</code> <code>csr_matrix</code> <p>Adjacency matrix.</p> <code>norm_adj_mat</code> <code>csr_matrix</code> <p>Normalized adjacency matrix.</p> <code>mean_adj_mat</code> <code>csr_matrix</code> <p>Mean adjacency matrix.</p> Source code in <code>datarec/io/frameworks/daisyrec/loader.py</code> <pre><code>def get_adj_mat(n_users, n_items):\n    \"\"\"\n    Get adjacency matrix.\n    Args:\n        n_users (int): Number of users.\n        n_items (int): Number of items.\n\n    Returns:\n        adj_mat (sp.csr_matrix): Adjacency matrix.\n        norm_adj_mat (sp.csr_matrix): Normalized adjacency matrix.\n        mean_adj_mat(sp.csr_matrix): Mean adjacency matrix.\n\n    \"\"\"\n    R = sp.dok_matrix((n_users, n_items), dtype=np.float32)\n    adj_mat = sp.dok_matrix((n_users + n_items, n_users + n_items), dtype=np.float32)\n    adj_mat = adj_mat.tolil()\n    R = R.tolil()\n\n    adj_mat[:n_users, n_users:] = R\n    adj_mat[n_users:, :n_users] = R.T\n    adj_mat = adj_mat.todok()\n    print('already create adjacency matrix', adj_mat.shape)\n\n    def mean_adj_single(adj):\n        \"\"\"\n        Compute row-normalized adjacency matrix (D\u207b\u00b9A).\n        Args:\n            adj (sp.spmatrix): Sparse adjacency matrix.\n\n        Returns:\n            (sp.coo_matrix): Row-normalized adjacency matrix in COO format.\n        \"\"\"\n        # D^-1 * A\n        rowsum = np.array(adj.sum(1))\n\n        d_inv = np.power(rowsum, -1).flatten()\n        d_inv[np.isinf(d_inv)] = 0.\n        d_mat_inv = sp.diags(d_inv)\n\n        norm_adj = d_mat_inv.dot(adj)\n        # norm_adj = adj.dot(d_mat_inv)\n        print('generate single-normalized adjacency matrix.')\n        return norm_adj.tocoo()\n\n    def normalized_adj_single(adj):\n        \"\"\"\n        Compute symmetric normalized adjacency matrix (D\u207b\u00b9/\u00b2 A D\u207b\u00b9/\u00b2).\n        Args:\n            adj (sp.spmatrix): Sparse adjacency matrix.\n\n        Returns:\n            (sp.coo_matrix): Symmetric normalized adjacency matrix in COO format.\n        \"\"\"\n        # D^-1/2 * A * D^-1/2\n        rowsum = np.array(adj.sum(1))\n\n        d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n        d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n        d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n\n        # bi_lap = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt)\n        bi_lap = d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt)\n        return bi_lap.tocoo()\n\n    def check_adj_if_equal(adj):\n        \"\"\"\n        Check if normalized adjacency is equivalent to Laplacian-based transformation.\n        Args:\n            adj (sp.spmatrix): Sparse adjacency matrix.\n\n        Returns:\n            (np.ndarray): Dense matrix representing the normalized adjacency for verification\n\n        \"\"\"\n        dense_A = np.array(adj.todense())\n        degree = np.sum(dense_A, axis=1, keepdims=False)\n\n        temp = np.dot(np.diag(np.power(degree, -1)), dense_A)\n        print('check normalized adjacency matrix whether equal to this laplacian matrix.')\n        return temp\n\n    norm_adj_mat = mean_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))\n    # norm_adj_mat = normalized_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))\n    mean_adj_mat = mean_adj_single(adj_mat)\n\n    print('already normalize adjacency matrix')\n    return adj_mat.tocsr(), norm_adj_mat.tocsr(), mean_adj_mat.tocsr()\n</code></pre>"},{"location":"documentation/io/#elliot","title":"Elliot","text":""},{"location":"documentation/io/#datarec.io.frameworks.elliot.elliot.Elliot","title":"<code>Elliot</code>","text":"<p>               Bases: <code>Framework</code></p> <p>Elliot framework adapter.</p> <p>Provide metadata, citation, and usage examples for Elliot framework.</p> Source code in <code>datarec/io/frameworks/elliot/elliot.py</code> <pre><code>class Elliot(Framework):\n    \"\"\"\n    Elliot framework adapter.\n\n    Provide metadata, citation, and usage examples for Elliot framework.\n    \"\"\"\n\n    def __init__(self, timestamp, path):\n        \"\"\"\n        Initialize Elliot adapter.\n        Args:\n            timestamp (bool): Whether timestamps are included.\n            path (str): Path where the Elliot-compatible dataset is stored.\n        \"\"\"\n        self.timestamp = timestamp\n\n        self.directory = os.path.abspath(path)\n        if os.path.exists(self.directory) is False:\n            os.makedirs(self.directory)\n\n        self.train_path, self.test_path, self.val_path = \\\n            os.path.join(self.directory, 'train.tsv'), \\\n                os.path.join(self.directory, 'test.tsv'), \\\n                os.path.join(self.directory, 'validation.tsv')\n\n        self.file = os.path.basename(path)\n        self.file_path = os.path.join(self.directory, self.file)\n\n        # create configuration file\n        config_file = \\\n            CONF.format(path=self.file_path,\n                        dataset='datarec2elliot',\n                        train=self.train_path,\n                        test=self.test_path,\n                        val=self.val_path)\n\n        self.config_path = os.path.join(self.directory, 'datarec_config.yml')\n        with open(self.config_path, 'w') as file:\n            file.write(config_file)\n\n    FRAMEWORK_NAME = 'Elliot'\n\n    REPOSITORY = 'https://github.com/sisinflab/elliot'\n\n    PAPER = \"\"\"Elliot: a Comprehensive and Rigorous Framework for Reproducible Recommender Systems Evaluation\"\"\"\n\n    DOI = \"https://doi.org/10.1145/3404835.3463245\"\n\n    CITATION = r\"\"\"\n            @inproceedings{DBLP:conf/sigir/AnelliBFMMPDN21,\n              author       = {Vito Walter Anelli and\n                              Alejandro Bellog{\\'{\\i}}n and\n                              Antonio Ferrara and\n                              Daniele Malitesta and\n                              Felice Antonio Merra and\n                              Claudio Pomo and\n                              Francesco Maria Donini and\n                              Tommaso Di Noia},\n              editor       = {Fernando Diaz and\n                              Chirag Shah and\n                              Torsten Suel and\n                              Pablo Castells and\n                              Rosie Jones and\n                              Tetsuya Sakai},\n              title        = {Elliot: {A} Comprehensive and Rigorous Framework for Reproducible\n                              Recommender Systems Evaluation},\n              booktitle    = {{SIGIR} '21: The 44th International {ACM} {SIGIR} Conference on Research\n                              and Development in Information Retrieval, Virtual Event, Canada, July\n                              11-15, 2021},\n              pages        = {2405--2414},\n              publisher    = {{ACM}},\n              year         = {2021},\n              url          = {https://doi.org/10.1145/3404835.3463245},\n              doi          = {10.1145/3404835.3463245},\n              timestamp    = {Sun, 12 Nov 2023 02:10:04 +0100},\n              biburl       = {https://dblp.org/rec/conf/sigir/AnelliBFMMPDN21.bib},\n              bibsource    = {dblp computer science bibliography, https://dblp.org}\n            }\"\"\"\n\n    CODE = \"  \"\n\n    DOC = 'https://elliot.readthedocs.io/en/latest/'\n\n    def info_code(self):\n        \"\"\"\n        Provide the code to use in Elliot to run experiments.\n        \"\"\"\n        self.CODE = \"\"\"\n            A configuration file for Elliot has been created here:\n            \\'{config_path}\\'\n            You can now run the script.\n             If you move the configuration file remember to change the path in the script below.\n\n            Elliot script:\n            python start_experiments.py --config {config_path}\n\n            This script contains a basic recommendation example. Change it if you need.\n            \"\"\".format(config_path=self.config_path)\n\n        super().info_code()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.elliot.elliot.Elliot.__init__","title":"<code>__init__(timestamp, path)</code>","text":"<p>Initialize Elliot adapter. Args:     timestamp (bool): Whether timestamps are included.     path (str): Path where the Elliot-compatible dataset is stored.</p> Source code in <code>datarec/io/frameworks/elliot/elliot.py</code> <pre><code>def __init__(self, timestamp, path):\n    \"\"\"\n    Initialize Elliot adapter.\n    Args:\n        timestamp (bool): Whether timestamps are included.\n        path (str): Path where the Elliot-compatible dataset is stored.\n    \"\"\"\n    self.timestamp = timestamp\n\n    self.directory = os.path.abspath(path)\n    if os.path.exists(self.directory) is False:\n        os.makedirs(self.directory)\n\n    self.train_path, self.test_path, self.val_path = \\\n        os.path.join(self.directory, 'train.tsv'), \\\n            os.path.join(self.directory, 'test.tsv'), \\\n            os.path.join(self.directory, 'validation.tsv')\n\n    self.file = os.path.basename(path)\n    self.file_path = os.path.join(self.directory, self.file)\n\n    # create configuration file\n    config_file = \\\n        CONF.format(path=self.file_path,\n                    dataset='datarec2elliot',\n                    train=self.train_path,\n                    test=self.test_path,\n                    val=self.val_path)\n\n    self.config_path = os.path.join(self.directory, 'datarec_config.yml')\n    with open(self.config_path, 'w') as file:\n        file.write(config_file)\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.elliot.elliot.Elliot.info_code","title":"<code>info_code()</code>","text":"<p>Provide the code to use in Elliot to run experiments.</p> Source code in <code>datarec/io/frameworks/elliot/elliot.py</code> <pre><code>def info_code(self):\n    \"\"\"\n    Provide the code to use in Elliot to run experiments.\n    \"\"\"\n    self.CODE = \"\"\"\n        A configuration file for Elliot has been created here:\n        \\'{config_path}\\'\n        You can now run the script.\n         If you move the configuration file remember to change the path in the script below.\n\n        Elliot script:\n        python start_experiments.py --config {config_path}\n\n        This script contains a basic recommendation example. Change it if you need.\n        \"\"\".format(config_path=self.config_path)\n\n    super().info_code()\n</code></pre>"},{"location":"documentation/io/#lenskit","title":"LensKit","text":""},{"location":"documentation/io/#datarec.io.frameworks.lenskit.lenskit.LensKit","title":"<code>LensKit</code>","text":"<p>               Bases: <code>Framework</code></p> <p>LensKit framework adapter.</p> <p>Provide metadata, citation, and usage examples for LensKit framework.</p> Source code in <code>datarec/io/frameworks/lenskit/lenskit.py</code> <pre><code>class LensKit(Framework):\n    \"\"\"\n    LensKit framework adapter.\n\n    Provide metadata, citation, and usage examples for LensKit framework.\n    \"\"\"\n\n    def __init__(self, timestamp, path):\n        \"\"\"\n        Initialize LensKit adapter.\n        Args:\n            timestamp (bool): Whether timestamps are included.\n            path (str): Path where the LensKit-compatible dataset is stored.\n        \"\"\"\n        self.timestamp = timestamp\n        self.path = path\n\n    FRAMEWORK_NAME = 'LensKit'\n\n    REPOSITORY = 'https://github.com/lenskit/lkpy'\n\n    PAPER = \"\"\"LensKit for Python: Next-Generation Software for Recommender Systems Experiments\"\"\"\n\n    DOI = \"https://doi.org/10.1145/3340531.3412778\"\n\n    CITATION = \"\"\"\n            @inproceedings{DBLP:conf/cikm/Ekstrand20,\n              author       = {Michael D. Ekstrand},\n              editor       = {Mathieu d'Aquin and\n                              Stefan Dietze and\n                              Claudia Hauff and\n                              Edward Curry and\n                              Philippe Cudr{\\'{e}}{-}Mauroux},\n              title        = {LensKit for Python: Next-Generation Software for Recommender Systems\n                              Experiments},\n              booktitle    = {{CIKM} '20: The 29th {ACM} International Conference on Information\n                              and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020},\n              pages        = {2999--3006},\n              publisher    = {{ACM}},\n              year         = {2020},\n              url          = {https://doi.org/10.1145/3340531.3412778},\n              doi          = {10.1145/3340531.3412778},\n              timestamp    = {Tue, 29 Dec 2020 18:42:41 +0100},\n              biburl       = {https://dblp.org/rec/conf/cikm/Ekstrand20.bib},\n              bibsource    = {dblp computer science bibliography, https://dblp.org}\n            }\"\"\"\n\n    CODE = \"\"\"\n\n    \"\"\"\n\n    DOC = 'https://lkpy.lenskit.org/en/stable/'\n\n    def info_code(self):\n        \"\"\"\n        Provide the code to use in LensKit to run experiments.\n        \"\"\"\n        self.CODE = \"\"\"\n        LensKit accepts pandas DataFrames with specific column naming. DataRec will do that for you!\n\n        import pandas as pd\n\n        ratings = pd.read_csv({path}, sep='\\\\t', header=False)\n        \"\"\".format(path=self.path)\n\n        super().info_code()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.lenskit.lenskit.LensKit.__init__","title":"<code>__init__(timestamp, path)</code>","text":"<p>Initialize LensKit adapter. Args:     timestamp (bool): Whether timestamps are included.     path (str): Path where the LensKit-compatible dataset is stored.</p> Source code in <code>datarec/io/frameworks/lenskit/lenskit.py</code> <pre><code>def __init__(self, timestamp, path):\n    \"\"\"\n    Initialize LensKit adapter.\n    Args:\n        timestamp (bool): Whether timestamps are included.\n        path (str): Path where the LensKit-compatible dataset is stored.\n    \"\"\"\n    self.timestamp = timestamp\n    self.path = path\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.lenskit.lenskit.LensKit.info_code","title":"<code>info_code()</code>","text":"<p>Provide the code to use in LensKit to run experiments.</p> Source code in <code>datarec/io/frameworks/lenskit/lenskit.py</code> <pre><code>def info_code(self):\n    \"\"\"\n    Provide the code to use in LensKit to run experiments.\n    \"\"\"\n    self.CODE = \"\"\"\n    LensKit accepts pandas DataFrames with specific column naming. DataRec will do that for you!\n\n    import pandas as pd\n\n    ratings = pd.read_csv({path}, sep='\\\\t', header=False)\n    \"\"\".format(path=self.path)\n\n    super().info_code()\n</code></pre>"},{"location":"documentation/io/#recbole","title":"RecBole","text":""},{"location":"documentation/io/#datarec.io.frameworks.recbole.recbole.RecBole","title":"<code>RecBole</code>","text":"<p>               Bases: <code>Framework</code></p> <p>RecBole framework adapter.</p> <p>Provide metadata, citation, and usage examples for RecBole framework.</p> Source code in <code>datarec/io/frameworks/recbole/recbole.py</code> <pre><code>class RecBole(Framework):\n    \"\"\"\n    RecBole framework adapter.\n\n    Provide metadata, citation, and usage examples for RecBole framework.\n    \"\"\"\n\n    def __init__(self, timestamp, path):\n        \"\"\"\n        Initialize RecBole adapter.\n        Args:\n            timestamp (bool): Whether timestamps are included.\n            path (str): Path where the RecBole-compatible dataset is stored.\n        \"\"\"\n        self.timestamp = timestamp\n        directory = os.path.dirname(path)\n        self.directory = os.path.join(directory, 'DataRec2RecBole')\n        print('RecBole requires a directory named as the the dataset.\\n'\n              f'Based on your path the directory that will be used is \\'{self.directory}\\'')\n        if os.path.exists(self.directory) is False:\n            os.makedirs(self.directory)\n        self.path = os.path.join(self.directory, path)\n\n    FRAMEWORK_NAME = 'RecBole'\n\n    REPOSITORY = 'https://github.com/RUCAIBox/RecBole2.0'\n\n    PAPER = \"\"\"RecBole 2.0: Towards a More Up-to-Date Recommendation Library\"\"\"\n\n    DOI = \"https://doi.org/10.1145/3511808.3557680\"\n\n    CITATION = r\"\"\"\n            @inproceedings{DBLP:conf/cikm/ZhaoMHLCPLLWTMF21,\n              author       = {Wayne Xin Zhao and\n                              Shanlei Mu and\n                              Yupeng Hou and\n                              Zihan Lin and\n                              Yushuo Chen and\n                              Xingyu Pan and\n                              Kaiyuan Li and\n                              Yujie Lu and\n                              Hui Wang and\n                              Changxin Tian and\n                              Yingqian Min and\n                              Zhichao Feng and\n                              Xinyan Fan and\n                              Xu Chen and\n                              Pengfei Wang and\n                              Wendi Ji and\n                              Yaliang Li and\n                              Xiaoling Wang and\n                              Ji{-}Rong Wen},\n              editor       = {Gianluca Demartini and\n                              Guido Zuccon and\n                              J. Shane Culpepper and\n                              Zi Huang and\n                              Hanghang Tong},\n              title        = {RecBole: Towards a Unified, Comprehensive and Efficient Framework\n                              for Recommendation Algorithms},\n              booktitle    = {{CIKM} '21: The 30th {ACM} International Conference on Information\n                              and Knowledge Management, Virtual Event, Queensland, Australia, November\n                              1 - 5, 2021},\n              pages        = {4653--4664},\n              publisher    = {{ACM}},\n              year         = {2021},\n              url          = {https://doi.org/10.1145/3459637.3482016},\n              doi          = {10.1145/3459637.3482016},\n              timestamp    = {Tue, 07 May 2024 20:05:19 +0200},\n              biburl       = {https://dblp.org/rec/conf/cikm/ZhaoMHLCPLLWTMF21.bib},\n              bibsource    = {dblp computer science bibliography, https://dblp.org}\n            }\n            @inproceedings{DBLP:conf/cikm/ZhaoHPYZLZBTSCX22,\n              author       = {Wayne Xin Zhao and\n                              Yupeng Hou and\n                              Xingyu Pan and\n                              Chen Yang and\n                              Zeyu Zhang and\n                              Zihan Lin and\n                              Jingsen Zhang and\n                              Shuqing Bian and\n                              Jiakai Tang and\n                              Wenqi Sun and\n                              Yushuo Chen and\n                              Lanling Xu and\n                              Gaowei Zhang and\n                              Zhen Tian and\n                              Changxin Tian and\n                              Shanlei Mu and\n                              Xinyan Fan and\n                              Xu Chen and\n                              Ji{-}Rong Wen},\n              editor       = {Mohammad Al Hasan and\n                              Li Xiong},\n              title        = {RecBole 2.0: Towards a More Up-to-Date Recommendation Library},\n              booktitle    = {Proceedings of the 31st {ACM} International Conference on Information\n                              {\\&amp;} Knowledge Management, Atlanta, GA, USA, October 17-21, 2022},\n              pages        = {4722--4726},\n              publisher    = {{ACM}},\n              year         = {2022},\n              url          = {https://doi.org/10.1145/3511808.3557680},\n              doi          = {10.1145/3511808.3557680},\n              timestamp    = {Sun, 20 Aug 2023 12:23:03 +0200},\n              biburl       = {https://dblp.org/rec/conf/cikm/ZhaoHPYZLZBTSCX22.bib},\n              bibsource    = {dblp computer science bibliography, https://dblp.org}\n            }\"\"\"\n\n    CODE = \"\"\"\n\n    \"\"\"\n\n    DOC = 'https://recbole.io/'\n\n    def info_code(self):\n        \"\"\"\n        Provide the code to use in RecBole to run experiments.\n        \"\"\"\n        self.CODE = \"\"\"\n            from recbole.data import create_dataset\n            from recbole.config import Config\n\n            config_dict = {{\n                \"dataset\": \"datarec\",\n                \"data_path\": {path},\n            }}\n            config = Config(config_dict=config_dict, config_file_list=config_file_list)\n            dataset = create_dataset(config)\n        \"\"\".format(path=self.path)\n\n        super().info_code()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.recbole.recbole.RecBole.__init__","title":"<code>__init__(timestamp, path)</code>","text":"<p>Initialize RecBole adapter. Args:     timestamp (bool): Whether timestamps are included.     path (str): Path where the RecBole-compatible dataset is stored.</p> Source code in <code>datarec/io/frameworks/recbole/recbole.py</code> <pre><code>def __init__(self, timestamp, path):\n    \"\"\"\n    Initialize RecBole adapter.\n    Args:\n        timestamp (bool): Whether timestamps are included.\n        path (str): Path where the RecBole-compatible dataset is stored.\n    \"\"\"\n    self.timestamp = timestamp\n    directory = os.path.dirname(path)\n    self.directory = os.path.join(directory, 'DataRec2RecBole')\n    print('RecBole requires a directory named as the the dataset.\\n'\n          f'Based on your path the directory that will be used is \\'{self.directory}\\'')\n    if os.path.exists(self.directory) is False:\n        os.makedirs(self.directory)\n    self.path = os.path.join(self.directory, path)\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.recbole.recbole.RecBole.info_code","title":"<code>info_code()</code>","text":"<p>Provide the code to use in RecBole to run experiments.</p> Source code in <code>datarec/io/frameworks/recbole/recbole.py</code> <pre><code>def info_code(self):\n    \"\"\"\n    Provide the code to use in RecBole to run experiments.\n    \"\"\"\n    self.CODE = \"\"\"\n        from recbole.data import create_dataset\n        from recbole.config import Config\n\n        config_dict = {{\n            \"dataset\": \"datarec\",\n            \"data_path\": {path},\n        }}\n        config = Config(config_dict=config_dict, config_file_list=config_file_list)\n        dataset = create_dataset(config)\n    \"\"\".format(path=self.path)\n\n    super().info_code()\n</code></pre>"},{"location":"documentation/io/#rechorus","title":"ReChorus","text":""},{"location":"documentation/io/#datarec.io.frameworks.rechorus.rechorus.ReChorus","title":"<code>ReChorus</code>","text":"<p>               Bases: <code>Framework</code></p> <p>ReChorus framework adapter.</p> <p>Provide metadata, citation, and usage examples for ReChorus framework.</p> Source code in <code>datarec/io/frameworks/rechorus/rechorus.py</code> <pre><code>class ReChorus(Framework):\n    \"\"\"\n    ReChorus framework adapter.\n\n    Provide metadata, citation, and usage examples for ReChorus framework.\n    \"\"\"\n\n    def __init__(self, timestamp, path):\n        \"\"\"\n        Initialize ReChorus adapter.\n        Args:\n            timestamp (bool): Whether timestamps are included.\n            path (str): Path where the ReChorus-compatible dataset is stored.\n        \"\"\"\n        self.timestamp = timestamp\n        directory = os.path.dirname(path)\n        self.directory = os.path.abspath(os.path.join(directory, 'DataRec2ReChorus'))\n        print('RecBole requires a directory named as the the dataset.\\n'\n              f'Based on your path the directory that will be used is \\'{self.directory}\\'')\n        if os.path.exists(self.directory) is False:\n            os.makedirs(self.directory)\n\n    FRAMEWORK_NAME = 'ReChorus'\n\n    REPOSITORY = 'https://github.com/THUwangcy/ReChorus'\n\n    PAPER = \"\"\"Make It a Chorus: Knowledge- and Time-aware Item Modeling for Sequential Recommendation\"\"\"\n\n    DOI = \"https://doi.org/10.1145/3397271.3401131\"\n\n    CITATION = \"\"\"\n            @inproceedings{DBLP:conf/sigir/WangZMLM20,\n              author       = {Chenyang Wang and\n                              Min Zhang and\n                              Weizhi Ma and\n                              Yiqun Liu and\n                              Shaoping Ma},\n              editor       = {Jimmy X. Huang and\n                              Yi Chang and\n                              Xueqi Cheng and\n                              Jaap Kamps and\n                              Vanessa Murdock and\n                              Ji{-}Rong Wen and\n                              Yiqun Liu},\n              title        = {Make It a Chorus: Knowledge- and Time-aware Item Modeling for Sequential\n                              Recommendation},\n              booktitle    = {Proceedings of the 43rd International {ACM} {SIGIR} conference on\n                              research and development in Information Retrieval, {SIGIR} 2020, Virtual\n                              Event, China, July 25-30, 2020},\n              pages        = {109--118},\n              publisher    = {{ACM}},\n              year         = {2020},\n              url          = {https://doi.org/10.1145/3397271.3401131},\n              doi          = {10.1145/3397271.3401131},\n              timestamp    = {Mon, 31 Oct 2022 08:39:18 +0100},\n              biburl       = {https://dblp.org/rec/conf/sigir/WangZMLM20.bib},\n              bibsource    = {dblp computer science bibliography, https://dblp.org}\n            }\"\"\"\n\n    CODE = \"\"\"\n\n    \"\"\"\n\n    DOC = None\n\n    def info_code(self):\n        \"\"\"\n        Provide the code to use in RecBole to run experiments.\n        \"\"\"\n        self.CODE = \"\"\"\n            Dataset must be split and provided in a single folder within the \\'data\\' folder of the project.\\n\n            This data will be supported by ReChorus models that adopt a dataset \\'BaseModel.Dataset\\' \\n\n            DataRec created this directory here \\'{directory}\\'.\n        \"\"\".format(directory=self.directory)\n\n        super().info_code()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.rechorus.rechorus.ReChorus.__init__","title":"<code>__init__(timestamp, path)</code>","text":"<p>Initialize ReChorus adapter. Args:     timestamp (bool): Whether timestamps are included.     path (str): Path where the ReChorus-compatible dataset is stored.</p> Source code in <code>datarec/io/frameworks/rechorus/rechorus.py</code> <pre><code>def __init__(self, timestamp, path):\n    \"\"\"\n    Initialize ReChorus adapter.\n    Args:\n        timestamp (bool): Whether timestamps are included.\n        path (str): Path where the ReChorus-compatible dataset is stored.\n    \"\"\"\n    self.timestamp = timestamp\n    directory = os.path.dirname(path)\n    self.directory = os.path.abspath(os.path.join(directory, 'DataRec2ReChorus'))\n    print('RecBole requires a directory named as the the dataset.\\n'\n          f'Based on your path the directory that will be used is \\'{self.directory}\\'')\n    if os.path.exists(self.directory) is False:\n        os.makedirs(self.directory)\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.rechorus.rechorus.ReChorus.info_code","title":"<code>info_code()</code>","text":"<p>Provide the code to use in RecBole to run experiments.</p> Source code in <code>datarec/io/frameworks/rechorus/rechorus.py</code> <pre><code>def info_code(self):\n    \"\"\"\n    Provide the code to use in RecBole to run experiments.\n    \"\"\"\n    self.CODE = \"\"\"\n        Dataset must be split and provided in a single folder within the \\'data\\' folder of the project.\\n\n        This data will be supported by ReChorus models that adopt a dataset \\'BaseModel.Dataset\\' \\n\n        DataRec created this directory here \\'{directory}\\'.\n    \"\"\".format(directory=self.directory)\n\n    super().info_code()\n</code></pre>"},{"location":"documentation/io/#recommenders","title":"Recommenders","text":""},{"location":"documentation/io/#datarec.io.frameworks.recommenders.recommenders.Recommenders","title":"<code>Recommenders</code>","text":"<p>               Bases: <code>Framework</code></p> <p>Recommenders framework adapter.</p> <p>Provide metadata, citation, and usage examples for Recommenders framework.</p> Source code in <code>datarec/io/frameworks/recommenders/recommenders.py</code> <pre><code>class Recommenders(Framework):\n    \"\"\"\n    Recommenders framework adapter.\n\n    Provide metadata, citation, and usage examples for Recommenders framework.\n    \"\"\"\n\n    def __init__(self, timestamp, path):\n        \"\"\"\n        Initialize Recommenders adapter.\n        Args:\n            timestamp (bool): Whether timestamps are included.\n            path (str): Path where the Recommenders-compatible dataset is stored.\n        \"\"\"\n        self.timestamp = timestamp\n        self.directory = os.path.abspath(os.path.dirname(path))\n        if os.path.exists(self.directory) is False:\n            os.makedirs(self.directory)\n        self.file = os.path.basename(path)\n        self.file_path = os.path.join(self.directory, self.file)\n\n    FRAMEWORK_NAME = 'Recommenders'\n\n    REPOSITORY = 'https://github.com/recommenders-team/recommenders?tab=readme-ov-file'\n\n    PAPER = \"\"\"Microsoft recommenders: tools to accelerate developing recommender systems\"\"\"\n\n    DOI = \"https://doi.org/10.1145/3298689.3346967\"\n\n    CITATION = \"\"\"\n            @inproceedings{DBLP:conf/recsys/GrahamMW19,\n              author       = {Scott Graham and\n                              Jun{-}Ki Min and\n                              Tao Wu},\n              editor       = {Toine Bogers and\n                              Alan Said and\n                              Peter Brusilovsky and\n                              Domonkos Tikk},\n              title        = {Microsoft recommenders: tools to accelerate developing recommender\n                              systems},\n              booktitle    = {Proceedings of the 13th {ACM} Conference on Recommender Systems, RecSys\n                              2019, Copenhagen, Denmark, September 16-20, 2019},\n              pages        = {542--543},\n              publisher    = {{ACM}},\n              year         = {2019},\n              url          = {https://doi.org/10.1145/3298689.3346967},\n              doi          = {10.1145/3298689.3346967},\n              timestamp    = {Wed, 09 Oct 2019 14:20:04 +0200},\n              biburl       = {https://dblp.org/rec/conf/recsys/GrahamMW19.bib},\n              bibsource    = {dblp computer science bibliography, https://dblp.org}\n            }\"\"\"\n\n    CODE = \"\"\"\n\n    \"\"\"\n\n    DOC = 'https://recommenders-team.github.io/recommenders'\n\n    def info_code(self):\n        \"\"\"\n        Provide the code to use in Recommenders to run experiments.\n        \"\"\"\n        if self.timestamp:\n            self.CODE = \"\"\"\n                import pandas as pd\n\n                data = pd.read_csv({file}, sep=\"\\\\t\", names=['user', 'item', 'rating', 'timestamp'])\n                \"\"\".format(file=self.file_path)\n        else:\n            self.CODE = \"\"\"\n                import pandas as pd\n\n                data = pd.read_csv({file}, sep=\"\\\\t\", names=['user', 'item', 'rating'])\n                \"\"\".format(file=self.file_path)\n\n        super().info_code()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.recommenders.recommenders.Recommenders.__init__","title":"<code>__init__(timestamp, path)</code>","text":"<p>Initialize Recommenders adapter. Args:     timestamp (bool): Whether timestamps are included.     path (str): Path where the Recommenders-compatible dataset is stored.</p> Source code in <code>datarec/io/frameworks/recommenders/recommenders.py</code> <pre><code>def __init__(self, timestamp, path):\n    \"\"\"\n    Initialize Recommenders adapter.\n    Args:\n        timestamp (bool): Whether timestamps are included.\n        path (str): Path where the Recommenders-compatible dataset is stored.\n    \"\"\"\n    self.timestamp = timestamp\n    self.directory = os.path.abspath(os.path.dirname(path))\n    if os.path.exists(self.directory) is False:\n        os.makedirs(self.directory)\n    self.file = os.path.basename(path)\n    self.file_path = os.path.join(self.directory, self.file)\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.recommenders.recommenders.Recommenders.info_code","title":"<code>info_code()</code>","text":"<p>Provide the code to use in Recommenders to run experiments.</p> Source code in <code>datarec/io/frameworks/recommenders/recommenders.py</code> <pre><code>def info_code(self):\n    \"\"\"\n    Provide the code to use in Recommenders to run experiments.\n    \"\"\"\n    if self.timestamp:\n        self.CODE = \"\"\"\n            import pandas as pd\n\n            data = pd.read_csv({file}, sep=\"\\\\t\", names=['user', 'item', 'rating', 'timestamp'])\n            \"\"\".format(file=self.file_path)\n    else:\n        self.CODE = \"\"\"\n            import pandas as pd\n\n            data = pd.read_csv({file}, sep=\"\\\\t\", names=['user', 'item', 'rating'])\n            \"\"\".format(file=self.file_path)\n\n    super().info_code()\n</code></pre>"},{"location":"documentation/io/#recpack","title":"RecPack","text":""},{"location":"documentation/io/#datarec.io.frameworks.recpack.recpack.RecPack","title":"<code>RecPack</code>","text":"<p>               Bases: <code>Framework</code></p> <p>RecPack framework adapter.</p> <p>Provide metadata, citation, and usage examples for RecPack framework.</p> Source code in <code>datarec/io/frameworks/recpack/recpack.py</code> <pre><code>class RecPack(Framework):\n    \"\"\"\n    RecPack framework adapter.\n\n    Provide metadata, citation, and usage examples for RecPack framework.\n    \"\"\"\n\n    def __init__(self, timestamp, path):\n        \"\"\"\n        Initialize RecPack adapter.\n        Args:\n            timestamp (bool): Whether timestamps are included.\n            path (str): Path where the RecPack-compatible dataset is stored.\n        \"\"\"\n        self.timestamp = timestamp\n        self.directory = os.path.abspath(os.path.dirname(path))\n        if os.path.exists(self.directory) is False:\n            os.makedirs(self.directory)\n        self.file = os.path.basename(path)\n        self.file_path = os.path.join(self.directory, self.file)\n\n    FRAMEWORK_NAME = 'RecPack'\n\n    REPOSITORY = 'https://github.com/LienM/recpack'\n\n    PAPER = \"\"\"RecPack: An(other) Experimentation Toolkit for Top-N Recommendation using Implicit Feedback Data\"\"\"\n\n    DOI = \"https://doi.org/10.1145/3523227.3551472\"\n\n    CITATION = \"\"\"\n            @inproceedings{DBLP:conf/recsys/MichielsVG22,\n              author       = {Lien Michiels and\n                              Robin Verachtert and\n                              Bart Goethals},\n              editor       = {Jennifer Golbeck and\n                              F. Maxwell Harper and\n                              Vanessa Murdock and\n                              Michael D. Ekstrand and\n                              Bracha Shapira and\n                              Justin Basilico and\n                              Keld T. Lundgaard and\n                              Even Oldridge},\n              title        = {RecPack: An(other) Experimentation Toolkit for Top-N Recommendation\n                              using Implicit Feedback Data},\n              booktitle    = {RecSys '22: Sixteenth {ACM} Conference on Recommender Systems, Seattle,\n                              WA, USA, September 18 - 23, 2022},\n              pages        = {648--651},\n              publisher    = {{ACM}},\n              year         = {2022},\n              url          = {https://doi.org/10.1145/3523227.3551472},\n              doi          = {10.1145/3523227.3551472},\n              timestamp    = {Mon, 01 May 2023 13:01:24 +0200},\n              biburl       = {https://dblp.org/rec/conf/recsys/MichielsVG22.bib},\n              bibsource    = {dblp computer science bibliography, https://dblp.org}\n            }\"\"\"\n\n    CODE = \"\"\"\n\n    \"\"\"\n\n    DOC = 'https://recpack.froomle.ai/'\n\n    def info_code(self):\n        \"\"\"\n        Provide the code to use in RecPack to run experiments.\n        \"\"\"\n        self.CODE = \"\"\"\n            For using a dataset from DataRec you need to:\n            1) copy/move the file \n            \\'datarec/io/frameworks/recpack/datarec.py\\'\n            at \\'recpack/datasets/datarec.py\\'\n            2) replace the content of the init file in RecPack\n            \\'datarec/io/frameworks/recpack/__init__.py\\'\n            with the content of\n            \\'datarec/io/frameworks/recpack/copy_me_in__init__.py\\'\n            Then you can use this code\n\n            from recpack.datasets import DummyDataset\n            dataset = (path={file}, filename={directory}, use_default_filters=False)\n        \"\"\".format(file=self.file, directory=self.directory)\n\n        super().info_code()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.recpack.recpack.RecPack.__init__","title":"<code>__init__(timestamp, path)</code>","text":"<p>Initialize RecPack adapter. Args:     timestamp (bool): Whether timestamps are included.     path (str): Path where the RecPack-compatible dataset is stored.</p> Source code in <code>datarec/io/frameworks/recpack/recpack.py</code> <pre><code>def __init__(self, timestamp, path):\n    \"\"\"\n    Initialize RecPack adapter.\n    Args:\n        timestamp (bool): Whether timestamps are included.\n        path (str): Path where the RecPack-compatible dataset is stored.\n    \"\"\"\n    self.timestamp = timestamp\n    self.directory = os.path.abspath(os.path.dirname(path))\n    if os.path.exists(self.directory) is False:\n        os.makedirs(self.directory)\n    self.file = os.path.basename(path)\n    self.file_path = os.path.join(self.directory, self.file)\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.recpack.recpack.RecPack.info_code","title":"<code>info_code()</code>","text":"<p>Provide the code to use in RecPack to run experiments.</p> Source code in <code>datarec/io/frameworks/recpack/recpack.py</code> <pre><code>def info_code(self):\n    \"\"\"\n    Provide the code to use in RecPack to run experiments.\n    \"\"\"\n    self.CODE = \"\"\"\n        For using a dataset from DataRec you need to:\n        1) copy/move the file \n        \\'datarec/io/frameworks/recpack/datarec.py\\'\n        at \\'recpack/datasets/datarec.py\\'\n        2) replace the content of the init file in RecPack\n        \\'datarec/io/frameworks/recpack/__init__.py\\'\n        with the content of\n        \\'datarec/io/frameworks/recpack/copy_me_in__init__.py\\'\n        Then you can use this code\n\n        from recpack.datasets import DummyDataset\n        dataset = (path={file}, filename={directory}, use_default_filters=False)\n    \"\"\".format(file=self.file, directory=self.directory)\n\n    super().info_code()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.recpack.datarec.DataRec","title":"<code>DataRec</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Base class for DataRec Datasets</p> Source code in <code>datarec/io/frameworks/recpack/datarec.py</code> <pre><code>class DataRec(Dataset):\n    \"\"\"\n    Base class for DataRec Datasets\n    \"\"\"\n    USER_IX = \"userId\"\n    \"\"\"Name of the column in the DataFrame that contains user identifiers.\"\"\"\n    ITEM_IX = \"itemId\"\n    \"\"\"Name of the column in the DataFrame that contains item identifiers.\"\"\"\n    TIMESTAMP_IX = \"timestamp\"\n    \"\"\"Name of the column in the DataFrame that contains time of interaction in seconds since epoch.\"\"\"\n\n    @property\n    def DEFAULT_FILENAME(self) -&gt; str:\n        \"\"\"\n        Default filename that will be used if it is not specified by the user.\n        \"\"\"\n        return f\"datarec.tsv\"\n\n    def _load_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Dataset from DataRec will be loaded as a pandas DataFrame\n\n        Warning:: This does not apply any preprocessing, and returns the raw dataset.\n\n        Returns:\n            (pd.DataFrame): The interaction data as a DataFrame with a row per interaction.\n\n        \"\"\"\n        df = pd.read_csv(os.path.join(self.path, self.filename), sep='\\t', header=True, dtype={\n                self.USER_IX: str,\n                self.TIMESTAMP_IX: np.int64,\n                self.ITEM_IX: str,\n            })\n        return df\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.recpack.datarec.DataRec.USER_IX","title":"<code>USER_IX = 'userId'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name of the column in the DataFrame that contains user identifiers.</p>"},{"location":"documentation/io/#datarec.io.frameworks.recpack.datarec.DataRec.ITEM_IX","title":"<code>ITEM_IX = 'itemId'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name of the column in the DataFrame that contains item identifiers.</p>"},{"location":"documentation/io/#datarec.io.frameworks.recpack.datarec.DataRec.TIMESTAMP_IX","title":"<code>TIMESTAMP_IX = 'timestamp'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name of the column in the DataFrame that contains time of interaction in seconds since epoch.</p>"},{"location":"documentation/io/#datarec.io.frameworks.recpack.datarec.DataRec.DEFAULT_FILENAME","title":"<code>DEFAULT_FILENAME</code>  <code>property</code>","text":"<p>Default filename that will be used if it is not specified by the user.</p>"},{"location":"documentation/pipeline/","title":"Pipeline Module Reference","text":"<p>This section provides the API reference for modules that handle the creation, management, and execution of reproducible data processing workflows. Pipelines support <code>load</code>, <code>read</code>, <code>process</code>, <code>split</code>, <code>export</code>, and <code>write</code> steps.</p>"},{"location":"documentation/pipeline/#on-this-page","title":"On This Page","text":"<ul> <li>Minimal usage</li> <li>Pipeline API</li> </ul>"},{"location":"documentation/pipeline/#minimal-usage","title":"Minimal usage","text":"<pre><code>from datarec.pipeline import Pipeline\n\npipeline = Pipeline()\npipeline.add_step(\"load\", \"registry_dataset\", {\"dataset_name\": \"movielens\", \"version\": \"1m\"})\npipeline.add_step(\"process\", \"Binarize\", {\"threshold\": 4})\npipeline.add_step(\"split\", \"RandomHoldOut\", {\"test_ratio\": 0.2, \"val_ratio\": 0.1})\npipeline.add_step(\"export\", \"Elliot\", {\"filename\": \"elliot/\"})\npipeline.apply(output_folder=\"./out\")\n</code></pre>"},{"location":"documentation/pipeline/#pipeline-api","title":"Pipeline API","text":""},{"location":"documentation/pipeline/#datarec.pipeline.pipeline.Pipeline","title":"<code>Pipeline</code>","text":"<p>Pipeline for reproducible data transformations.</p> Source code in <code>datarec/pipeline/pipeline.py</code> <pre><code>class Pipeline:\n    \"\"\"Pipeline for reproducible data transformations.\"\"\"\n    def __init__(self):\n        self.steps: List[PipelineStep] = []\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a readable summary of pipeline steps.\"\"\"\n        if not self.steps:\n            return \"Pipeline(steps=0)\"\n        lines = [\"Pipeline:\"]\n        for idx, step in enumerate(self.steps, start=1):\n            params = \", \".join(f\"{k}={v!r}\" for k, v in step.params.items())\n            lines.append(f\"{idx}. {step.name} -&gt; {step.operation}({params})\")\n        return \"\\n\".join(lines)\n\n    def add_step(self, name: str, operation: str, params: Dict[str, Any]) -&gt; None:\n        \"\"\"Add a pipeline step.\n\n        Args:\n            name (str): Step name (load/read/process/split/export/write).\n            operation (str): Operation or class name for the step.\n            params (Dict[str, Any]): Parameters for the step.\n        \"\"\"\n        self.steps.append(PipelineStep(name, operation, params))\n\n    def to_yaml(self, file_path: str) -&gt; None:\n        \"\"\"Serialize pipeline to YAML.\n\n        Args:\n            file_path (str): Path to the output YAML file.\n        \"\"\"\n        with open(file_path, \"w\") as f:\n            yaml.dump({\"pipeline\": [step.to_dict() for step in self.steps]}, f)\n\n    @classmethod\n    def from_yaml(cls, file_path: str) -&gt; \"Pipeline\":\n        \"\"\"Load a pipeline from YAML.\n\n        Args:\n            file_path (str): Path to a YAML pipeline file.\n\n        Returns:\n            Pipeline: The loaded pipeline instance.\n        \"\"\"\n        with open(file_path, \"r\") as f:\n            config = yaml.safe_load(f)\n\n        pipeline = cls()\n        for step in config.get(\"pipeline\", []):\n            pipeline.add_step(step['name'], step[\"operation\"], step[\"params\"])\n\n        return pipeline\n\n    def copy(self) -&gt; \"Pipeline\":\n        \"\"\"Deep-copy the pipeline steps.\n\n        Returns:\n            Pipeline: A copy of this pipeline with duplicated steps.\n        \"\"\"\n        pipeline = Pipeline()\n        pipeline.steps = [step.copy() for step in self.steps]\n        return pipeline\n\n    def apply(self, input_folder: Optional[str] = None, output_folder: Optional[str] = None) -&gt; Any:\n        \"\"\"Execute the pipeline.\n\n        Args:\n            input_folder (Optional[str]): Base folder for file-based read steps.\n            output_folder (Optional[str]): Base folder for export/write steps.\n\n        Returns:\n            Any: The final DataRec or split dict, depending on pipeline steps.\n        \"\"\"\n        frameworks = {\n            'Elliot': 'to_elliot',\n            'ClayRS': 'to_clayrs',\n            'Cornac': 'to_cornac',\n            'DaisyRec': 'to_daisyrec',\n            'LensKit': 'to_lenskit',\n            'RecBole': 'to_recbole',\n            'ReChorus': 'to_rechorus',\n            'RecPack': 'to_recpack',\n            'Recommenders': 'to_recommenders'\n        }\n        split_required = {'Elliot', 'ReChorus'}\n\n        if not self.steps:\n            raise ValueError(\"Pipeline is empty. Add at least a load step.\")\n        if self.steps[0].name not in {'load', 'read'}:\n            raise ValueError(f\"The first pipeline step must be a load or read, not {self.steps[0].name}\")\n\n        print(f\"\\n\\n --- Reproducing Pipeline --- \\n\\n\")\n\n        for step in self.steps:\n            print(f\"\\n--- Step: {step.name} -&gt; {step.operation} ---\\n\")\n            func = self.get_transformation_class(step.name, step.operation)\n            if not func:\n                raise ValueError(f\"Unknown operation: {step.operation}\")\n\n            if step.name == 'load':\n                result = self._apply_load(step, func, input_folder)\n\n            elif step.name == 'read':\n                result = self._apply_read(step, func, input_folder)\n\n            elif step.name == 'export':\n                if step.operation not in frameworks:\n                    raise ValueError(f\"Export step requires a framework operation. Unknown: {step.operation}\")\n                self._apply_export(step, func, result, output_folder, frameworks, split_required)\n                return\n\n            elif step.name == 'write':\n                self._apply_write(step, func, result, output_folder)\n                return\n            else:\n                result = self._apply_transform(step, func, result)\n\n        print(f\"\\n\\n --- Finished Pipeline --- \\n\\n\")\n        return result\n\n    def _apply_load(self, step: PipelineStep, func, input_folder: Optional[str]) -&gt; Any:\n        print(f\"Pipeline step {step.name}.\")\n        print(f\"Loading dataset via: {step.operation}.\")\n        params = dict(step.params)\n        if step.operation == \"registry_dataset\":\n            loaded = func(**params)\n        else:\n            raise ValueError(\"Load step supports only 'registry_dataset'. Use 'read' for file-based inputs.\")\n        if hasattr(loaded, \"prepare_and_load\"):\n            return loaded.prepare_and_load()\n        if hasattr(loaded, \"data\"):\n            return loaded\n        raise ValueError(f\"Loader '{step.operation}' did not return a DataRec or an object with prepare_and_load.\")\n\n    def _apply_read(self, step: PipelineStep, func, input_folder: Optional[str]) -&gt; Any:\n        print(f\"Pipeline step {step.name}.\")\n        print(f\"Reading dataset via: {step.operation}.\")\n        params = dict(step.params)\n        if input_folder is None:\n            raise ValueError(\"Read step requires input_folder and 'filename' in params.\")\n        if \"filepath\" in params:\n            raise ValueError(\"Read step does not accept 'filepath' in pipelines. Use 'filename' and pass input_folder to apply().\")\n        filename = params.pop(\"filename\", None)\n        if not filename:\n            raise ValueError(\"Read step requires 'filename' in params.\")\n        dataset_name = params.pop(\"dataset_name\", None)\n        version_name = params.pop(\"version_name\", \"file\")\n        params[\"filepath\"] = str(Path(input_folder) / filename)\n        loaded = func(**params)\n        if isinstance(loaded, RawData):\n            from datarec.data.dataset import DataRec\n            if dataset_name is None:\n                dataset_name = Path(filename).stem\n            loaded = DataRec(rawdata=loaded, dataset_name=dataset_name, version_name=version_name)\n        if hasattr(loaded, \"data\"):\n            return loaded\n        raise ValueError(f\"Reader '{step.operation}' did not return a DataRec/RawData.\")\n\n    def _apply_transform(self, step: PipelineStep, func, result):\n        print(f\"Pipeline step {step.name}.\")\n        print(f\"Applying {func}.\")\n        return func(**step.params).run(result)\n\n    def _apply_export(\n        self,\n        step: PipelineStep,\n        func,\n        result,\n        output_folder: Optional[str],\n        frameworks: Dict[str, str],\n        split_required: set,\n    ) -&gt; None:\n        print(f\"Pipeline step {step.name}.\")\n        print(f\"Exporting dataset for {step.operation}.\")\n        params = dict(step.params)\n        if output_folder is None:\n            if \"filename\" in params and \"output_path\" in params:\n                raise ValueError(\"Framework export should not include both 'filename' and 'output_path'.\")\n            if \"filename\" in params and \"output_path\" not in params:\n                raise ValueError(\"Framework export requires output_folder when using 'filename' in params.\")\n            if \"output_path\" not in params:\n                raise ValueError(\"Framework export requires 'output_path' or 'filename' in params.\")\n        else:\n            if \"output_path\" in params:\n                raise ValueError(\"Framework export does not accept 'output_path' when output_folder is provided. Use 'filename'.\")\n            filename = params.pop(\"filename\", None)\n            if not filename:\n                raise ValueError(\"Framework export requires 'filename' in params.\")\n            params[\"output_path\"] = str(Path(output_folder) / filename)\n        exporter = func(**params)\n        function_name = frameworks[step.operation]\n        if isinstance(result, dict):\n            if step.operation in split_required:\n                train = result.get('train')\n                test = result.get('test')\n                val = result.get('val')\n                if train is None or test is None:\n                    raise ValueError(f\"Export '{step.operation}' requires train/test splits.\")\n                getattr(exporter, function_name)(train, test, val)\n            else:\n                export_data = result.get('train')\n                if export_data is None and len(result) == 1:\n                    export_data = next(iter(result.values()))\n                if export_data is None:\n                    raise ValueError(f\"Export '{step.operation}' does not support split dicts without a train split.\")\n                getattr(exporter, function_name)(export_data)\n        else:\n            if step.operation in split_required:\n                raise ValueError(f\"Export '{step.operation}' requires a split result (train/test[/val]).\")\n            getattr(exporter, function_name)(result)\n        return\n\n    def _apply_write(\n        self,\n        step: PipelineStep,\n        func,\n        result,\n        output_folder: Optional[str],\n    ) -&gt; None:\n        print(f\"Pipeline step {step.name}.\")\n        print(f\"Writing dataset via: {step.operation}.\")\n        params = dict(step.params)\n        if output_folder is None:\n            if \"filename\" in params and \"filepath\" not in params:\n                raise ValueError(\"Write step requires output_folder when using 'filename' in params.\")\n            if \"filename\" in params and \"filepath\" in params:\n                raise ValueError(\"Write step should not include both 'filename' and 'filepath'.\")\n            if \"filepath\" not in params:\n                raise ValueError(\"Write step requires 'filepath' or 'filename' in params.\")\n            base_path = Path(params.get(\"filepath\"))\n        else:\n            if \"filepath\" in params:\n                raise ValueError(\"Write step does not accept 'filepath' when output_folder is provided. Use 'filename'.\")\n            filename = params.pop(\"filename\", None)\n            if not filename:\n                raise ValueError(\"Write step requires 'filename' in params.\")\n            base_path = Path(output_folder) / filename\n        if isinstance(result, dict):\n            split_param = params.pop(\"split\", None)\n            if split_param:\n                split_data = result.get(split_param)\n                if split_data is None:\n                    raise ValueError(f\"Split '{split_param}' not found for writer export.\")\n                params[\"filepath\"] = str(base_path)\n                func(split_data, **params)\n            else:\n                for split_name, split_data in result.items():\n                    split_path = _with_split_suffix(base_path, split_name)\n                    params[\"filepath\"] = str(split_path)\n                    func(split_data, **params)\n        else:\n            params[\"filepath\"] = str(base_path)\n            func(result, **params)\n\n    @staticmethod\n    def get_transformation_class(package_name: str, class_name: str) -&gt; Any:\n        \"\"\"Resolve a pipeline step to its callable class/function.\n\n        Args:\n            package_name (str): Pipeline step name (load/process/split/export).\n            class_name (str): Class name or special loader name.\n\n        Returns:\n            Any: Callable class/function implementing the step.\n        \"\"\"\n        mapping = {\n            'load': 'datasets',\n            'process': 'processing',\n            'split': 'splitters',\n            'export': 'io',\n            'read': 'io',\n            'write': 'io',\n        }\n\n        if package_name not in mapping.keys():\n            raise ValueError(f\"Unknown package name '{package_name}'\")\n\n        module_name = \"datarec.\" + mapping[package_name]\n\n        if package_name == 'load':\n            if class_name == \"registry_dataset\":\n                return _load_registry_dataset\n            readers_module = importlib.import_module(\"datarec.io.readers\")\n            if hasattr(readers_module, class_name):\n                return getattr(readers_module, class_name)\n            raise ImportError(\n                f\"Could not find reader '{class_name}' in datarec.io.readers.\"\n            )\n        if package_name == 'read':\n            readers_module = importlib.import_module(\"datarec.io.readers\")\n            if hasattr(readers_module, class_name):\n                return getattr(readers_module, class_name)\n            raise ImportError(\n                f\"Could not find reader '{class_name}' in datarec.io.readers.\"\n            )\n        if package_name == 'export':\n            module = importlib.import_module(module_name)\n            return getattr(module, \"FrameworkExporter\")\n        if package_name == 'write':\n            writers_module = importlib.import_module(\"datarec.io.writers\")\n            if hasattr(writers_module, class_name):\n                return getattr(writers_module, class_name)\n            raise ImportError(\n                f\"Could not find writer '{class_name}' in datarec.io.writers.\"\n            )\n        try:\n            module = importlib.import_module(module_name)\n            return getattr(module, class_name)\n        except (ModuleNotFoundError, AttributeError) as e:\n            raise ImportError(f\"Could not find class {class_name} in module {module_name}\") from e\n</code></pre>"},{"location":"documentation/pipeline/#datarec.pipeline.pipeline.Pipeline.__str__","title":"<code>__str__()</code>","text":"<p>Return a readable summary of pipeline steps.</p> Source code in <code>datarec/pipeline/pipeline.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a readable summary of pipeline steps.\"\"\"\n    if not self.steps:\n        return \"Pipeline(steps=0)\"\n    lines = [\"Pipeline:\"]\n    for idx, step in enumerate(self.steps, start=1):\n        params = \", \".join(f\"{k}={v!r}\" for k, v in step.params.items())\n        lines.append(f\"{idx}. {step.name} -&gt; {step.operation}({params})\")\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"documentation/pipeline/#datarec.pipeline.pipeline.Pipeline.add_step","title":"<code>add_step(name, operation, params)</code>","text":"<p>Add a pipeline step.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Step name (load/read/process/split/export/write).</p> required <code>operation</code> <code>str</code> <p>Operation or class name for the step.</p> required <code>params</code> <code>Dict[str, Any]</code> <p>Parameters for the step.</p> required Source code in <code>datarec/pipeline/pipeline.py</code> <pre><code>def add_step(self, name: str, operation: str, params: Dict[str, Any]) -&gt; None:\n    \"\"\"Add a pipeline step.\n\n    Args:\n        name (str): Step name (load/read/process/split/export/write).\n        operation (str): Operation or class name for the step.\n        params (Dict[str, Any]): Parameters for the step.\n    \"\"\"\n    self.steps.append(PipelineStep(name, operation, params))\n</code></pre>"},{"location":"documentation/pipeline/#datarec.pipeline.pipeline.Pipeline.to_yaml","title":"<code>to_yaml(file_path)</code>","text":"<p>Serialize pipeline to YAML.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the output YAML file.</p> required Source code in <code>datarec/pipeline/pipeline.py</code> <pre><code>def to_yaml(self, file_path: str) -&gt; None:\n    \"\"\"Serialize pipeline to YAML.\n\n    Args:\n        file_path (str): Path to the output YAML file.\n    \"\"\"\n    with open(file_path, \"w\") as f:\n        yaml.dump({\"pipeline\": [step.to_dict() for step in self.steps]}, f)\n</code></pre>"},{"location":"documentation/pipeline/#datarec.pipeline.pipeline.Pipeline.from_yaml","title":"<code>from_yaml(file_path)</code>  <code>classmethod</code>","text":"<p>Load a pipeline from YAML.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to a YAML pipeline file.</p> required <p>Returns:</p> Name Type Description <code>Pipeline</code> <code>Pipeline</code> <p>The loaded pipeline instance.</p> Source code in <code>datarec/pipeline/pipeline.py</code> <pre><code>@classmethod\ndef from_yaml(cls, file_path: str) -&gt; \"Pipeline\":\n    \"\"\"Load a pipeline from YAML.\n\n    Args:\n        file_path (str): Path to a YAML pipeline file.\n\n    Returns:\n        Pipeline: The loaded pipeline instance.\n    \"\"\"\n    with open(file_path, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    pipeline = cls()\n    for step in config.get(\"pipeline\", []):\n        pipeline.add_step(step['name'], step[\"operation\"], step[\"params\"])\n\n    return pipeline\n</code></pre>"},{"location":"documentation/pipeline/#datarec.pipeline.pipeline.Pipeline.copy","title":"<code>copy()</code>","text":"<p>Deep-copy the pipeline steps.</p> <p>Returns:</p> Name Type Description <code>Pipeline</code> <code>Pipeline</code> <p>A copy of this pipeline with duplicated steps.</p> Source code in <code>datarec/pipeline/pipeline.py</code> <pre><code>def copy(self) -&gt; \"Pipeline\":\n    \"\"\"Deep-copy the pipeline steps.\n\n    Returns:\n        Pipeline: A copy of this pipeline with duplicated steps.\n    \"\"\"\n    pipeline = Pipeline()\n    pipeline.steps = [step.copy() for step in self.steps]\n    return pipeline\n</code></pre>"},{"location":"documentation/pipeline/#datarec.pipeline.pipeline.Pipeline.apply","title":"<code>apply(input_folder=None, output_folder=None)</code>","text":"<p>Execute the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>input_folder</code> <code>Optional[str]</code> <p>Base folder for file-based read steps.</p> <code>None</code> <code>output_folder</code> <code>Optional[str]</code> <p>Base folder for export/write steps.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The final DataRec or split dict, depending on pipeline steps.</p> Source code in <code>datarec/pipeline/pipeline.py</code> <pre><code>def apply(self, input_folder: Optional[str] = None, output_folder: Optional[str] = None) -&gt; Any:\n    \"\"\"Execute the pipeline.\n\n    Args:\n        input_folder (Optional[str]): Base folder for file-based read steps.\n        output_folder (Optional[str]): Base folder for export/write steps.\n\n    Returns:\n        Any: The final DataRec or split dict, depending on pipeline steps.\n    \"\"\"\n    frameworks = {\n        'Elliot': 'to_elliot',\n        'ClayRS': 'to_clayrs',\n        'Cornac': 'to_cornac',\n        'DaisyRec': 'to_daisyrec',\n        'LensKit': 'to_lenskit',\n        'RecBole': 'to_recbole',\n        'ReChorus': 'to_rechorus',\n        'RecPack': 'to_recpack',\n        'Recommenders': 'to_recommenders'\n    }\n    split_required = {'Elliot', 'ReChorus'}\n\n    if not self.steps:\n        raise ValueError(\"Pipeline is empty. Add at least a load step.\")\n    if self.steps[0].name not in {'load', 'read'}:\n        raise ValueError(f\"The first pipeline step must be a load or read, not {self.steps[0].name}\")\n\n    print(f\"\\n\\n --- Reproducing Pipeline --- \\n\\n\")\n\n    for step in self.steps:\n        print(f\"\\n--- Step: {step.name} -&gt; {step.operation} ---\\n\")\n        func = self.get_transformation_class(step.name, step.operation)\n        if not func:\n            raise ValueError(f\"Unknown operation: {step.operation}\")\n\n        if step.name == 'load':\n            result = self._apply_load(step, func, input_folder)\n\n        elif step.name == 'read':\n            result = self._apply_read(step, func, input_folder)\n\n        elif step.name == 'export':\n            if step.operation not in frameworks:\n                raise ValueError(f\"Export step requires a framework operation. Unknown: {step.operation}\")\n            self._apply_export(step, func, result, output_folder, frameworks, split_required)\n            return\n\n        elif step.name == 'write':\n            self._apply_write(step, func, result, output_folder)\n            return\n        else:\n            result = self._apply_transform(step, func, result)\n\n    print(f\"\\n\\n --- Finished Pipeline --- \\n\\n\")\n    return result\n</code></pre>"},{"location":"documentation/pipeline/#datarec.pipeline.pipeline.Pipeline.get_transformation_class","title":"<code>get_transformation_class(package_name, class_name)</code>  <code>staticmethod</code>","text":"<p>Resolve a pipeline step to its callable class/function.</p> <p>Parameters:</p> Name Type Description Default <code>package_name</code> <code>str</code> <p>Pipeline step name (load/process/split/export).</p> required <code>class_name</code> <code>str</code> <p>Class name or special loader name.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Callable class/function implementing the step.</p> Source code in <code>datarec/pipeline/pipeline.py</code> <pre><code>@staticmethod\ndef get_transformation_class(package_name: str, class_name: str) -&gt; Any:\n    \"\"\"Resolve a pipeline step to its callable class/function.\n\n    Args:\n        package_name (str): Pipeline step name (load/process/split/export).\n        class_name (str): Class name or special loader name.\n\n    Returns:\n        Any: Callable class/function implementing the step.\n    \"\"\"\n    mapping = {\n        'load': 'datasets',\n        'process': 'processing',\n        'split': 'splitters',\n        'export': 'io',\n        'read': 'io',\n        'write': 'io',\n    }\n\n    if package_name not in mapping.keys():\n        raise ValueError(f\"Unknown package name '{package_name}'\")\n\n    module_name = \"datarec.\" + mapping[package_name]\n\n    if package_name == 'load':\n        if class_name == \"registry_dataset\":\n            return _load_registry_dataset\n        readers_module = importlib.import_module(\"datarec.io.readers\")\n        if hasattr(readers_module, class_name):\n            return getattr(readers_module, class_name)\n        raise ImportError(\n            f\"Could not find reader '{class_name}' in datarec.io.readers.\"\n        )\n    if package_name == 'read':\n        readers_module = importlib.import_module(\"datarec.io.readers\")\n        if hasattr(readers_module, class_name):\n            return getattr(readers_module, class_name)\n        raise ImportError(\n            f\"Could not find reader '{class_name}' in datarec.io.readers.\"\n        )\n    if package_name == 'export':\n        module = importlib.import_module(module_name)\n        return getattr(module, \"FrameworkExporter\")\n    if package_name == 'write':\n        writers_module = importlib.import_module(\"datarec.io.writers\")\n        if hasattr(writers_module, class_name):\n            return getattr(writers_module, class_name)\n        raise ImportError(\n            f\"Could not find writer '{class_name}' in datarec.io.writers.\"\n        )\n    try:\n        module = importlib.import_module(module_name)\n        return getattr(module, class_name)\n    except (ModuleNotFoundError, AttributeError) as e:\n        raise ImportError(f\"Could not find class {class_name} in module {module_name}\") from e\n</code></pre>"},{"location":"documentation/processing/","title":"Processing Module Reference","text":"<p>This section provides a detailed API reference for all modules related to preprocessing and filtering datasets. Processing steps operate on <code>DataRec</code> and return a new <code>DataRec</code> with the pipeline updated.</p>"},{"location":"documentation/processing/#on-this-page","title":"On This Page","text":"<ul> <li>Processing Components</li> <li>Minimal usage</li> </ul>"},{"location":"documentation/processing/#minimal-usage","title":"Minimal usage","text":"<pre><code>from datarec.processing import FilterOutDuplicatedInteractions, UserItemIterativeKCore\n\ndata = FilterOutDuplicatedInteractions().run(data)\ndata = UserItemIterativeKCore(cores=5).run(data)\n</code></pre>"},{"location":"documentation/processing/#processing-components","title":"Processing Components","text":""},{"location":"documentation/processing/#datarec.processing.binarizer.Binarize","title":"<code>Binarize</code>","text":"<p>               Bases: <code>Processor</code></p> <p>A class for binarizing rating values in a dataset based on a given threshold. </p> <p>This class processes a dataset wrapped in a DataRec object and modifies the rating column based on the specified threshold. If <code>implicit</code> is set to True, rows with ratings below the threshold are removed, and the rating column is dropped. Otherwise, ratings are binarized to either <code>over_threshold</code> or <code>under_threshold</code> values.</p> Source code in <code>datarec/processing/binarizer.py</code> <pre><code>class Binarize(Processor):\n\n    \"\"\"\n    A class for binarizing rating values in a dataset based on a given threshold. \n\n    This class processes a dataset wrapped in a DataRec object and modifies the rating column\n    based on the specified threshold. If `implicit` is set to True, rows with ratings below\n    the threshold are removed, and the rating column is dropped. Otherwise, ratings are binarized\n    to either `over_threshold` or `under_threshold` values.\n    \"\"\"\n\n    def __init__(self, threshold: float, implicit: bool = False,\n                 over_threshold: float = 1, under_threshold: float = 0):\n        \"\"\"\n        Initializes the Binarize object.\n\n        Args:\n            threshold (float): The threshold for binarization.\n            implicit (bool): If True, removes rows below the threshold and drops the rating column.\n            over_threshold (int, float): The value assigned to ratings equal to or above the threshold.\n            under_threshold (int, float): The value assigned to ratings below the threshold.\n        \"\"\"\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self._threshold = threshold\n        self._over_threshold = over_threshold\n        self._under_threshold = under_threshold\n        self._implicit = implicit\n\n    def run(self, datarec: DataRec) -&gt; DataRec:\n\n        \"\"\"\n        Binarizes the rating values in the given dataset based on a threshold.\n\n        If `implicit` is True, removes rows where the rating is below the threshold\n        and drops the rating column. If `implicit` is False, replaces the rating\n        values with binary values (over_threshold if &gt;= threshold, under_threshold otherwise).\n\n        Args:\n            datarec (DataRec): The input dataset wrapped in a DataRec object.\n\n        Returns:\n            (DataRec): A new DataRec object with the processed dataset.\n        \"\"\"\n\n        dataset = datarec.data.copy()\n        column = datarec.rating_col\n\n        positive = dataset[column] &gt;= self._threshold\n\n        if self._implicit:\n            dataset = dataset[positive].copy()\n            dataset.drop(columns=[column], inplace=True)\n        else:\n            dataset[column] = self._over_threshold\n            dataset.loc[~positive, column] = self._under_threshold\n\n        result = self.output(datarec, dataset,\n                             step_info={'operation': self.__class__.__name__, 'params': self.params})\n\n        return result\n\n    @property\n    def binary_threshold(self) -&gt; float:\n        \"\"\"\n        Returns the rating threshold used to distinguish positive interactions.\n        \"\"\"\n        return self._threshold\n\n    @property\n    def over_threshold(self) -&gt; float:\n        \"\"\"\n        Returns the value assigned to ratings at or above the threshold.\n        \"\"\"\n        return self._over_threshold\n\n    @property\n    def under_threshold(self) -&gt; float:\n        \"\"\"\n        Returns the value assigned to ratings below the threshold.\n        \"\"\"\n        return self._under_threshold\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.binarizer.Binarize.binary_threshold","title":"<code>binary_threshold</code>  <code>property</code>","text":"<p>Returns the rating threshold used to distinguish positive interactions.</p>"},{"location":"documentation/processing/#datarec.processing.binarizer.Binarize.over_threshold","title":"<code>over_threshold</code>  <code>property</code>","text":"<p>Returns the value assigned to ratings at or above the threshold.</p>"},{"location":"documentation/processing/#datarec.processing.binarizer.Binarize.under_threshold","title":"<code>under_threshold</code>  <code>property</code>","text":"<p>Returns the value assigned to ratings below the threshold.</p>"},{"location":"documentation/processing/#datarec.processing.binarizer.Binarize.__init__","title":"<code>__init__(threshold, implicit=False, over_threshold=1, under_threshold=0)</code>","text":"<p>Initializes the Binarize object.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The threshold for binarization.</p> required <code>implicit</code> <code>bool</code> <p>If True, removes rows below the threshold and drops the rating column.</p> <code>False</code> <code>over_threshold</code> <code>(int, float)</code> <p>The value assigned to ratings equal to or above the threshold.</p> <code>1</code> <code>under_threshold</code> <code>(int, float)</code> <p>The value assigned to ratings below the threshold.</p> <code>0</code> Source code in <code>datarec/processing/binarizer.py</code> <pre><code>def __init__(self, threshold: float, implicit: bool = False,\n             over_threshold: float = 1, under_threshold: float = 0):\n    \"\"\"\n    Initializes the Binarize object.\n\n    Args:\n        threshold (float): The threshold for binarization.\n        implicit (bool): If True, removes rows below the threshold and drops the rating column.\n        over_threshold (int, float): The value assigned to ratings equal to or above the threshold.\n        under_threshold (int, float): The value assigned to ratings below the threshold.\n    \"\"\"\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self._threshold = threshold\n    self._over_threshold = over_threshold\n    self._under_threshold = under_threshold\n    self._implicit = implicit\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.binarizer.Binarize.run","title":"<code>run(datarec)</code>","text":"<p>Binarizes the rating values in the given dataset based on a threshold.</p> <p>If <code>implicit</code> is True, removes rows where the rating is below the threshold and drops the rating column. If <code>implicit</code> is False, replaces the rating values with binary values (over_threshold if &gt;= threshold, under_threshold otherwise).</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The input dataset wrapped in a DataRec object.</p> required <p>Returns:</p> Type Description <code>DataRec</code> <p>A new DataRec object with the processed dataset.</p> Source code in <code>datarec/processing/binarizer.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; DataRec:\n\n    \"\"\"\n    Binarizes the rating values in the given dataset based on a threshold.\n\n    If `implicit` is True, removes rows where the rating is below the threshold\n    and drops the rating column. If `implicit` is False, replaces the rating\n    values with binary values (over_threshold if &gt;= threshold, under_threshold otherwise).\n\n    Args:\n        datarec (DataRec): The input dataset wrapped in a DataRec object.\n\n    Returns:\n        (DataRec): A new DataRec object with the processed dataset.\n    \"\"\"\n\n    dataset = datarec.data.copy()\n    column = datarec.rating_col\n\n    positive = dataset[column] &gt;= self._threshold\n\n    if self._implicit:\n        dataset = dataset[positive].copy()\n        dataset.drop(columns=[column], inplace=True)\n    else:\n        dataset[column] = self._over_threshold\n        dataset.loc[~positive, column] = self._under_threshold\n\n    result = self.output(datarec, dataset,\n                         step_info={'operation': self.__class__.__name__, 'params': self.params})\n\n    return result\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.cold.ColdFilter","title":"<code>ColdFilter</code>","text":"<p>               Bases: <code>Processor</code></p> <p>A filtering class to retain only cold users or cold items, i.e., those with at most <code>interactions</code> interactions in the original DataRec dataset.</p> Source code in <code>datarec/processing/cold.py</code> <pre><code>class ColdFilter(Processor):\n    \"\"\"\n    A filtering class to retain only cold users or cold items, i.e., those with at most `interactions` interactions\n    in the original DataRec dataset.\n    \"\"\"\n\n    def __init__(self, interactions: int, mode: str = \"user\"):\n        \"\"\"\n        Initializes the ColdFilter object.\n\n        Args:\n            interactions (int): The maximum number of interactions a user or item can have to be retained.\n            mode (str): Filtering mode, either \"user\" for cold users or \"item\" for cold items.\n\n        Raises:\n            TypeError: If `interactions` is not an integer.\n            ValueError: If `mode` is not \"user\" or \"item\".\n        \"\"\"\n        if not isinstance(interactions, int):\n            raise TypeError('Interactions must be an integer.')\n\n        if mode not in {\"user\", \"item\"}:\n            raise ValueError('Mode must be \"user\" or \"item\".')\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.interactions = interactions\n        self.mode = mode\n\n    def run(self, datarec: DataRec) -&gt; DataRec:\n        \"\"\"\n        Filters the dataset to keep only cold users or cold items with at most `self.interactions` interactions.\n\n        Args:\n            datarec (DataRec): The input dataset wrapped in a DataRec object.\n\n        Returns:\n            (DataRec): A new DataRec object containing only the filtered users or items.\n        \"\"\"\n\n        dataset = datarec.data.copy()\n        group_col = datarec.user_col if self.mode == \"user\" else datarec.item_col\n        groups = dataset.groupby(group_col)\n        result = groups.filter(lambda x: len(x) &lt;= self.interactions).reset_index(drop=True)\n\n        return self.output(datarec, result, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.cold.ColdFilter.__init__","title":"<code>__init__(interactions, mode='user')</code>","text":"<p>Initializes the ColdFilter object.</p> <p>Parameters:</p> Name Type Description Default <code>interactions</code> <code>int</code> <p>The maximum number of interactions a user or item can have to be retained.</p> required <code>mode</code> <code>str</code> <p>Filtering mode, either \"user\" for cold users or \"item\" for cold items.</p> <code>'user'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>interactions</code> is not an integer.</p> <code>ValueError</code> <p>If <code>mode</code> is not \"user\" or \"item\".</p> Source code in <code>datarec/processing/cold.py</code> <pre><code>def __init__(self, interactions: int, mode: str = \"user\"):\n    \"\"\"\n    Initializes the ColdFilter object.\n\n    Args:\n        interactions (int): The maximum number of interactions a user or item can have to be retained.\n        mode (str): Filtering mode, either \"user\" for cold users or \"item\" for cold items.\n\n    Raises:\n        TypeError: If `interactions` is not an integer.\n        ValueError: If `mode` is not \"user\" or \"item\".\n    \"\"\"\n    if not isinstance(interactions, int):\n        raise TypeError('Interactions must be an integer.')\n\n    if mode not in {\"user\", \"item\"}:\n        raise ValueError('Mode must be \"user\" or \"item\".')\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.interactions = interactions\n    self.mode = mode\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.cold.ColdFilter.run","title":"<code>run(datarec)</code>","text":"<p>Filters the dataset to keep only cold users or cold items with at most <code>self.interactions</code> interactions.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The input dataset wrapped in a DataRec object.</p> required <p>Returns:</p> Type Description <code>DataRec</code> <p>A new DataRec object containing only the filtered users or items.</p> Source code in <code>datarec/processing/cold.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; DataRec:\n    \"\"\"\n    Filters the dataset to keep only cold users or cold items with at most `self.interactions` interactions.\n\n    Args:\n        datarec (DataRec): The input dataset wrapped in a DataRec object.\n\n    Returns:\n        (DataRec): A new DataRec object containing only the filtered users or items.\n    \"\"\"\n\n    dataset = datarec.data.copy()\n    group_col = datarec.user_col if self.mode == \"user\" else datarec.item_col\n    groups = dataset.groupby(group_col)\n    result = groups.filter(lambda x: len(x) &lt;= self.interactions).reset_index(drop=True)\n\n    return self.output(datarec, result, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.KCore","title":"<code>KCore</code>","text":"<p>This class filters a dataset based on a minimum number of records (core) for each group defined by a specific column.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>class KCore:\n\n    \"\"\"\n    This class filters a dataset based on a minimum number of records (core) for each group\n    defined by a specific column.\n    \"\"\"\n\n    def __init__(self, column: str, core: int):\n        \"\"\"\n        Initializes the KCore object.\n\n        Args:\n            column (str): The column name used to group the data (e.g., user or item).\n            core (int): The minimum number of records required for each group to be kept.\n\n        Raises:\n            TypeError: If 'core' is not an integer.\n        \"\"\"\n\n        if not isinstance(core, int):\n            raise TypeError('Core must be an integer.')\n\n        self._column = column\n        self._core = core\n\n    def run(self, dataset: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Filters the dataset by keeping only groups with at least the specified number of records.\n\n        Args:\n            dataset (pd.DataFrame): The dataset to be filtered.\n\n        Returns:\n            (pd.DataFrame): A new dataframe with groups filtered by the core condition.\n\n        Raises: \n            ValueError: If 'self._column' is not in the dataset.\n\n        \"\"\"\n\n        if self._column not in dataset.columns:\n            raise ValueError(f'Column \"{self._column}\" not in the dataset.')\n\n        dataset = dataset.copy()\n        groups = dataset.groupby([self._column])\n        dataset = groups.filter(lambda x: len(x) &gt;= self._core)\n        return dataset\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.KCore.__init__","title":"<code>__init__(column, core)</code>","text":"<p>Initializes the KCore object.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>str</code> <p>The column name used to group the data (e.g., user or item).</p> required <code>core</code> <code>int</code> <p>The minimum number of records required for each group to be kept.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If 'core' is not an integer.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def __init__(self, column: str, core: int):\n    \"\"\"\n    Initializes the KCore object.\n\n    Args:\n        column (str): The column name used to group the data (e.g., user or item).\n        core (int): The minimum number of records required for each group to be kept.\n\n    Raises:\n        TypeError: If 'core' is not an integer.\n    \"\"\"\n\n    if not isinstance(core, int):\n        raise TypeError('Core must be an integer.')\n\n    self._column = column\n    self._core = core\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.KCore.run","title":"<code>run(dataset)</code>","text":"<p>Filters the dataset by keeping only groups with at least the specified number of records.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataFrame</code> <p>The dataset to be filtered.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new dataframe with groups filtered by the core condition.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'self._column' is not in the dataset.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def run(self, dataset: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters the dataset by keeping only groups with at least the specified number of records.\n\n    Args:\n        dataset (pd.DataFrame): The dataset to be filtered.\n\n    Returns:\n        (pd.DataFrame): A new dataframe with groups filtered by the core condition.\n\n    Raises: \n        ValueError: If 'self._column' is not in the dataset.\n\n    \"\"\"\n\n    if self._column not in dataset.columns:\n        raise ValueError(f'Column \"{self._column}\" not in the dataset.')\n\n    dataset = dataset.copy()\n    groups = dataset.groupby([self._column])\n    dataset = groups.filter(lambda x: len(x) &gt;= self._core)\n    return dataset\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.UserKCore","title":"<code>UserKCore</code>","text":"<p>               Bases: <code>Processor</code></p> <p>Filters a dataset based on a minimum number of records (core) for each user.</p> <p>This class applies a KCore filter on the user column of the dataset.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>class UserKCore(Processor):\n    \"\"\"\n    Filters a dataset based on a minimum number of records (core) for each user.\n\n    This class applies a KCore filter on the user column of the dataset.\n    \"\"\"\n    def __init__(self, core: int):\n        \"\"\"\n        Initializes the UserKCore object.\n\n        Args:\n            core (int): The minimum number of records required for each user to be kept.\n\n        Raises: \n            TypeErrore: If 'core' is not an integer.\n        \"\"\"\n        if not isinstance(core, int):\n            raise TypeError('Core must be an integer.')\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.core = core\n\n    def run(self, datarec: DataRec) -&gt; DataRec:\n        \"\"\"\n        Filters the dataset by user, applying the KCore filter, and returns a new DataRec object\n        containing the filtered data.\n\n        Args:\n            datarec (DataRec): The DataRec object containing the dataset to be filtered.\n\n        Returns:\n            (DataRec): A new DataRec object with the filtered data.\n\n        \"\"\"\n\n        core_obj = KCore(column=datarec.user_col, core=self.core)\n        result = core_obj.run(datarec.data)\n\n        return self.output(datarec, result, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.UserKCore.__init__","title":"<code>__init__(core)</code>","text":"<p>Initializes the UserKCore object.</p> <p>Parameters:</p> Name Type Description Default <code>core</code> <code>int</code> <p>The minimum number of records required for each user to be kept.</p> required <p>Raises:</p> Type Description <code>TypeErrore</code> <p>If 'core' is not an integer.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def __init__(self, core: int):\n    \"\"\"\n    Initializes the UserKCore object.\n\n    Args:\n        core (int): The minimum number of records required for each user to be kept.\n\n    Raises: \n        TypeErrore: If 'core' is not an integer.\n    \"\"\"\n    if not isinstance(core, int):\n        raise TypeError('Core must be an integer.')\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.core = core\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.UserKCore.run","title":"<code>run(datarec)</code>","text":"<p>Filters the dataset by user, applying the KCore filter, and returns a new DataRec object containing the filtered data.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The DataRec object containing the dataset to be filtered.</p> required <p>Returns:</p> Type Description <code>DataRec</code> <p>A new DataRec object with the filtered data.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; DataRec:\n    \"\"\"\n    Filters the dataset by user, applying the KCore filter, and returns a new DataRec object\n    containing the filtered data.\n\n    Args:\n        datarec (DataRec): The DataRec object containing the dataset to be filtered.\n\n    Returns:\n        (DataRec): A new DataRec object with the filtered data.\n\n    \"\"\"\n\n    core_obj = KCore(column=datarec.user_col, core=self.core)\n    result = core_obj.run(datarec.data)\n\n    return self.output(datarec, result, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.ItemKCore","title":"<code>ItemKCore</code>","text":"<p>               Bases: <code>Processor</code></p> <p>Filters a dataset based on a minimum number of records (core) for each item.</p> <p>This class applies a KCore filter on the item column of the dataset.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>class ItemKCore(Processor):\n    \"\"\"\n    Filters a dataset based on a minimum number of records (core) for each item.\n\n    This class applies a KCore filter on the item column of the dataset.\n    \"\"\"\n    def __init__(self, core: int):\n        \"\"\"\n        Initializes the ItemKCore object.\n\n        Args:\n            core (int): The minimum number of records required for each item to be kept.\n\n        Raises:\n            TypeError: If \"core\" is not an integer.\n        \"\"\"\n\n        if not isinstance(core, int):\n            raise TypeError('Core must be an integer.')\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.core = core\n\n    def run(self, datarec: DataRec) -&gt; DataRec:\n        \"\"\"\n        Filters the dataset by item, applying the KCore filter, and returns a new DataRec object\n        containing the filtered data.\n\n        Args:\n            datarec (DataRec): The DataRec object containing the dataset to be filtered.\n\n        Returns:\n            (DataRec): A new DataRec object with the filtered data.\n        \"\"\"\n\n        core_obj = KCore(column=datarec.item_col, core=self.core)\n        result = core_obj.run(datarec.data)\n\n        return self.output(datarec, result, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.ItemKCore.__init__","title":"<code>__init__(core)</code>","text":"<p>Initializes the ItemKCore object.</p> <p>Parameters:</p> Name Type Description Default <code>core</code> <code>int</code> <p>The minimum number of records required for each item to be kept.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If \"core\" is not an integer.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def __init__(self, core: int):\n    \"\"\"\n    Initializes the ItemKCore object.\n\n    Args:\n        core (int): The minimum number of records required for each item to be kept.\n\n    Raises:\n        TypeError: If \"core\" is not an integer.\n    \"\"\"\n\n    if not isinstance(core, int):\n        raise TypeError('Core must be an integer.')\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.core = core\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.ItemKCore.run","title":"<code>run(datarec)</code>","text":"<p>Filters the dataset by item, applying the KCore filter, and returns a new DataRec object containing the filtered data.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The DataRec object containing the dataset to be filtered.</p> required <p>Returns:</p> Type Description <code>DataRec</code> <p>A new DataRec object with the filtered data.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; DataRec:\n    \"\"\"\n    Filters the dataset by item, applying the KCore filter, and returns a new DataRec object\n    containing the filtered data.\n\n    Args:\n        datarec (DataRec): The DataRec object containing the dataset to be filtered.\n\n    Returns:\n        (DataRec): A new DataRec object with the filtered data.\n    \"\"\"\n\n    core_obj = KCore(column=datarec.item_col, core=self.core)\n    result = core_obj.run(datarec.data)\n\n    return self.output(datarec, result, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.IterativeKCore","title":"<code>IterativeKCore</code>","text":"<p>Iteratively filters a dataset based on a set of columns and minimum core values.</p> <p>This class applies KCore filters to multiple columns and iteratively removes groups that do not meet the core requirement until no further changes occur.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>class IterativeKCore:\n    \"\"\"        \n    Iteratively filters a dataset based on a set of columns and minimum core values.\n\n    This class applies KCore filters to multiple columns and iteratively removes groups\n    that do not meet the core requirement until no further changes occur.\n    \"\"\"\n    def __init__(self, columns: list, cores: Union[int, list]):\n        \"\"\"\n        Initializes the IterativeKCore object.\n\n        Args:\n            columns (list): A list of column names to apply the KCore filter on.\n            cores (list of int or int): The minimum number of records required for each column to be kept.\n\n        Raises:\n            TypeError: If 'cores' in not a list or an integer.\n        \"\"\"\n\n        self._columns = columns\n\n        if isinstance(cores, list):\n            self._cores = list(zip(columns, cores))\n        elif isinstance(cores, int):\n            self._cores = [(c, cores) for c in columns]\n        else:\n            raise TypeError('Cores must be a list or an integer.')\n\n    def run(self, dataset: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Iteratively applies the KCore filters on the dataset until no changes occur, then returns the filtered dataset.\n\n        Args:\n            dataset (pd.DataFrame): The dataset to be iteratively filtered.\n\n        Returns:\n            (pd.DataFrame): The filtered dataset after all iterations.\n        \"\"\"\n\n        data = dataset.copy()\n\n        filters = {c: KCore(column=c, core=k) for c, k in self._cores}\n        checks = [False for _ in self._columns]\n        prev_len = len(data)\n\n        while not all(checks):\n            checks = []\n            for c, f in filters.items():\n                data = f.run(data)\n                checks.append((prev_len - len(data)) == 0)\n                prev_len = len(data)\n\n        return data\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.IterativeKCore.__init__","title":"<code>__init__(columns, cores)</code>","text":"<p>Initializes the IterativeKCore object.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list</code> <p>A list of column names to apply the KCore filter on.</p> required <code>cores</code> <code>list of int or int</code> <p>The minimum number of records required for each column to be kept.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If 'cores' in not a list or an integer.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def __init__(self, columns: list, cores: Union[int, list]):\n    \"\"\"\n    Initializes the IterativeKCore object.\n\n    Args:\n        columns (list): A list of column names to apply the KCore filter on.\n        cores (list of int or int): The minimum number of records required for each column to be kept.\n\n    Raises:\n        TypeError: If 'cores' in not a list or an integer.\n    \"\"\"\n\n    self._columns = columns\n\n    if isinstance(cores, list):\n        self._cores = list(zip(columns, cores))\n    elif isinstance(cores, int):\n        self._cores = [(c, cores) for c in columns]\n    else:\n        raise TypeError('Cores must be a list or an integer.')\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.IterativeKCore.run","title":"<code>run(dataset)</code>","text":"<p>Iteratively applies the KCore filters on the dataset until no changes occur, then returns the filtered dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataFrame</code> <p>The dataset to be iteratively filtered.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The filtered dataset after all iterations.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def run(self, dataset: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Iteratively applies the KCore filters on the dataset until no changes occur, then returns the filtered dataset.\n\n    Args:\n        dataset (pd.DataFrame): The dataset to be iteratively filtered.\n\n    Returns:\n        (pd.DataFrame): The filtered dataset after all iterations.\n    \"\"\"\n\n    data = dataset.copy()\n\n    filters = {c: KCore(column=c, core=k) for c, k in self._cores}\n    checks = [False for _ in self._columns]\n    prev_len = len(data)\n\n    while not all(checks):\n        checks = []\n        for c, f in filters.items():\n            data = f.run(data)\n            checks.append((prev_len - len(data)) == 0)\n            prev_len = len(data)\n\n    return data\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.UserItemIterativeKCore","title":"<code>UserItemIterativeKCore</code>","text":"<p>               Bases: <code>Processor</code></p> <p>Iteratively filters a dataset based on both user and item columns with specified core values.</p> <p>This class applies the IterativeKCore filter to both the user and item columns of the dataset.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>class UserItemIterativeKCore(Processor):\n\n    \"\"\"\n    Iteratively filters a dataset based on both user and item columns with specified core values.\n\n    This class applies the IterativeKCore filter to both the user and item columns of the dataset.\n    \"\"\"\n    def __init__(self, cores: Union[int, list]):\n        \"\"\"\n        Initializes the UserItemIterativeKCore object.\n\n        Args:\n            cores (list or int): A list of core values for the user and item columns.\n\n        Raises:\n            TypeError: If \"cores\" is not a list or an integer.\n        \"\"\"\n\n        if not isinstance(cores, (list, int)):\n            raise TypeError('Cores must be a list or an integer.')\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self._cores = cores\n\n    def run(self, datarec: DataRec) -&gt; DataRec:\n        \"\"\"\n        Applies the iterative KCore filter to both user and item columns, and returns a new DataRec object\n        containing the filtered data.\n\n        Args:\n            datarec (DataRec): The DataRec object containing the dataset to be filtered.\n\n        Returns:\n            (DataRec): A new DataRec object with the filtered data.\n        \"\"\"\n\n        core_obj = IterativeKCore(columns=[datarec.user_col, datarec.item_col],\n                                  cores=self._cores)\n        result = core_obj.run(datarec.data)\n\n        return self.output(datarec, result, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.UserItemIterativeKCore.__init__","title":"<code>__init__(cores)</code>","text":"<p>Initializes the UserItemIterativeKCore object.</p> <p>Parameters:</p> Name Type Description Default <code>cores</code> <code>list or int</code> <p>A list of core values for the user and item columns.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If \"cores\" is not a list or an integer.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def __init__(self, cores: Union[int, list]):\n    \"\"\"\n    Initializes the UserItemIterativeKCore object.\n\n    Args:\n        cores (list or int): A list of core values for the user and item columns.\n\n    Raises:\n        TypeError: If \"cores\" is not a list or an integer.\n    \"\"\"\n\n    if not isinstance(cores, (list, int)):\n        raise TypeError('Cores must be a list or an integer.')\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self._cores = cores\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.UserItemIterativeKCore.run","title":"<code>run(datarec)</code>","text":"<p>Applies the iterative KCore filter to both user and item columns, and returns a new DataRec object containing the filtered data.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The DataRec object containing the dataset to be filtered.</p> required <p>Returns:</p> Type Description <code>DataRec</code> <p>A new DataRec object with the filtered data.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; DataRec:\n    \"\"\"\n    Applies the iterative KCore filter to both user and item columns, and returns a new DataRec object\n    containing the filtered data.\n\n    Args:\n        datarec (DataRec): The DataRec object containing the dataset to be filtered.\n\n    Returns:\n        (DataRec): A new DataRec object with the filtered data.\n    \"\"\"\n\n    core_obj = IterativeKCore(columns=[datarec.user_col, datarec.item_col],\n                              cores=self._cores)\n    result = core_obj.run(datarec.data)\n\n    return self.output(datarec, result, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.NRoundsKCore","title":"<code>NRoundsKCore</code>","text":"<p>Filters a dataset based on a minimum number of records (core) for each column over multiple rounds.</p> <p>This class applies KCore filters iteratively over a specified number of rounds.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>class NRoundsKCore:\n    \"\"\"\n    Filters a dataset based on a minimum number of records (core) for each column over multiple rounds.\n\n    This class applies KCore filters iteratively over a specified number of rounds.\n    \"\"\"\n\n    def __init__(self, columns: list, cores: Union[int, list], rounds: int):\n        \"\"\"\n        Initializes the NRoundsKCore object.\n\n        Args:\n            columns (list): A list of column names to apply the KCore filter on.\n            cores (list of int or int): The minimum number of records required for each column to be kept.\n            rounds (int): The number of rounds to apply the filtering process.\n\n        Raises:\n            TypeError: If 'cores' is not a list or an integer.\n            TypeError: If 'rounds' is not an integer.\n        \"\"\"\n\n        self._columns = columns\n\n        if isinstance(cores, list):\n            self._cores = list(zip(columns, cores))\n        elif isinstance(cores, int):\n            self._cores = [(c, cores) for c in columns]\n        else:\n            raise TypeError('Cores must be a list or an integer.')\n\n        if not isinstance(rounds, int):\n            raise TypeError('Rounds must be an integer.')\n\n        self._rounds = rounds\n\n    def run(self, dataset: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Applies the KCore filters over the specified number of rounds and returns the filtered dataset.\n\n        Args:\n            dataset (pd.DataFrame): The dataset to be filtered.\n\n        Returns:\n            (pd.DataFrame): The dataset after filtering over the specified number of rounds.\n        \"\"\"\n\n        data = dataset.copy()\n\n        filters = {c: KCore(column=c, core=k) for c, k in self._cores}\n        checks = [False for _ in self._columns]\n        prev_len = len(data)\n\n        for _ in range(self._rounds) or all(checks):\n            checks = []\n            for c, f in filters.items():\n                data = f.run(data)\n                checks.append((prev_len - len(data)) == 0)\n                prev_len = len(data)\n        return data\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.NRoundsKCore.__init__","title":"<code>__init__(columns, cores, rounds)</code>","text":"<p>Initializes the NRoundsKCore object.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list</code> <p>A list of column names to apply the KCore filter on.</p> required <code>cores</code> <code>list of int or int</code> <p>The minimum number of records required for each column to be kept.</p> required <code>rounds</code> <code>int</code> <p>The number of rounds to apply the filtering process.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If 'cores' is not a list or an integer.</p> <code>TypeError</code> <p>If 'rounds' is not an integer.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def __init__(self, columns: list, cores: Union[int, list], rounds: int):\n    \"\"\"\n    Initializes the NRoundsKCore object.\n\n    Args:\n        columns (list): A list of column names to apply the KCore filter on.\n        cores (list of int or int): The minimum number of records required for each column to be kept.\n        rounds (int): The number of rounds to apply the filtering process.\n\n    Raises:\n        TypeError: If 'cores' is not a list or an integer.\n        TypeError: If 'rounds' is not an integer.\n    \"\"\"\n\n    self._columns = columns\n\n    if isinstance(cores, list):\n        self._cores = list(zip(columns, cores))\n    elif isinstance(cores, int):\n        self._cores = [(c, cores) for c in columns]\n    else:\n        raise TypeError('Cores must be a list or an integer.')\n\n    if not isinstance(rounds, int):\n        raise TypeError('Rounds must be an integer.')\n\n    self._rounds = rounds\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.NRoundsKCore.run","title":"<code>run(dataset)</code>","text":"<p>Applies the KCore filters over the specified number of rounds and returns the filtered dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataFrame</code> <p>The dataset to be filtered.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The dataset after filtering over the specified number of rounds.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def run(self, dataset: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Applies the KCore filters over the specified number of rounds and returns the filtered dataset.\n\n    Args:\n        dataset (pd.DataFrame): The dataset to be filtered.\n\n    Returns:\n        (pd.DataFrame): The dataset after filtering over the specified number of rounds.\n    \"\"\"\n\n    data = dataset.copy()\n\n    filters = {c: KCore(column=c, core=k) for c, k in self._cores}\n    checks = [False for _ in self._columns]\n    prev_len = len(data)\n\n    for _ in range(self._rounds) or all(checks):\n        checks = []\n        for c, f in filters.items():\n            data = f.run(data)\n            checks.append((prev_len - len(data)) == 0)\n            prev_len = len(data)\n    return data\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.UserItemNRoundsKCore","title":"<code>UserItemNRoundsKCore</code>","text":"<p>               Bases: <code>Processor</code></p> <p>Filters a dataset based on both user and item columns with specified core values over multiple rounds.</p> <p>This class applies the NRoundsKCore filter to both the user and item columns of the dataset.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>class UserItemNRoundsKCore(Processor):\n\n    \"\"\"\n    Filters a dataset based on both user and item columns with specified core values over multiple rounds.\n\n    This class applies the NRoundsKCore filter to both the user and item columns of the dataset.\n    \"\"\"\n\n    def __init__(self, cores: Union[int, list], rounds: int):\n        \"\"\"\n        Initializes the UserItemNRoundsKCore object.\n\n        Args:\n            cores (int, list): A list of core values for the user and item columns.\n            rounds (int): The number of rounds to apply the filtering process.\n\n        Raises:\n            TypeError: If 'cores' is not a list or an integer.\n            TypeError: If 'rounds' is not an integer.\n        \"\"\"\n\n        if not isinstance(cores, (list, int)):\n            raise TypeError('Cores must be a list or an integer.')\n\n        if not isinstance(rounds, int):\n            raise TypeError('Rounds must be an integer.')\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self._cores = cores\n        self._rounds = rounds\n\n    def run(self, datarec: DataRec) -&gt; DataRec:\n        \"\"\"\n        Applies the NRoundsKCore filter to both user and item columns over multiple rounds, and returns a new DataRec object\n        containing the filtered data.\n\n        Args:\n            datarec (DataRec): The DataRec object containing the dataset to be filtered.\n\n        Returns:\n            (DataRec): A new DataRec object with the filtered data.\n        \"\"\"\n\n        core_obj = NRoundsKCore(columns=[datarec.user_col, datarec.item_col],\n                                cores=self._cores, rounds=self._rounds)\n        result = core_obj.run(datarec.data)\n\n        return self.output(datarec, result, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.UserItemNRoundsKCore.__init__","title":"<code>__init__(cores, rounds)</code>","text":"<p>Initializes the UserItemNRoundsKCore object.</p> <p>Parameters:</p> Name Type Description Default <code>cores</code> <code>(int, list)</code> <p>A list of core values for the user and item columns.</p> required <code>rounds</code> <code>int</code> <p>The number of rounds to apply the filtering process.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If 'cores' is not a list or an integer.</p> <code>TypeError</code> <p>If 'rounds' is not an integer.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def __init__(self, cores: Union[int, list], rounds: int):\n    \"\"\"\n    Initializes the UserItemNRoundsKCore object.\n\n    Args:\n        cores (int, list): A list of core values for the user and item columns.\n        rounds (int): The number of rounds to apply the filtering process.\n\n    Raises:\n        TypeError: If 'cores' is not a list or an integer.\n        TypeError: If 'rounds' is not an integer.\n    \"\"\"\n\n    if not isinstance(cores, (list, int)):\n        raise TypeError('Cores must be a list or an integer.')\n\n    if not isinstance(rounds, int):\n        raise TypeError('Rounds must be an integer.')\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self._cores = cores\n    self._rounds = rounds\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.UserItemNRoundsKCore.run","title":"<code>run(datarec)</code>","text":"<p>Applies the NRoundsKCore filter to both user and item columns over multiple rounds, and returns a new DataRec object containing the filtered data.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The DataRec object containing the dataset to be filtered.</p> required <p>Returns:</p> Type Description <code>DataRec</code> <p>A new DataRec object with the filtered data.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; DataRec:\n    \"\"\"\n    Applies the NRoundsKCore filter to both user and item columns over multiple rounds, and returns a new DataRec object\n    containing the filtered data.\n\n    Args:\n        datarec (DataRec): The DataRec object containing the dataset to be filtered.\n\n    Returns:\n        (DataRec): A new DataRec object with the filtered data.\n    \"\"\"\n\n    core_obj = NRoundsKCore(columns=[datarec.user_col, datarec.item_col],\n                            cores=self._cores, rounds=self._rounds)\n    result = core_obj.run(datarec.data)\n\n    return self.output(datarec, result, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.processor.Processor","title":"<code>Processor</code>","text":"<p>Utility class for handling the output of preprocessing steps on <code>DataRec</code>  objects.</p> <p>This class provides functionality to build a new <code>DataRec</code> from  transformation results while updating the processing pipeline accordingly.</p> Source code in <code>datarec/processing/processor.py</code> <pre><code>class Processor:\n    \"\"\"\n    Utility class for handling the output of preprocessing steps on `DataRec` \n    objects.\n\n    This class provides functionality to build a new `DataRec` from \n    transformation results while updating the processing pipeline accordingly.\n    \"\"\"\n\n    @staticmethod\n    def output(datarec: DataRec, result: pd.DataFrame, step_info: dict) -&gt; DataRec:\n        \"\"\"\n        Create a new `DataRec` object from a transformation result and update \n        the processing pipeline with a new step.\n\n        Args:\n            datarec (DataRec): The original `DataRec` object from which the \n                transformation is derived.\n            result (pd.DataFrame): The result of the transformation.\n            step_info (dict): Metadata of the transformation.\n\n        Returns:\n            (DataRec): A new `DataRec` object wrapping the transformation result\n                with an updated pipeline.\n        \"\"\"\n        pipeline = datarec.pipeline.copy()\n        pipeline.add_step(name='process', operation=step_info['operation'], params=step_info['params'])\n\n        new_datarec = DataRec(\n            RawData(result,\n                    user=datarec.user_col,\n                    item=datarec.item_col,\n                    rating=datarec.rating_col if datarec.rating_col in result.columns else None,\n                    timestamp=datarec.timestamp_col),\n            derives_from=datarec,\n            dataset_name=datarec.dataset_name,\n            pipeline=pipeline\n        )\n\n        return new_datarec\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.processor.Processor.output","title":"<code>output(datarec, result, step_info)</code>  <code>staticmethod</code>","text":"<p>Create a new <code>DataRec</code> object from a transformation result and update  the processing pipeline with a new step.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The original <code>DataRec</code> object from which the  transformation is derived.</p> required <code>result</code> <code>DataFrame</code> <p>The result of the transformation.</p> required <code>step_info</code> <code>dict</code> <p>Metadata of the transformation.</p> required <p>Returns:</p> Type Description <code>DataRec</code> <p>A new <code>DataRec</code> object wrapping the transformation result with an updated pipeline.</p> Source code in <code>datarec/processing/processor.py</code> <pre><code>@staticmethod\ndef output(datarec: DataRec, result: pd.DataFrame, step_info: dict) -&gt; DataRec:\n    \"\"\"\n    Create a new `DataRec` object from a transformation result and update \n    the processing pipeline with a new step.\n\n    Args:\n        datarec (DataRec): The original `DataRec` object from which the \n            transformation is derived.\n        result (pd.DataFrame): The result of the transformation.\n        step_info (dict): Metadata of the transformation.\n\n    Returns:\n        (DataRec): A new `DataRec` object wrapping the transformation result\n            with an updated pipeline.\n    \"\"\"\n    pipeline = datarec.pipeline.copy()\n    pipeline.add_step(name='process', operation=step_info['operation'], params=step_info['params'])\n\n    new_datarec = DataRec(\n        RawData(result,\n                user=datarec.user_col,\n                item=datarec.item_col,\n                rating=datarec.rating_col if datarec.rating_col in result.columns else None,\n                timestamp=datarec.timestamp_col),\n        derives_from=datarec,\n        dataset_name=datarec.dataset_name,\n        pipeline=pipeline\n    )\n\n    return new_datarec\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.rating.FilterByRatingThreshold","title":"<code>FilterByRatingThreshold</code>","text":"<p>               Bases: <code>Processor</code></p> <p>Filters the dataset by removing interactions with a rating below a given threshold.</p> Source code in <code>datarec/processing/rating.py</code> <pre><code>class FilterByRatingThreshold(Processor):\n    \"\"\"\n    Filters the dataset by removing interactions with a rating below a given threshold.\n    \"\"\"\n\n    def __init__(self, rating_threshold: float):\n        \"\"\"\n        Initializes the FilterByRatingThreshold object.\n\n        Args:\n            rating_threshold (float): The minimum rating required for an interaction to be kept.\n\n        Raises:\n            ValueError: If `rating_threshold` is a negative number.\n        \"\"\"\n        if not isinstance(rating_threshold, (int, float)):\n            raise ValueError(\"rating_threshold must be a number.\")\n        if rating_threshold &lt; 0:\n            raise ValueError(\"rating_threshold must be non-negative.\")\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.rating_threshold = rating_threshold\n\n    def run(self, datarec: DataRec) -&gt; DataRec:\n        \"\"\"\n        Filters interactions with a rating below the threshold.\n\n        Args:\n            datarec (DataRec): The input dataset wrapped in a DataRec object.\n\n        Returns:\n            (DataRec): A new DataRec object with the processed dataset.\n        \"\"\"\n\n        dataset = datarec.data\n        filtered_data = dataset[dataset[datarec.rating_col] &gt;= self.rating_threshold]\n\n        return self.output(datarec, filtered_data, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.rating.FilterByRatingThreshold.__init__","title":"<code>__init__(rating_threshold)</code>","text":"<p>Initializes the FilterByRatingThreshold object.</p> <p>Parameters:</p> Name Type Description Default <code>rating_threshold</code> <code>float</code> <p>The minimum rating required for an interaction to be kept.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>rating_threshold</code> is a negative number.</p> Source code in <code>datarec/processing/rating.py</code> <pre><code>def __init__(self, rating_threshold: float):\n    \"\"\"\n    Initializes the FilterByRatingThreshold object.\n\n    Args:\n        rating_threshold (float): The minimum rating required for an interaction to be kept.\n\n    Raises:\n        ValueError: If `rating_threshold` is a negative number.\n    \"\"\"\n    if not isinstance(rating_threshold, (int, float)):\n        raise ValueError(\"rating_threshold must be a number.\")\n    if rating_threshold &lt; 0:\n        raise ValueError(\"rating_threshold must be non-negative.\")\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.rating_threshold = rating_threshold\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.rating.FilterByRatingThreshold.run","title":"<code>run(datarec)</code>","text":"<p>Filters interactions with a rating below the threshold.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The input dataset wrapped in a DataRec object.</p> required <p>Returns:</p> Type Description <code>DataRec</code> <p>A new DataRec object with the processed dataset.</p> Source code in <code>datarec/processing/rating.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; DataRec:\n    \"\"\"\n    Filters interactions with a rating below the threshold.\n\n    Args:\n        datarec (DataRec): The input dataset wrapped in a DataRec object.\n\n    Returns:\n        (DataRec): A new DataRec object with the processed dataset.\n    \"\"\"\n\n    dataset = datarec.data\n    filtered_data = dataset[dataset[datarec.rating_col] &gt;= self.rating_threshold]\n\n    return self.output(datarec, filtered_data, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.rating.FilterByUserMeanRating","title":"<code>FilterByUserMeanRating</code>","text":"<p>               Bases: <code>Processor</code></p> <p>Filters the dataset by removing interactions with a rating below the user's average rating.</p> <p>This filter calculates the average rating given by each user and removes interactions where the rating is below that average.</p> Source code in <code>datarec/processing/rating.py</code> <pre><code>class FilterByUserMeanRating(Processor):\n    \"\"\"\n    Filters the dataset by removing interactions with a rating below the user's average rating.\n\n    This filter calculates the average rating given by each user and removes\n    interactions where the rating is below that average.\n    \"\"\"\n\n    def run(self, datarec: DataRec) -&gt; DataRec:\n        \"\"\"\n        Filters interactions with a rating below the user's mean rating.\n\n        Args:\n            datarec (DataRec): The input dataset wrapped in a DataRec object.\n\n        Returns:\n            (DataRec): A new DataRec object with the processed dataset.\n        \"\"\"\n\n        dataset = datarec.data\n        user_means = dataset.groupby(datarec.user_col)[datarec.rating_col].mean()\n\n        filtered_data = dataset[\n            dataset.apply(lambda row: row[datarec.rating_col] &gt;= user_means[row[datarec.user_col]], axis=1)\n        ]\n\n        return self.output(datarec, filtered_data, {'operation': self.__class__.__name__, 'params': ''})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.rating.FilterByUserMeanRating.run","title":"<code>run(datarec)</code>","text":"<p>Filters interactions with a rating below the user's mean rating.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The input dataset wrapped in a DataRec object.</p> required <p>Returns:</p> Type Description <code>DataRec</code> <p>A new DataRec object with the processed dataset.</p> Source code in <code>datarec/processing/rating.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; DataRec:\n    \"\"\"\n    Filters interactions with a rating below the user's mean rating.\n\n    Args:\n        datarec (DataRec): The input dataset wrapped in a DataRec object.\n\n    Returns:\n        (DataRec): A new DataRec object with the processed dataset.\n    \"\"\"\n\n    dataset = datarec.data\n    user_means = dataset.groupby(datarec.user_col)[datarec.rating_col].mean()\n\n    filtered_data = dataset[\n        dataset.apply(lambda row: row[datarec.rating_col] &gt;= user_means[row[datarec.user_col]], axis=1)\n    ]\n\n    return self.output(datarec, filtered_data, {'operation': self.__class__.__name__, 'params': ''})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.rating.FilterOutDuplicatedInteractions","title":"<code>FilterOutDuplicatedInteractions</code>","text":"<p>               Bases: <code>Processor</code></p> <p>Filters a dataset by removing duplicated (user, item) interactions based on a specified strategy.</p> Source code in <code>datarec/processing/rating.py</code> <pre><code>class FilterOutDuplicatedInteractions(Processor):\n    \"\"\"\n    Filters a dataset by removing duplicated (user, item) interactions based on a specified strategy.\n    \"\"\"\n\n    STRATEGIES = ['first', 'last', 'earliest', 'latest', 'random']\n\n    def __init__(self, keep='first', random_seed=42):\n        \"\"\"\n        Initializes the FilterOutDuplicatedInteractions object.\n\n        Args:\n            keep (str): Strategy to determine which interaction to keep when duplicates are found.\n                Must be one of ['first', 'last', 'earliest', 'latest', 'random'].\n            random_seed (int): Random seed used for reproducibility when using the 'random' strategy.\n\n        Raises:\n            ValueError: If the provided strategy (`keep`) is not among the supported options.\n        \"\"\"\n\n        if keep not in self.STRATEGIES:\n            raise ValueError(f\"Invalid strategy '{keep}'. Choose from {self.STRATEGIES}.\")\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.keep = keep\n        self.random_seed = random_seed\n\n    def run(self, datarec: DataRec, verbose=True) -&gt; DataRec:\n        \"\"\"\n        Filter out duplicated (user, item) interactions in the dataset using the specified strategy.\n\n        Args:\n            datarec (DataRec): An object containing the dataset and metadata (user, item, timestamp columns, etc.)\n            verbose (bool): Whether to print logging information during execution.\n\n        Returns:\n            (DataRec): A new DataRec object with duplicated (user, item) interactions removed according to the selected strategy.\n\n        Raises:\n            ValueError: If Date colum is not provided for 'earliest' and 'latest' strategies.\n            ValueError: If the provided strategy (`keep`) is not among the supported options.\n        \"\"\"\n\n        if verbose:\n            print(f'Running filter-out duplicated interactions with strategy {self.keep}')\n            print(f'Filtering DataRec: {datarec.dataset_name}')\n\n        dataset = datarec.data\n        subset = [datarec.user_col, datarec.item_col]\n\n        # Random strategy\n        if self.keep == 'random':\n            dataset = dataset.sample(frac=1, random_state=self.random_seed).drop_duplicates(subset=subset, keep='first')\n\n        # Ordering-based strategies\n        elif self.keep in ['first', 'last']:\n            dataset = dataset.drop_duplicates(subset=subset, keep=self.keep)\n\n        # Temporal strategies\n        elif self.keep in ['earliest', 'latest']:\n            if datarec.timestamp_col is None:\n                raise ValueError(f\"Date column is required for '{self.keep}' strategy.\")\n            dataset = dataset.sort_values(by=datarec.timestamp_col, ascending=True)\n            if self.keep == 'earliest':\n                dataset = dataset.drop_duplicates(subset=subset, keep='first')\n            else:\n                dataset = dataset.drop_duplicates(subset=subset, keep='last')\n        else:\n            raise ValueError(f\"Invalid strategy '{self.keep}'. Choose from {self.STRATEGIES}.\")\n\n        dataset = dataset.sort_values(by=[datarec.user_col, datarec.item_col], ascending=True)\n\n        return self.output(datarec, dataset, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.rating.FilterOutDuplicatedInteractions.__init__","title":"<code>__init__(keep='first', random_seed=42)</code>","text":"<p>Initializes the FilterOutDuplicatedInteractions object.</p> <p>Parameters:</p> Name Type Description Default <code>keep</code> <code>str</code> <p>Strategy to determine which interaction to keep when duplicates are found. Must be one of ['first', 'last', 'earliest', 'latest', 'random'].</p> <code>'first'</code> <code>random_seed</code> <code>int</code> <p>Random seed used for reproducibility when using the 'random' strategy.</p> <code>42</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided strategy (<code>keep</code>) is not among the supported options.</p> Source code in <code>datarec/processing/rating.py</code> <pre><code>def __init__(self, keep='first', random_seed=42):\n    \"\"\"\n    Initializes the FilterOutDuplicatedInteractions object.\n\n    Args:\n        keep (str): Strategy to determine which interaction to keep when duplicates are found.\n            Must be one of ['first', 'last', 'earliest', 'latest', 'random'].\n        random_seed (int): Random seed used for reproducibility when using the 'random' strategy.\n\n    Raises:\n        ValueError: If the provided strategy (`keep`) is not among the supported options.\n    \"\"\"\n\n    if keep not in self.STRATEGIES:\n        raise ValueError(f\"Invalid strategy '{keep}'. Choose from {self.STRATEGIES}.\")\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.keep = keep\n    self.random_seed = random_seed\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.rating.FilterOutDuplicatedInteractions.run","title":"<code>run(datarec, verbose=True)</code>","text":"<p>Filter out duplicated (user, item) interactions in the dataset using the specified strategy.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>An object containing the dataset and metadata (user, item, timestamp columns, etc.)</p> required <code>verbose</code> <code>bool</code> <p>Whether to print logging information during execution.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataRec</code> <p>A new DataRec object with duplicated (user, item) interactions removed according to the selected strategy.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If Date colum is not provided for 'earliest' and 'latest' strategies.</p> <code>ValueError</code> <p>If the provided strategy (<code>keep</code>) is not among the supported options.</p> Source code in <code>datarec/processing/rating.py</code> <pre><code>def run(self, datarec: DataRec, verbose=True) -&gt; DataRec:\n    \"\"\"\n    Filter out duplicated (user, item) interactions in the dataset using the specified strategy.\n\n    Args:\n        datarec (DataRec): An object containing the dataset and metadata (user, item, timestamp columns, etc.)\n        verbose (bool): Whether to print logging information during execution.\n\n    Returns:\n        (DataRec): A new DataRec object with duplicated (user, item) interactions removed according to the selected strategy.\n\n    Raises:\n        ValueError: If Date colum is not provided for 'earliest' and 'latest' strategies.\n        ValueError: If the provided strategy (`keep`) is not among the supported options.\n    \"\"\"\n\n    if verbose:\n        print(f'Running filter-out duplicated interactions with strategy {self.keep}')\n        print(f'Filtering DataRec: {datarec.dataset_name}')\n\n    dataset = datarec.data\n    subset = [datarec.user_col, datarec.item_col]\n\n    # Random strategy\n    if self.keep == 'random':\n        dataset = dataset.sample(frac=1, random_state=self.random_seed).drop_duplicates(subset=subset, keep='first')\n\n    # Ordering-based strategies\n    elif self.keep in ['first', 'last']:\n        dataset = dataset.drop_duplicates(subset=subset, keep=self.keep)\n\n    # Temporal strategies\n    elif self.keep in ['earliest', 'latest']:\n        if datarec.timestamp_col is None:\n            raise ValueError(f\"Date column is required for '{self.keep}' strategy.\")\n        dataset = dataset.sort_values(by=datarec.timestamp_col, ascending=True)\n        if self.keep == 'earliest':\n            dataset = dataset.drop_duplicates(subset=subset, keep='first')\n        else:\n            dataset = dataset.drop_duplicates(subset=subset, keep='last')\n    else:\n        raise ValueError(f\"Invalid strategy '{self.keep}'. Choose from {self.STRATEGIES}.\")\n\n    dataset = dataset.sort_values(by=[datarec.user_col, datarec.item_col], ascending=True)\n\n    return self.output(datarec, dataset, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.temporal.FilterByTime","title":"<code>FilterByTime</code>","text":"<p>               Bases: <code>Processor</code></p> <p>Filters the dataset based on a time threshold and specified drop condition.</p> <p>This class allows filtering a dataset by a time threshold, either dropping records before or after the specified time.</p> Source code in <code>datarec/processing/temporal.py</code> <pre><code>class FilterByTime(Processor):\n    \"\"\"\n    Filters the dataset based on a time threshold and specified drop condition.\n\n    This class allows filtering a dataset by a time threshold, either dropping\n    records before or after the specified time.\n    \"\"\"\n\n    def __init__(self, time_threshold: float = 0, drop: str = 'after'):\n        \"\"\"  \n        Initializes the FilterByTime object.\n\n        Args:\n            time_threshold (float): The time threshold used for filtering. The dataset\n                                    will be filtered based on this value.\n            drop (str, optional): Specifies whether to drop records 'before' or 'after' the time threshold.\n\n        Raises:\n            ValueError: If `time_threshold` is negative or not a float, or if drop is\n                        neither 'after' nor 'before'.\n        \"\"\"\n        if not isinstance(time_threshold, (int, float)):\n            raise ValueError('time_threshold must be positive number.')\n        if isinstance(time_threshold, float) and time_threshold &lt; 0:\n            raise ValueError('time_threshold must be positive number.')\n\n        if drop not in ['after', 'before']:\n            raise ValueError(f'Drop must be \"after\" or \"before\".')\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n        self.time_threshold = time_threshold\n        self.drop = drop\n\n    def run(self, datarec: DataRec) -&gt; DataRec:\n        \"\"\"\n        Filters the dataset of the given DataRec based on the specified time threshold\n        and drop condition, returning a new DataRec object with the filtered data.\n\n        Args:\n            datarec (DataRec): The input dataset wrapped in a DataRec object.\n\n        Returns:\n            (DataRec): A new DataRec object with the processed dataset.\n\n        Raises:\n            TypeError: If the DataRec does not contain temporal information.\n        \"\"\"\n\n        if datarec.timestamp_col is None:\n            raise TypeError('This DataRec does not contain temporal information')\n\n        dataset = datarec.data\n\n        if self.drop == 'before':\n            data = dataset[dataset[datarec.timestamp_col] &lt; self.time_threshold]\n        else:\n            data = dataset[dataset[datarec.timestamp_col] &gt;= self.time_threshold]\n\n        return self.output(datarec, data, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.temporal.FilterByTime.__init__","title":"<code>__init__(time_threshold=0, drop='after')</code>","text":"<p>Initializes the FilterByTime object.</p> <p>Parameters:</p> Name Type Description Default <code>time_threshold</code> <code>float</code> <p>The time threshold used for filtering. The dataset                     will be filtered based on this value.</p> <code>0</code> <code>drop</code> <code>str</code> <p>Specifies whether to drop records 'before' or 'after' the time threshold.</p> <code>'after'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>time_threshold</code> is negative or not a float, or if drop is         neither 'after' nor 'before'.</p> Source code in <code>datarec/processing/temporal.py</code> <pre><code>def __init__(self, time_threshold: float = 0, drop: str = 'after'):\n    \"\"\"  \n    Initializes the FilterByTime object.\n\n    Args:\n        time_threshold (float): The time threshold used for filtering. The dataset\n                                will be filtered based on this value.\n        drop (str, optional): Specifies whether to drop records 'before' or 'after' the time threshold.\n\n    Raises:\n        ValueError: If `time_threshold` is negative or not a float, or if drop is\n                    neither 'after' nor 'before'.\n    \"\"\"\n    if not isinstance(time_threshold, (int, float)):\n        raise ValueError('time_threshold must be positive number.')\n    if isinstance(time_threshold, float) and time_threshold &lt; 0:\n        raise ValueError('time_threshold must be positive number.')\n\n    if drop not in ['after', 'before']:\n        raise ValueError(f'Drop must be \"after\" or \"before\".')\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n    self.time_threshold = time_threshold\n    self.drop = drop\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.temporal.FilterByTime.run","title":"<code>run(datarec)</code>","text":"<p>Filters the dataset of the given DataRec based on the specified time threshold and drop condition, returning a new DataRec object with the filtered data.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The input dataset wrapped in a DataRec object.</p> required <p>Returns:</p> Type Description <code>DataRec</code> <p>A new DataRec object with the processed dataset.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the DataRec does not contain temporal information.</p> Source code in <code>datarec/processing/temporal.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; DataRec:\n    \"\"\"\n    Filters the dataset of the given DataRec based on the specified time threshold\n    and drop condition, returning a new DataRec object with the filtered data.\n\n    Args:\n        datarec (DataRec): The input dataset wrapped in a DataRec object.\n\n    Returns:\n        (DataRec): A new DataRec object with the processed dataset.\n\n    Raises:\n        TypeError: If the DataRec does not contain temporal information.\n    \"\"\"\n\n    if datarec.timestamp_col is None:\n        raise TypeError('This DataRec does not contain temporal information')\n\n    dataset = datarec.data\n\n    if self.drop == 'before':\n        data = dataset[dataset[datarec.timestamp_col] &lt; self.time_threshold]\n    else:\n        data = dataset[dataset[datarec.timestamp_col] &gt;= self.time_threshold]\n\n    return self.output(datarec, data, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/registry/","title":"Datasets Registry","text":"<p>The <code>datarec/registry</code> folder is the source of truth for built-in datasets. It defines what datasets exist, which versions are available, where to download resources, and how to interpret them. This registry powers the dataset builder and the reproducible pipeline system.</p>"},{"location":"documentation/registry/#on-this-page","title":"On This Page","text":"<ul> <li>Folder structure</li> <li>How DataRec uses the registry</li> <li>Add a new dataset</li> <li>Minimal resource example</li> <li>Tips</li> </ul>"},{"location":"documentation/registry/#folder-structure","title":"Folder structure","text":"<pre><code>datarec/registry/\n\u251c\u2500 datasets/        # Dataset-level metadata (name, description, versions, citation)\n\u251c\u2500 versions/        # Version-specific sources/resources and schemas\n\u2514\u2500 metrics/         # Precomputed dataset characteristics (generated)\n</code></pre>"},{"location":"documentation/registry/#datasets","title":"datasets/","text":"<p>Each file describes a dataset at a high level: - <code>description</code> and <code>source</code> - <code>citation</code> (used in docs and references) - <code>versions</code> list - <code>latest_version</code></p> <p>These files are used to validate dataset names and versions.</p>"},{"location":"documentation/registry/#versions","title":"versions/","text":"<p>Each file defines a specific version of a dataset. It contains: - <code>sources</code>: how to download data (URLs, archives, checksums) - <code>resources</code>: what to extract and how to parse it - optional <code>schema</code> definitions for interactions or content</p> <p>The schema drives how RawData/DataRec columns are interpreted.</p>"},{"location":"documentation/registry/#metrics","title":"metrics/","text":"<p>YAML files generated by DataRec with dataset characteristics (e.g., sparsity, users/items, density). They are produced by <code>datarec/registry/utils.py</code>.</p>"},{"location":"documentation/registry/#how-datarec-uses-the-registry","title":"How DataRec uses the registry","text":"<ol> <li>User requests a dataset name/version.</li> <li>Registry metadata validates the request.</li> <li>Version file provides sources and resources.</li> <li>Dataset builder prepares and loads the data into <code>DataRec</code>.</li> </ol>"},{"location":"documentation/registry/#add-a-new-dataset","title":"Add a new dataset","text":"<ol> <li>Create metadata in <code>datarec/registry/datasets/&lt;name&gt;.yml</code>.</li> <li>Create a version file in <code>datarec/registry/versions/&lt;name&gt;_&lt;version&gt;.yml</code>.</li> <li>(Optional) compute metrics with <code>compute_dataset_characteristics</code> in <code>datarec/registry/utils.py</code>.</li> </ol>"},{"location":"documentation/registry/#minimal-resource-example","title":"Minimal resource example","text":"<pre><code>resources:\n  interactions:\n    type: interactions\n    format: csv\n    required: true\n    source_name: main_source\n    filename: ratings.csv\n    schema:\n      user: user_id\n      item: item_id\n      rating: rating\n      timestamp: timestamp\n</code></pre>"},{"location":"documentation/registry/#tips","title":"Tips","text":"<ul> <li>Keep <code>latest_version</code> aligned with the newest version.</li> <li>Use checksums for reproducibility.</li> <li>If the schema changes, introduce a new version file.</li> </ul>"},{"location":"documentation/splitters/","title":"Splitters Module Reference","text":"<p>This section provides a detailed API reference for all modules related to splitting datasets into training, validation, and test sets. Use uniform splitters to sample globally, and user\u2011stratified splitters to split each user\u2019s history independently. Temporal splitters preserve time ordering when timestamps are available.</p>"},{"location":"documentation/splitters/#on-this-page","title":"On This Page","text":"<ul> <li>Core Splitting Utilities</li> <li>Uniform Splitting Strategies</li> <li>User-Stratified Splitting Strategies</li> </ul> <p>Minimal usage:</p> <pre><code>from datarec.splitters import RandomHoldOut\n\nsplitter = RandomHoldOut(test_ratio=0.2, val_ratio=0.1, seed=42)\nsplits = splitter.run(datarec)\ntrain, val, test = splits[\"train\"], splits[\"val\"], splits[\"test\"]\n</code></pre>"},{"location":"documentation/splitters/#core-splitting-utilities","title":"Core Splitting Utilities","text":"<p>These modules define the base class and common utilities used by all splitters.</p>"},{"location":"documentation/splitters/#datarec.splitters.splitter.Splitter","title":"<code>Splitter</code>","text":"<p>Base class for dataset splitters.</p> <p>This class provides a common interface for splitting datasets into training, validation, and test sets. Subclasses should implement specific splitting strategies.</p> Source code in <code>datarec/splitters/splitter.py</code> <pre><code>class Splitter:\n    \"\"\"\n    Base class for dataset splitters.\n\n    This class provides a common interface for splitting datasets into training,\n    validation, and test sets. Subclasses should implement specific splitting strategies.\n    \"\"\"\n\n    @staticmethod\n    def output(datarec: DataRec, train: pd.DataFrame, test: pd.DataFrame, validation: pd.DataFrame,\n               step_info: Dict[str, Dict]) -&gt; Dict[str, DataRec]:\n        \"\"\"\n        Creates a dictionary of `DataRec` objects for train, test, and validation splits.\n\n        Args:\n            datarec (DataRec): The original dataset wrapped in a `DataRec` object.\n            train (pd.DataFrame): The training split of the dataset.\n            test (pd.DataFrame): The test split of the dataset.\n            validation (pd.DataFrame): The validation split of the dataset.\n            step_info (Dict[str, Dict]): Metadata of the transformation.\n\n        Returns:\n            (Dict[str, DataRec]): A dictionary containing the split datasets:\n                - 'train': The training dataset as a `DataRec` object (if not empty).\n                - 'test': The test dataset as a `DataRec` object (if not empty).\n                - 'val': The validation dataset as a `DataRec` object (if not empty).\n        \"\"\"\n\n        pipeline = datarec.pipeline.copy()\n        pipeline.add_step(name='split', operation=step_info['operation'], params=step_info['params'])\n\n        result = dict()\n        for k, d in zip(['train', 'test', 'val'], [train, test, validation]):\n            if len(d) &gt; 0:\n                new_datarec = DataRec(RawData(d,\n                                              user=datarec.user_col,\n                                              item=datarec.item_col,\n                                              rating=datarec.rating_col,\n                                              timestamp=datarec.timestamp_col),\n                                      derives_from=datarec,\n                                      dataset_name=datarec.dataset_name,\n                                      pipeline=pipeline.copy())\n                result[k] = new_datarec\n        return result\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.splitter.Splitter.output","title":"<code>output(datarec, train, test, validation, step_info)</code>  <code>staticmethod</code>","text":"<p>Creates a dictionary of <code>DataRec</code> objects for train, test, and validation splits.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The original dataset wrapped in a <code>DataRec</code> object.</p> required <code>train</code> <code>DataFrame</code> <p>The training split of the dataset.</p> required <code>test</code> <code>DataFrame</code> <p>The test split of the dataset.</p> required <code>validation</code> <code>DataFrame</code> <p>The validation split of the dataset.</p> required <code>step_info</code> <code>Dict[str, Dict]</code> <p>Metadata of the transformation.</p> required <p>Returns:</p> Type Description <code>Dict[str, DataRec]</code> <p>A dictionary containing the split datasets: - 'train': The training dataset as a <code>DataRec</code> object (if not empty). - 'test': The test dataset as a <code>DataRec</code> object (if not empty). - 'val': The validation dataset as a <code>DataRec</code> object (if not empty).</p> Source code in <code>datarec/splitters/splitter.py</code> <pre><code>@staticmethod\ndef output(datarec: DataRec, train: pd.DataFrame, test: pd.DataFrame, validation: pd.DataFrame,\n           step_info: Dict[str, Dict]) -&gt; Dict[str, DataRec]:\n    \"\"\"\n    Creates a dictionary of `DataRec` objects for train, test, and validation splits.\n\n    Args:\n        datarec (DataRec): The original dataset wrapped in a `DataRec` object.\n        train (pd.DataFrame): The training split of the dataset.\n        test (pd.DataFrame): The test split of the dataset.\n        validation (pd.DataFrame): The validation split of the dataset.\n        step_info (Dict[str, Dict]): Metadata of the transformation.\n\n    Returns:\n        (Dict[str, DataRec]): A dictionary containing the split datasets:\n            - 'train': The training dataset as a `DataRec` object (if not empty).\n            - 'test': The test dataset as a `DataRec` object (if not empty).\n            - 'val': The validation dataset as a `DataRec` object (if not empty).\n    \"\"\"\n\n    pipeline = datarec.pipeline.copy()\n    pipeline.add_step(name='split', operation=step_info['operation'], params=step_info['params'])\n\n    result = dict()\n    for k, d in zip(['train', 'test', 'val'], [train, test, validation]):\n        if len(d) &gt; 0:\n            new_datarec = DataRec(RawData(d,\n                                          user=datarec.user_col,\n                                          item=datarec.item_col,\n                                          rating=datarec.rating_col,\n                                          timestamp=datarec.timestamp_col),\n                                  derives_from=datarec,\n                                  dataset_name=datarec.dataset_name,\n                                  pipeline=pipeline.copy())\n            result[k] = new_datarec\n    return result\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.utils.random_sample","title":"<code>random_sample(dataframe, seed, n_samples=1)</code>","text":"<p>Randomly selects a specified number of samples from a given DataFrame.</p> <p>This function splits the input DataFrame into two subsets: - One containing <code>n_samples</code> randomly selected rows. - One containing the remaining rows after the selection.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The input DataFrame from which to sample.</p> required <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> required <code>n_samples</code> <code>int</code> <p>The number of samples to extract. Must be at least 1. Default is 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <ul> <li>The first DataFrame contains the remaining data after sampling.</li> <li>The second DataFrame contains the randomly selected samples.</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>n_samples</code> is less than 1 or lesser/greater than the number of rows in the DataFrame.</p> Source code in <code>datarec/splitters/utils.py</code> <pre><code>def random_sample(dataframe: pd.DataFrame, seed: int, n_samples: int = 1) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Randomly selects a specified number of samples from a given DataFrame.\n\n    This function splits the input DataFrame into two subsets:\n    - One containing `n_samples` randomly selected rows.\n    - One containing the remaining rows after the selection.\n\n    Args:\n        dataframe (pd.DataFrame): The input DataFrame from which to sample.\n        seed (int): Random seed for reproducibility.\n        n_samples (int, optional): The number of samples to extract. Must be at least 1. Default is 1.\n\n    Returns:\n        (Tuple[pd.DataFrame, pd.DataFrame]):\n            - The first DataFrame contains the remaining data after sampling.\n            - The second DataFrame contains the randomly selected samples.\n\n    Raises:\n        ValueError: If `n_samples` is less than 1 or lesser/greater than the number of rows in the DataFrame.\n    \"\"\"\n\n    if n_samples &lt; 1:\n        raise ValueError('number of samples must be greater than 1.')\n\n    if n_samples &gt; len(dataframe):\n        raise ValueError('number of samples greater than the number of samples in the DataFrame.')\n\n    samples = dataframe.sample(n=n_samples, random_state=seed)\n\n    if len(samples) != n_samples:\n        raise ValueError('number of samples lesser or greater than the number of rows in the DataFrame.')\n    else:\n        return dataframe.drop(samples.index), samples\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.utils.max_by_col","title":"<code>max_by_col(dataframe, discriminative_column, seed)</code>","text":"<p>Selects the row with the minimum value in the specified column from the given DataFrame. If multiple rows have the same minimum value, one is randomly selected.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <code>discriminative_column</code> <code>str</code> <p>The column used to determine the minimum value.</p> required <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <code>DataFrame</code> <ul> <li>The first DataFrame contains the remaining rows after removing the selected row.</li> </ul> <code>Tuple[DataFrame, DataFrame]</code> <ul> <li>The second DataFrame contains the selected row with the minimum value.</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified column is not present in the DataFrame.</p> <code>ValueError</code> <p>If no candidates are found (should not happen unless DataFrame is empty).</p> <code>ValueError</code> <p>If the random selection fails to return exactly one row.</p> Source code in <code>datarec/splitters/utils.py</code> <pre><code>def max_by_col(dataframe: pd.DataFrame, discriminative_column: str, seed: int) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Selects the row with the minimum value in the specified column from the given DataFrame.\n    If multiple rows have the same minimum value, one is randomly selected.\n\n    Args:\n        dataframe (pd.DataFrame): The input DataFrame.\n        discriminative_column (str): The column used to determine the minimum value.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        (Tuple[pd.DataFrame, pd.DataFrame]):\n        - The first DataFrame contains the remaining rows after removing the selected row.\n        - The second DataFrame contains the selected row with the minimum value.\n\n    Raises:\n        ValueError: If the specified column is not present in the DataFrame.\n        ValueError: If no candidates are found (should not happen unless DataFrame is empty).\n        ValueError: If the random selection fails to return exactly one row.\n    \"\"\"\n\n    if discriminative_column not in dataframe:\n        raise ValueError(f'Column \\'{discriminative_column}\\' must be in the dataframe.')\n\n    max_value = dataframe[discriminative_column].max()\n    candidates = dataframe.loc[dataframe[discriminative_column] == max_value]\n    n_candidates = len(candidates)\n\n    if n_candidates == 0:\n        raise ValueError('No candidate.')\n    elif n_candidates == 1:\n        return dataframe.drop(candidates.index), candidates\n    else:\n        candidates = candidates.sample(n=1, random_state=seed)\n        if len(candidates) != 1:\n            raise ValueError('Number of candidates lesser or greater than 1.')\n        return dataframe.drop(candidates.index), candidates\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.utils.temporal_holdout","title":"<code>temporal_holdout(dataframe, test_ratio, val_ratio, temporal_col)</code>","text":"<p>Splits a dataset into training, validation, and test sets based on temporal ordering.</p> <p>The function sorts the dataset according to a specified timestamp column and assigns the oldest interactions to the training set, followed by the validation set (if applicable), and the most recent interactions to the test set.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The input dataset containing interaction data.</p> required <code>test_ratio</code> <code>float</code> <p>The proportion of the dataset to allocate to the test set. Must be between 0 and 1.</p> required <code>val_ratio</code> <code>float</code> <p>The proportion of the dataset to allocate to the validation set. Must be between 0 and 1.</p> required <code>temporal_col</code> <code>str</code> <p>The name of the column containing timestamp information.</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame, DataFrame]</code> <p>A tuple containing the train, validation, and test sets.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>test_ratio</code> or <code>val_ratio</code> are not in the range [0, 1].</p> Source code in <code>datarec/splitters/utils.py</code> <pre><code>def temporal_holdout(dataframe: pd.DataFrame, test_ratio: float, val_ratio: float, temporal_col: str) \\\n        -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Splits a dataset into training, validation, and test sets based on temporal ordering.\n\n    The function sorts the dataset according to a specified timestamp column and assigns\n    the oldest interactions to the training set, followed by the validation set (if applicable),\n    and the most recent interactions to the test set.\n\n    Args:\n        dataframe (pd.DataFrame): The input dataset containing interaction data.\n        test_ratio (float): The proportion of the dataset to allocate to the test set. Must be between 0 and 1.\n        val_ratio (float): The proportion of the dataset to allocate to the validation set. Must be between 0 and 1.\n        temporal_col (str): The name of the column containing timestamp information.\n\n    Returns:\n        (Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]): A tuple containing the train, validation, and test sets.\n\n    Raises:\n        ValueError: If `test_ratio` or `val_ratio` are not in the range [0, 1].\n    \"\"\"\n\n    if test_ratio &lt; 0 or test_ratio &gt; 1:\n        raise ValueError('test ratio must be between 0 and 1.')\n\n    if val_ratio &lt; 0 or val_ratio &gt; 1:\n        raise ValueError('val ratio must be between 0 and 1.')\n\n    train, test, val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n    total_samples = len(dataframe)\n\n    test_samples = round(total_samples * test_ratio)\n    train_samples = total_samples - test_samples\n    val_samples = round(train_samples * val_ratio)\n\n    train_samples = total_samples - test_samples - val_samples\n\n    assert (train_samples + val_samples + test_samples) == total_samples\n\n    ordered = dataframe.sort_values(by=temporal_col)\n\n    train = ordered.iloc[:train_samples]\n    if val_samples:\n        val = ordered.iloc[train_samples:(train_samples + val_samples)]\n    if test_samples:\n        test = ordered.iloc[(train_samples + val_samples):]\n\n    assert len(train) == train_samples\n    assert len(val) == val_samples\n    assert len(test) == test_samples\n\n    return train, test, val\n</code></pre>"},{"location":"documentation/splitters/#uniform-splitting-strategies","title":"Uniform Splitting Strategies","text":"<p>These splitters operate on the entire dataset globally.</p>"},{"location":"documentation/splitters/#datarec.splitters.uniform.hold_out.RandomHoldOut","title":"<code>RandomHoldOut</code>","text":"<p>               Bases: <code>Splitter</code></p> <p>Implements a random holdout split for recommendation datasets.</p> <p>This splitter partitions the dataset into training, validation, and test sets using a random sampling approach. The proportions of the dataset allocated to the validation and test sets are controlled by <code>val_ratio</code> and <code>test_ratio</code>, respectively.</p> Source code in <code>datarec/splitters/uniform/hold_out.py</code> <pre><code>class RandomHoldOut(Splitter):\n    \"\"\"\n    Implements a random holdout split for recommendation datasets.\n\n    This splitter partitions the dataset into training, validation, and test sets\n    using a random sampling approach. The proportions of the dataset allocated to\n    the validation and test sets are controlled by `val_ratio` and `test_ratio`, respectively.\n\n    \"\"\"\n\n    def __init__(self, test_ratio: float = 0, val_ratio: float = 0, seed: int = 42):\n        \"\"\"\n        Initializes the RandomHoldOut object.\n\n        Args:\n            test_ratio (float, optional): The proportion of the dataset to include in the test set.\n                Must be between 0 and 1. Default is 0.\n            val_ratio (float, optional): The proportion of the training set to include in the validation set.\n                Must be between 0 and 1. Default is 0.\n            seed (int, optional): The random seed for reproducibility. Defaults to 42.\n\n        Raises:\n            ValueError: If `test_ratio` or `val_ratio` is not in the range [0, 1].\n        \"\"\"\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.test_ratio = test_ratio\n        self.val_ratio = val_ratio\n        self.seed = seed\n\n    @property\n    def test_ratio(self) -&gt; float:\n        \"\"\"\n        The proportion of the dataset for the test set.\n        \"\"\"\n        return self._test_ratio\n\n    @test_ratio.setter\n    def test_ratio(self, value: float) -&gt; None:\n        \"\"\"\n        Sets the proportion of the dataset for the test set.\n\n        Args:\n            value (float): The proportion to allocate to the test set.\n                Must be between 0 and 1.\n\n        Raises:\n            ValueError: If `value` is not in the range [0, 1].\n        \"\"\"\n        if value &lt; 0 or value &gt; 1:\n            raise ValueError('ratio must be between 0 and 1')\n        self._test_ratio = value\n\n    @property\n    def val_ratio(self) -&gt; float:\n        \"\"\"\n        The proportion of the dataset for the validation set.\n        \"\"\"\n        return self._val_ratio\n\n    @val_ratio.setter\n    def val_ratio(self, value: float) -&gt; None:\n        \"\"\"\n        Sets the proportion of the dataset for the validation set.\n\n        Args:\n            value (float): The proportion to allocate to the validation set.\n                Must be between 0 and 1.\n\n        Raises:\n            ValueError: If `value` is not in the range [0, 1].\n        \"\"\"\n        if value &lt; 0 or value &gt; 1:\n            raise ValueError('ratio must be between 0 and 1')\n        self._val_ratio = value\n\n    def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n        \"\"\"\n        Splits the dataset into training, validation, and test sets according to the specified ratios, \n        with the val_ratio being applied to the dataset after the test set has been partitioned.\n\n        Args:\n            datarec (DataRec): The dataset to be split.\n\n        Returns:\n            (Dict[str, DataRec]): A dictionary with the following keys:\n                - \"train\": The training dataset (`DataRec`).\n                - \"test\": The test dataset (`DataRec`), if `test_ratio` &gt; 0.\n                - \"val\": The validation dataset (`DataRec`), if `val_ratio` &gt; 0.\n        \"\"\"\n\n        train, val, test = datarec.data, pd.DataFrame(), pd.DataFrame()\n\n        if self.test_ratio:\n            train, test = split(train, test_size=self._test_ratio, random_state=self.seed)\n\n        if self.val_ratio:\n            train, val = split(train, test_size=self._val_ratio, random_state=self.seed)\n\n        return self.output(datarec=datarec, train=train, test=test, validation=val,\n                           step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.uniform.hold_out.RandomHoldOut.test_ratio","title":"<code>test_ratio</code>  <code>property</code> <code>writable</code>","text":"<p>The proportion of the dataset for the test set.</p>"},{"location":"documentation/splitters/#datarec.splitters.uniform.hold_out.RandomHoldOut.val_ratio","title":"<code>val_ratio</code>  <code>property</code> <code>writable</code>","text":"<p>The proportion of the dataset for the validation set.</p>"},{"location":"documentation/splitters/#datarec.splitters.uniform.hold_out.RandomHoldOut.__init__","title":"<code>__init__(test_ratio=0, val_ratio=0, seed=42)</code>","text":"<p>Initializes the RandomHoldOut object.</p> <p>Parameters:</p> Name Type Description Default <code>test_ratio</code> <code>float</code> <p>The proportion of the dataset to include in the test set. Must be between 0 and 1. Default is 0.</p> <code>0</code> <code>val_ratio</code> <code>float</code> <p>The proportion of the training set to include in the validation set. Must be between 0 and 1. Default is 0.</p> <code>0</code> <code>seed</code> <code>int</code> <p>The random seed for reproducibility. Defaults to 42.</p> <code>42</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>test_ratio</code> or <code>val_ratio</code> is not in the range [0, 1].</p> Source code in <code>datarec/splitters/uniform/hold_out.py</code> <pre><code>def __init__(self, test_ratio: float = 0, val_ratio: float = 0, seed: int = 42):\n    \"\"\"\n    Initializes the RandomHoldOut object.\n\n    Args:\n        test_ratio (float, optional): The proportion of the dataset to include in the test set.\n            Must be between 0 and 1. Default is 0.\n        val_ratio (float, optional): The proportion of the training set to include in the validation set.\n            Must be between 0 and 1. Default is 0.\n        seed (int, optional): The random seed for reproducibility. Defaults to 42.\n\n    Raises:\n        ValueError: If `test_ratio` or `val_ratio` is not in the range [0, 1].\n    \"\"\"\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.test_ratio = test_ratio\n    self.val_ratio = val_ratio\n    self.seed = seed\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.uniform.hold_out.RandomHoldOut.run","title":"<code>run(datarec)</code>","text":"<p>Splits the dataset into training, validation, and test sets according to the specified ratios,  with the val_ratio being applied to the dataset after the test set has been partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The dataset to be split.</p> required <p>Returns:</p> Type Description <code>Dict[str, DataRec]</code> <p>A dictionary with the following keys: - \"train\": The training dataset (<code>DataRec</code>). - \"test\": The test dataset (<code>DataRec</code>), if <code>test_ratio</code> &gt; 0. - \"val\": The validation dataset (<code>DataRec</code>), if <code>val_ratio</code> &gt; 0.</p> Source code in <code>datarec/splitters/uniform/hold_out.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n    \"\"\"\n    Splits the dataset into training, validation, and test sets according to the specified ratios, \n    with the val_ratio being applied to the dataset after the test set has been partitioned.\n\n    Args:\n        datarec (DataRec): The dataset to be split.\n\n    Returns:\n        (Dict[str, DataRec]): A dictionary with the following keys:\n            - \"train\": The training dataset (`DataRec`).\n            - \"test\": The test dataset (`DataRec`), if `test_ratio` &gt; 0.\n            - \"val\": The validation dataset (`DataRec`), if `val_ratio` &gt; 0.\n    \"\"\"\n\n    train, val, test = datarec.data, pd.DataFrame(), pd.DataFrame()\n\n    if self.test_ratio:\n        train, test = split(train, test_size=self._test_ratio, random_state=self.seed)\n\n    if self.val_ratio:\n        train, val = split(train, test_size=self._val_ratio, random_state=self.seed)\n\n    return self.output(datarec=datarec, train=train, test=test, validation=val,\n                       step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.uniform.temporal.hold_out.TemporalHoldOut","title":"<code>TemporalHoldOut</code>","text":"<p>               Bases: <code>Splitter</code></p> <p>Implements a temporal hold-out splitting strategy for recommendation datasets.</p> <p>This splitter partitions a dataset into training, validation, and test sets based on the timestamps associated with interactions. The training set contains the oldest interactions, while the test set contains the most recent ones.</p> Source code in <code>datarec/splitters/uniform/temporal/hold_out.py</code> <pre><code>class TemporalHoldOut(Splitter):\n    \"\"\"\n    Implements a temporal hold-out splitting strategy for recommendation datasets.\n\n    This splitter partitions a dataset into training, validation, and test sets based on\n    the timestamps associated with interactions. The training set contains the oldest interactions,\n    while the test set contains the most recent ones.\n\n    \"\"\"\n\n    def __init__(self, test_ratio: float = 0, val_ratio: float = 0):\n\n        \"\"\"\n        Initializes the TemporalHoldOut object.\n        Args:\n            test_ratio (float, optional): The proportion of the dataset to allocate to the test set.\n                Must be between 0 and 1. Default is 0.\n            val_ratio (float, optional): The proportion of the dataset to allocate to the validation set.\n                Must be between 0 and 1. Default is 0.\n\n        Raises:\n            ValueError: If `test_ratio` or `val_ratio` are not in the range [0, 1].\n        \"\"\"\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.test_ratio = test_ratio\n        self.val_ratio = val_ratio\n\n    @property\n    def test_ratio(self) -&gt; float:\n        \"The proportion of the dataset allocated to the test set.\"\n        return self._test_ratio\n\n    @test_ratio.setter\n    def test_ratio(self, value: float) -&gt; None:\n        \"\"\"\n        Sets the test ratio.\n\n        Args:\n            value (float): The proportion of the dataset to allocate to the test set.\n                Must be between 0 and 1.\n\n        Raises:\n            ValueError: If `value` is not in the range [0, 1].\n        \"\"\"\n        if value &lt; 0 or value &gt; 1:\n            raise ValueError('ratio must be between 0 and 1')\n        self._test_ratio = value\n\n    @property\n    def val_ratio(self) -&gt; float:\n        \"\"\"\n        The proportion of the dataset allocated to the validation set.\n        \"\"\"\n        return self._val_ratio\n\n    @val_ratio.setter\n    def val_ratio(self, value: float) -&gt; None:\n        \"\"\"\n        Sets the validation ratio.\n\n        Args:\n            value (float): The proportion of the dataset to allocate to the validation set.\n                Must be between 0 and 1.\n\n        Raises:\n            ValueError: If `value` is not in the range [0, 1].\n        \"\"\"\n        if value &lt; 0 or value &gt; 1:\n            raise ValueError('ratio must be between 0 and 1')\n        self._val_ratio = value\n\n    def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n        \"\"\"\n        Splits the dataset using a temporal hold-out strategy.\n\n        This method partitions the dataset into training, validation, and test sets based on\n        the timestamps present in the `datarec` object. The split is performed such that the\n        training set contains older interactions, while the test set contains more recent ones.\n\n        Args:\n            datarec (DataRec): A DataRec object containing the dataset and a timestamp column.\n\n        Returns:\n            (Dict[str, DataRec]): A dictionary with three keys:\n                - `'train'`: A DataRec object containing the training set.\n                - `'val'`: A DataRec object containing the validation set (if `val_ratio` &gt; 0).\n                - `'test'`: A DataRec object containing the test set (if `test_ratio` &gt; 0).\n\n        Raises:\n            TypeError: If the `datarec` object does not contain a timestamp column.\n        \"\"\"\n\n        if datarec.timestamp_col is None:\n            raise TypeError('This DataRec does not contain temporal information')\n\n        train, test, val = temporal_holdout(dataframe=datarec.data,\n                                            test_ratio=self.test_ratio, val_ratio=self.val_ratio,\n                                            temporal_col=datarec.timestamp_col)\n\n        return self.output(datarec, train, test, val,\n                           step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.uniform.temporal.hold_out.TemporalHoldOut.test_ratio","title":"<code>test_ratio</code>  <code>property</code> <code>writable</code>","text":"<p>The proportion of the dataset allocated to the test set.</p>"},{"location":"documentation/splitters/#datarec.splitters.uniform.temporal.hold_out.TemporalHoldOut.val_ratio","title":"<code>val_ratio</code>  <code>property</code> <code>writable</code>","text":"<p>The proportion of the dataset allocated to the validation set.</p>"},{"location":"documentation/splitters/#datarec.splitters.uniform.temporal.hold_out.TemporalHoldOut.__init__","title":"<code>__init__(test_ratio=0, val_ratio=0)</code>","text":"<p>Initializes the TemporalHoldOut object. Args:     test_ratio (float, optional): The proportion of the dataset to allocate to the test set.         Must be between 0 and 1. Default is 0.     val_ratio (float, optional): The proportion of the dataset to allocate to the validation set.         Must be between 0 and 1. Default is 0.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>test_ratio</code> or <code>val_ratio</code> are not in the range [0, 1].</p> Source code in <code>datarec/splitters/uniform/temporal/hold_out.py</code> <pre><code>def __init__(self, test_ratio: float = 0, val_ratio: float = 0):\n\n    \"\"\"\n    Initializes the TemporalHoldOut object.\n    Args:\n        test_ratio (float, optional): The proportion of the dataset to allocate to the test set.\n            Must be between 0 and 1. Default is 0.\n        val_ratio (float, optional): The proportion of the dataset to allocate to the validation set.\n            Must be between 0 and 1. Default is 0.\n\n    Raises:\n        ValueError: If `test_ratio` or `val_ratio` are not in the range [0, 1].\n    \"\"\"\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.test_ratio = test_ratio\n    self.val_ratio = val_ratio\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.uniform.temporal.hold_out.TemporalHoldOut.run","title":"<code>run(datarec)</code>","text":"<p>Splits the dataset using a temporal hold-out strategy.</p> <p>This method partitions the dataset into training, validation, and test sets based on the timestamps present in the <code>datarec</code> object. The split is performed such that the training set contains older interactions, while the test set contains more recent ones.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>A DataRec object containing the dataset and a timestamp column.</p> required <p>Returns:</p> Type Description <code>Dict[str, DataRec]</code> <p>A dictionary with three keys: - <code>'train'</code>: A DataRec object containing the training set. - <code>'val'</code>: A DataRec object containing the validation set (if <code>val_ratio</code> &gt; 0). - <code>'test'</code>: A DataRec object containing the test set (if <code>test_ratio</code> &gt; 0).</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the <code>datarec</code> object does not contain a timestamp column.</p> Source code in <code>datarec/splitters/uniform/temporal/hold_out.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n    \"\"\"\n    Splits the dataset using a temporal hold-out strategy.\n\n    This method partitions the dataset into training, validation, and test sets based on\n    the timestamps present in the `datarec` object. The split is performed such that the\n    training set contains older interactions, while the test set contains more recent ones.\n\n    Args:\n        datarec (DataRec): A DataRec object containing the dataset and a timestamp column.\n\n    Returns:\n        (Dict[str, DataRec]): A dictionary with three keys:\n            - `'train'`: A DataRec object containing the training set.\n            - `'val'`: A DataRec object containing the validation set (if `val_ratio` &gt; 0).\n            - `'test'`: A DataRec object containing the test set (if `test_ratio` &gt; 0).\n\n    Raises:\n        TypeError: If the `datarec` object does not contain a timestamp column.\n    \"\"\"\n\n    if datarec.timestamp_col is None:\n        raise TypeError('This DataRec does not contain temporal information')\n\n    train, test, val = temporal_holdout(dataframe=datarec.data,\n                                        test_ratio=self.test_ratio, val_ratio=self.val_ratio,\n                                        temporal_col=datarec.timestamp_col)\n\n    return self.output(datarec, train, test, val,\n                       step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.uniform.temporal.threshold.TemporalThresholdSplit","title":"<code>TemporalThresholdSplit</code>","text":"<p>               Bases: <code>Splitter</code></p> <p>Splits a dataset into training, validation, and test sets based on two timestamp thresholds.</p> <p>The dataset is divided such that: - The training set contains interactions occurring strictly before <code>val_threshold</code>. - The validation set contains interactions occurring between <code>val_threshold</code> (inclusive)   and <code>test_threshold</code> (exclusive). - The test set contains interactions occurring at or after <code>test_threshold</code>.</p> Source code in <code>datarec/splitters/uniform/temporal/threshold.py</code> <pre><code>class TemporalThresholdSplit(Splitter):\n    \"\"\"\n    Splits a dataset into training, validation, and test sets based on two timestamp thresholds.\n\n    The dataset is divided such that:\n    - The training set contains interactions occurring strictly before `val_threshold`.\n    - The validation set contains interactions occurring between `val_threshold` (inclusive)\n      and `test_threshold` (exclusive).\n    - The test set contains interactions occurring at or after `test_threshold`.\n    \"\"\"\n\n    def __init__(self, val_threshold: float, test_threshold: float):\n        \"\"\"Initializes the TemporalThresholdSplit object.\n\n        Args:\n            val_threshold (float): The timestamp value that defines the split between training and validation.\n            test_threshold (float): The timestamp value that defines the split between validation and test.\n\n        Raises:\n            ValueError: If `val_threshold` is not strictly less than `test_threshold`.\n        \"\"\"\n\n        if val_threshold &gt;= test_threshold:\n            raise ValueError('val_threshold must be strictly less than test_threshold')\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.val_threshold = val_threshold\n        self.test_threshold = test_threshold\n\n    def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n        \"\"\"\n        Splits the dataset into training, validation, and test sets based on two thresholds.\n\n        Args:\n            datarec (DataRec): A DataRec object containing the dataset with a timestamp column.\n\n        Returns:\n            Dict[str, DataRec]: A dictionary with:\n                - `'train'`: Training set (timestamps &lt; `val_threshold`).\n                - `'val'`: Validation set (timestamps between `val_threshold` and `test_threshold`).\n                - `'test'`: Test set (timestamps &gt;= `test_threshold`).\n\n        Raises:\n            TypeError: If the `datarec` object does not contain a timestamp column.\n        \"\"\"\n\n        if datarec.timestamp_col is None:\n            raise TypeError('This DataRec does not contain temporal information')\n\n        dataset = datarec.data\n\n        train = dataset[dataset[datarec.timestamp_col] &lt; self.val_threshold]\n\n        val = dataset[(dataset[datarec.timestamp_col] &gt;= self.val_threshold) &amp;\n                      (dataset[datarec.timestamp_col] &lt; self.test_threshold)]\n\n        test = dataset[dataset[datarec.timestamp_col] &gt;= self.test_threshold]\n\n        return self.output(datarec, train, test, val,\n                           step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.uniform.temporal.threshold.TemporalThresholdSplit.__init__","title":"<code>__init__(val_threshold, test_threshold)</code>","text":"<p>Initializes the TemporalThresholdSplit object.</p> <p>Parameters:</p> Name Type Description Default <code>val_threshold</code> <code>float</code> <p>The timestamp value that defines the split between training and validation.</p> required <code>test_threshold</code> <code>float</code> <p>The timestamp value that defines the split between validation and test.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>val_threshold</code> is not strictly less than <code>test_threshold</code>.</p> Source code in <code>datarec/splitters/uniform/temporal/threshold.py</code> <pre><code>def __init__(self, val_threshold: float, test_threshold: float):\n    \"\"\"Initializes the TemporalThresholdSplit object.\n\n    Args:\n        val_threshold (float): The timestamp value that defines the split between training and validation.\n        test_threshold (float): The timestamp value that defines the split between validation and test.\n\n    Raises:\n        ValueError: If `val_threshold` is not strictly less than `test_threshold`.\n    \"\"\"\n\n    if val_threshold &gt;= test_threshold:\n        raise ValueError('val_threshold must be strictly less than test_threshold')\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.val_threshold = val_threshold\n    self.test_threshold = test_threshold\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.uniform.temporal.threshold.TemporalThresholdSplit.run","title":"<code>run(datarec)</code>","text":"<p>Splits the dataset into training, validation, and test sets based on two thresholds.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>A DataRec object containing the dataset with a timestamp column.</p> required <p>Returns:</p> Type Description <code>Dict[str, DataRec]</code> <p>Dict[str, DataRec]: A dictionary with: - <code>'train'</code>: Training set (timestamps &lt; <code>val_threshold</code>). - <code>'val'</code>: Validation set (timestamps between <code>val_threshold</code> and <code>test_threshold</code>). - <code>'test'</code>: Test set (timestamps &gt;= <code>test_threshold</code>).</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the <code>datarec</code> object does not contain a timestamp column.</p> Source code in <code>datarec/splitters/uniform/temporal/threshold.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n    \"\"\"\n    Splits the dataset into training, validation, and test sets based on two thresholds.\n\n    Args:\n        datarec (DataRec): A DataRec object containing the dataset with a timestamp column.\n\n    Returns:\n        Dict[str, DataRec]: A dictionary with:\n            - `'train'`: Training set (timestamps &lt; `val_threshold`).\n            - `'val'`: Validation set (timestamps between `val_threshold` and `test_threshold`).\n            - `'test'`: Test set (timestamps &gt;= `test_threshold`).\n\n    Raises:\n        TypeError: If the `datarec` object does not contain a timestamp column.\n    \"\"\"\n\n    if datarec.timestamp_col is None:\n        raise TypeError('This DataRec does not contain temporal information')\n\n    dataset = datarec.data\n\n    train = dataset[dataset[datarec.timestamp_col] &lt; self.val_threshold]\n\n    val = dataset[(dataset[datarec.timestamp_col] &gt;= self.val_threshold) &amp;\n                  (dataset[datarec.timestamp_col] &lt; self.test_threshold)]\n\n    test = dataset[dataset[datarec.timestamp_col] &gt;= self.test_threshold]\n\n    return self.output(datarec, train, test, val,\n                       step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#user-stratified-splitting-strategies","title":"User-Stratified Splitting Strategies","text":"<p>These splitters operate on a per-user basis, ensuring that each user's interaction history is partitioned across the splits.</p>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.hold_out.UserStratifiedHoldOut","title":"<code>UserStratifiedHoldOut</code>","text":"<p>               Bases: <code>Splitter</code></p> <p>Implements a user-stratified holdout split for a recommendation dataset.</p> <p>This splitter ensures that each user's interactions are split into training, validation, and test sets while maintaining the proportion specified by <code>test_ratio</code> and <code>val_ratio</code>.</p> Source code in <code>datarec/splitters/user_stratified/hold_out.py</code> <pre><code>class UserStratifiedHoldOut(Splitter):\n    \"\"\"\n    Implements a user-stratified holdout split for a recommendation dataset.\n\n    This splitter ensures that each user's interactions are split into training, validation,\n    and test sets while maintaining the proportion specified by `test_ratio` and `val_ratio`.\n\n    \"\"\"\n\n    def __init__(self, test_ratio: float = 0, val_ratio: float = 0, seed: int = 42):\n        \"\"\"Initializes the UserStratifiedHoldOut splitter.\n\n        Args:\n            test_ratio (float, optional): The proportion of interactions per user to include in the test set.\n                Must be between 0 and 1. Default is 0.\n            val_ratio (float, optional): The proportion of interactions per user to include in the validation set.\n                Must be between 0 and 1. Default is 0.\n            seed (int, optional): Random seed for reproducibility. Defaults to 42.\n\n         Raises:\n            ValueError: If `test_ratio` or `val_ratio` is not in the range [0, 1].\n        \"\"\"\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.test_ratio = test_ratio\n        self.val_ratio = val_ratio\n        self.seed = seed\n\n    @property\n    def test_ratio(self) -&gt; float:\n        \"\"\"The proportion of interactions per user for the test set.\"\"\"\n        return self._test_ratio\n\n    @test_ratio.setter\n    def test_ratio(self, value: float) -&gt; None:\n        \"\"\"\n        Sets the proportion of interactions per user for the test set.\n\n        Args:\n            value (float): Ratio for the test set. Must be between 0 and 1.\n\n        Raises:\n            ValueError: If the ratio is not between 0 and 1.\n        \"\"\"\n        if value &lt; 0 or value &gt; 1:\n            raise ValueError('ratio must be between 0 and 1')\n        self._test_ratio = value\n\n    @property\n    def val_ratio(self) -&gt; float:\n        \"\"\" \n        The proportion of interactions per user for the validation set.\n        \"\"\"\n        return self._val_ratio\n\n    @val_ratio.setter\n    def val_ratio(self, value: float) -&gt; None:\n        \"\"\"\n        Sets the proportion of remaining interactions per user for the validation set.\n\n        Args:\n            value (float): Ratio for the validation set. Must be between 0 and 1.\n\n        Raises:\n            ValueError: If the ratio is not between 0 and 1.\n        \"\"\"\n        if value &lt; 0 or value &gt; 1:\n            raise ValueError('ratio must be between 0 and 1')\n        self._val_ratio = value\n\n    def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n        \"\"\"\n        Splits the dataset into train, validation, and test sets using a user-stratified holdout approach.\n\n        Each user's interactions are split independently according to `test_ratio` and `val_ratio`, ensuring\n        that the distribution is preserved per user. The function returns a dictionary containing the three\n        resulting subsets.\n\n        Args:\n            datarec (DataRec): The dataset to be split.\n\n        Returns:\n            (Dict[str, DataRec]): A dictionary with the following keys:\n                - \"train\": DataRec containing the training set.\n                - \"test\": DataRec containing the test set, if `test_ratio` &gt; 0.\n                - \"val\": DataRec containing the validation set, if `val_ratio` &gt; 0.\n        \"\"\"\n\n        train, test, val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n        data = datarec.data\n        for u in datarec.users:\n\n            u_train, u_val, u_test = data[data.iloc[:, 0] == u], pd.DataFrame(), pd.DataFrame()\n\n            if self.test_ratio:\n                u_train, u_test = split(u_train, test_size=self._test_ratio, random_state=self.seed)\n            if self.val_ratio:\n                u_train, u_val = split(u_train, test_size=self._val_ratio, random_state=self.seed)\n\n            train = pd.concat([train, u_train], axis=0, ignore_index=True)\n            test = pd.concat([test, u_test], axis=0, ignore_index=True)\n            val = pd.concat([val, u_val], axis=0, ignore_index=True)\n\n        return self.output(datarec=datarec, train=train, test=test, validation=val,\n                           step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.hold_out.UserStratifiedHoldOut.test_ratio","title":"<code>test_ratio</code>  <code>property</code> <code>writable</code>","text":"<p>The proportion of interactions per user for the test set.</p>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.hold_out.UserStratifiedHoldOut.val_ratio","title":"<code>val_ratio</code>  <code>property</code> <code>writable</code>","text":"<p>The proportion of interactions per user for the validation set.</p>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.hold_out.UserStratifiedHoldOut.__init__","title":"<code>__init__(test_ratio=0, val_ratio=0, seed=42)</code>","text":"<p>Initializes the UserStratifiedHoldOut splitter.</p> <p>Parameters:</p> Name Type Description Default <code>test_ratio</code> <code>float</code> <p>The proportion of interactions per user to include in the test set. Must be between 0 and 1. Default is 0.</p> <code>0</code> <code>val_ratio</code> <code>float</code> <p>The proportion of interactions per user to include in the validation set. Must be between 0 and 1. Default is 0.</p> <code>0</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> <p>Raises:     ValueError: If <code>test_ratio</code> or <code>val_ratio</code> is not in the range [0, 1].</p> Source code in <code>datarec/splitters/user_stratified/hold_out.py</code> <pre><code>def __init__(self, test_ratio: float = 0, val_ratio: float = 0, seed: int = 42):\n    \"\"\"Initializes the UserStratifiedHoldOut splitter.\n\n    Args:\n        test_ratio (float, optional): The proportion of interactions per user to include in the test set.\n            Must be between 0 and 1. Default is 0.\n        val_ratio (float, optional): The proportion of interactions per user to include in the validation set.\n            Must be between 0 and 1. Default is 0.\n        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n\n     Raises:\n        ValueError: If `test_ratio` or `val_ratio` is not in the range [0, 1].\n    \"\"\"\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.test_ratio = test_ratio\n    self.val_ratio = val_ratio\n    self.seed = seed\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.hold_out.UserStratifiedHoldOut.run","title":"<code>run(datarec)</code>","text":"<p>Splits the dataset into train, validation, and test sets using a user-stratified holdout approach.</p> <p>Each user's interactions are split independently according to <code>test_ratio</code> and <code>val_ratio</code>, ensuring that the distribution is preserved per user. The function returns a dictionary containing the three resulting subsets.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The dataset to be split.</p> required <p>Returns:</p> Type Description <code>Dict[str, DataRec]</code> <p>A dictionary with the following keys: - \"train\": DataRec containing the training set. - \"test\": DataRec containing the test set, if <code>test_ratio</code> &gt; 0. - \"val\": DataRec containing the validation set, if <code>val_ratio</code> &gt; 0.</p> Source code in <code>datarec/splitters/user_stratified/hold_out.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n    \"\"\"\n    Splits the dataset into train, validation, and test sets using a user-stratified holdout approach.\n\n    Each user's interactions are split independently according to `test_ratio` and `val_ratio`, ensuring\n    that the distribution is preserved per user. The function returns a dictionary containing the three\n    resulting subsets.\n\n    Args:\n        datarec (DataRec): The dataset to be split.\n\n    Returns:\n        (Dict[str, DataRec]): A dictionary with the following keys:\n            - \"train\": DataRec containing the training set.\n            - \"test\": DataRec containing the test set, if `test_ratio` &gt; 0.\n            - \"val\": DataRec containing the validation set, if `val_ratio` &gt; 0.\n    \"\"\"\n\n    train, test, val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n    data = datarec.data\n    for u in datarec.users:\n\n        u_train, u_val, u_test = data[data.iloc[:, 0] == u], pd.DataFrame(), pd.DataFrame()\n\n        if self.test_ratio:\n            u_train, u_test = split(u_train, test_size=self._test_ratio, random_state=self.seed)\n        if self.val_ratio:\n            u_train, u_val = split(u_train, test_size=self._val_ratio, random_state=self.seed)\n\n        train = pd.concat([train, u_train], axis=0, ignore_index=True)\n        test = pd.concat([test, u_test], axis=0, ignore_index=True)\n        val = pd.concat([val, u_val], axis=0, ignore_index=True)\n\n    return self.output(datarec=datarec, train=train, test=test, validation=val,\n                       step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.leave_out.LeaveNOut","title":"<code>LeaveNOut</code>","text":"<p>               Bases: <code>Splitter</code></p> <p>Implements the Leave-N-Out splitting strategy for recommendation datasets.</p> <p>This splitter ensures that for each user, a fixed number of interactions (<code>test_n</code> and <code>validation_n</code>) are randomly selected and moved to the test and validation sets, respectively. The remaining interactions are kept in the training set.</p> Source code in <code>datarec/splitters/user_stratified/leave_out.py</code> <pre><code>class LeaveNOut(Splitter):\n    \"\"\"\n    Implements the Leave-N-Out splitting strategy for recommendation datasets.\n\n    This splitter ensures that for each user, a fixed number of interactions (`test_n` and `validation_n`)\n    are randomly selected and moved to the test and validation sets, respectively. The remaining interactions\n    are kept in the training set.\n    \"\"\"\n\n    def __init__(self, test_n: int = 0, validation_n: int = 0, seed: int = 42):\n        \"\"\"Initializes the LeaveNOut splitter.\n\n        Args:\n            test_n (int, optional): Number of interactions to move to the test set per user. Default is 0.\n            validation_n (int, optional): Number of interactions to move to the validation set per user. Default is 0.\n            seed (int, optional): Random seed for reproducibility. Default is 42.\n\n        Raises:\n            ValueError: If `test_n` or `validation_n` are negative.\n            TypeError: If `test_n` or `validation_n` are not integers.\n        \"\"\"\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.test_n = test_n\n        self.validation_n = validation_n\n        self.seed = seed\n\n    @property\n    def test_n(self) -&gt; int:\n        \"\"\"Number of interactions to move to the test set per user.\"\"\"\n        return self._test_n\n\n    @test_n.setter\n    def test_n(self, value: int) -&gt; None:\n        \"\"\"\n        Sets the number of interactions to move to the test set per user.\n\n        Args:\n            value (int): Number of interactions.\n\n        Raises:\n            ValueError: If `value` is negative.\n            TypeError: If `value` is not an integer.\n        \"\"\"\n        if value &lt; 0:\n            raise ValueError(\"test_n must be greater or equal to 0.\")\n        if isinstance(value, float):\n            raise TypeError(\"test_n must be an integer.\")\n        self._test_n = value\n\n    @property\n    def validation_n(self) -&gt; int:\n        \"\"\"Number of interactions to move to the test set per user.\"\"\"\n        return self._validation_n\n\n    @validation_n.setter\n    def validation_n(self, value: int) -&gt; None:\n        \"\"\"\n        Sets the number of interactions to move to the validation set per user.\n\n        Args:\n            value (int): Number of interactions.\n\n        Raises:\n            ValueError: If `value` is negative.\n            TypeError: If `value` is not an integer.\n        \"\"\"\n        if value &lt; 0:\n            raise ValueError(\"validation_n must be greater or equal to 0.\")\n        if isinstance(value, float):\n            raise TypeError(\"validation_n must be an integer.\")\n        self._validation_n = value\n\n    def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n        \"\"\"\n        Splits the dataset into train, validation, and test sets using a Leave-N-Out approach.\n\n        For each user, `test_n` interactions are randomly assigned to the test set, and `validation_n`\n        interactions are assigned to the validation set. The remaining interactions are used for training.\n\n        Args:\n            datarec (DataRec): The dataset to be split.\n\n        Returns:\n            (Dict[str, DataRec]): A dictionary with the following keys:\n                - \"train\": DataRec containing the training set.\n                - \"test\": DataRec containing the test set, if `test_n` &gt; 0.\n                - \"validation\": DataRec containing the validation set, if `val_n` &gt; 0.\n        \"\"\"\n\n        train, test, val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n        data = datarec.data\n        for u in datarec.users:\n\n            u_train, u_val, u_test = data[data.iloc[:, 0] == u], pd.DataFrame(), pd.DataFrame()\n\n            if self.test_n:\n                u_train, sample = random_sample(dataframe=u_train, n_samples=self.test_n, seed=self.seed)\n                u_test = pd.concat([u_test, sample], axis=0, ignore_index=True)\n\n            if self.validation_n:\n                u_train, sample = random_sample(dataframe=u_train, n_samples=self.validation_n, seed=self.seed)\n                u_val = pd.concat([u_val, sample], axis=0, ignore_index=True)\n\n            train = pd.concat([train, u_train], axis=0, ignore_index=True)\n            test = pd.concat([test, u_test], axis=0, ignore_index=True)\n            val = pd.concat([val, u_val], axis=0, ignore_index=True)\n\n        return self.output(datarec, train, test, val,\n                           step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.leave_out.LeaveNOut.test_n","title":"<code>test_n</code>  <code>property</code> <code>writable</code>","text":"<p>Number of interactions to move to the test set per user.</p>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.leave_out.LeaveNOut.validation_n","title":"<code>validation_n</code>  <code>property</code> <code>writable</code>","text":"<p>Number of interactions to move to the test set per user.</p>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.leave_out.LeaveNOut.__init__","title":"<code>__init__(test_n=0, validation_n=0, seed=42)</code>","text":"<p>Initializes the LeaveNOut splitter.</p> <p>Parameters:</p> Name Type Description Default <code>test_n</code> <code>int</code> <p>Number of interactions to move to the test set per user. Default is 0.</p> <code>0</code> <code>validation_n</code> <code>int</code> <p>Number of interactions to move to the validation set per user. Default is 0.</p> <code>0</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Default is 42.</p> <code>42</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>test_n</code> or <code>validation_n</code> are negative.</p> <code>TypeError</code> <p>If <code>test_n</code> or <code>validation_n</code> are not integers.</p> Source code in <code>datarec/splitters/user_stratified/leave_out.py</code> <pre><code>def __init__(self, test_n: int = 0, validation_n: int = 0, seed: int = 42):\n    \"\"\"Initializes the LeaveNOut splitter.\n\n    Args:\n        test_n (int, optional): Number of interactions to move to the test set per user. Default is 0.\n        validation_n (int, optional): Number of interactions to move to the validation set per user. Default is 0.\n        seed (int, optional): Random seed for reproducibility. Default is 42.\n\n    Raises:\n        ValueError: If `test_n` or `validation_n` are negative.\n        TypeError: If `test_n` or `validation_n` are not integers.\n    \"\"\"\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.test_n = test_n\n    self.validation_n = validation_n\n    self.seed = seed\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.leave_out.LeaveNOut.run","title":"<code>run(datarec)</code>","text":"<p>Splits the dataset into train, validation, and test sets using a Leave-N-Out approach.</p> <p>For each user, <code>test_n</code> interactions are randomly assigned to the test set, and <code>validation_n</code> interactions are assigned to the validation set. The remaining interactions are used for training.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The dataset to be split.</p> required <p>Returns:</p> Type Description <code>Dict[str, DataRec]</code> <p>A dictionary with the following keys: - \"train\": DataRec containing the training set. - \"test\": DataRec containing the test set, if <code>test_n</code> &gt; 0. - \"validation\": DataRec containing the validation set, if <code>val_n</code> &gt; 0.</p> Source code in <code>datarec/splitters/user_stratified/leave_out.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n    \"\"\"\n    Splits the dataset into train, validation, and test sets using a Leave-N-Out approach.\n\n    For each user, `test_n` interactions are randomly assigned to the test set, and `validation_n`\n    interactions are assigned to the validation set. The remaining interactions are used for training.\n\n    Args:\n        datarec (DataRec): The dataset to be split.\n\n    Returns:\n        (Dict[str, DataRec]): A dictionary with the following keys:\n            - \"train\": DataRec containing the training set.\n            - \"test\": DataRec containing the test set, if `test_n` &gt; 0.\n            - \"validation\": DataRec containing the validation set, if `val_n` &gt; 0.\n    \"\"\"\n\n    train, test, val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n    data = datarec.data\n    for u in datarec.users:\n\n        u_train, u_val, u_test = data[data.iloc[:, 0] == u], pd.DataFrame(), pd.DataFrame()\n\n        if self.test_n:\n            u_train, sample = random_sample(dataframe=u_train, n_samples=self.test_n, seed=self.seed)\n            u_test = pd.concat([u_test, sample], axis=0, ignore_index=True)\n\n        if self.validation_n:\n            u_train, sample = random_sample(dataframe=u_train, n_samples=self.validation_n, seed=self.seed)\n            u_val = pd.concat([u_val, sample], axis=0, ignore_index=True)\n\n        train = pd.concat([train, u_train], axis=0, ignore_index=True)\n        test = pd.concat([test, u_test], axis=0, ignore_index=True)\n        val = pd.concat([val, u_val], axis=0, ignore_index=True)\n\n    return self.output(datarec, train, test, val,\n                       step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.leave_out.LeaveOneOut","title":"<code>LeaveOneOut</code>","text":"<p>               Bases: <code>LeaveNOut</code></p> <p>Implements the Leave-One-Out splitting strategy for recommendation datasets.</p> <p>This splitter ensures that for each user, at most one interaction is randomly selected and moved to the test and/or validation set, depending on the specified parameters. The remaining interactions are kept in the training set.</p> <p>This is a special case of <code>LeaveNOut</code> where <code>test_n=1</code> and/or <code>validation_n=1</code> if <code>test</code> and <code>validation</code> are set to <code>True</code>, respectively.</p> Source code in <code>datarec/splitters/user_stratified/leave_out.py</code> <pre><code>class LeaveOneOut(LeaveNOut):\n    \"\"\"\n    Implements the Leave-One-Out splitting strategy for recommendation datasets.\n\n    This splitter ensures that for each user, at most one interaction is randomly selected and moved\n    to the test and/or validation set, depending on the specified parameters. The remaining interactions\n    are kept in the training set.\n\n    This is a special case of `LeaveNOut` where `test_n=1` and/or `validation_n=1` if `test` and `validation`\n    are set to `True`, respectively.\n    \"\"\"\n\n    def __init__(self, test: bool = True, validation: bool = True, seed: int = 42):\n        \"\"\"Initializes the LeaveOneOut splitter.\n\n        Args:\n            test (bool, optional): Whether to include a test set. Defaults to True.\n            validation (bool, optional): Whether to include a validation set. Defaults to True.\n            seed (int, optional): Random seed for reproducibility. Default is 42.\n\n        Raises:\n            TypeError: If `test` or `validation` is not a boolean.\n        \"\"\"\n        if not isinstance(test, bool):\n            raise TypeError(\"test must be a boolean.\")\n        if not isinstance(validation, bool):\n            raise TypeError(\"validation must be an boolean.\")\n\n        test_n = 1 if test else 0\n        validation_n = 1 if validation else 0\n\n        super().__init__(test_n=test_n, validation_n=validation_n, seed=seed)\n        self.params = {\n            \"test\": test,\n            \"validation\": validation,\n            \"seed\": seed,\n        }\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.leave_out.LeaveOneOut.__init__","title":"<code>__init__(test=True, validation=True, seed=42)</code>","text":"<p>Initializes the LeaveOneOut splitter.</p> <p>Parameters:</p> Name Type Description Default <code>test</code> <code>bool</code> <p>Whether to include a test set. Defaults to True.</p> <code>True</code> <code>validation</code> <code>bool</code> <p>Whether to include a validation set. Defaults to True.</p> <code>True</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Default is 42.</p> <code>42</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>test</code> or <code>validation</code> is not a boolean.</p> Source code in <code>datarec/splitters/user_stratified/leave_out.py</code> <pre><code>def __init__(self, test: bool = True, validation: bool = True, seed: int = 42):\n    \"\"\"Initializes the LeaveOneOut splitter.\n\n    Args:\n        test (bool, optional): Whether to include a test set. Defaults to True.\n        validation (bool, optional): Whether to include a validation set. Defaults to True.\n        seed (int, optional): Random seed for reproducibility. Default is 42.\n\n    Raises:\n        TypeError: If `test` or `validation` is not a boolean.\n    \"\"\"\n    if not isinstance(test, bool):\n        raise TypeError(\"test must be a boolean.\")\n    if not isinstance(validation, bool):\n        raise TypeError(\"validation must be an boolean.\")\n\n    test_n = 1 if test else 0\n    validation_n = 1 if validation else 0\n\n    super().__init__(test_n=test_n, validation_n=validation_n, seed=seed)\n    self.params = {\n        \"test\": test,\n        \"validation\": validation,\n        \"seed\": seed,\n    }\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.leave_out.LeaveRatioOut","title":"<code>LeaveRatioOut</code>","text":"<p>               Bases: <code>Splitter</code></p> <p>Splits the dataset into training, test, and validation sets based on a ratio instead of a fixed number of samples.</p> <p>This splitter selects a fraction of interactions for each user to be assigned to the test and validation sets, ensuring that the splits are proportional to the user's total number of interactions.</p> Source code in <code>datarec/splitters/user_stratified/leave_out.py</code> <pre><code>class LeaveRatioOut(Splitter):\n    \"\"\"\n    Splits the dataset into training, test, and validation sets based on a ratio instead of a fixed number of samples.\n\n    This splitter selects a fraction of interactions for each user to be assigned to the test and validation sets,\n    ensuring that the splits are proportional to the user's total number of interactions.\n    \"\"\"\n\n    def __init__(self, test_ratio: float = 0, val_ratio: float = 0, seed: int = 42):\n        \"\"\"Initializes the LeaveRatioOut splitter.\n\n        Args:\n            test_ratio (float, optional): Proportion of each user's interactions assigned to the test set. Default is 0.\n            val_ratio (float, optional): Proportion of each user's interactions assigned to the validation set. Default is 0.\n            seed (int, optional): Random seed for reproducibility. Default is 42.\n\n        Raises:\n            ValueError: If `test_ratio` or `val_ratio` are not in the range [0, 1].\n            ValueError: If the sum of `test_ratio` and `val_ratio` exceeds 1.\n        \"\"\"\n        if not (0 &lt;= test_ratio &lt;= 1):\n            raise ValueError('ratio must be between 0 and 1')\n        if not (0 &lt;= val_ratio &lt;= 1):\n            raise ValueError('ratio must be between 0 and 1')\n        if test_ratio + val_ratio &gt; 1:\n            raise ValueError(\"sum of test_ratio and val_ratio must not exceed 1\")\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.test_ratio = test_ratio\n        self.val_ratio = val_ratio\n        self.seed = seed\n\n    def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n        \"\"\"\n         Splits the dataset into train, test, and validation sets based on the specified ratios.\n\n         The interactions of each user are sampled proportionally to create the test and validation sets.\n         The remaining interactions are used as the training set.\n\n         Args:\n             datarec (DataRec): The dataset containing interactions and user-item relationships.\n\n         Returns:\n             (Dict[str, DataRec]): A dictionary containing the following keys:\n                 - `\"train\"` (`DataRec`): The training dataset.\n                 - `\"test\"` (`DataRec`): The test dataset, if `test_ratio` &gt; 0.\n                 - `\"val\"` (`DataRec`): The validation dataset, if `val_ratio` &gt; 0.\n\n         Raises:\n             ValueError: If an empty dataset is encountered after sampling.\n         \"\"\"\n\n        train, test, val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n        data = datarec.data\n        for u in datarec.users:\n            u_train, u_val, u_test = data[data.iloc[:, 0] == u], pd.DataFrame(), pd.DataFrame()\n\n            user_total = len(u_train)\n\n            test_n_samples = round(self.test_ratio * user_total)\n            val_n_samples = round(self.val_ratio * user_total)\n\n            if test_n_samples &gt; 0:\n                u_train, sample = random_sample(dataframe=u_train, n_samples=min(test_n_samples, len(u_train)),\n                                                seed=self.seed)\n                u_test = pd.concat([u_test, sample], axis=0, ignore_index=True)\n\n            if val_n_samples &gt; 0:\n                u_train, sample = random_sample(dataframe=u_train, n_samples=min(val_n_samples, len(u_train)),\n                                                seed=self.seed)\n                u_val = pd.concat([u_val, sample], axis=0, ignore_index=True)\n\n            train = pd.concat([train, u_train], axis=0, ignore_index=True)\n            test = pd.concat([test, u_test], axis=0, ignore_index=True)\n            val = pd.concat([val, u_val], axis=0, ignore_index=True)\n\n        return self.output(datarec, train, test, val,\n                           step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.leave_out.LeaveRatioOut.__init__","title":"<code>__init__(test_ratio=0, val_ratio=0, seed=42)</code>","text":"<p>Initializes the LeaveRatioOut splitter.</p> <p>Parameters:</p> Name Type Description Default <code>test_ratio</code> <code>float</code> <p>Proportion of each user's interactions assigned to the test set. Default is 0.</p> <code>0</code> <code>val_ratio</code> <code>float</code> <p>Proportion of each user's interactions assigned to the validation set. Default is 0.</p> <code>0</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Default is 42.</p> <code>42</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>test_ratio</code> or <code>val_ratio</code> are not in the range [0, 1].</p> <code>ValueError</code> <p>If the sum of <code>test_ratio</code> and <code>val_ratio</code> exceeds 1.</p> Source code in <code>datarec/splitters/user_stratified/leave_out.py</code> <pre><code>def __init__(self, test_ratio: float = 0, val_ratio: float = 0, seed: int = 42):\n    \"\"\"Initializes the LeaveRatioOut splitter.\n\n    Args:\n        test_ratio (float, optional): Proportion of each user's interactions assigned to the test set. Default is 0.\n        val_ratio (float, optional): Proportion of each user's interactions assigned to the validation set. Default is 0.\n        seed (int, optional): Random seed for reproducibility. Default is 42.\n\n    Raises:\n        ValueError: If `test_ratio` or `val_ratio` are not in the range [0, 1].\n        ValueError: If the sum of `test_ratio` and `val_ratio` exceeds 1.\n    \"\"\"\n    if not (0 &lt;= test_ratio &lt;= 1):\n        raise ValueError('ratio must be between 0 and 1')\n    if not (0 &lt;= val_ratio &lt;= 1):\n        raise ValueError('ratio must be between 0 and 1')\n    if test_ratio + val_ratio &gt; 1:\n        raise ValueError(\"sum of test_ratio and val_ratio must not exceed 1\")\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.test_ratio = test_ratio\n    self.val_ratio = val_ratio\n    self.seed = seed\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.leave_out.LeaveRatioOut.run","title":"<code>run(datarec)</code>","text":"<p>Splits the dataset into train, test, and validation sets based on the specified ratios.</p> <p>The interactions of each user are sampled proportionally to create the test and validation sets. The remaining interactions are used as the training set.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The dataset containing interactions and user-item relationships.</p> required <p>Returns:</p> Type Description <code>Dict[str, DataRec]</code> <p>A dictionary containing the following keys: - <code>\"train\"</code> (<code>DataRec</code>): The training dataset. - <code>\"test\"</code> (<code>DataRec</code>): The test dataset, if <code>test_ratio</code> &gt; 0. - <code>\"val\"</code> (<code>DataRec</code>): The validation dataset, if <code>val_ratio</code> &gt; 0.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an empty dataset is encountered after sampling.</p> Source code in <code>datarec/splitters/user_stratified/leave_out.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n    \"\"\"\n     Splits the dataset into train, test, and validation sets based on the specified ratios.\n\n     The interactions of each user are sampled proportionally to create the test and validation sets.\n     The remaining interactions are used as the training set.\n\n     Args:\n         datarec (DataRec): The dataset containing interactions and user-item relationships.\n\n     Returns:\n         (Dict[str, DataRec]): A dictionary containing the following keys:\n             - `\"train\"` (`DataRec`): The training dataset.\n             - `\"test\"` (`DataRec`): The test dataset, if `test_ratio` &gt; 0.\n             - `\"val\"` (`DataRec`): The validation dataset, if `val_ratio` &gt; 0.\n\n     Raises:\n         ValueError: If an empty dataset is encountered after sampling.\n     \"\"\"\n\n    train, test, val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n    data = datarec.data\n    for u in datarec.users:\n        u_train, u_val, u_test = data[data.iloc[:, 0] == u], pd.DataFrame(), pd.DataFrame()\n\n        user_total = len(u_train)\n\n        test_n_samples = round(self.test_ratio * user_total)\n        val_n_samples = round(self.val_ratio * user_total)\n\n        if test_n_samples &gt; 0:\n            u_train, sample = random_sample(dataframe=u_train, n_samples=min(test_n_samples, len(u_train)),\n                                            seed=self.seed)\n            u_test = pd.concat([u_test, sample], axis=0, ignore_index=True)\n\n        if val_n_samples &gt; 0:\n            u_train, sample = random_sample(dataframe=u_train, n_samples=min(val_n_samples, len(u_train)),\n                                            seed=self.seed)\n            u_val = pd.concat([u_val, sample], axis=0, ignore_index=True)\n\n        train = pd.concat([train, u_train], axis=0, ignore_index=True)\n        test = pd.concat([test, u_test], axis=0, ignore_index=True)\n        val = pd.concat([val, u_val], axis=0, ignore_index=True)\n\n    return self.output(datarec, train, test, val,\n                       step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.temporal.leave_out.LeaveNLast","title":"<code>LeaveNLast</code>","text":"<p>               Bases: <code>Splitter</code></p> <p>Splits the dataset by removing the last <code>n</code> interactions per user based on a timestamp column.</p> <p>This splitter selects the last <code>test_n</code> interactions for the test set and the last <code>validation_n</code> interactions for the validation set while keeping the remaining interactions in the training set.</p> Source code in <code>datarec/splitters/user_stratified/temporal/leave_out.py</code> <pre><code>class LeaveNLast(Splitter):\n    \"\"\"\n    Splits the dataset by removing the last `n` interactions per user based on a timestamp column.\n\n    This splitter selects the last `test_n` interactions for the test set and the last `validation_n`\n    interactions for the validation set while keeping the remaining interactions in the training set.\n    \"\"\"\n    def __init__(self, test_n: int = 0, validation_n: int = 0, seed: int = 42):\n        \"\"\"Initializes the LeaveNLast splitter.\n\n        Args:\n            test_n (int, optional): Number of last interactions for the test set. Defaults to 0.\n            validation_n (int, optional): Number of last interactions for the validation set. Defaults to 0.\n            seed (int, optional): Random seed for reproducibility. Defaults to 42.\n        \"\"\"\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.test_n = test_n\n        self.validation_n = validation_n\n        self.seed = seed\n\n    @property\n    def test_n(self) -&gt; int:\n        \"\"\"The number of last interactions per user for the test set.\"\"\"\n        return self._test_n\n\n    @test_n.setter\n    def test_n(self, value: int) -&gt; None:\n        \"\"\"\n        Sets the number of last interactions per user for the test set.\n\n        Args:\n            value (int): Number of interactions. Must be &gt;= 0.\n\n        Raises:\n            ValueError: If `value` &lt; 0.\n            TypeError: If `value` is not an integer.\n        \"\"\"\n        if value &lt; 0:\n            raise ValueError(\"test_n must be greater or equal than 0.\")\n        if isinstance(value, float):\n            raise TypeError(\"test_n must be an integer.\")\n        self._test_n = value\n\n    @property\n    def validation_n(self) -&gt; int:\n        \"\"\"The number of last interactions per user for the validation set.\"\"\"\n        return self._validation_n\n\n    @validation_n.setter\n    def validation_n(self, value: int) -&gt; None:\n        \"\"\"\n        Sets the number of last interactions per user for the validation set.\n\n        Args:\n            value (int): Number of interactions. Must be &gt;= 0.\n\n        Raises:\n            ValueError: If `value` &lt; 0.\n            TypeError: If `value` is not an integer.\n        \"\"\"\n        if value &lt; 0:\n            raise ValueError(\"validation_n must be greater or equal than 0.\")\n        if isinstance(value, float):\n            raise TypeError(\"validation_n must be and integer.\")\n        self._validation_n = value\n\n    def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n        \"\"\"\n        Splits the dataset into train, test, and validation sets based on the last `n` interactions.\n\n        Args:\n            datarec (DataRec): The dataset containing the interactions and timestamp column.\n\n        Returns:\n            (Dict[str, DataRec]): A dictionary with the following keys:\n                - \"train\": The training dataset (`DataRec`).\n                - \"test\": The test dataset (`DataRec`), if `test_n` &gt; 0.\n                - \"val\": The validation dataset (`DataRec`), if `val_n` &gt; 0.\n\n        Raises:\n            TypeError: If the dataset does not contain a timestamp column.\n        \"\"\"\n\n        if datarec.timestamp_col is None:\n            raise TypeError('This DataRec does not contain temporal information')\n\n        train, test, val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n        data = datarec.data\n        for u in datarec.users:\n\n            u_train, u_val, u_test = data[data.iloc[:, 0] == u], pd.DataFrame(), pd.DataFrame()\n\n            if self.test_n:\n                for _ in range(self.test_n):\n                    u_train, sample = max_by_col(u_train, datarec.timestamp_col, seed=self.seed)\n                    u_test = pd.concat([u_test, sample], axis=0, ignore_index=True)\n\n            if self.validation_n:\n                for _ in range(self.validation_n):\n                    u_train, sample = max_by_col(u_train, datarec.timestamp_col, seed=self.seed)\n                    u_val = pd.concat([u_val, sample], axis=0, ignore_index=True)\n\n            train = pd.concat([train, u_train], axis=0, ignore_index=True)\n            test = pd.concat([test, u_test], axis=0, ignore_index=True)\n            val = pd.concat([val, u_val], axis=0, ignore_index=True)\n\n        return self.output(datarec, train, test, val,\n                           step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.temporal.leave_out.LeaveNLast.test_n","title":"<code>test_n</code>  <code>property</code> <code>writable</code>","text":"<p>The number of last interactions per user for the test set.</p>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.temporal.leave_out.LeaveNLast.validation_n","title":"<code>validation_n</code>  <code>property</code> <code>writable</code>","text":"<p>The number of last interactions per user for the validation set.</p>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.temporal.leave_out.LeaveNLast.__init__","title":"<code>__init__(test_n=0, validation_n=0, seed=42)</code>","text":"<p>Initializes the LeaveNLast splitter.</p> <p>Parameters:</p> Name Type Description Default <code>test_n</code> <code>int</code> <p>Number of last interactions for the test set. Defaults to 0.</p> <code>0</code> <code>validation_n</code> <code>int</code> <p>Number of last interactions for the validation set. Defaults to 0.</p> <code>0</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> Source code in <code>datarec/splitters/user_stratified/temporal/leave_out.py</code> <pre><code>def __init__(self, test_n: int = 0, validation_n: int = 0, seed: int = 42):\n    \"\"\"Initializes the LeaveNLast splitter.\n\n    Args:\n        test_n (int, optional): Number of last interactions for the test set. Defaults to 0.\n        validation_n (int, optional): Number of last interactions for the validation set. Defaults to 0.\n        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n    \"\"\"\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.test_n = test_n\n    self.validation_n = validation_n\n    self.seed = seed\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.temporal.leave_out.LeaveNLast.run","title":"<code>run(datarec)</code>","text":"<p>Splits the dataset into train, test, and validation sets based on the last <code>n</code> interactions.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The dataset containing the interactions and timestamp column.</p> required <p>Returns:</p> Type Description <code>Dict[str, DataRec]</code> <p>A dictionary with the following keys: - \"train\": The training dataset (<code>DataRec</code>). - \"test\": The test dataset (<code>DataRec</code>), if <code>test_n</code> &gt; 0. - \"val\": The validation dataset (<code>DataRec</code>), if <code>val_n</code> &gt; 0.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the dataset does not contain a timestamp column.</p> Source code in <code>datarec/splitters/user_stratified/temporal/leave_out.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n    \"\"\"\n    Splits the dataset into train, test, and validation sets based on the last `n` interactions.\n\n    Args:\n        datarec (DataRec): The dataset containing the interactions and timestamp column.\n\n    Returns:\n        (Dict[str, DataRec]): A dictionary with the following keys:\n            - \"train\": The training dataset (`DataRec`).\n            - \"test\": The test dataset (`DataRec`), if `test_n` &gt; 0.\n            - \"val\": The validation dataset (`DataRec`), if `val_n` &gt; 0.\n\n    Raises:\n        TypeError: If the dataset does not contain a timestamp column.\n    \"\"\"\n\n    if datarec.timestamp_col is None:\n        raise TypeError('This DataRec does not contain temporal information')\n\n    train, test, val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n    data = datarec.data\n    for u in datarec.users:\n\n        u_train, u_val, u_test = data[data.iloc[:, 0] == u], pd.DataFrame(), pd.DataFrame()\n\n        if self.test_n:\n            for _ in range(self.test_n):\n                u_train, sample = max_by_col(u_train, datarec.timestamp_col, seed=self.seed)\n                u_test = pd.concat([u_test, sample], axis=0, ignore_index=True)\n\n        if self.validation_n:\n            for _ in range(self.validation_n):\n                u_train, sample = max_by_col(u_train, datarec.timestamp_col, seed=self.seed)\n                u_val = pd.concat([u_val, sample], axis=0, ignore_index=True)\n\n        train = pd.concat([train, u_train], axis=0, ignore_index=True)\n        test = pd.concat([test, u_test], axis=0, ignore_index=True)\n        val = pd.concat([val, u_val], axis=0, ignore_index=True)\n\n    return self.output(datarec, train, test, val,\n                       step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.temporal.leave_out.LeaveOneLast","title":"<code>LeaveOneLast</code>","text":"<p>               Bases: <code>LeaveNLast</code></p> <p>Special case of LeaveNLast that removes only the last interaction per user for test and validation.</p> <p>This class sets <code>test_n</code> and <code>validation_n</code> to 1 if their corresponding boolean parameters are True.</p> Source code in <code>datarec/splitters/user_stratified/temporal/leave_out.py</code> <pre><code>class LeaveOneLast(LeaveNLast):\n    \"\"\"\n    Special case of LeaveNLast that removes only the last interaction per user for test and validation.\n\n    This class sets `test_n` and `validation_n` to 1 if their corresponding boolean parameters are True.\n\n    \"\"\"\n\n    def __init__(self, test: bool = True, validation: bool = True, seed: int = 42):\n        \"\"\"\n        Initializes the LeaveOneLast splitter.\n\n        Args:\n            test (bool, optional): Whether to remove the last interaction for the test set. Defaults to True.\n            validation (bool, optional): Whether to remove the last interaction for the validation set. Defaults to True.\n            seed (int, optional): Random seed for reproducibility. Default is 42.\n\n        Raises:\n            TypeError: If `test` or `validation` are not boolean.\n        \"\"\"\n        if not isinstance(test, bool):\n            raise TypeError(\"test must be a boolean.\")\n        if not isinstance(validation, bool):\n            raise TypeError(\"validation must be an boolean.\")\n\n        test = 1 if test else 0\n        validation = 1 if validation else 0\n\n        super().__init__(test_n=test, validation_n=validation, seed=seed)\n\n        self.params = {\n            \"test\": test,\n            \"validation\": validation,\n            \"seed\": seed,\n        }\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.temporal.leave_out.LeaveOneLast.__init__","title":"<code>__init__(test=True, validation=True, seed=42)</code>","text":"<p>Initializes the LeaveOneLast splitter.</p> <p>Parameters:</p> Name Type Description Default <code>test</code> <code>bool</code> <p>Whether to remove the last interaction for the test set. Defaults to True.</p> <code>True</code> <code>validation</code> <code>bool</code> <p>Whether to remove the last interaction for the validation set. Defaults to True.</p> <code>True</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Default is 42.</p> <code>42</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>test</code> or <code>validation</code> are not boolean.</p> Source code in <code>datarec/splitters/user_stratified/temporal/leave_out.py</code> <pre><code>def __init__(self, test: bool = True, validation: bool = True, seed: int = 42):\n    \"\"\"\n    Initializes the LeaveOneLast splitter.\n\n    Args:\n        test (bool, optional): Whether to remove the last interaction for the test set. Defaults to True.\n        validation (bool, optional): Whether to remove the last interaction for the validation set. Defaults to True.\n        seed (int, optional): Random seed for reproducibility. Default is 42.\n\n    Raises:\n        TypeError: If `test` or `validation` are not boolean.\n    \"\"\"\n    if not isinstance(test, bool):\n        raise TypeError(\"test must be a boolean.\")\n    if not isinstance(validation, bool):\n        raise TypeError(\"validation must be an boolean.\")\n\n    test = 1 if test else 0\n    validation = 1 if validation else 0\n\n    super().__init__(test_n=test, validation_n=validation, seed=seed)\n\n    self.params = {\n        \"test\": test,\n        \"validation\": validation,\n        \"seed\": seed,\n    }\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.temporal.leave_out.LeaveRatioLast","title":"<code>LeaveRatioLast</code>","text":"<p>               Bases: <code>Splitter</code></p> <p>Splits the dataset into training, test, and validation sets by selecting the most recent interactions for each user based on a specified ratio.</p> <p>Unlike <code>LeaveNLast</code>, which selects a fixed number of interactions, this splitter chooses a fraction of the total interactions per user, preserving temporal order.</p> Source code in <code>datarec/splitters/user_stratified/temporal/leave_out.py</code> <pre><code>class LeaveRatioLast(Splitter):\n    \"\"\"\n    Splits the dataset into training, test, and validation sets by selecting the most recent interactions\n    for each user based on a specified ratio.\n\n    Unlike `LeaveNLast`, which selects a fixed number of interactions, this splitter chooses a fraction\n    of the total interactions per user, preserving temporal order.\n\n    \"\"\"\n\n    def __init__(self, test_ratio: float = 0, val_ratio: float = 0, seed: int = 42):\n        \"\"\"\n        Args:\n            test_ratio (float, optional): Proportion of each user's interactions assigned to the test set. Default is 0.\n            val_ratio (float, optional): Proportion of each user's interactions assigned to the validation set. Default is 0.\n            seed (int, optional): Random seed for reproducibility. Default is 42.\n\n        Raises:\n            ValueError: If `test_ratio` or `val_ratio` are not in the range [0, 1].\n            ValueError: If `test_ratio + val_ratio` &gt; 1.\n        \"\"\"\n        if not (0 &lt;= test_ratio &lt;= 1):\n            raise ValueError('ratio must be between 0 and 1')\n        if not (0 &lt;= val_ratio &lt;= 1):\n            raise ValueError('ratio must be between 0 and 1')\n        if test_ratio + val_ratio &gt; 1:\n            raise ValueError(\"sum of test_ratio and val_ratio must not exceed 1\")\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.test_ratio = test_ratio\n        self.val_ratio = val_ratio\n        self.seed = seed\n\n    def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n        \"\"\"\n        Splits the dataset into train, test, and validation sets by selecting the last interactions\n        (in chronological order) for each user.\n\n        The most recent interactions are removed first for the test set, then for the validation set,\n        leaving the remaining interactions for training.\n\n        Args:\n            datarec (DataRec): The dataset containing interactions with a timestamp column.\n\n        Returns:\n            (Dict[str, DataRec]): A dictionary containing the following keys:\n                - `\"train\"` (`DataRec`): The training dataset.\n                - `\"test\"` (`DataRec`): The test dataset, if `test_ratio` &gt; 0.\n                - `\"val\"` (`DataRec`): The validation dataset, if `val_ratio` &gt; 0.\n\n        Raises:\n            TypeError: If the dataset does not contain a timestamp column.\n        \"\"\"\n\n        if datarec.timestamp_col is None:\n            raise TypeError('This DataRec does not contain temporal information')\n\n        train, test, val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n        data = datarec.data\n        for u in datarec.users:\n            u_train, u_val, u_test = data[data.iloc[:, 0] == u], pd.DataFrame(), pd.DataFrame()\n\n            user_total = len(u_train)\n\n            test_n_samples = round(self.test_ratio * user_total)\n            val_n_samples = round(self.val_ratio * user_total)\n\n            if test_n_samples &gt; 0:\n                for _ in range(min(test_n_samples, len(u_train))):\n                    u_train, sample = max_by_col(u_train, datarec.timestamp_col, seed=self.seed)\n                    u_test = pd.concat([u_test, sample], axis=0, ignore_index=True)\n\n            if val_n_samples &gt; 0:\n                for _ in range(min(val_n_samples, len(u_train))):\n                    u_train, sample = max_by_col(u_train, datarec.timestamp_col, seed=self.seed)\n                    u_val = pd.concat([u_val, sample], axis=0, ignore_index=True)\n\n            train = pd.concat([train, u_train], axis=0, ignore_index=True)\n            test = pd.concat([test, u_test], axis=0, ignore_index=True)\n            val = pd.concat([val, u_val], axis=0, ignore_index=True)\n\n        return self.output(datarec, train, test, val,\n                           step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.temporal.leave_out.LeaveRatioLast.__init__","title":"<code>__init__(test_ratio=0, val_ratio=0, seed=42)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>test_ratio</code> <code>float</code> <p>Proportion of each user's interactions assigned to the test set. Default is 0.</p> <code>0</code> <code>val_ratio</code> <code>float</code> <p>Proportion of each user's interactions assigned to the validation set. Default is 0.</p> <code>0</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Default is 42.</p> <code>42</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>test_ratio</code> or <code>val_ratio</code> are not in the range [0, 1].</p> <code>ValueError</code> <p>If <code>test_ratio + val_ratio</code> &gt; 1.</p> Source code in <code>datarec/splitters/user_stratified/temporal/leave_out.py</code> <pre><code>def __init__(self, test_ratio: float = 0, val_ratio: float = 0, seed: int = 42):\n    \"\"\"\n    Args:\n        test_ratio (float, optional): Proportion of each user's interactions assigned to the test set. Default is 0.\n        val_ratio (float, optional): Proportion of each user's interactions assigned to the validation set. Default is 0.\n        seed (int, optional): Random seed for reproducibility. Default is 42.\n\n    Raises:\n        ValueError: If `test_ratio` or `val_ratio` are not in the range [0, 1].\n        ValueError: If `test_ratio + val_ratio` &gt; 1.\n    \"\"\"\n    if not (0 &lt;= test_ratio &lt;= 1):\n        raise ValueError('ratio must be between 0 and 1')\n    if not (0 &lt;= val_ratio &lt;= 1):\n        raise ValueError('ratio must be between 0 and 1')\n    if test_ratio + val_ratio &gt; 1:\n        raise ValueError(\"sum of test_ratio and val_ratio must not exceed 1\")\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.test_ratio = test_ratio\n    self.val_ratio = val_ratio\n    self.seed = seed\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.temporal.leave_out.LeaveRatioLast.run","title":"<code>run(datarec)</code>","text":"<p>Splits the dataset into train, test, and validation sets by selecting the last interactions (in chronological order) for each user.</p> <p>The most recent interactions are removed first for the test set, then for the validation set, leaving the remaining interactions for training.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The dataset containing interactions with a timestamp column.</p> required <p>Returns:</p> Type Description <code>Dict[str, DataRec]</code> <p>A dictionary containing the following keys: - <code>\"train\"</code> (<code>DataRec</code>): The training dataset. - <code>\"test\"</code> (<code>DataRec</code>): The test dataset, if <code>test_ratio</code> &gt; 0. - <code>\"val\"</code> (<code>DataRec</code>): The validation dataset, if <code>val_ratio</code> &gt; 0.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the dataset does not contain a timestamp column.</p> Source code in <code>datarec/splitters/user_stratified/temporal/leave_out.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n    \"\"\"\n    Splits the dataset into train, test, and validation sets by selecting the last interactions\n    (in chronological order) for each user.\n\n    The most recent interactions are removed first for the test set, then for the validation set,\n    leaving the remaining interactions for training.\n\n    Args:\n        datarec (DataRec): The dataset containing interactions with a timestamp column.\n\n    Returns:\n        (Dict[str, DataRec]): A dictionary containing the following keys:\n            - `\"train\"` (`DataRec`): The training dataset.\n            - `\"test\"` (`DataRec`): The test dataset, if `test_ratio` &gt; 0.\n            - `\"val\"` (`DataRec`): The validation dataset, if `val_ratio` &gt; 0.\n\n    Raises:\n        TypeError: If the dataset does not contain a timestamp column.\n    \"\"\"\n\n    if datarec.timestamp_col is None:\n        raise TypeError('This DataRec does not contain temporal information')\n\n    train, test, val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n    data = datarec.data\n    for u in datarec.users:\n        u_train, u_val, u_test = data[data.iloc[:, 0] == u], pd.DataFrame(), pd.DataFrame()\n\n        user_total = len(u_train)\n\n        test_n_samples = round(self.test_ratio * user_total)\n        val_n_samples = round(self.val_ratio * user_total)\n\n        if test_n_samples &gt; 0:\n            for _ in range(min(test_n_samples, len(u_train))):\n                u_train, sample = max_by_col(u_train, datarec.timestamp_col, seed=self.seed)\n                u_test = pd.concat([u_test, sample], axis=0, ignore_index=True)\n\n        if val_n_samples &gt; 0:\n            for _ in range(min(val_n_samples, len(u_train))):\n                u_train, sample = max_by_col(u_train, datarec.timestamp_col, seed=self.seed)\n                u_val = pd.concat([u_val, sample], axis=0, ignore_index=True)\n\n        train = pd.concat([train, u_train], axis=0, ignore_index=True)\n        test = pd.concat([test, u_test], axis=0, ignore_index=True)\n        val = pd.concat([val, u_val], axis=0, ignore_index=True)\n\n    return self.output(datarec, train, test, val,\n                       step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"}]}