{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Documentation of DataRec: A Python Library for Standardized and Reproducible Data Management in Recommender Systems","text":"<p>This is the official Documentation Website for the paper \"DataRec: A Python Library for Standardized and Reproducible Data Management in Recommender Systems\", accepted for publication at the \"The 48th International ACM SIGIR Conference on Research and Development in Information Retrieval\", SIGIR 2025.</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>What is DataRec</li> <li>Filtering Strategies</li> <li>Splitting Strategies</li> <li>Authors</li> </ul>"},{"location":"#what-is-datarec","title":"What is DataRec","text":"<p>DataRec is a Python library that focuses on the data management phase of recommendation systems. It aims to promote standardization, interoperability, and best practices for processing and analyzing recommendation datasets.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Dataset Management: Supports reading and writing various data formats and allows dynamic format specification.</li> <li>Reference Datasets: Include commonly used recommendation datasets with traceable sources and versioning.</li> <li>Filtering Strategies: Implements popular filtering techniques.</li> <li>Splitting Strategies: Implements widely used data splitting strategies.</li> <li>Data Characteristics Analysis: Enables computing data characteristics that impact recommendation performance.</li> <li>Interoperability: Designed to be modular and compatible with existing recommendation frameworks by allowing dataset export in various formats.</li> </ul>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Alberto Carlo Maria Mancino (alberto.mancino@poliba.it)</li> <li>Salvatore Bufi (salvatore.bufi@poliba.it)</li> <li>Angela Di Fazio (angela.difazio@poliba.it)</li> <li>Daniele Malitesta (daniele.malitesta@centralesupelec.fr)</li> <li>Antonio Ferrara (antonio.ferrara@poliba.it)</li> <li>Claudio Pomo (claudio.pomo@poliba.it)</li> <li>Tommaso Di Noia (tommaso.dinoia@poliba.it)</li> </ul>"},{"location":"datasets_nav/","title":"Datasets","text":"<p>DataRec includes several commonly used recommendation datasets to facilitate reproducibility and standardization. These datasets have been carefully curated, with traceable sources and versioning information maintained whenever possible. For each dataset, DataRec provides metadata such as the number of users, items, and interactions and data characteristics known to impact recommendation performance (e.g., sparsity and user/item distribution shifts). The dataset collection in DataRec is continuously updated to include more recent and widely used datasets from the recommendation systems literature. The most recent and widely used version is included when the original data source is unavailable to ensure backward compatibility.</p> <p>The following datasets are currently included in DataRec:</p> Dataset Name Source Alibaba iFashion https://drive.google.com/drive/folders/1xFdx5xuNXHGsUVG2VIohFTXf9S7G5veq Amazon Beauty https://amazon-reviews-2023.github.io Amazon Books https://amazon-reviews-2023.github.io/ Amazon Clothing https://amazon-reviews-2023.github.io/ Amazon Sports and Outdoors https://amazon-reviews-2023.github.io/ Amazon Toys and Games https://amazon-reviews-2023.github.io/ Amazon Video Games https://amazon-reviews-2023.github.io/ Ciao https://guoguibing.github.io/librec/datasets.html Epinions https://snap.stanford.edu/data/soc-Epinions1.html Gowalla https://snap.stanford.edu/data/loc-gowalla.html LastFM https://grouplens.org/datasets/hetrec-2011/ MovieLens https://grouplens.org/datasets/movielens/ Tmall https://tianchi.aliyun.com/dataset/53?t=1716541860503 Yelp https://www.yelp.com/dataset"},{"location":"get_started/","title":"Get Started","text":""},{"location":"get_started/#installation-guidelines","title":"Installation guidelines","text":"<p>Please make sure to have the following installed on your system:</p> <ul> <li>Python <code>3.9.0</code> or later</li> </ul> <p>you first need to clone this repository: <pre><code>git clone https://github.com/sisinflab/DataRec.git\n</code></pre> You may create the virtual environment with the requirements files we included in the repository, as follows: <pre><code>$ python3.9 -m venv venv\n$ source venv/bin/activate\n$ pip install --upgrade pip\n$ pip install -r requirements.txt\n</code></pre></p>"},{"location":"documentation/data/","title":"Data Module Reference","text":"<p>This section provides a detailed API reference for all modules related to managing the datasets.</p>"},{"location":"documentation/data/#core-data-utilities","title":"Core Data Utilities","text":"<p>These modules define the common utilities used by all splitters.</p>"},{"location":"documentation/data/#datarec.data.utils.set_column_name","title":"<code>set_column_name(columns, value, rename=True, default_name=None)</code>","text":"<p>Identifies a column by its name or index and optionally renames it.</p> <p>This utility function provides a flexible way to handle DataFrame columns. It can find a column based on its current name (string) or its position (integer). If <code>rename</code> is True, it replaces the found column name in the list of columns with a <code>default_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list</code> <p>The list of current column names in the DataFrame.</p> required <code>value</code> <code>Union[str, int]</code> <p>The identifier for the column, either its name or its integer index.</p> required <code>rename</code> <code>bool</code> <p>If True, the identified column's name is changed to <code>default_name</code> in the returned list. Defaults to True.</p> <code>True</code> <code>default_name</code> <code>str</code> <p>The new name for the column if <code>rename</code> is True. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[list, str]</code> <p>A tuple containing: - The (potentially modified) list of column names. - The final name of the selected column (either the original or the   <code>default_name</code> if renamed).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>value</code> is not a valid column name or index, or if it is not a string or integer.</p> Source code in <code>datarec/data/utils.py</code> <pre><code>def set_column_name(columns: list, value: Union[str, int], rename=True, default_name=None) -&gt; (list, str):\n    \"\"\"\n    Identifies a column by its name or index and optionally renames it.\n\n    This utility function provides a flexible way to handle DataFrame columns. It\n    can find a column based on its current name (string) or its position (integer).\n    If `rename` is True, it replaces the found column name in the list of columns\n    with a `default_name`.\n\n    Args:\n        columns (list): The list of current column names in the DataFrame.\n        value (Union[str, int]): The identifier for the column, either its name\n            or its integer index.\n        rename (bool, optional): If True, the identified column's name is\n            changed to `default_name` in the returned list. Defaults to True.\n        default_name (str, optional): The new name for the column if `rename` is\n            True. Defaults to None.\n\n    Returns:\n        (tuple[list, str]): A tuple containing:\n            - The (potentially modified) list of column names.\n            - The final name of the selected column (either the original or the\n              `default_name` if renamed).\n\n    Raises:\n        ValueError: If the `value` is not a valid column name or index, or if\n            it is not a string or integer.\n    \"\"\"\n    columns = list(columns)\n\n    if isinstance(value, str):\n        if value not in columns:\n            raise ValueError(f'column \\'{value}\\' is not a valid column name.')\n        selected_column = value\n\n    elif isinstance(value, int):\n        if value in columns:\n            selected_column = value\n        else:\n            if value not in range(len(columns)):\n                raise ValueError(f'column int \\'{value}\\' is out of range ({len(columns)} columns).')\n            selected_column = columns[value]\n    else:\n        raise ValueError(f'column value must be either a string (column name) or an integer (column index)')\n\n    if rename is True:\n        columns[columns.index(selected_column)] = default_name\n        return columns, default_name\n\n    return columns, selected_column\n</code></pre>"},{"location":"documentation/data/#datarec.data.utils.quartiles","title":"<code>quartiles(count)</code>","text":"<p>Assigns quartile indices (0-3) to items based on their frequency counts.</p> <p>The function divides the input values into four quartiles using the  median and quantiles. Each item is assigned an integer:     0: long tail (lowest quartile)     1: common     2: popular     3: most popular (highest quartile)</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>dict</code> <p>A dictionary mapping items to numeric counts or frequencies.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping each item to its quartile index (0-3).</p> Source code in <code>datarec/data/utils.py</code> <pre><code>def quartiles(count: dict):\n    \"\"\" \n    Assigns quartile indices (0-3) to items based on their frequency counts.\n\n    The function divides the input values into four quartiles using the \n    median and quantiles. Each item is assigned an integer:\n        0: long tail (lowest quartile)\n        1: common\n        2: popular\n        3: most popular (highest quartile)\n\n    Args:\n        count (dict): A dictionary mapping items to numeric counts or frequencies.\n\n    Returns:\n        (dict): A dictionary mapping each item to its quartile index (0-3).\n    \"\"\"\n    q1, q2, q3 = statistics.quantiles(count.values())\n\n    def assign(value):\n        if value &lt;= q2:\n            if value &lt;= q1:\n                return 0\n            else:\n                return 1\n        else:\n            if value &lt;= q3:\n                return 2\n            else:\n                return 3\n\n    return {k: assign(f) for k, f in count.items()}\n</code></pre>"},{"location":"documentation/data/#datarec.data.utils.popularity","title":"<code>popularity(quartiles)</code>","text":"<p>Categorizes items based on their quartile indices.</p> <p>Converts quartile indices (0-3) into descriptive popularity categories:     0 -&gt; 'long tail'     1 -&gt; 'common'     2 -&gt; 'popular'     3 -&gt; 'most popular'</p> <p>Parameters:</p> Name Type Description Default <code>quartiles</code> <code>dict</code> <p>A dictionary mapping items to quartile indices (0-3).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping each popularity category to a list of items.</p> Source code in <code>datarec/data/utils.py</code> <pre><code>def popularity(quartiles: dict):\n    \"\"\" \n    Categorizes items based on their quartile indices.\n\n    Converts quartile indices (0-3) into descriptive popularity categories:\n        0 -&gt; 'long tail'\n        1 -&gt; 'common'\n        2 -&gt; 'popular'\n        3 -&gt; 'most popular'\n\n    Args:\n        quartiles (dict): A dictionary mapping items to quartile indices (0-3).\n\n    Returns:\n        (dict): A dictionary mapping each popularity category to a list of items.\n    \"\"\"\n    categories_map = \\\n        {3: 'most popular',\n         2: 'popular',\n         1: 'common',\n         0: 'long tail'}\n\n    categories = \\\n        {'most popular': [],\n         'popular': [],\n         'common': [],\n         'long tail': []}\n\n    for k, q in quartiles.items():\n        categories[categories_map[q]].append(k)\n\n    return categories\n</code></pre>"},{"location":"documentation/data/#datarec.data.utils.verify_checksum","title":"<code>verify_checksum(file_path, checksum)</code>","text":"<p>Verifies the MD5 checksum of a file.</p> <p>This function computes the MD5 hash of the file at the given path and compares it to the expected checksum. If the file does not exist, a FileNotFoundError is raised. If the checksum does not match, a RuntimeError is raised indicating possible corruption or version mismatch.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the file to verify.</p> required <code>checksum</code> <code>str</code> <p>The expected MD5 checksum.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>RuntimeError</code> <p>If the computed checksum does not match the expected value.</p> Source code in <code>datarec/data/utils.py</code> <pre><code>def verify_checksum(file_path: str, checksum: str) -&gt; None:\n    \"\"\"\n    Verifies the MD5 checksum of a file.\n\n    This function computes the MD5 hash of the file at the given path and\n    compares it to the expected checksum. If the file does not exist, a\n    FileNotFoundError is raised. If the checksum does not match, a RuntimeError\n    is raised indicating possible corruption or version mismatch.\n\n    Args:\n        file_path (str): The path to the file to verify.\n        checksum (str): The expected MD5 checksum.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n        RuntimeError: If the computed checksum does not match the expected value.\n    \"\"\"\n\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f'File \\'{file_path}\\ not found.')\n\n    md5 = hashlib.md5()\n    with open(file_path, \"rb\") as f:\n        for chunck in iter(lambda: f.read(65536), b\"\"):\n            md5.update(chunck)\n\n    digest = md5.hexdigest()\n    if not digest == checksum:\n        raise RuntimeError(f\"Checksum mismatch for '{file_path}': expected {checksum}, but got {digest}. \"\n                           f\"The file may be corrupted or a new version has been downloaded.\")\n\n    print(f'Checksum verified.')\n</code></pre>"},{"location":"documentation/data/#dataset-wrappers","title":"Dataset wrappers","text":""},{"location":"documentation/data/#datarec.data.torch_dataset.BaseTorchDataset","title":"<code>BaseTorchDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Base class for Torch datasets wrapping a DataRec dataset.</p> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>class BaseTorchDataset(Dataset):\n    \"\"\"\n    Base class for Torch datasets wrapping a DataRec dataset.\n    \"\"\"\n    def __init__(self, datarec, copy_data=False):\n        \"\"\"\n        Initializes the BaseTorchDataset object.    \n\n        Args:\n            datarec (DataRec): An instance of a DataRec dataset.\n            copy_data (bool): Whether to copy the dataset or use it by reference.\n        \"\"\"\n        self.df = datarec.data.copy() if copy_data else datarec.data\n        self.user_col = datarec.user_col\n        self.item_col = datarec.item_col\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.BaseTorchDataset.__init__","title":"<code>__init__(datarec, copy_data=False)</code>","text":"<p>Initializes the BaseTorchDataset object.    </p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>An instance of a DataRec dataset.</p> required <code>copy_data</code> <code>bool</code> <p>Whether to copy the dataset or use it by reference.</p> <code>False</code> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>def __init__(self, datarec, copy_data=False):\n    \"\"\"\n    Initializes the BaseTorchDataset object.    \n\n    Args:\n        datarec (DataRec): An instance of a DataRec dataset.\n        copy_data (bool): Whether to copy the dataset or use it by reference.\n    \"\"\"\n    self.df = datarec.data.copy() if copy_data else datarec.data\n    self.user_col = datarec.user_col\n    self.item_col = datarec.item_col\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.PointwiseTorchDataset","title":"<code>PointwiseTorchDataset</code>","text":"<p>               Bases: <code>BaseTorchDataset</code></p> <p>Torch dataset for pointwise recommendation tasks.</p> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>class PointwiseTorchDataset(BaseTorchDataset):\n    \"\"\"\n    Torch dataset for pointwise recommendation tasks.\n    \"\"\"\n    def __init__(self, datarec, copy_data=False):\n        \"\"\"\n        Initializes the PointwiseTorchDataset object.\n\n        Args:\n            datarec (DataRec): An instance of a DataRec dataset.\n            copy_data (bool): Whether to copy the dataset or use it by reference.\n        \"\"\"\n        super().__init__(datarec, copy_data)\n        self.rating_col = datarec.rating_col\n\n    def __len__(self):\n        \"\"\"\n        Returns the total number of samples in the dataset.\n\n        This is required by PyTorch's DataLoader to iterate over the dataset.\n\n        Returns:\n            (int): Number of samples in the dataset.\n        \"\"\"\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Returns a sample with user, item, and rating.\n\n        Args:\n            idx (int): Sample index to be returned.\n\n        Returns:\n            (dict): Sample with user, item, and rating.\n        \"\"\"\n        row = self.df.iloc[idx]\n        return {\n            \"user\": row[self.user_col],\n            \"item\": row[self.item_col],\n            \"rating\": row.get(self.rating_col, 1.0)\n        }\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.PointwiseTorchDataset.__init__","title":"<code>__init__(datarec, copy_data=False)</code>","text":"<p>Initializes the PointwiseTorchDataset object.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>An instance of a DataRec dataset.</p> required <code>copy_data</code> <code>bool</code> <p>Whether to copy the dataset or use it by reference.</p> <code>False</code> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>def __init__(self, datarec, copy_data=False):\n    \"\"\"\n    Initializes the PointwiseTorchDataset object.\n\n    Args:\n        datarec (DataRec): An instance of a DataRec dataset.\n        copy_data (bool): Whether to copy the dataset or use it by reference.\n    \"\"\"\n    super().__init__(datarec, copy_data)\n    self.rating_col = datarec.rating_col\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.PointwiseTorchDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the total number of samples in the dataset.</p> <p>This is required by PyTorch's DataLoader to iterate over the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of samples in the dataset.</p> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Returns the total number of samples in the dataset.\n\n    This is required by PyTorch's DataLoader to iterate over the dataset.\n\n    Returns:\n        (int): Number of samples in the dataset.\n    \"\"\"\n    return len(self.df)\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.PointwiseTorchDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns a sample with user, item, and rating.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Sample index to be returned.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Sample with user, item, and rating.</p> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Returns a sample with user, item, and rating.\n\n    Args:\n        idx (int): Sample index to be returned.\n\n    Returns:\n        (dict): Sample with user, item, and rating.\n    \"\"\"\n    row = self.df.iloc[idx]\n    return {\n        \"user\": row[self.user_col],\n        \"item\": row[self.item_col],\n        \"rating\": row.get(self.rating_col, 1.0)\n    }\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.PairwiseTorchDataset","title":"<code>PairwiseTorchDataset</code>","text":"<p>               Bases: <code>BaseTorchDataset</code></p> <p>Torch dataset for pairwise recommendation tasks with negative sampling.</p> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>class PairwiseTorchDataset(BaseTorchDataset):\n    \"\"\"\n    Torch dataset for pairwise recommendation tasks with negative sampling.\n    \"\"\"\n    def __init__(self, datarec, num_negatives=1, item_pool=None, copy_data=False):\n        \"\"\" \n        Initializes the PairwiseTorchDataset object.\n\n        Args:\n            datarec (DataRec): An instance of a DataRec dataset.\n            num_negatives (int): Number of negative samples to generate per interaction.\n            item_pool (array-like): Pool of items to sample from. Defaults to all items in the dataset.\n            copy_data (bool): Whether to copy the dataset or use it by reference.\n        \"\"\"\n        super().__init__(datarec, copy_data)\n        self.num_negatives = num_negatives\n        self.item_pool = item_pool or self.df[self.item_col].unique()\n        self.user_pos_items = self.df.groupby(self.user_col)[self.item_col].apply(set).to_dict()\n\n    def sample_negatives(self, user):\n        \"\"\"\n        Samples negative items for a given user, avoiding known positive items.\n\n        This method is designed to be overridden to implement custom negative\n        sampling strategies (e.g., popularity-based, adversarial, or\n        distribution-aware sampling). The default implementation draws\n        uniformly from the item pool, excluding items the user has already interacted with.\n\n        Args:\n            user: The user ID for which to sample negatives.\n\n        Returns:\n            (List): List of sampled negative item IDs.\n        \"\"\"\n        neg_items = []\n        user_positives = self.user_pos_items.get(user, set())\n        while len(neg_items) &lt; self.num_negatives:\n            candidate = np.random.choice(self.item_pool)\n            if candidate not in user_positives:\n                neg_items.append(candidate)\n        return neg_items\n\n    def __len__(self):\n        \"\"\"\n        Returns the total number of samples in the dataset.\n\n        This is required by PyTorch's DataLoader to iterate over the dataset.\n\n        Returns:\n            (int): number of samples in the dataset.\n        \"\"\"\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Returns a sample with user, positive item, and negative items.\n\n        Args:\n            idx (int): Sample index to be returned.\n\n        Returns:\n            (dict): Sample with user, positive item, and negative items.\n        \"\"\"\n        row = self.df.iloc[idx]\n        user = row[self.user_col]\n        pos_item = row[self.item_col]\n        neg_items = self.sample_negatives(user)\n        return {\n            \"user\": user,\n            \"pos_item\": pos_item,\n            \"neg_items\": neg_items\n        }\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.PairwiseTorchDataset.__init__","title":"<code>__init__(datarec, num_negatives=1, item_pool=None, copy_data=False)</code>","text":"<p>Initializes the PairwiseTorchDataset object.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>An instance of a DataRec dataset.</p> required <code>num_negatives</code> <code>int</code> <p>Number of negative samples to generate per interaction.</p> <code>1</code> <code>item_pool</code> <code>array - like</code> <p>Pool of items to sample from. Defaults to all items in the dataset.</p> <code>None</code> <code>copy_data</code> <code>bool</code> <p>Whether to copy the dataset or use it by reference.</p> <code>False</code> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>def __init__(self, datarec, num_negatives=1, item_pool=None, copy_data=False):\n    \"\"\" \n    Initializes the PairwiseTorchDataset object.\n\n    Args:\n        datarec (DataRec): An instance of a DataRec dataset.\n        num_negatives (int): Number of negative samples to generate per interaction.\n        item_pool (array-like): Pool of items to sample from. Defaults to all items in the dataset.\n        copy_data (bool): Whether to copy the dataset or use it by reference.\n    \"\"\"\n    super().__init__(datarec, copy_data)\n    self.num_negatives = num_negatives\n    self.item_pool = item_pool or self.df[self.item_col].unique()\n    self.user_pos_items = self.df.groupby(self.user_col)[self.item_col].apply(set).to_dict()\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.PairwiseTorchDataset.sample_negatives","title":"<code>sample_negatives(user)</code>","text":"<p>Samples negative items for a given user, avoiding known positive items.</p> <p>This method is designed to be overridden to implement custom negative sampling strategies (e.g., popularity-based, adversarial, or distribution-aware sampling). The default implementation draws uniformly from the item pool, excluding items the user has already interacted with.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <p>The user ID for which to sample negatives.</p> required <p>Returns:</p> Type Description <code>List</code> <p>List of sampled negative item IDs.</p> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>def sample_negatives(self, user):\n    \"\"\"\n    Samples negative items for a given user, avoiding known positive items.\n\n    This method is designed to be overridden to implement custom negative\n    sampling strategies (e.g., popularity-based, adversarial, or\n    distribution-aware sampling). The default implementation draws\n    uniformly from the item pool, excluding items the user has already interacted with.\n\n    Args:\n        user: The user ID for which to sample negatives.\n\n    Returns:\n        (List): List of sampled negative item IDs.\n    \"\"\"\n    neg_items = []\n    user_positives = self.user_pos_items.get(user, set())\n    while len(neg_items) &lt; self.num_negatives:\n        candidate = np.random.choice(self.item_pool)\n        if candidate not in user_positives:\n            neg_items.append(candidate)\n    return neg_items\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.PairwiseTorchDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the total number of samples in the dataset.</p> <p>This is required by PyTorch's DataLoader to iterate over the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>number of samples in the dataset.</p> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Returns the total number of samples in the dataset.\n\n    This is required by PyTorch's DataLoader to iterate over the dataset.\n\n    Returns:\n        (int): number of samples in the dataset.\n    \"\"\"\n    return len(self.df)\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.PairwiseTorchDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns a sample with user, positive item, and negative items.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Sample index to be returned.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Sample with user, positive item, and negative items.</p> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Returns a sample with user, positive item, and negative items.\n\n    Args:\n        idx (int): Sample index to be returned.\n\n    Returns:\n        (dict): Sample with user, positive item, and negative items.\n    \"\"\"\n    row = self.df.iloc[idx]\n    user = row[self.user_col]\n    pos_item = row[self.item_col]\n    neg_items = self.sample_negatives(user)\n    return {\n        \"user\": user,\n        \"pos_item\": pos_item,\n        \"neg_items\": neg_items\n    }\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.RankingTorchDataset","title":"<code>RankingTorchDataset</code>","text":"<p>               Bases: <code>BaseTorchDataset</code></p> <p>Torch dataset for full softmax-style ranking tasks.</p> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>class RankingTorchDataset(BaseTorchDataset):\n    \"\"\"\n    Torch dataset for full softmax-style ranking tasks.\n    \"\"\"\n    def __init__(self, datarec, copy_data=False):\n        \"\"\"\n        Initializes the RankingTorchDataset object.\n\n        Args:\n            datarec (DataRec): An instance of a DataRec dataset.\n            copy_data (bool): Whether to copy the dataset or use it by reference.\n        \"\"\"\n        super().__init__(datarec, copy_data)\n        # Could prepare user-&gt;items mapping here for evaluation\n\n    def __len__(self):\n        \"\"\"\n        Returns the total number of samples in the dataset.\n\n        This is required by PyTorch's DataLoader to iterate over the dataset.\n\n        Returns:\n            (int): Number of samples in the dataset.\n        \"\"\"\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Returns a sample with user and item.\n\n        Args:\n            idx (int): Sample index to be returned.\n\n        Returns:\n            (dict): Sample with user and item data.\n        \"\"\"\n        row = self.df.iloc[idx]\n        return {\n            \"user\": row[self.user_col],\n            \"item\": row[self.item_col]\n            # No target \u2014 implicit ranking\n        }\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.RankingTorchDataset.__init__","title":"<code>__init__(datarec, copy_data=False)</code>","text":"<p>Initializes the RankingTorchDataset object.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>An instance of a DataRec dataset.</p> required <code>copy_data</code> <code>bool</code> <p>Whether to copy the dataset or use it by reference.</p> <code>False</code> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>def __init__(self, datarec, copy_data=False):\n    \"\"\"\n    Initializes the RankingTorchDataset object.\n\n    Args:\n        datarec (DataRec): An instance of a DataRec dataset.\n        copy_data (bool): Whether to copy the dataset or use it by reference.\n    \"\"\"\n    super().__init__(datarec, copy_data)\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.RankingTorchDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the total number of samples in the dataset.</p> <p>This is required by PyTorch's DataLoader to iterate over the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of samples in the dataset.</p> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Returns the total number of samples in the dataset.\n\n    This is required by PyTorch's DataLoader to iterate over the dataset.\n\n    Returns:\n        (int): Number of samples in the dataset.\n    \"\"\"\n    return len(self.df)\n</code></pre>"},{"location":"documentation/data/#datarec.data.torch_dataset.RankingTorchDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns a sample with user and item.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Sample index to be returned.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Sample with user and item data.</p> Source code in <code>datarec/data/torch_dataset.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Returns a sample with user and item.\n\n    Args:\n        idx (int): Sample index to be returned.\n\n    Returns:\n        (dict): Sample with user and item data.\n    \"\"\"\n    row = self.df.iloc[idx]\n    return {\n        \"user\": row[self.user_col],\n        \"item\": row[self.item_col]\n        # No target \u2014 implicit ranking\n    }\n</code></pre>"},{"location":"documentation/data/#the-datarec-class","title":"The DataRec class","text":"<p>These modules define the core part of the framework: the DataRec class.</p>"},{"location":"documentation/data/#datarec.data.datarec_builder.BaseDataRecBuilder","title":"<code>BaseDataRecBuilder</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for building <code>DataRec</code> datasets.</p> <p>This class defines the interface for preparing, downloading, and loading  datasets into <code>DataRec</code> objects.</p> Source code in <code>datarec/data/datarec_builder.py</code> <pre><code>class BaseDataRecBuilder(ABC):\n    \"\"\"\n    Abstract base class for building `DataRec` datasets.\n\n    This class defines the interface for preparing, downloading, and loading \n    datasets into `DataRec` objects. \n    \"\"\"\n\n    @abstractmethod\n    def prepare(self) -&gt; None:\n        \"\"\"Download and process the dataset, without loading it into memory.\"\"\"\n        pass\n\n    @abstractmethod\n    def load(self) -&gt; DataRec:\n        \"\"\"Load the processed dataset into a DataRec object.\"\"\"\n        pass\n\n    def prepare_and_load(self) -&gt; DataRec:\n        \"\"\"\n        A convenience method that runs the full prepare and load pipeline.\n\n        Returns:\n            (DataRec): The fully prepared and loaded dataset.\n        \"\"\"\n        self.prepare()\n        return self.load()\n\n    @abstractmethod\n    def download(self) -&gt; str:\n        \"\"\"Download the raw dataset files.\"\"\"\n        pass\n</code></pre>"},{"location":"documentation/data/#datarec.data.datarec_builder.BaseDataRecBuilder.prepare","title":"<code>prepare()</code>  <code>abstractmethod</code>","text":"<p>Download and process the dataset, without loading it into memory.</p> Source code in <code>datarec/data/datarec_builder.py</code> <pre><code>@abstractmethod\ndef prepare(self) -&gt; None:\n    \"\"\"Download and process the dataset, without loading it into memory.\"\"\"\n    pass\n</code></pre>"},{"location":"documentation/data/#datarec.data.datarec_builder.BaseDataRecBuilder.load","title":"<code>load()</code>  <code>abstractmethod</code>","text":"<p>Load the processed dataset into a DataRec object.</p> Source code in <code>datarec/data/datarec_builder.py</code> <pre><code>@abstractmethod\ndef load(self) -&gt; DataRec:\n    \"\"\"Load the processed dataset into a DataRec object.\"\"\"\n    pass\n</code></pre>"},{"location":"documentation/data/#datarec.data.datarec_builder.BaseDataRecBuilder.prepare_and_load","title":"<code>prepare_and_load()</code>","text":"<p>A convenience method that runs the full prepare and load pipeline.</p> <p>Returns:</p> Type Description <code>DataRec</code> <p>The fully prepared and loaded dataset.</p> Source code in <code>datarec/data/datarec_builder.py</code> <pre><code>def prepare_and_load(self) -&gt; DataRec:\n    \"\"\"\n    A convenience method that runs the full prepare and load pipeline.\n\n    Returns:\n        (DataRec): The fully prepared and loaded dataset.\n    \"\"\"\n    self.prepare()\n    return self.load()\n</code></pre>"},{"location":"documentation/data/#datarec.data.datarec_builder.BaseDataRecBuilder.download","title":"<code>download()</code>  <code>abstractmethod</code>","text":"<p>Download the raw dataset files.</p> Source code in <code>datarec/data/datarec_builder.py</code> <pre><code>@abstractmethod\ndef download(self) -&gt; str:\n    \"\"\"Download the raw dataset files.\"\"\"\n    pass\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec","title":"<code>DataRec</code>","text":"<p>Core data structure for recommendation datasets in the DataRec framework.</p> <p>This class wraps a Pandas DataFrame and standardizes common columns (user, item, rating, timestamp) to provide a consistent interface for recommendation tasks. It supports data preprocessing,  user/item remapping (public vs private IDs), frequency analysis,  sparsity/density metrics, Gini coefficients, and conversion into  PyTorch datasets for training.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>class DataRec:\n\n    \"\"\"\n    Core data structure for recommendation datasets in the DataRec framework.\n\n    This class wraps a Pandas DataFrame and standardizes common columns\n    (user, item, rating, timestamp) to provide a consistent interface\n    for recommendation tasks. It supports data preprocessing, \n    user/item remapping (public vs private IDs), frequency analysis, \n    sparsity/density metrics, Gini coefficients, and conversion into \n    PyTorch datasets for training.\n    \"\"\"\n\n    def __init__(\n            self,\n            rawdata: RawData = None,\n            copy: bool = False,\n            dataset_name: str = 'datarec',\n            version_name: str = 'no_version_provided',\n            pipeline: Optional[Pipeline] = None,\n            *args,\n            **kwargs\n    ):\n        \"\"\"\n        Initializes the DataRec object.\n\n        Args:\n            rawdata (RawData): The input dataset wrapped in a RawData object.\n                If None, the DataRec is initialized empty.\n            copy (bool): Whether to copy the input DataFrame to avoid \n                modifying the original RawData.\n            dataset_name (str): A name to identify the dataset.\n            version_name (str): A version identifier \n                for the dataset.\n            pipeline (Pipeline): A pipeline object to track preprocessing steps.\n\n        \"\"\"\n\n\n\n        self.path = None\n        self._data = None\n        self.dataset_name = dataset_name\n        self.version_name = version_name\n\n        if pipeline:\n            self.pipeline = pipeline\n        else:\n            self.pipeline = Pipeline()\n            self.pipeline.add_step(\"load\", self.dataset_name, {'version': self.version_name})\n\n        if rawdata is not None:\n            if copy:\n                self._data: pd.DataFrame = rawdata.data.copy()\n            else:\n                self._data: pd.DataFrame = rawdata.data\n\n        # ------------------------------------\n        # --------- STANDARD COLUMNS ---------\n        # if a column is None it means that the DataRec does not have that information\n        self.__assigned_columns = []\n\n        self._user_col = None\n        self._item_col = None\n        self._rating_col = None\n        self._timestamp_col = None\n\n        if rawdata:\n            self.set_columns(rawdata)\n\n        # dataset is assumed to be the public version of the dataset\n        self._is_private = False\n        self.__implicit = False\n\n        # ------------------------------\n        # --------- PROPERTIES ---------\n        self._sorted_users = None\n        self._sorted_items = None\n\n        # map users and items with a 0-indexed mapping\n        self._public_to_private_users = None\n        self._public_to_private_items = None\n        self._private_to_public_users = None\n        self._private_to_public_items = None\n\n        # metrics\n        self._transactions = None\n        self._space_size = None\n        self._space_size_log = None\n        self._shape = None\n        self._shape_log = None\n        self._density = None\n        self._density_log = None\n        self._gini_item = None\n        self._gini_user = None\n        self._ratings_per_user = None\n        self._ratings_per_item = None\n\n        # more analyses\n        self.metrics = ['transactions', 'space_size', 'space_size_log', 'shape', 'shape_log', 'density', 'density_log',\n                        'gini_item', 'gini_user', 'ratings_per_user', 'ratings_per_item']\n\n    def __str__(self):\n        \"\"\"\n        Returns 'self.data' as a string variable.\n\n        Returns:\n            (str): 'self.data' as a string variable.\n        \"\"\"\n        return self.data.__str__()\n\n    def __repr__(self):\n        \"\"\"\n        Returns the official string representation of the internal DataFrame.\n        \"\"\"\n        return self.data.__repr__()\n\n    def _repr_html_(self):\n        \"\"\"\n        Returns an HTML representation of the internal DataFrame for rich displays.\n        \"\"\"\n        return self.data._repr_html_()\n\n    def __len__(self):\n        \"\"\"\n        Returns the total number of samples in the dataset.\n\n        Returns:\n            (int): number of samples in the dataset.\n        \"\"\"\n        return len(self.data)\n\n    def set_columns(self, rawdata):\n        \"\"\"\n        Assign dataset column names from a RawData object and reorder the data accordingly.\n\n        Args:\n            rawdata (RawData): A RawData object containing the column names for \n                user, item, rating, and timestamp.\n        \"\"\"\n        if rawdata.user is not None:\n            self.user_col = rawdata.user\n            self.__assigned_columns.append(self.user_col)\n        if rawdata.item is not None:\n            self.item_col = rawdata.item\n            self.__assigned_columns.append(self.item_col)\n        if rawdata.rating is not None:\n            self.rating_col = rawdata.rating\n            self.__assigned_columns.append(self.rating_col)\n        if rawdata.timestamp is not None:\n            self.timestamp_col = rawdata.timestamp\n            self.__assigned_columns.append(self.timestamp_col)\n\n        # re-order columns\n        self._data = self.data[self.__assigned_columns]\n\n    def reset(self):\n\n        \"\"\"\n        Reset cached statistics and assigned columns of the DataRec object.\n\n        This method clears all precomputed dataset statistics (e.g., sorted users, \n        density, Gini indices, shape, ratings per user/item) and empties the list \n        of assigned columns. It is automatically called when the underlying data is changed.\n        \"\"\"\n        self._sorted_users = None\n        self._sorted_items = None\n        self._transactions = None\n        self._space_size = None\n        self._space_size_log = None\n        self._shape = None\n        self._shape_log = None\n        self._density = None\n        self._density_log = None\n        self._gini_item = None\n        self._gini_user = None\n        self._ratings_per_user = None\n        self._ratings_per_item = None\n\n        self.__assigned_columns = []\n\n    @property\n    def data(self):\n        \"\"\"\n        The underlying pandas DataFrame holding the interaction data.\n        \"\"\"\n        return self._data\n\n    @data.setter\n    def data(self, value: RawData):\n        \"\"\"\n        Sets the internal DataFrame from a RawData object and resets stats.\n        \"\"\"\n        if (value is not None and\n                not isinstance(value, RawData)):\n            raise ValueError(f'Data must be RawData or None if empty. Found {type(value)}')\n        value = value if value is not None else pd.DataFrame()\n\n        self._data = value.data\n        self.reset()\n        self.set_columns(value)\n\n    @property\n    def user_col(self):\n        \"\"\"\n        The name of the user ID column.\n        \"\"\"\n        return self._user_col\n\n    @user_col.setter\n    def user_col(self, value: Union[str, int]):\n        \"\"\"\n        Sets and renames the user column to the internal standard name.\n        \"\"\"\n        self.set_user_col(value, rename=True)\n\n    def set_user_col(self, value: Union[str, int] = DATAREC_USER_COL, rename=True):\n        \"\"\"\n        Identifies and optionally renames the user column.\n\n        Args:\n            value (Union[str, int]): The current name or index of the user column.\n            rename (bool): If True, renames the column to the standard internal name.\n        \"\"\"\n        self.data.columns, self._user_col = set_column_name(columns=list(self.data.columns),\n                                                            value=value,\n                                                            default_name=DATAREC_USER_COL,\n                                                            rename=rename)\n\n    @property\n    def item_col(self):\n        \"\"\"\n        The name of the item ID column.\n        \"\"\"\n        return self._item_col\n\n    @item_col.setter\n    def item_col(self, value: Union[str, int]):\n        \"\"\"\n        Sets and renames the item column to the internal standard name.\n        \"\"\"\n        self.set_item_col(value, rename=True)\n\n    def set_item_col(self, value: Union[str, int] = DATAREC_ITEM_COL, rename=True):\n        \"\"\"\n        Identifies and optionally renames the item column.\n\n        Args:\n            value (Union[str, int]): The current name or index of the item column.\n            rename (bool): If True, renames the column to the standard internal name.\n        \"\"\"\n        self.data.columns, self._item_col = set_column_name(columns=list(self.data.columns),\n                                                            value=value,\n                                                            default_name=DATAREC_ITEM_COL,\n                                                            rename=rename)\n\n    @property\n    def rating_col(self):\n        \"\"\"\n        The name of the rating column.\n        \"\"\"\n        return self._rating_col\n\n    @rating_col.setter\n    def rating_col(self, value: Union[str, int]):\n        \"\"\"\n        Sets and renames the rating column to the internal standard name.\n        \"\"\"\n        self.set_rating_col(value, rename=True)\n\n    def set_rating_col(self, value: Union[str, int] = DATAREC_RATING_COL, rename=True):\n        \"\"\"\n        Identifies and optionally renames the rating column.\n\n        Args:\n            value (Union[str, int]): The current name or index of the rating column.\n            rename (bool): If True, renames the column to the standard internal name.\n        \"\"\"\n        self.data.columns, self._rating_col = set_column_name(columns=list(self.data.columns),\n                                                              value=value,\n                                                              default_name=DATAREC_RATING_COL,\n                                                              rename=rename)\n\n    @property\n    def timestamp_col(self):\n        \"\"\"\n        The name of the timestamp column.\n        \"\"\"\n        return self._timestamp_col\n\n    @timestamp_col.setter\n    def timestamp_col(self, value: Union[str, int]):\n        \"\"\"\n        Sets and renames the timestamp column to the internal standard name.\n        \"\"\"\n        self.set_timestamp_col(value, rename=True)\n\n    def set_timestamp_col(self, value: Union[str, int] = DATAREC_TIMESTAMP_COL, rename=True):\n        \"\"\"\n        Identifies and optionally renames the timestamp column.\n\n        Args:\n            value (Union[str, int]): The current name or index of the timestamp column.\n            rename (bool): If True, renames the column to the standard internal name.\n        \"\"\"\n        self.data.columns, self._timestamp_col = set_column_name(columns=list(self.data.columns),\n                                                                 value=value,\n                                                                 default_name=DATAREC_TIMESTAMP_COL,\n                                                                 rename=rename)\n\n    @property\n    def users(self):\n        \"\"\"\n        Returns a list of unique user IDs in the dataset.\n        \"\"\"\n        return self.data[self.user_col].unique().tolist()\n\n    @property\n    def items(self):\n        \"\"\"\n        Returns a list of unique item IDs in the dataset.\n        \"\"\"\n        return self.data[self.item_col].unique().tolist()\n\n    @property\n    def n_users(self):\n        \"\"\"\n        Returns the number of unique users.\n        \"\"\"\n        return int(self.data[self.user_col].nunique())\n\n    @property\n    def n_items(self):\n        \"\"\"\n        Returns the number of unique items.\n        \"\"\"\n        return int(self.data[self.item_col].nunique())\n\n    @property\n    def columns(self):\n        \"\"\"\n        Returns the list of column names of the internal DataFrame.\n        \"\"\"\n        return self.data.columns\n\n    @columns.setter\n    def columns(self, columns):\n        \"\"\"\n        Sets the column names of the internal DataFrame.\n        \"\"\"\n        self.data.columns = columns\n\n    @property\n    def sorted_items(self):\n        \"\"\"\n        Returns a dictionary of items sorted by their interaction count.\n        \"\"\"\n        if self._sorted_items is None:\n            count_items = self.data.groupby(self.item_col).count().sort_values(by=[self.user_col])\n            self._sorted_items = dict(zip(count_items.index, count_items[self.user_col]))\n        return self._sorted_items\n\n    @property\n    def sorted_users(self):\n        \"\"\"\n        Returns a dictionary of users sorted by their interaction count.\n        \"\"\"\n        if self._sorted_users is None:\n            count_users = self.data.groupby(self.user_col).count().sort_values(by=[self.item_col])\n            self._sorted_users = dict(zip(count_users.index, count_users[self.item_col]))\n        return self._sorted_users\n\n    # --- MAPPING FUNCTIONS ---\n    @property\n    def transactions(self):\n        \"\"\"\n        Returns the total number of interactions (rows) in the dataset.\n        \"\"\"\n        if self._transactions is None:\n            self._transactions = len(self.data)\n        return self._transactions\n\n    @staticmethod\n    def public_to_private(lst, offset=0):\n        \"\"\"\n        Creates a mapping from public (original) IDs to private (integer) IDs.\n\n        Args:\n            lst (list): A list of public IDs.\n            offset (int): The starting integer for the private IDs.\n\n        Returns:\n            (dict): A dictionary mapping public IDs to private integer IDs.\n        \"\"\"\n        return dict(zip(lst, range(offset, offset + len(lst))))\n\n    @staticmethod\n    def private_to_public(pub_to_prvt: dict):\n        \"\"\"\n        Creates a reverse mapping from private IDs back to public IDs.\n\n        Args:\n            pub_to_prvt (dict): A dictionary mapping public IDs to private IDs.\n\n        Returns:\n            (dict): A dictionary mapping private IDs to public IDs.\n        \"\"\"\n        mapping = {el: idx for idx, el in pub_to_prvt.items()}\n        if len(pub_to_prvt) != len(mapping):\n            print('WARNING: private to public mapping could be incorrect. Please, check your code.')\n        return mapping\n\n    def map_users_and_items(self, offset=0, items_shift=False):\n        \"\"\"\n        Generates the public-to-private and private-to-public ID mappings.\n\n        This method creates the dictionaries needed to convert user and item IDs\n        to a dense, zero-indexed integer range suitable for machine learning models.\n\n        Args:\n            offset (int): The starting integer for the ID mappings. Defaults to 0.\n            items_shift (bool): If True, item private IDs will start after the last\n                user private ID, creating a single contiguous ID space. Defaults to False.\n        \"\"\"\n        # map users and items with a 0-indexed mapping\n        users_offset = offset\n        items_offset = offset\n\n        # users\n        self._public_to_private_users = self.public_to_private(self.users, offset=users_offset)\n        self._private_to_public_users = self.private_to_public(self._public_to_private_users)\n\n        # items\n        if items_shift:\n            items_offset = offset + self.n_users\n        self._public_to_private_items = self.public_to_private(self.items, offset=items_offset)\n        self._private_to_public_items = self.private_to_public(self._public_to_private_items)\n\n    def map_dataset(self, user_mapping, item_mapping):\n        \"\"\"\n        Applies ID mappings to the user and item columns of the DataFrame.\n\n        This is an in-place operation that modifies the internal DataFrame.\n\n        Args:\n            user_mapping (dict): The dictionary to map user IDs.\n            item_mapping (dict): The dictionary to map item IDs.\n        \"\"\"\n        self.data[self.user_col] = self.data[self.user_col].map(user_mapping)\n        self.data[self.item_col] = self.data[self.item_col].map(item_mapping)\n\n    def to_public(self):\n        \"\"\"\n        Converts user and item IDs back to their original (public) values.\n        \"\"\"\n        if self._is_private:\n            self.map_dataset(self._private_to_public_users, self._private_to_public_items)\n        self._is_private = False\n\n    def to_private(self):\n        \"\"\"\n        Converts user and item IDs to their dense, zero-indexed (private) integer values.\n        \"\"\"\n        if not self._is_private:\n            self.map_dataset(self._public_to_private_users, self._public_to_private_items)\n        self._is_private = True\n\n    # -- METRICS --\n    def get_metric(self, metric):\n        \"\"\"\n        Retrieves a calculated dataset metric by name.\n\n        Args:\n            metric (str): The name of the metric to compute (e.g., 'density', 'gini_user').\n\n        Returns:\n            The value of the computed metric.\n        \"\"\"\n        assert metric in self.metrics, f'{self.__class__.__name__}: metric \\'{metric}\\' not found.'\n        func = getattr(self, metric)\n        return func()\n\n    @property\n    def space_size(self):\n        \"\"\"\n        Calculates the scaled square root of the user-item interaction space.\n        \"\"\"\n        if self._space_size is None:\n            scale_factor = 1000\n            self._space_size = math.sqrt(self.n_users * self.n_items) / scale_factor\n        return self._space_size\n\n    @property\n    def space_size_log(self):\n        \"\"\"\n        Calculates the log10 of the space_size metric.\n        \"\"\"\n        if self._space_size_log is None:\n            self._space_size_log = math.log10(self.space_size)\n        return self._space_size_log\n\n    @property\n    def shape(self):\n        \"\"\"\n        Calculates the shape of the interaction matrix (n_users / n_items).\n        \"\"\"\n        if self._shape is None:\n            self._shape = self.n_users / self.n_items\n        return self._shape\n\n    @property\n    def shape_log(self):\n        \"\"\"\n        Calculates the log10 of the shape metric.\n        \"\"\"\n        if self._shape_log is None:\n            self._shape_log = math.log10(self.shape)\n        return self._shape_log\n\n    @property\n    def density(self):\n        \"\"\"\n        Calculates the density of the user-item interaction matrix.\n        \"\"\"\n        if self._density is None:\n            self._density = self.transactions / (self.n_users * self.n_items)\n        return self._density\n\n    @property\n    def density_log(self):\n        \"\"\"\n        Calculates the log10 of the density metric.\n        \"\"\"\n        if self._density_log is None:\n            self._density_log = math.log10(self.density)\n        return self._density_log\n\n    @staticmethod\n    def gini(x):\n        \"\"\"\n        Calculates the Gini coefficient for a numpy array.\n\n        Args:\n            x (np.ndarray): An array of non-negative values.\n\n        Returns:\n            (float): The Gini coefficient, a measure of inequality.\n        \"\"\"\n        x = np.sort(x)  # O(n log n)\n        n = len(x)\n        cum_index = np.arange(1, n + 1)\n        return (np.sum((2 * cum_index - n - 1) * x)) / (n * np.sum(x))\n\n\n    @property\n    def gini_item(self):\n        \"\"\"\n        Calculates the Gini coefficient for item popularity.\n        \"\"\"\n        if self._gini_item is None:\n            self._gini_item = self.gini(np.array(list(self.sorted_items.values())))\n        return self._gini_item\n\n    @property\n    def gini_user(self):\n        \"\"\"\n        Calculates the Gini coefficient for user activity.\n        \"\"\"\n        if self._gini_user is None:\n            self._gini_user = self.gini(np.array(list(self.sorted_users.values())))\n        return self._gini_user\n\n    @property\n    def ratings_per_user(self):\n        \"\"\"\n        Calculates the average number of ratings per user.\n        \"\"\"\n        if self._ratings_per_user is None:\n            self._ratings_per_user = self.transactions / self.n_users\n        return self._ratings_per_user\n\n    @property\n    def ratings_per_item(self):\n        \"\"\"\n        Calculates the average number of ratings per item.\n        \"\"\"\n        if self._ratings_per_item is None:\n            self._ratings_per_item = self.transactions / self.n_items\n        return self._ratings_per_item\n\n    def users_frequency(self):\n        \"\"\"\n        Computes the absolute frequency of each user in the dataset.\n\n        Returns:\n            (dict): A dictionary mapping user IDs to the number of interactions, \n                sorted in descending order of frequency.\n        \"\"\"\n        fr = dict(Counter(self.data[self.user_col]))\n        return dict(sorted(fr.items(), key=lambda item: item[1], reverse=True))\n\n    def users_relative_frequency(self):\n        \"\"\"\n        Computes the relative frequency of each user in the dataset.\n\n        Returns:\n            (dict): A dictionary mapping user IDs to their relative frequency \n                (fraction of total transactions).\n        \"\"\"\n        return {u: (f / self.transactions) for u, f in self.users_frequency().items()}\n\n    def items_frequency(self):\n        \"\"\"\n        Computes the absolute frequency of each item in the dataset.\n\n        Returns:\n            (dict): A dictionary mapping item IDs to the number of interactions, \n                sorted in descending order of frequency.\n        \"\"\"\n        fr = dict(Counter(self.data[self.item_col]))\n        return dict(sorted(fr.items(), key=lambda item: item[1], reverse=True))\n\n    def items_relative_frequency(self):\n        \"\"\"\n        Computes the relative frequency of each item in the dataset.\n\n        Returns:\n            (dict): A dictionary mapping item IDs to their relative frequency \n                (fraction of total transactions).\n        \"\"\"\n        return {u: (f / self.transactions) for u, f in self.items_frequency().items()}\n\n    def users_quartiles(self):\n        \"\"\"\n        Assigns quartile indices to users based on their frequency.\n\n        Returns:\n            (dict): A dictionary mapping each user ID to a quartile index (0-3),\n                where 0 = lowest, 3 = highest frequency.\n        \"\"\" \n        return quartiles(self.users_frequency())\n\n    def items_quartiles(self):\n        \"\"\"\n        Assigns quartile indices to items based on their frequency.\n\n        Returns:\n            (dict): A dictionary mapping each item ID to a quartile index (0-3),\n                where 0 = lowest, 3 = highest frequency.\n        \"\"\"\n        return quartiles(self.items_frequency())\n\n    def users_popularity(self):\n        \"\"\"\n        Categorizes users into descriptive popularity groups based on quartiles.\n\n        Returns:\n            (dict): A dictionary mapping popularity categories ('long tail', \n                'common', 'popular', 'most popular') to lists of user IDs.\n        \"\"\"\n        return popularity(self.users_quartiles())\n\n    def items_popularity(self):\n        \"\"\"\n        Categorizes items into descriptive popularity groups based on quartiles.\n\n        Returns:\n            (dict): A dictionary mapping popularity categories ('long tail', \n                'common', 'popular', 'most popular') to lists of item IDs.\n        \"\"\"\n        return popularity(self.items_quartiles())\n\n    def copy(self):\n        \"\"\"\n        Create a deep copy of the current DataRec object.\n\n        This method duplicates the DataRec instance, including its data,\n        metadata (user, item, rating, timestamp columns), pipeline, and internal\n        state such as privacy settings and implicit flags.\n\n        Returns:\n            (DataRec): A new DataRec object that is a deep copy of the current instance.\n        \"\"\"\n        pipeline = self.pipeline.copy()\n\n        new_dr = DataRec(rawdata=self.to_rawdata(),\n                         pipeline=pipeline,\n                         copy=True)\n\n        new_dr.__implicit = self.__implicit\n        new_dr._user_col = self.user_col\n        new_dr._item_col = self.item_col\n        new_dr._rating_col = self.rating_col\n        new_dr._timestamp_col = self.timestamp_col\n        new_dr._is_private = self._is_private\n        return new_dr\n\n    def to_rawdata(self):\n        \"\"\"\n        Convert the current DataRec object into a RawData object.\n\n        This method creates a RawData instance containing the same data and\n        metadata (user, item, rating, timestamp columns) as the DataRec object.\n\n        Returns:\n            (RawData): A new RawData object containing the DataRec's data and column information.\n        \"\"\"\n        raw = RawData(self.data)\n        raw.user = self.user_col\n        raw.item = self.item_col\n        raw.rating = self.rating_col\n        raw.timestamp = self.timestamp_col\n        return raw\n\n    def save_pipeline(self, filepath: str) -&gt; None:\n        \"\"\"\n        Save the current processing pipeline to a YAML file.\n\n        Args:\n            filepath (str): The path (including filename) where the pipeline \n                YAML file will be saved.\n        \"\"\"\n        print(f'Saving pipeline to {filepath}')\n\n        self.pipeline.to_yaml(filepath)\n\n        print(f'Pipeline correctly saved to {filepath}')\n\n    def to_torch_dataset(self, task=\"pointwise\", autoprepare=True, **kwargs):\n        \"\"\"\n        Converts the current DataRec object into a PyTorch-compatible dataset.\n\n        This method prepares the dataset (e.g., remaps user/item IDs to a dense index space)\n        and returns a `torch.utils.data.Dataset` object suitable for training with PyTorch.\n\n        Args:\n            task (str): The recommendation task type. Must be one of:\n                - \"pointwise\": returns PointwiseTorchDataset\n                - \"pairwise\": returns PairwiseTorchDataset\n                - \"ranking\": returns RankingTorchDataset\n            autoprepare (bool): If True, automatically applies user/item remapping\n                and switches the dataset to private IDs. If False, assumes the dataset\n                is already properly prepared.\n            **kwargs: Additional arguments passed to the specific torch dataset class.\n\n        Returns:\n            (torch.utils.data.Dataset): A PyTorch dataset instance corresponding to the selected task.\n\n        Raises:\n            ImportError: If PyTorch is not installed.\n            ValueError: If an unknown task name is provided.\n        \"\"\"\n\n        try:\n            import torch\n        except ImportError:\n            raise ImportError(\n                \"PyTorch is required to use the to_torch_dataset() method. \"\n                \"Please install it with `pip install torch`.\"\n            )\n\n        # Preparazione automatica del dataset\n        if autoprepare:\n            self.map_users_and_items()\n            self.to_private()\n        else:\n            warnings.warn(\n                \"Autoprepare is set to False. \"\n                \"Ensure that the dataset is prepared correctly before using it with PyTorch.\"\n            )\n\n        # Selezione del dataset PyTorch\n        if task == \"pointwise\":\n            from datarec.data.torch_dataset import PointwiseTorchDataset\n            return PointwiseTorchDataset(self, **kwargs)\n        elif task == \"pairwise\":\n            from datarec.data.torch_dataset import PairwiseTorchDataset\n            return PairwiseTorchDataset(self, **kwargs)\n        elif task == \"ranking\":\n            from datarec.data.torch_dataset import RankingTorchDataset\n            return RankingTorchDataset(self, **kwargs)\n        else:\n            raise ValueError(f\"Unknown task: {task}\")\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.data","title":"<code>data</code>  <code>property</code> <code>writable</code>","text":"<p>The underlying pandas DataFrame holding the interaction data.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.user_col","title":"<code>user_col</code>  <code>property</code> <code>writable</code>","text":"<p>The name of the user ID column.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.item_col","title":"<code>item_col</code>  <code>property</code> <code>writable</code>","text":"<p>The name of the item ID column.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.rating_col","title":"<code>rating_col</code>  <code>property</code> <code>writable</code>","text":"<p>The name of the rating column.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.timestamp_col","title":"<code>timestamp_col</code>  <code>property</code> <code>writable</code>","text":"<p>The name of the timestamp column.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.users","title":"<code>users</code>  <code>property</code>","text":"<p>Returns a list of unique user IDs in the dataset.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.items","title":"<code>items</code>  <code>property</code>","text":"<p>Returns a list of unique item IDs in the dataset.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.n_users","title":"<code>n_users</code>  <code>property</code>","text":"<p>Returns the number of unique users.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.n_items","title":"<code>n_items</code>  <code>property</code>","text":"<p>Returns the number of unique items.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.columns","title":"<code>columns</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the list of column names of the internal DataFrame.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.sorted_items","title":"<code>sorted_items</code>  <code>property</code>","text":"<p>Returns a dictionary of items sorted by their interaction count.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.sorted_users","title":"<code>sorted_users</code>  <code>property</code>","text":"<p>Returns a dictionary of users sorted by their interaction count.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.transactions","title":"<code>transactions</code>  <code>property</code>","text":"<p>Returns the total number of interactions (rows) in the dataset.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.space_size","title":"<code>space_size</code>  <code>property</code>","text":"<p>Calculates the scaled square root of the user-item interaction space.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.space_size_log","title":"<code>space_size_log</code>  <code>property</code>","text":"<p>Calculates the log10 of the space_size metric.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>Calculates the shape of the interaction matrix (n_users / n_items).</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.shape_log","title":"<code>shape_log</code>  <code>property</code>","text":"<p>Calculates the log10 of the shape metric.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.density","title":"<code>density</code>  <code>property</code>","text":"<p>Calculates the density of the user-item interaction matrix.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.density_log","title":"<code>density_log</code>  <code>property</code>","text":"<p>Calculates the log10 of the density metric.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.gini_item","title":"<code>gini_item</code>  <code>property</code>","text":"<p>Calculates the Gini coefficient for item popularity.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.gini_user","title":"<code>gini_user</code>  <code>property</code>","text":"<p>Calculates the Gini coefficient for user activity.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.ratings_per_user","title":"<code>ratings_per_user</code>  <code>property</code>","text":"<p>Calculates the average number of ratings per user.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.ratings_per_item","title":"<code>ratings_per_item</code>  <code>property</code>","text":"<p>Calculates the average number of ratings per item.</p>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.__init__","title":"<code>__init__(rawdata=None, copy=False, dataset_name='datarec', version_name='no_version_provided', pipeline=None, *args, **kwargs)</code>","text":"<p>Initializes the DataRec object.</p> <p>Parameters:</p> Name Type Description Default <code>rawdata</code> <code>RawData</code> <p>The input dataset wrapped in a RawData object. If None, the DataRec is initialized empty.</p> <code>None</code> <code>copy</code> <code>bool</code> <p>Whether to copy the input DataFrame to avoid  modifying the original RawData.</p> <code>False</code> <code>dataset_name</code> <code>str</code> <p>A name to identify the dataset.</p> <code>'datarec'</code> <code>version_name</code> <code>str</code> <p>A version identifier  for the dataset.</p> <code>'no_version_provided'</code> <code>pipeline</code> <code>Pipeline</code> <p>A pipeline object to track preprocessing steps.</p> <code>None</code> Source code in <code>datarec/data/dataset.py</code> <pre><code>def __init__(\n        self,\n        rawdata: RawData = None,\n        copy: bool = False,\n        dataset_name: str = 'datarec',\n        version_name: str = 'no_version_provided',\n        pipeline: Optional[Pipeline] = None,\n        *args,\n        **kwargs\n):\n    \"\"\"\n    Initializes the DataRec object.\n\n    Args:\n        rawdata (RawData): The input dataset wrapped in a RawData object.\n            If None, the DataRec is initialized empty.\n        copy (bool): Whether to copy the input DataFrame to avoid \n            modifying the original RawData.\n        dataset_name (str): A name to identify the dataset.\n        version_name (str): A version identifier \n            for the dataset.\n        pipeline (Pipeline): A pipeline object to track preprocessing steps.\n\n    \"\"\"\n\n\n\n    self.path = None\n    self._data = None\n    self.dataset_name = dataset_name\n    self.version_name = version_name\n\n    if pipeline:\n        self.pipeline = pipeline\n    else:\n        self.pipeline = Pipeline()\n        self.pipeline.add_step(\"load\", self.dataset_name, {'version': self.version_name})\n\n    if rawdata is not None:\n        if copy:\n            self._data: pd.DataFrame = rawdata.data.copy()\n        else:\n            self._data: pd.DataFrame = rawdata.data\n\n    # ------------------------------------\n    # --------- STANDARD COLUMNS ---------\n    # if a column is None it means that the DataRec does not have that information\n    self.__assigned_columns = []\n\n    self._user_col = None\n    self._item_col = None\n    self._rating_col = None\n    self._timestamp_col = None\n\n    if rawdata:\n        self.set_columns(rawdata)\n\n    # dataset is assumed to be the public version of the dataset\n    self._is_private = False\n    self.__implicit = False\n\n    # ------------------------------\n    # --------- PROPERTIES ---------\n    self._sorted_users = None\n    self._sorted_items = None\n\n    # map users and items with a 0-indexed mapping\n    self._public_to_private_users = None\n    self._public_to_private_items = None\n    self._private_to_public_users = None\n    self._private_to_public_items = None\n\n    # metrics\n    self._transactions = None\n    self._space_size = None\n    self._space_size_log = None\n    self._shape = None\n    self._shape_log = None\n    self._density = None\n    self._density_log = None\n    self._gini_item = None\n    self._gini_user = None\n    self._ratings_per_user = None\n    self._ratings_per_item = None\n\n    # more analyses\n    self.metrics = ['transactions', 'space_size', 'space_size_log', 'shape', 'shape_log', 'density', 'density_log',\n                    'gini_item', 'gini_user', 'ratings_per_user', 'ratings_per_item']\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.__str__","title":"<code>__str__()</code>","text":"<p>Returns 'self.data' as a string variable.</p> <p>Returns:</p> Type Description <code>str</code> <p>'self.data' as a string variable.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def __str__(self):\n    \"\"\"\n    Returns 'self.data' as a string variable.\n\n    Returns:\n        (str): 'self.data' as a string variable.\n    \"\"\"\n    return self.data.__str__()\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns the official string representation of the internal DataFrame.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def __repr__(self):\n    \"\"\"\n    Returns the official string representation of the internal DataFrame.\n    \"\"\"\n    return self.data.__repr__()\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.__len__","title":"<code>__len__()</code>","text":"<p>Returns the total number of samples in the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>number of samples in the dataset.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Returns the total number of samples in the dataset.\n\n    Returns:\n        (int): number of samples in the dataset.\n    \"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.set_columns","title":"<code>set_columns(rawdata)</code>","text":"<p>Assign dataset column names from a RawData object and reorder the data accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>rawdata</code> <code>RawData</code> <p>A RawData object containing the column names for  user, item, rating, and timestamp.</p> required Source code in <code>datarec/data/dataset.py</code> <pre><code>def set_columns(self, rawdata):\n    \"\"\"\n    Assign dataset column names from a RawData object and reorder the data accordingly.\n\n    Args:\n        rawdata (RawData): A RawData object containing the column names for \n            user, item, rating, and timestamp.\n    \"\"\"\n    if rawdata.user is not None:\n        self.user_col = rawdata.user\n        self.__assigned_columns.append(self.user_col)\n    if rawdata.item is not None:\n        self.item_col = rawdata.item\n        self.__assigned_columns.append(self.item_col)\n    if rawdata.rating is not None:\n        self.rating_col = rawdata.rating\n        self.__assigned_columns.append(self.rating_col)\n    if rawdata.timestamp is not None:\n        self.timestamp_col = rawdata.timestamp\n        self.__assigned_columns.append(self.timestamp_col)\n\n    # re-order columns\n    self._data = self.data[self.__assigned_columns]\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.reset","title":"<code>reset()</code>","text":"<p>Reset cached statistics and assigned columns of the DataRec object.</p> <p>This method clears all precomputed dataset statistics (e.g., sorted users,  density, Gini indices, shape, ratings per user/item) and empties the list  of assigned columns. It is automatically called when the underlying data is changed.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def reset(self):\n\n    \"\"\"\n    Reset cached statistics and assigned columns of the DataRec object.\n\n    This method clears all precomputed dataset statistics (e.g., sorted users, \n    density, Gini indices, shape, ratings per user/item) and empties the list \n    of assigned columns. It is automatically called when the underlying data is changed.\n    \"\"\"\n    self._sorted_users = None\n    self._sorted_items = None\n    self._transactions = None\n    self._space_size = None\n    self._space_size_log = None\n    self._shape = None\n    self._shape_log = None\n    self._density = None\n    self._density_log = None\n    self._gini_item = None\n    self._gini_user = None\n    self._ratings_per_user = None\n    self._ratings_per_item = None\n\n    self.__assigned_columns = []\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.set_user_col","title":"<code>set_user_col(value=DATAREC_USER_COL, rename=True)</code>","text":"<p>Identifies and optionally renames the user column.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, int]</code> <p>The current name or index of the user column.</p> <code>DATAREC_USER_COL</code> <code>rename</code> <code>bool</code> <p>If True, renames the column to the standard internal name.</p> <code>True</code> Source code in <code>datarec/data/dataset.py</code> <pre><code>def set_user_col(self, value: Union[str, int] = DATAREC_USER_COL, rename=True):\n    \"\"\"\n    Identifies and optionally renames the user column.\n\n    Args:\n        value (Union[str, int]): The current name or index of the user column.\n        rename (bool): If True, renames the column to the standard internal name.\n    \"\"\"\n    self.data.columns, self._user_col = set_column_name(columns=list(self.data.columns),\n                                                        value=value,\n                                                        default_name=DATAREC_USER_COL,\n                                                        rename=rename)\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.set_item_col","title":"<code>set_item_col(value=DATAREC_ITEM_COL, rename=True)</code>","text":"<p>Identifies and optionally renames the item column.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, int]</code> <p>The current name or index of the item column.</p> <code>DATAREC_ITEM_COL</code> <code>rename</code> <code>bool</code> <p>If True, renames the column to the standard internal name.</p> <code>True</code> Source code in <code>datarec/data/dataset.py</code> <pre><code>def set_item_col(self, value: Union[str, int] = DATAREC_ITEM_COL, rename=True):\n    \"\"\"\n    Identifies and optionally renames the item column.\n\n    Args:\n        value (Union[str, int]): The current name or index of the item column.\n        rename (bool): If True, renames the column to the standard internal name.\n    \"\"\"\n    self.data.columns, self._item_col = set_column_name(columns=list(self.data.columns),\n                                                        value=value,\n                                                        default_name=DATAREC_ITEM_COL,\n                                                        rename=rename)\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.set_rating_col","title":"<code>set_rating_col(value=DATAREC_RATING_COL, rename=True)</code>","text":"<p>Identifies and optionally renames the rating column.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, int]</code> <p>The current name or index of the rating column.</p> <code>DATAREC_RATING_COL</code> <code>rename</code> <code>bool</code> <p>If True, renames the column to the standard internal name.</p> <code>True</code> Source code in <code>datarec/data/dataset.py</code> <pre><code>def set_rating_col(self, value: Union[str, int] = DATAREC_RATING_COL, rename=True):\n    \"\"\"\n    Identifies and optionally renames the rating column.\n\n    Args:\n        value (Union[str, int]): The current name or index of the rating column.\n        rename (bool): If True, renames the column to the standard internal name.\n    \"\"\"\n    self.data.columns, self._rating_col = set_column_name(columns=list(self.data.columns),\n                                                          value=value,\n                                                          default_name=DATAREC_RATING_COL,\n                                                          rename=rename)\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.set_timestamp_col","title":"<code>set_timestamp_col(value=DATAREC_TIMESTAMP_COL, rename=True)</code>","text":"<p>Identifies and optionally renames the timestamp column.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, int]</code> <p>The current name or index of the timestamp column.</p> <code>DATAREC_TIMESTAMP_COL</code> <code>rename</code> <code>bool</code> <p>If True, renames the column to the standard internal name.</p> <code>True</code> Source code in <code>datarec/data/dataset.py</code> <pre><code>def set_timestamp_col(self, value: Union[str, int] = DATAREC_TIMESTAMP_COL, rename=True):\n    \"\"\"\n    Identifies and optionally renames the timestamp column.\n\n    Args:\n        value (Union[str, int]): The current name or index of the timestamp column.\n        rename (bool): If True, renames the column to the standard internal name.\n    \"\"\"\n    self.data.columns, self._timestamp_col = set_column_name(columns=list(self.data.columns),\n                                                             value=value,\n                                                             default_name=DATAREC_TIMESTAMP_COL,\n                                                             rename=rename)\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.public_to_private","title":"<code>public_to_private(lst, offset=0)</code>  <code>staticmethod</code>","text":"<p>Creates a mapping from public (original) IDs to private (integer) IDs.</p> <p>Parameters:</p> Name Type Description Default <code>lst</code> <code>list</code> <p>A list of public IDs.</p> required <code>offset</code> <code>int</code> <p>The starting integer for the private IDs.</p> <code>0</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping public IDs to private integer IDs.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>@staticmethod\ndef public_to_private(lst, offset=0):\n    \"\"\"\n    Creates a mapping from public (original) IDs to private (integer) IDs.\n\n    Args:\n        lst (list): A list of public IDs.\n        offset (int): The starting integer for the private IDs.\n\n    Returns:\n        (dict): A dictionary mapping public IDs to private integer IDs.\n    \"\"\"\n    return dict(zip(lst, range(offset, offset + len(lst))))\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.private_to_public","title":"<code>private_to_public(pub_to_prvt)</code>  <code>staticmethod</code>","text":"<p>Creates a reverse mapping from private IDs back to public IDs.</p> <p>Parameters:</p> Name Type Description Default <code>pub_to_prvt</code> <code>dict</code> <p>A dictionary mapping public IDs to private IDs.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping private IDs to public IDs.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>@staticmethod\ndef private_to_public(pub_to_prvt: dict):\n    \"\"\"\n    Creates a reverse mapping from private IDs back to public IDs.\n\n    Args:\n        pub_to_prvt (dict): A dictionary mapping public IDs to private IDs.\n\n    Returns:\n        (dict): A dictionary mapping private IDs to public IDs.\n    \"\"\"\n    mapping = {el: idx for idx, el in pub_to_prvt.items()}\n    if len(pub_to_prvt) != len(mapping):\n        print('WARNING: private to public mapping could be incorrect. Please, check your code.')\n    return mapping\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.map_users_and_items","title":"<code>map_users_and_items(offset=0, items_shift=False)</code>","text":"<p>Generates the public-to-private and private-to-public ID mappings.</p> <p>This method creates the dictionaries needed to convert user and item IDs to a dense, zero-indexed integer range suitable for machine learning models.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>The starting integer for the ID mappings. Defaults to 0.</p> <code>0</code> <code>items_shift</code> <code>bool</code> <p>If True, item private IDs will start after the last user private ID, creating a single contiguous ID space. Defaults to False.</p> <code>False</code> Source code in <code>datarec/data/dataset.py</code> <pre><code>def map_users_and_items(self, offset=0, items_shift=False):\n    \"\"\"\n    Generates the public-to-private and private-to-public ID mappings.\n\n    This method creates the dictionaries needed to convert user and item IDs\n    to a dense, zero-indexed integer range suitable for machine learning models.\n\n    Args:\n        offset (int): The starting integer for the ID mappings. Defaults to 0.\n        items_shift (bool): If True, item private IDs will start after the last\n            user private ID, creating a single contiguous ID space. Defaults to False.\n    \"\"\"\n    # map users and items with a 0-indexed mapping\n    users_offset = offset\n    items_offset = offset\n\n    # users\n    self._public_to_private_users = self.public_to_private(self.users, offset=users_offset)\n    self._private_to_public_users = self.private_to_public(self._public_to_private_users)\n\n    # items\n    if items_shift:\n        items_offset = offset + self.n_users\n    self._public_to_private_items = self.public_to_private(self.items, offset=items_offset)\n    self._private_to_public_items = self.private_to_public(self._public_to_private_items)\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.map_dataset","title":"<code>map_dataset(user_mapping, item_mapping)</code>","text":"<p>Applies ID mappings to the user and item columns of the DataFrame.</p> <p>This is an in-place operation that modifies the internal DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>user_mapping</code> <code>dict</code> <p>The dictionary to map user IDs.</p> required <code>item_mapping</code> <code>dict</code> <p>The dictionary to map item IDs.</p> required Source code in <code>datarec/data/dataset.py</code> <pre><code>def map_dataset(self, user_mapping, item_mapping):\n    \"\"\"\n    Applies ID mappings to the user and item columns of the DataFrame.\n\n    This is an in-place operation that modifies the internal DataFrame.\n\n    Args:\n        user_mapping (dict): The dictionary to map user IDs.\n        item_mapping (dict): The dictionary to map item IDs.\n    \"\"\"\n    self.data[self.user_col] = self.data[self.user_col].map(user_mapping)\n    self.data[self.item_col] = self.data[self.item_col].map(item_mapping)\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.to_public","title":"<code>to_public()</code>","text":"<p>Converts user and item IDs back to their original (public) values.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def to_public(self):\n    \"\"\"\n    Converts user and item IDs back to their original (public) values.\n    \"\"\"\n    if self._is_private:\n        self.map_dataset(self._private_to_public_users, self._private_to_public_items)\n    self._is_private = False\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.to_private","title":"<code>to_private()</code>","text":"<p>Converts user and item IDs to their dense, zero-indexed (private) integer values.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def to_private(self):\n    \"\"\"\n    Converts user and item IDs to their dense, zero-indexed (private) integer values.\n    \"\"\"\n    if not self._is_private:\n        self.map_dataset(self._public_to_private_users, self._public_to_private_items)\n    self._is_private = True\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.get_metric","title":"<code>get_metric(metric)</code>","text":"<p>Retrieves a calculated dataset metric by name.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>str</code> <p>The name of the metric to compute (e.g., 'density', 'gini_user').</p> required <p>Returns:</p> Type Description <p>The value of the computed metric.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def get_metric(self, metric):\n    \"\"\"\n    Retrieves a calculated dataset metric by name.\n\n    Args:\n        metric (str): The name of the metric to compute (e.g., 'density', 'gini_user').\n\n    Returns:\n        The value of the computed metric.\n    \"\"\"\n    assert metric in self.metrics, f'{self.__class__.__name__}: metric \\'{metric}\\' not found.'\n    func = getattr(self, metric)\n    return func()\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.gini","title":"<code>gini(x)</code>  <code>staticmethod</code>","text":"<p>Calculates the Gini coefficient for a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>An array of non-negative values.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The Gini coefficient, a measure of inequality.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>@staticmethod\ndef gini(x):\n    \"\"\"\n    Calculates the Gini coefficient for a numpy array.\n\n    Args:\n        x (np.ndarray): An array of non-negative values.\n\n    Returns:\n        (float): The Gini coefficient, a measure of inequality.\n    \"\"\"\n    x = np.sort(x)  # O(n log n)\n    n = len(x)\n    cum_index = np.arange(1, n + 1)\n    return (np.sum((2 * cum_index - n - 1) * x)) / (n * np.sum(x))\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.users_frequency","title":"<code>users_frequency()</code>","text":"<p>Computes the absolute frequency of each user in the dataset.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping user IDs to the number of interactions,  sorted in descending order of frequency.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def users_frequency(self):\n    \"\"\"\n    Computes the absolute frequency of each user in the dataset.\n\n    Returns:\n        (dict): A dictionary mapping user IDs to the number of interactions, \n            sorted in descending order of frequency.\n    \"\"\"\n    fr = dict(Counter(self.data[self.user_col]))\n    return dict(sorted(fr.items(), key=lambda item: item[1], reverse=True))\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.users_relative_frequency","title":"<code>users_relative_frequency()</code>","text":"<p>Computes the relative frequency of each user in the dataset.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping user IDs to their relative frequency  (fraction of total transactions).</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def users_relative_frequency(self):\n    \"\"\"\n    Computes the relative frequency of each user in the dataset.\n\n    Returns:\n        (dict): A dictionary mapping user IDs to their relative frequency \n            (fraction of total transactions).\n    \"\"\"\n    return {u: (f / self.transactions) for u, f in self.users_frequency().items()}\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.items_frequency","title":"<code>items_frequency()</code>","text":"<p>Computes the absolute frequency of each item in the dataset.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping item IDs to the number of interactions,  sorted in descending order of frequency.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def items_frequency(self):\n    \"\"\"\n    Computes the absolute frequency of each item in the dataset.\n\n    Returns:\n        (dict): A dictionary mapping item IDs to the number of interactions, \n            sorted in descending order of frequency.\n    \"\"\"\n    fr = dict(Counter(self.data[self.item_col]))\n    return dict(sorted(fr.items(), key=lambda item: item[1], reverse=True))\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.items_relative_frequency","title":"<code>items_relative_frequency()</code>","text":"<p>Computes the relative frequency of each item in the dataset.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping item IDs to their relative frequency  (fraction of total transactions).</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def items_relative_frequency(self):\n    \"\"\"\n    Computes the relative frequency of each item in the dataset.\n\n    Returns:\n        (dict): A dictionary mapping item IDs to their relative frequency \n            (fraction of total transactions).\n    \"\"\"\n    return {u: (f / self.transactions) for u, f in self.items_frequency().items()}\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.users_quartiles","title":"<code>users_quartiles()</code>","text":"<p>Assigns quartile indices to users based on their frequency.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping each user ID to a quartile index (0-3), where 0 = lowest, 3 = highest frequency.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def users_quartiles(self):\n    \"\"\"\n    Assigns quartile indices to users based on their frequency.\n\n    Returns:\n        (dict): A dictionary mapping each user ID to a quartile index (0-3),\n            where 0 = lowest, 3 = highest frequency.\n    \"\"\" \n    return quartiles(self.users_frequency())\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.items_quartiles","title":"<code>items_quartiles()</code>","text":"<p>Assigns quartile indices to items based on their frequency.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping each item ID to a quartile index (0-3), where 0 = lowest, 3 = highest frequency.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def items_quartiles(self):\n    \"\"\"\n    Assigns quartile indices to items based on their frequency.\n\n    Returns:\n        (dict): A dictionary mapping each item ID to a quartile index (0-3),\n            where 0 = lowest, 3 = highest frequency.\n    \"\"\"\n    return quartiles(self.items_frequency())\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.users_popularity","title":"<code>users_popularity()</code>","text":"<p>Categorizes users into descriptive popularity groups based on quartiles.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping popularity categories ('long tail',  'common', 'popular', 'most popular') to lists of user IDs.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def users_popularity(self):\n    \"\"\"\n    Categorizes users into descriptive popularity groups based on quartiles.\n\n    Returns:\n        (dict): A dictionary mapping popularity categories ('long tail', \n            'common', 'popular', 'most popular') to lists of user IDs.\n    \"\"\"\n    return popularity(self.users_quartiles())\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.items_popularity","title":"<code>items_popularity()</code>","text":"<p>Categorizes items into descriptive popularity groups based on quartiles.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping popularity categories ('long tail',  'common', 'popular', 'most popular') to lists of item IDs.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def items_popularity(self):\n    \"\"\"\n    Categorizes items into descriptive popularity groups based on quartiles.\n\n    Returns:\n        (dict): A dictionary mapping popularity categories ('long tail', \n            'common', 'popular', 'most popular') to lists of item IDs.\n    \"\"\"\n    return popularity(self.items_quartiles())\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.copy","title":"<code>copy()</code>","text":"<p>Create a deep copy of the current DataRec object.</p> <p>This method duplicates the DataRec instance, including its data, metadata (user, item, rating, timestamp columns), pipeline, and internal state such as privacy settings and implicit flags.</p> <p>Returns:</p> Type Description <code>DataRec</code> <p>A new DataRec object that is a deep copy of the current instance.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def copy(self):\n    \"\"\"\n    Create a deep copy of the current DataRec object.\n\n    This method duplicates the DataRec instance, including its data,\n    metadata (user, item, rating, timestamp columns), pipeline, and internal\n    state such as privacy settings and implicit flags.\n\n    Returns:\n        (DataRec): A new DataRec object that is a deep copy of the current instance.\n    \"\"\"\n    pipeline = self.pipeline.copy()\n\n    new_dr = DataRec(rawdata=self.to_rawdata(),\n                     pipeline=pipeline,\n                     copy=True)\n\n    new_dr.__implicit = self.__implicit\n    new_dr._user_col = self.user_col\n    new_dr._item_col = self.item_col\n    new_dr._rating_col = self.rating_col\n    new_dr._timestamp_col = self.timestamp_col\n    new_dr._is_private = self._is_private\n    return new_dr\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.to_rawdata","title":"<code>to_rawdata()</code>","text":"<p>Convert the current DataRec object into a RawData object.</p> <p>This method creates a RawData instance containing the same data and metadata (user, item, rating, timestamp columns) as the DataRec object.</p> <p>Returns:</p> Type Description <code>RawData</code> <p>A new RawData object containing the DataRec's data and column information.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def to_rawdata(self):\n    \"\"\"\n    Convert the current DataRec object into a RawData object.\n\n    This method creates a RawData instance containing the same data and\n    metadata (user, item, rating, timestamp columns) as the DataRec object.\n\n    Returns:\n        (RawData): A new RawData object containing the DataRec's data and column information.\n    \"\"\"\n    raw = RawData(self.data)\n    raw.user = self.user_col\n    raw.item = self.item_col\n    raw.rating = self.rating_col\n    raw.timestamp = self.timestamp_col\n    return raw\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.save_pipeline","title":"<code>save_pipeline(filepath)</code>","text":"<p>Save the current processing pipeline to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path (including filename) where the pipeline  YAML file will be saved.</p> required Source code in <code>datarec/data/dataset.py</code> <pre><code>def save_pipeline(self, filepath: str) -&gt; None:\n    \"\"\"\n    Save the current processing pipeline to a YAML file.\n\n    Args:\n        filepath (str): The path (including filename) where the pipeline \n            YAML file will be saved.\n    \"\"\"\n    print(f'Saving pipeline to {filepath}')\n\n    self.pipeline.to_yaml(filepath)\n\n    print(f'Pipeline correctly saved to {filepath}')\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.DataRec.to_torch_dataset","title":"<code>to_torch_dataset(task='pointwise', autoprepare=True, **kwargs)</code>","text":"<p>Converts the current DataRec object into a PyTorch-compatible dataset.</p> <p>This method prepares the dataset (e.g., remaps user/item IDs to a dense index space) and returns a <code>torch.utils.data.Dataset</code> object suitable for training with PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>The recommendation task type. Must be one of: - \"pointwise\": returns PointwiseTorchDataset - \"pairwise\": returns PairwiseTorchDataset - \"ranking\": returns RankingTorchDataset</p> <code>'pointwise'</code> <code>autoprepare</code> <code>bool</code> <p>If True, automatically applies user/item remapping and switches the dataset to private IDs. If False, assumes the dataset is already properly prepared.</p> <code>True</code> <code>**kwargs</code> <p>Additional arguments passed to the specific torch dataset class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>A PyTorch dataset instance corresponding to the selected task.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If PyTorch is not installed.</p> <code>ValueError</code> <p>If an unknown task name is provided.</p> Source code in <code>datarec/data/dataset.py</code> <pre><code>def to_torch_dataset(self, task=\"pointwise\", autoprepare=True, **kwargs):\n    \"\"\"\n    Converts the current DataRec object into a PyTorch-compatible dataset.\n\n    This method prepares the dataset (e.g., remaps user/item IDs to a dense index space)\n    and returns a `torch.utils.data.Dataset` object suitable for training with PyTorch.\n\n    Args:\n        task (str): The recommendation task type. Must be one of:\n            - \"pointwise\": returns PointwiseTorchDataset\n            - \"pairwise\": returns PairwiseTorchDataset\n            - \"ranking\": returns RankingTorchDataset\n        autoprepare (bool): If True, automatically applies user/item remapping\n            and switches the dataset to private IDs. If False, assumes the dataset\n            is already properly prepared.\n        **kwargs: Additional arguments passed to the specific torch dataset class.\n\n    Returns:\n        (torch.utils.data.Dataset): A PyTorch dataset instance corresponding to the selected task.\n\n    Raises:\n        ImportError: If PyTorch is not installed.\n        ValueError: If an unknown task name is provided.\n    \"\"\"\n\n    try:\n        import torch\n    except ImportError:\n        raise ImportError(\n            \"PyTorch is required to use the to_torch_dataset() method. \"\n            \"Please install it with `pip install torch`.\"\n        )\n\n    # Preparazione automatica del dataset\n    if autoprepare:\n        self.map_users_and_items()\n        self.to_private()\n    else:\n        warnings.warn(\n            \"Autoprepare is set to False. \"\n            \"Ensure that the dataset is prepared correctly before using it with PyTorch.\"\n        )\n\n    # Selezione del dataset PyTorch\n    if task == \"pointwise\":\n        from datarec.data.torch_dataset import PointwiseTorchDataset\n        return PointwiseTorchDataset(self, **kwargs)\n    elif task == \"pairwise\":\n        from datarec.data.torch_dataset import PairwiseTorchDataset\n        return PairwiseTorchDataset(self, **kwargs)\n    elif task == \"ranking\":\n        from datarec.data.torch_dataset import RankingTorchDataset\n        return RankingTorchDataset(self, **kwargs)\n    else:\n        raise ValueError(f\"Unknown task: {task}\")\n</code></pre>"},{"location":"documentation/data/#datarec.data.dataset.share_info","title":"<code>share_info(datarec_source, datarec_target)</code>","text":"<p>Copy dataset metadata and mappings from one DataRec object to another</p> <p>This function transfers core attributes from a source DataRec to a target DataRec. </p> <p>Parameters:</p> Name Type Description Default <code>datarec_source</code> <code>DataRec</code> <p>The source DataRec from which information  is copied.</p> required <code>datarec_target</code> <code>DataRec</code> <p>The target DataRec that will be updated  with the source information.</p> required Source code in <code>datarec/data/dataset.py</code> <pre><code>def share_info(datarec_source: DataRec, datarec_target: DataRec) -&gt; None:\n    \"\"\"\n    Copy dataset metadata and mappings from one DataRec object to another\n\n    This function transfers core attributes from a source DataRec\n    to a target DataRec. \n\n    Args:\n        datarec_source (DataRec): The source DataRec from which information \n            is copied.\n        datarec_target (DataRec): The target DataRec that will be updated \n            with the source information.\n\n    \"\"\"\n    ds = datarec_source\n    dt = datarec_target\n\n    dt._is_private = ds._is_private\n    dt.__implicit = ds.__implicit\n\n    dt.dataset_name = ds.dataset_name\n    dt.version_name = ds.version_name\n    dt.user_col = ds.user_col\n    dt.item_col = ds.item_col\n    dt.rating_col = ds.rating_col\n    dt.timestamp_col = ds.timestamp_col\n\n    dt._sorted_users = ds._sorted_users\n    dt._sorted_items = ds._sorted_users\n    dt._public_to_private_users = ds._public_to_private_users\n    dt._public_to_private_items = ds._public_to_private_items\n    dt._private_to_public_users = ds._private_to_public_users\n    dt._private_to_public_items = ds._private_to_public_items\n</code></pre>"},{"location":"documentation/datasets/","title":"Datasets Reference","text":"<p>This section provides a detailed API reference for all modules related to built-in datasets in the <code>datarec</code> library.</p>"},{"location":"documentation/datasets/#download-utilities","title":"Download Utilities","text":"<p>Provides utility functions for downloading and decompressing dataset files.</p> <p>This module contains a set of helper functions used internally by the dataset builder classes (e.g., <code>MovieLens1M</code>, <code>Yelp_v1</code>) to handle the fetching of raw data from web sources and the extraction of various archive formats like .zip, .gz, .tar, and .7z.</p> <p>These functions are not typically called directly by the end-user but are fundamental to the automatic data preparation process of the library.</p>"},{"location":"documentation/datasets/#datarec.datasets.download.download_url","title":"<code>download_url(url, local_filepath)</code>","text":"<p>Downloads a file from a URL and saves it to a local path.</p> <p>Note: This is a basic downloader. For large files or more robust handling, <code>download_file</code> is generally preferred within this library.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the file to download.</p> required <code>local_filepath</code> <code>str</code> <p>The local path where the file will be saved.</p> required <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the HTTP request returned an unsuccessful status code.</p> Source code in <code>datarec/datasets/download.py</code> <pre><code>def download_url(url, local_filepath) -&gt; None:\n    \"\"\"\n    Downloads a file from a URL and saves it to a local path.\n\n    Note: This is a basic downloader. For large files or more robust handling,\n    `download_file` is generally preferred within this library.\n\n    Args:\n        url (str): The URL of the file to download.\n        local_filepath (str): The local path where the file will be saved.\n\n    Raises:\n        requests.exceptions.HTTPError: If the HTTP request returned an\n            unsuccessful status code.\n    \"\"\"\n    r = requests.get(url)\n    r.raise_for_status()\n    with open(local_filepath, 'wb') as file:\n        with tqdm(unit='byte', unit_scale=True) as progress_bar:\n            for chunk in r.iter_content(chunk_size=1024):\n                file.write(chunk)\n                progress_bar.update(len(chunk))\n                print(f\"File downloaded successfully and saved as {local_filepath}\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.download.download_file","title":"<code>download_file(url, local_filepath, size=None)</code>","text":"<p>Downloads a file by streaming its content, with a progress bar.</p> <p>This is the primary download function used for most datasets. It streams the response, making it suitable for large files. It attempts to infer the file size from response headers for the progress bar, but an expected size can also be provided.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the file to download.</p> required <code>local_filepath</code> <code>str</code> <p>The local path where the file will be saved.</p> required <code>size</code> <code>int</code> <p>The expected file size in bytes. Used for the progress bar if the 'Content-Length' header is not available. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The local file path if the download was successful, otherwise None.</p> Source code in <code>datarec/datasets/download.py</code> <pre><code>def download_file(url, local_filepath, size=None):\n    \"\"\"\n    Downloads a file by streaming its content, with a progress bar.\n\n    This is the primary download function used for most datasets. It streams the\n    response, making it suitable for large files. It attempts to infer the file\n    size from response headers for the progress bar, but an expected size can also\n    be provided.\n\n    Args:\n        url (str): The URL of the file to download.\n        local_filepath (str): The local path where the file will be saved.\n        size (int, optional): The expected file size in bytes. Used for the\n            progress bar if the 'Content-Length' header is not available.\n            Defaults to None.\n\n    Returns:\n        (str): The local file path if the download was successful, otherwise None.\n    \"\"\"\n    # Make a GET request to the URL\n    response = requests.get(url, stream=True)\n    # Check if the request was successful\n    if response.status_code == 200:\n        # try to infer the total size\n        try:\n            size = int(response.headers.get('Content-Length', 0))\n        except:\n            size = size\n        # Save the response content to a file\n        with open(local_filepath, 'wb') as f:\n            with tqdm(unit='byte', unit_scale=True, total=size) as progress_bar:\n                for chunk in response.iter_content(chunk_size=8192):\n                    f.write(chunk)\n                    progress_bar.update(len(chunk))\n            print(f\"File downloaded successfully and saved at \\'{local_filepath}\\'\")\n        return local_filepath\n    else:\n\n        print(f\"Failed to download the file. Response status code: {response.status_code}\")\n        return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.download.download_browser","title":"<code>download_browser(url, local_filepath, headers=None, chunk_size=8192)</code>","text":"<p>Downloads a file by mimicking a web browser request.</p> <p>This function is used for sources that may block simple scripted requests. It includes a default 'User-Agent' header to appear as a standard browser, which is necessary for some datasets (e.g., Yelp).</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the file to download.</p> required <code>local_filepath</code> <code>str</code> <p>The local path where the file will be saved.</p> required <code>headers</code> <code>dict</code> <p>Custom headers to use for the request. If None, a default browser User-Agent is used. Defaults to None.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>The size of chunks to download in bytes. Defaults to 8192.</p> <code>8192</code> <p>Returns:</p> Type Description <code>str</code> <p>The local file path if the download was successful, otherwise None.</p> Source code in <code>datarec/datasets/download.py</code> <pre><code>def download_browser(url, local_filepath, headers=None, chunk_size=8192):\n    \"\"\"\n    Downloads a file by mimicking a web browser request.\n\n    This function is used for sources that may block simple scripted requests.\n    It includes a default 'User-Agent' header to appear as a standard browser,\n    which is necessary for some datasets (e.g., Yelp).\n\n    Args:\n        url (str): The URL of the file to download.\n        local_filepath (str): The local path where the file will be saved.\n        headers (dict, optional): Custom headers to use for the request. If None,\n            a default browser User-Agent is used. Defaults to None.\n        chunk_size (int, optional): The size of chunks to download in bytes.\n            Defaults to 8192.\n\n    Returns:\n        (str): The local file path if the download was successful, otherwise None.\n    \"\"\"\n    # Default headers, or a custom one if provided\n    if headers is None:\n        headers = {'User-Agent': 'Mozilla/5.0...'}\n    response = requests.get(url, stream=True, headers=headers)\n    if response.status_code == 200:\n        total_size = int(response.headers.get('Content-Length', 0))\n        with open(local_filepath, 'wb') as f:\n            with tqdm(total=total_size, unit='iB', unit_scale=True) as progress_bar:\n                for chunk in response.iter_content(chunk_size=chunk_size):\n                    if chunk:\n                        f.write(chunk)\n                        progress_bar.update(len(chunk))\n        print(f\"Downloaded to '{local_filepath}'\")\n        return local_filepath\n    return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.download.decompress_gz","title":"<code>decompress_gz(input_file, output_file)</code>","text":"<p>Decompresses a .gz file.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>The path to the input .gz file.</p> required <code>output_file</code> <code>str</code> <p>The path where the decompressed file will be saved.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path to the decompressed output file.</p> Source code in <code>datarec/datasets/download.py</code> <pre><code>def decompress_gz(input_file, output_file):\n    \"\"\"\n    Decompresses a .gz file.\n\n    Args:\n        input_file (str): The path to the input .gz file.\n        output_file (str): The path where the decompressed file will be saved.\n\n    Returns:\n        (str): The path to the decompressed output file.\n    \"\"\"\n    print(f'Decompress: \\'{input_file}\\'')\n    with gzip.open(input_file, 'rb') as f_in:\n        with open(output_file, 'wb') as f_out:\n            shutil.copyfileobj(f_in, f_out)\n\n    print(f'File decompressed: \\'{output_file}\\'')\n    return output_file\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.download.decompress_tar_file","title":"<code>decompress_tar_file(input_file, output_dir)</code>","text":"<p>Decompresses a .tar archive.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>The path to the input .tar file.</p> required <code>output_dir</code> <code>str</code> <p>The directory where the contents will be extracted.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of the names of the extracted files and directories.</p> Source code in <code>datarec/datasets/download.py</code> <pre><code>def decompress_tar_file(input_file, output_dir):\n    \"\"\"\n    Decompresses a .tar archive.\n\n    Args:\n        input_file (str): The path to the input .tar file.\n        output_dir (str): The directory where the contents will be extracted.\n\n    Returns:\n        (list): A list of the names of the extracted files and directories.\n    \"\"\"\n\n    print(f'Decompress: \\'{input_file}\\'')\n    with tarfile.open(input_file, 'r') as tar:\n        tar.extractall(path=output_dir)\n\n        print(f'File decompressed in \\'{output_dir}\\'')\n    return os.listdir(output_dir)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.download.decompress_zip_file","title":"<code>decompress_zip_file(input_file, output_dir, allowZip64=False)</code>","text":"<p>Decompresses a .zip archive.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>The path to the input .zip file.</p> required <code>output_dir</code> <code>str</code> <p>The directory where the contents will be extracted.</p> required <code>allowZip64</code> <code>bool</code> <p>Whether to allow the Zip64 extension (for archives larger than 2 GB). Defaults to False, but should be True for large files.</p> <code>False</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of the names of the extracted files and directories.</p> Source code in <code>datarec/datasets/download.py</code> <pre><code>def decompress_zip_file(input_file, output_dir, allowZip64=False):\n    \"\"\"\n    Decompresses a .zip archive.\n\n    Args:\n        input_file (str): The path to the input .zip file.\n        output_dir (str): The directory where the contents will be extracted.\n        allowZip64 (bool): Whether to allow the Zip64 extension (for archives\n            larger than 2 GB). Defaults to False, but should be True for large files.\n\n    Returns:\n        (list): A list of the names of the extracted files and directories.\n    \"\"\"\n    with zipfile.ZipFile(input_file, 'r', allowZip64=allowZip64) as zip_ref:\n        zip_ref.extractall(output_dir)\n        print(f'File decompressed in \\'{output_dir}\\'')\n    return os.listdir(output_dir)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.download.decompress_7z_file","title":"<code>decompress_7z_file(input_file, output_dir)</code>","text":"<p>Decompresses a .7z archive.</p> <p>This function is used for datasets distributed in the 7-Zip format, such as the Alibaba-iFashion dataset.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>The path to the input .7z file.</p> required <code>output_dir</code> <code>str</code> <p>The directory where the contents will be extracted.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path to the output directory.</p> Source code in <code>datarec/datasets/download.py</code> <pre><code>def decompress_7z_file(input_file, output_dir):\n    \"\"\"\n    Decompresses a .7z archive.\n\n    This function is used for datasets distributed in the 7-Zip format, such as\n    the Alibaba-iFashion dataset.\n\n    Args:\n        input_file (str): The path to the input .7z file.\n        output_dir (str): The directory where the contents will be extracted.\n\n    Returns:\n        (str): The path to the output directory.\n    \"\"\"\n    print(f\"Decompressing: {input_file}\")\n    with py7zr.SevenZipFile(input_file, mode='r') as archive:\n        archive.extractall(path=output_dir)\n    print(f\"File decompressed in '{output_dir}'\")\n    return output_dir\n</code></pre>"},{"location":"documentation/datasets/#alibaba-ifashion","title":"Alibaba-iFashion","text":"<p>Entry point for loading different versions of the Alibaba-iFashion dataset.</p> <p>Builder class for version 'v1' of the Alibaba-iFashion dataset.</p>"},{"location":"documentation/datasets/#datarec.datasets.alibaba_ifashion.alibaba_ifashion.AlibabaIFashion","title":"<code>AlibabaIFashion</code>","text":"<p>Entry point class to load various versions of the Alibaba-iFashion dataset.</p> <p>This class provides a single, convenient interface for accessing the dataset. Based on the <code>version</code> parameter, it selects and returns the appropriate dataset builder.</p> <p>The default version is 'latest', which currently corresponds to 'v1'.</p> <p>Examples:</p> <p>To load the latest version:</p> <pre><code>&gt;&gt;&gt; data_loader = AlibabaIFashion()\n</code></pre> <p>To load a specific version:</p> <pre><code>&gt;&gt;&gt; data_loader = AlibabaIFashion(version='v1')\n</code></pre> Source code in <code>datarec/datasets/alibaba_ifashion/alibaba_ifashion.py</code> <pre><code>class AlibabaIFashion:\n    \"\"\"\n    Entry point class to load various versions of the Alibaba-iFashion dataset.\n\n    This class provides a single, convenient interface for accessing the dataset.\n    Based on the `version` parameter, it selects and returns the appropriate\n    dataset builder.\n\n    The default version is 'latest', which currently corresponds to 'v1'.\n\n    Examples:\n        To load the latest version:\n        &gt;&gt;&gt; data_loader = AlibabaIFashion()\n\n        To load a specific version:\n        &gt;&gt;&gt; data_loader = AlibabaIFashion(version='v1')\n    \"\"\"\n    latest_version = 'v1'\n\n    def __new__(cls, version: str = 'latest', **kwargs):\n        \"\"\"\n        Initializes and returns the specified version of the Alibaba-iFashion dataset builder.\n\n        This method acts as a dispatcher, instantiating the correct builder class\n        that handles the downloading, caching, and loading for a specific dataset version.\n\n        Args:\n            version (str): The version of the dataset to load. Currently, only\n            'v1' and 'latest' are supported. Defaults to 'latest'.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            (AlibabaIFashion_V1): An instance of the dataset builder class, ready to be used.\n\n        Raises:\n            ValueError: If an unsupported version string is provided.\n        \"\"\"\n        versions = {'v1': AlibabaIFashion_V1}\n        if version == 'latest':\n            version = cls.latest_version\n        if version in versions:\n            return versions[version]()\n        else:\n            raise ValueError(\"Alibaba iFashion: Unsupported version\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.alibaba_ifashion.alibaba_ifashion.AlibabaIFashion.__new__","title":"<code>__new__(version='latest', **kwargs)</code>","text":"<p>Initializes and returns the specified version of the Alibaba-iFashion dataset builder.</p> <p>This method acts as a dispatcher, instantiating the correct builder class that handles the downloading, caching, and loading for a specific dataset version.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>The version of the dataset to load. Currently, only</p> <code>'latest'</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AlibabaIFashion_V1</code> <p>An instance of the dataset builder class, ready to be used.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported version string is provided.</p> Source code in <code>datarec/datasets/alibaba_ifashion/alibaba_ifashion.py</code> <pre><code>def __new__(cls, version: str = 'latest', **kwargs):\n    \"\"\"\n    Initializes and returns the specified version of the Alibaba-iFashion dataset builder.\n\n    This method acts as a dispatcher, instantiating the correct builder class\n    that handles the downloading, caching, and loading for a specific dataset version.\n\n    Args:\n        version (str): The version of the dataset to load. Currently, only\n        'v1' and 'latest' are supported. Defaults to 'latest'.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        (AlibabaIFashion_V1): An instance of the dataset builder class, ready to be used.\n\n    Raises:\n        ValueError: If an unsupported version string is provided.\n    \"\"\"\n    versions = {'v1': AlibabaIFashion_V1}\n    if version == 'latest':\n        version = cls.latest_version\n    if version in versions:\n        return versions[version]()\n    else:\n        raise ValueError(\"Alibaba iFashion: Unsupported version\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.alibaba_ifashion.alibaba_ifashion_v1.AlibabaIFashion_V1","title":"<code>AlibabaIFashion_V1</code>","text":"<p>               Bases: <code>DataRec</code></p> <p>Builder class for the Alibaba-iFashion dataset (KDD 2019 version).</p> <p>This class handles the logic for downloading, preparing, and loading the Alibaba-iFashion dataset. It is not typically instantiated directly but is called by the <code>AlibabaIFashion</code> entry point class.</p> <p>The dataset was released for the paper \"POG: Personalized Outfit Generation for Fashion Recommendation at Alibaba iFashion\". It contains user-item interactions, item metadata, and outfit compositions. This loader focuses on processing the user-item interaction data from <code>user_data.txt</code>.</p> <p>Attributes:</p> Name Type Description <code>item_data_url</code> <code>str</code> <p>The URL for the item metadata file.</p> <code>outfit_data_url</code> <code>str</code> <p>The URL for the outfit composition file.</p> <code>user_data_url</code> <code>str</code> <p>The URL for the user-item interaction file.</p> <code>CHECKSUM_ITEM</code> <code>str</code> <p>MD5 checksum for the compressed item data archive.</p> <code>CHECKSUM_USER</code> <code>str</code> <p>MD5 checksum for the compressed user data archive.</p> <code>CHECKSUM_OUTFIT</code> <code>str</code> <p>MD5 checksum for the compressed outfit data archive.</p> Source code in <code>datarec/datasets/alibaba_ifashion/alibaba_ifashion_v1.py</code> <pre><code>class AlibabaIFashion_V1(DataRec):\n    \"\"\"\n    Builder class for the Alibaba-iFashion dataset (KDD 2019 version).\n\n    This class handles the logic for downloading, preparing, and loading the\n    Alibaba-iFashion dataset. It is not typically instantiated directly but is\n    called by the `AlibabaIFashion` entry point class.\n\n    The dataset was released for the paper \"POG: Personalized Outfit Generation\n    for Fashion Recommendation at Alibaba iFashion\". It contains user-item interactions,\n    item metadata, and outfit compositions. This loader focuses on processing the\n    user-item interaction data from `user_data.txt`.\n\n    Attributes:\n        item_data_url (str): The URL for the item metadata file.\n        outfit_data_url (str): The URL for the outfit composition file.\n        user_data_url (str): The URL for the user-item interaction file.\n        CHECKSUM_ITEM (str): MD5 checksum for the compressed item data archive.\n        CHECKSUM_USER (str): MD5 checksum for the compressed user data archive.\n        CHECKSUM_OUTFIT (str): MD5 checksum for the compressed outfit data archive.\n    \"\"\"\n    item_data_url = 'https://drive.google.com/uc?id=17MAGl20_mf9V8j0-J6c7T3ayfZd-dIx8'\n    outfit_data_url = 'https://drive.google.com/uc?id=1HFKUqBe5oMizU0lxy6sQE5Er1w9x-cC4'\n    user_data_url = 'https://drive.google.com/uc?id=1G_1SV9H7fQMPPJOBmZpCnCkgifSsb9Ar'\n\n    compressed_item_file_name = 'item_data.txt.zip'\n    compressed_outfit_file_name = 'outfit_data.txt.zip'\n    compressed_user_file_name = 'user_data.7z'\n\n    data_file_name = 'alibaba_ifashion'\n\n    uncompressed_item_file_name = 'item_data.txt'\n    uncompressed_outfit_file_name = 'outfit_data.txt'\n    uncompressed_user_file_name = 'user_data.txt'\n\n    REQUIRED_FILES = [uncompressed_item_file_name, uncompressed_outfit_file_name, uncompressed_user_file_name]\n    CHECKSUM_ITEM = 'f501244e784ae33defb71b3478d1125c'\n    CHECKSUM_USER = '2ff9254d67fb13d04824621ca1387622'\n    CHECKSUM_OUTFIT = 'f24078606235c122bd1d1c988766e83f'\n\n    def __init__(self, folder=None):\n        \"\"\"\n        Initializes the builder and orchestrates the data preparation workflow.\n\n        This constructor sets up the necessary paths and automatically triggers\n        the download, verification, and processing steps if the data is not\n        already present in the specified cache directory.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n        \"\"\"\n\n        super().__init__(user=True, item=True, rating='implicit')\n\n        self.dataset_name = 'alibaba_ifashion'\n        self.version_name = 'v1'\n\n        self._data_folder = folder if folder \\\n            else dataset_directory(self.dataset_name)\n        self._raw_folder = os.path.abspath(os.path.join(self._data_folder, RAW_DATA_FOLDER)) if folder \\\n            else dataset_raw_directory(self.dataset_name)\n\n        self.return_type = None\n\n        # check if the required files have been already downloaded\n        rq_found, rq_missing = self.required_files()\n        # download and decompress the required files that are missing\n        rq_found, rq_missing = self.download(found=rq_found, missing=rq_missing)\n        assert len(rq_found) == len(self.REQUIRED_FILES), len(rq_missing) == 0\n\n        data_path = None\n        for p, n in rq_found:\n            if n == self.uncompressed_user_file_name:\n                data_path = p\n                break\n        assert data_path is not None, 'User data file not found'\n\n        self.path = self.process(data_path=data_path)\n\n    def required_files(self):\n        \"\"\"\n        Checks for the presence of the required decompressed data files.\n\n        Returns:\n            (tuple[list, list]): A tuple where the first element is a list of\n                found files and the second is a list of missing files. Each\n                item in the lists is a tuple of (path, filename).\n        \"\"\"\n        # check if the file is there\n        req_files = [(os.path.join(self._raw_folder, f), f) for f in self.REQUIRED_FILES]\n        found, missing = [], []\n        # required file path, required file name\n        for rfp, rfn in req_files:\n            if os.path.isfile(rfp):\n                found.append((rfp, rfn))\n                print(f'Required file \\'{rfn}\\' found')\n            else:\n                missing.append((rfp, rfn))\n        return found, missing\n\n    def download_item_data(self):\n        \"\"\"\n        Downloads, verifies, and decompresses the item data file.\n\n        Returns:\n            (str): The path to the decompressed item data file.\n        \"\"\"\n        file_path = os.path.join(self._raw_folder, self.compressed_item_file_name)\n        print(f'Downloading {self.dataset_name} item data...')\n        gdown.download(self.item_data_url, file_path, quiet=False)\n        print(f'{self.dataset_name} item data downloaded at \\'{file_path}\\'')\n        verify_checksum(file_path, self.CHECKSUM_ITEM)\n        print('Decompressing zip file...')\n        decompress_zip_file(file_path, self._raw_folder)\n        print('Deleting zip file...')\n        os.remove(file_path)\n        return os.path.join(self._raw_folder, self.uncompressed_item_file_name)\n\n    def download_outfit_data(self):\n        \"\"\"\n        Downloads, verifies, and decompresses the outfit data file.\n\n        Returns:\n            (str): The path to the decompressed outfit data file.\n        \"\"\"\n        file_path = os.path.join(self._raw_folder, self.compressed_outfit_file_name)\n        print(f'Downloading {self.dataset_name} outfit data...')\n        gdown.download(self.outfit_data_url, file_path, quiet=False)\n        print(f'{self.dataset_name} outfit data downloaded at \\'{file_path}\\'')\n        verify_checksum(file_path, self.CHECKSUM_OUTFIT)\n        print('Decompressing zip file...')\n        decompress_zip_file(file_path, self._raw_folder)\n        print('Deleting zip file...')\n        os.remove(file_path)\n        return os.path.join(self._raw_folder, self.uncompressed_outfit_file_name)\n\n    def download_user_data(self):\n        \"\"\"\n        Downloads, verifies, and decompresses the user interaction data file.\n\n        Returns:\n            (str): The path to the decompressed user data file.\n        \"\"\"\n        file_path = os.path.join(self._raw_folder, self.compressed_user_file_name)\n        print(f'Downloading {self.dataset_name} user data...')\n        gdown.download(self.user_data_url, file_path, quiet=False)\n        print(f'{self.dataset_name} user data downloaded at \\'{file_path}\\'')\n        verify_checksum(file_path, self.CHECKSUM_USER)\n        print('Decompressing 7z file...')\n        decompress_7z_file(file_path, self._raw_folder)\n        print('Deleting 7z file...')\n        os.remove(file_path)\n        return os.path.join(self._raw_folder, self.uncompressed_user_file_name)\n\n    def download(self, found: list, missing: list) -&gt; (str, str):\n        \"\"\"\n        Downloads all missing files for the dataset.\n\n        Iterates through the list of missing files and calls the appropriate download helper function for each one.\n\n        Args:\n            found (list): A list of file tuples that were already found locally.\n            missing (list): A list of file tuples that need to be downloaded.\n\n        Returns:\n            (tuple[list, list]): The updated lists of found and missing files after\n                the download and verification process.\n        \"\"\"\n        if not os.path.exists(self._raw_folder):\n            os.makedirs(self._raw_folder)\n            print('Created raw files folder at \\'{}\\''.format(self._raw_folder))\n\n        downloaded = []\n        for required_file in missing:\n            file_path, file_name = required_file\n\n            if file_name == 'item_data.txt':\n                self.download_item_data()\n                downloaded.append(required_file)\n            elif file_name == 'outfit_data.txt':\n                self.download_outfit_data()\n                downloaded.append(required_file)\n            elif file_name == 'user_data.txt':\n                self.download_user_data()\n                downloaded.append(required_file)\n            else:\n                raise warnings.warn(f'You are trying to download a not required file for {self.dataset_name}.'\n                                    f' \\n The file will not be downloaded.', UserWarning)\n\n        for required_file in downloaded:\n            missing.remove(required_file)\n            found.append(required_file)\n        return found, missing\n\n    def process(self, data_path) -&gt; None:\n        \"\"\"\n        Processes the raw user interaction data and loads it into the class.\n\n        The user interaction data is in an 'inline' format, where each line\n        contains a user followed by a semicolon-separated list of their item\n        interactions. This method uses `read_inline` to parse this format into\n        a standard user-item pair DataFrame.\n\n        Args:\n            data_path (str): The path to the raw `user_data.txt` file.\n\n        Returns:\n            (None): This method assigns the processed data to `self.data` directly.\n        \"\"\"\n        from datarec.io.readers import read_inline\n        self.data = read_inline(data_path, cols=['user', 'item', 'outfit'],\n                                user_col='user', item_col='item',\n                                col_sep=',', history_sep=';')\n        return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.alibaba_ifashion.alibaba_ifashion_v1.AlibabaIFashion_V1.__init__","title":"<code>__init__(folder=None)</code>","text":"<p>Initializes the builder and orchestrates the data preparation workflow.</p> <p>This constructor sets up the necessary paths and automatically triggers the download, verification, and processing steps if the data is not already present in the specified cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> Source code in <code>datarec/datasets/alibaba_ifashion/alibaba_ifashion_v1.py</code> <pre><code>def __init__(self, folder=None):\n    \"\"\"\n    Initializes the builder and orchestrates the data preparation workflow.\n\n    This constructor sets up the necessary paths and automatically triggers\n    the download, verification, and processing steps if the data is not\n    already present in the specified cache directory.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n    \"\"\"\n\n    super().__init__(user=True, item=True, rating='implicit')\n\n    self.dataset_name = 'alibaba_ifashion'\n    self.version_name = 'v1'\n\n    self._data_folder = folder if folder \\\n        else dataset_directory(self.dataset_name)\n    self._raw_folder = os.path.abspath(os.path.join(self._data_folder, RAW_DATA_FOLDER)) if folder \\\n        else dataset_raw_directory(self.dataset_name)\n\n    self.return_type = None\n\n    # check if the required files have been already downloaded\n    rq_found, rq_missing = self.required_files()\n    # download and decompress the required files that are missing\n    rq_found, rq_missing = self.download(found=rq_found, missing=rq_missing)\n    assert len(rq_found) == len(self.REQUIRED_FILES), len(rq_missing) == 0\n\n    data_path = None\n    for p, n in rq_found:\n        if n == self.uncompressed_user_file_name:\n            data_path = p\n            break\n    assert data_path is not None, 'User data file not found'\n\n    self.path = self.process(data_path=data_path)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.alibaba_ifashion.alibaba_ifashion_v1.AlibabaIFashion_V1.required_files","title":"<code>required_files()</code>","text":"<p>Checks for the presence of the required decompressed data files.</p> <p>Returns:</p> Type Description <code>tuple[list, list]</code> <p>A tuple where the first element is a list of found files and the second is a list of missing files. Each item in the lists is a tuple of (path, filename).</p> Source code in <code>datarec/datasets/alibaba_ifashion/alibaba_ifashion_v1.py</code> <pre><code>def required_files(self):\n    \"\"\"\n    Checks for the presence of the required decompressed data files.\n\n    Returns:\n        (tuple[list, list]): A tuple where the first element is a list of\n            found files and the second is a list of missing files. Each\n            item in the lists is a tuple of (path, filename).\n    \"\"\"\n    # check if the file is there\n    req_files = [(os.path.join(self._raw_folder, f), f) for f in self.REQUIRED_FILES]\n    found, missing = [], []\n    # required file path, required file name\n    for rfp, rfn in req_files:\n        if os.path.isfile(rfp):\n            found.append((rfp, rfn))\n            print(f'Required file \\'{rfn}\\' found')\n        else:\n            missing.append((rfp, rfn))\n    return found, missing\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.alibaba_ifashion.alibaba_ifashion_v1.AlibabaIFashion_V1.download_item_data","title":"<code>download_item_data()</code>","text":"<p>Downloads, verifies, and decompresses the item data file.</p> <p>Returns:</p> Type Description <code>str</code> <p>The path to the decompressed item data file.</p> Source code in <code>datarec/datasets/alibaba_ifashion/alibaba_ifashion_v1.py</code> <pre><code>def download_item_data(self):\n    \"\"\"\n    Downloads, verifies, and decompresses the item data file.\n\n    Returns:\n        (str): The path to the decompressed item data file.\n    \"\"\"\n    file_path = os.path.join(self._raw_folder, self.compressed_item_file_name)\n    print(f'Downloading {self.dataset_name} item data...')\n    gdown.download(self.item_data_url, file_path, quiet=False)\n    print(f'{self.dataset_name} item data downloaded at \\'{file_path}\\'')\n    verify_checksum(file_path, self.CHECKSUM_ITEM)\n    print('Decompressing zip file...')\n    decompress_zip_file(file_path, self._raw_folder)\n    print('Deleting zip file...')\n    os.remove(file_path)\n    return os.path.join(self._raw_folder, self.uncompressed_item_file_name)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.alibaba_ifashion.alibaba_ifashion_v1.AlibabaIFashion_V1.download_outfit_data","title":"<code>download_outfit_data()</code>","text":"<p>Downloads, verifies, and decompresses the outfit data file.</p> <p>Returns:</p> Type Description <code>str</code> <p>The path to the decompressed outfit data file.</p> Source code in <code>datarec/datasets/alibaba_ifashion/alibaba_ifashion_v1.py</code> <pre><code>def download_outfit_data(self):\n    \"\"\"\n    Downloads, verifies, and decompresses the outfit data file.\n\n    Returns:\n        (str): The path to the decompressed outfit data file.\n    \"\"\"\n    file_path = os.path.join(self._raw_folder, self.compressed_outfit_file_name)\n    print(f'Downloading {self.dataset_name} outfit data...')\n    gdown.download(self.outfit_data_url, file_path, quiet=False)\n    print(f'{self.dataset_name} outfit data downloaded at \\'{file_path}\\'')\n    verify_checksum(file_path, self.CHECKSUM_OUTFIT)\n    print('Decompressing zip file...')\n    decompress_zip_file(file_path, self._raw_folder)\n    print('Deleting zip file...')\n    os.remove(file_path)\n    return os.path.join(self._raw_folder, self.uncompressed_outfit_file_name)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.alibaba_ifashion.alibaba_ifashion_v1.AlibabaIFashion_V1.download_user_data","title":"<code>download_user_data()</code>","text":"<p>Downloads, verifies, and decompresses the user interaction data file.</p> <p>Returns:</p> Type Description <code>str</code> <p>The path to the decompressed user data file.</p> Source code in <code>datarec/datasets/alibaba_ifashion/alibaba_ifashion_v1.py</code> <pre><code>def download_user_data(self):\n    \"\"\"\n    Downloads, verifies, and decompresses the user interaction data file.\n\n    Returns:\n        (str): The path to the decompressed user data file.\n    \"\"\"\n    file_path = os.path.join(self._raw_folder, self.compressed_user_file_name)\n    print(f'Downloading {self.dataset_name} user data...')\n    gdown.download(self.user_data_url, file_path, quiet=False)\n    print(f'{self.dataset_name} user data downloaded at \\'{file_path}\\'')\n    verify_checksum(file_path, self.CHECKSUM_USER)\n    print('Decompressing 7z file...')\n    decompress_7z_file(file_path, self._raw_folder)\n    print('Deleting 7z file...')\n    os.remove(file_path)\n    return os.path.join(self._raw_folder, self.uncompressed_user_file_name)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.alibaba_ifashion.alibaba_ifashion_v1.AlibabaIFashion_V1.download","title":"<code>download(found, missing)</code>","text":"<p>Downloads all missing files for the dataset.</p> <p>Iterates through the list of missing files and calls the appropriate download helper function for each one.</p> <p>Parameters:</p> Name Type Description Default <code>found</code> <code>list</code> <p>A list of file tuples that were already found locally.</p> required <code>missing</code> <code>list</code> <p>A list of file tuples that need to be downloaded.</p> required <p>Returns:</p> Type Description <code>tuple[list, list]</code> <p>The updated lists of found and missing files after the download and verification process.</p> Source code in <code>datarec/datasets/alibaba_ifashion/alibaba_ifashion_v1.py</code> <pre><code>def download(self, found: list, missing: list) -&gt; (str, str):\n    \"\"\"\n    Downloads all missing files for the dataset.\n\n    Iterates through the list of missing files and calls the appropriate download helper function for each one.\n\n    Args:\n        found (list): A list of file tuples that were already found locally.\n        missing (list): A list of file tuples that need to be downloaded.\n\n    Returns:\n        (tuple[list, list]): The updated lists of found and missing files after\n            the download and verification process.\n    \"\"\"\n    if not os.path.exists(self._raw_folder):\n        os.makedirs(self._raw_folder)\n        print('Created raw files folder at \\'{}\\''.format(self._raw_folder))\n\n    downloaded = []\n    for required_file in missing:\n        file_path, file_name = required_file\n\n        if file_name == 'item_data.txt':\n            self.download_item_data()\n            downloaded.append(required_file)\n        elif file_name == 'outfit_data.txt':\n            self.download_outfit_data()\n            downloaded.append(required_file)\n        elif file_name == 'user_data.txt':\n            self.download_user_data()\n            downloaded.append(required_file)\n        else:\n            raise warnings.warn(f'You are trying to download a not required file for {self.dataset_name}.'\n                                f' \\n The file will not be downloaded.', UserWarning)\n\n    for required_file in downloaded:\n        missing.remove(required_file)\n        found.append(required_file)\n    return found, missing\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.alibaba_ifashion.alibaba_ifashion_v1.AlibabaIFashion_V1.process","title":"<code>process(data_path)</code>","text":"<p>Processes the raw user interaction data and loads it into the class.</p> <p>The user interaction data is in an 'inline' format, where each line contains a user followed by a semicolon-separated list of their item interactions. This method uses <code>read_inline</code> to parse this format into a standard user-item pair DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>The path to the raw <code>user_data.txt</code> file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This method assigns the processed data to <code>self.data</code> directly.</p> Source code in <code>datarec/datasets/alibaba_ifashion/alibaba_ifashion_v1.py</code> <pre><code>def process(self, data_path) -&gt; None:\n    \"\"\"\n    Processes the raw user interaction data and loads it into the class.\n\n    The user interaction data is in an 'inline' format, where each line\n    contains a user followed by a semicolon-separated list of their item\n    interactions. This method uses `read_inline` to parse this format into\n    a standard user-item pair DataFrame.\n\n    Args:\n        data_path (str): The path to the raw `user_data.txt` file.\n\n    Returns:\n        (None): This method assigns the processed data to `self.data` directly.\n    \"\"\"\n    from datarec.io.readers import read_inline\n    self.data = read_inline(data_path, cols=['user', 'item', 'outfit'],\n                            user_col='user', item_col='item',\n                            col_sep=',', history_sep=';')\n    return None\n</code></pre>"},{"location":"documentation/datasets/#amazon-beauty","title":"Amazon Beauty","text":"<p>Entry point for loading different versions of the Amazon Beauty dataset.</p> <p>Builder class for the 2023 version of the Amazon Beauty dataset.</p>"},{"location":"documentation/datasets/#datarec.datasets.amazon_beauty.amz_beauty.AmazonBeauty","title":"<code>AmazonBeauty</code>","text":"<p>Entry point class to load various versions of the Amazon Beauty dataset.</p> <p>This class provides a single, convenient interface for accessing the dataset. Based on the <code>version</code> parameter, it selects and returns the appropriate dataset builder.</p> <p>The Amazon Beauty dataset contains product reviews and metadata from Amazon, specialized for the \"Beauty and Personal Care\" category.</p> <p>The default version is 'latest', which currently corresponds to the '2023' version.</p> <p>Examples:</p> <p>To load the latest version:</p> <pre><code>&gt;&gt;&gt; data_loader = AmazonBeauty()\n</code></pre> <p>To load a specific version:</p> <pre><code>&gt;&gt;&gt; data_loader = AmazonBeauty(version='2023')\n</code></pre> Source code in <code>datarec/datasets/amazon_beauty/amz_beauty.py</code> <pre><code>class AmazonBeauty:\n    \"\"\"\n    Entry point class to load various versions of the Amazon Beauty dataset.\n\n    This class provides a single, convenient interface for accessing the dataset.\n    Based on the `version` parameter, it selects and returns the appropriate\n    dataset builder.\n\n    The Amazon Beauty dataset contains product reviews and metadata from Amazon,\n    specialized for the \"Beauty and Personal Care\" category.\n\n    The default version is 'latest', which currently corresponds to the '2023' version.\n\n    Examples:\n        To load the latest version:\n        &gt;&gt;&gt; data_loader = AmazonBeauty()\n\n        To load a specific version:\n        &gt;&gt;&gt; data_loader = AmazonBeauty(version='2023')\n    \"\"\"\n    latest_version = '2023'\n\n    def __new__(cls, version: str = 'latest', **kwargs) -&gt; DataRec:\n        \"\"\"\n        Initializes and returns the specified version of the Amazon Beauty dataset builder.\n\n        This method acts as a dispatcher, instantiating the correct builder class\n        that handles the downloading, caching, and loading for a specific dataset version.\n\n        Args:\n            version (str): The version of the dataset to load. Currently, only\n                '2023' and 'latest' are supported. Defaults to 'latest'.\n            **kwargs: Additional keyword arguments (not currently used for this dataset).\n\n        Returns:\n            (AMZ_Beauty_2023): An instance of the dataset builder class, populated with data.\n\n        Raises:\n            ValueError: If an unsupported version string is provided.\n        \"\"\"\n        versions = {'2023': AMZ_Beauty_2023}\n        if version == 'latest':\n            version = cls.latest_version\n        if version in versions:\n            return versions[version]()\n        else:\n            raise ValueError(f\"Amazon Beauty {version}: Unsupported version \\n Supported version:\"\n                             f\"\\n \\t version \\t name \"\n                             f\"\\n \\t 2023 \\t Amazon Beauty and Personal Care 2023\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_beauty.amz_beauty.AmazonBeauty.__new__","title":"<code>__new__(version='latest', **kwargs)</code>","text":"<p>Initializes and returns the specified version of the Amazon Beauty dataset builder.</p> <p>This method acts as a dispatcher, instantiating the correct builder class that handles the downloading, caching, and loading for a specific dataset version.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>The version of the dataset to load. Currently, only '2023' and 'latest' are supported. Defaults to 'latest'.</p> <code>'latest'</code> <code>**kwargs</code> <p>Additional keyword arguments (not currently used for this dataset).</p> <code>{}</code> <p>Returns:</p> Type Description <code>AMZ_Beauty_2023</code> <p>An instance of the dataset builder class, populated with data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported version string is provided.</p> Source code in <code>datarec/datasets/amazon_beauty/amz_beauty.py</code> <pre><code>def __new__(cls, version: str = 'latest', **kwargs) -&gt; DataRec:\n    \"\"\"\n    Initializes and returns the specified version of the Amazon Beauty dataset builder.\n\n    This method acts as a dispatcher, instantiating the correct builder class\n    that handles the downloading, caching, and loading for a specific dataset version.\n\n    Args:\n        version (str): The version of the dataset to load. Currently, only\n            '2023' and 'latest' are supported. Defaults to 'latest'.\n        **kwargs: Additional keyword arguments (not currently used for this dataset).\n\n    Returns:\n        (AMZ_Beauty_2023): An instance of the dataset builder class, populated with data.\n\n    Raises:\n        ValueError: If an unsupported version string is provided.\n    \"\"\"\n    versions = {'2023': AMZ_Beauty_2023}\n    if version == 'latest':\n        version = cls.latest_version\n    if version in versions:\n        return versions[version]()\n    else:\n        raise ValueError(f\"Amazon Beauty {version}: Unsupported version \\n Supported version:\"\n                         f\"\\n \\t version \\t name \"\n                         f\"\\n \\t 2023 \\t Amazon Beauty and Personal Care 2023\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_beauty.amz_beauty_2023.AMZ_Beauty_2023","title":"<code>AMZ_Beauty_2023</code>","text":"<p>               Bases: <code>DataRec</code></p> <p>Builder class for the Amazon Beauty dataset (2023 version).</p> <p>This class handles the logic for downloading, preparing, and loading the 2023 version of the Amazon Beauty dataset. It is not typically instantiated directly but is called by the <code>AmazonBeauty</code> entry point class.</p> <p>The dataset is from the \"Bridging Language and Items for Retrieval and Recommendation\" paper and contains user ratings for beauty products.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL from which the raw dataset is downloaded.</p> <code>CHECKSUM</code> <code>str</code> <p>The MD5 checksum to verify the integrity of the downloaded file.</p> Source code in <code>datarec/datasets/amazon_beauty/amz_beauty_2023.py</code> <pre><code>class AMZ_Beauty_2023(DataRec):\n    \"\"\"\n    Builder class for the Amazon Beauty dataset (2023 version).\n\n    This class handles the logic for downloading, preparing, and loading the\n    2023 version of the Amazon Beauty dataset. It is not typically instantiated\n    directly but is called by the `AmazonBeauty` entry point class.\n\n    The dataset is from the \"Bridging Language and Items for Retrieval and\n    Recommendation\" paper and contains user ratings for beauty products.\n\n    Attributes:\n        url (str): The URL from which the raw dataset is downloaded.\n        CHECKSUM (str): The MD5 checksum to verify the integrity of the downloaded file.\n    \"\"\"\n    url = 'https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/benchmark/0core/rating_only/Beauty_and_Personal_Care.csv.gz'\n    data_file_name = os.path.basename(url)\n    decompressed_data_file_name = data_file_name.replace('.gz', '')\n    CHECKSUM = '2e7f69fa6d738f1ee7756d8a46ad7930'\n\n    def __init__(self, folder=None):\n        \"\"\"\n        Initializes the builder and orchestrates the data preparation workflow.\n\n        This constructor sets up the necessary paths and automatically triggers\n        the download, verification, and processing steps if the data is not\n        already present in the specified cache directory.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n        \"\"\"\n        super().__init__(None)\n\n        self.dataset_name = 'amazon_beauty'\n        self.version_name = '2023'\n\n        self._data_folder = folder if folder \\\n            else dataset_directory(self.dataset_name)\n        self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n            else dataset_raw_directory(self.dataset_name)\n\n        self.return_type = None\n\n        # check if the required files have been already downloaded\n        file_path = self.required_files()\n\n        if file_path is None:\n            file_path = self.download()\n            file_path = self.decompress(file_path)\n\n        self.process(file_path)\n\n    def required_files(self):\n        \"\"\"\n        Checks for the presence of the required decompressed data file.\n\n        It first looks for the final, uncompressed file. If not found, it\n        looks for the compressed archive and decompresses it.\n\n        Returns:\n            (str or None): The path to the required data file if it exists or can be\n                created by decompression. Otherwise, returns None.\n        \"\"\"\n        # compressed data file\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n        uncompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n\n        # check if the file is there\n        if os.path.exists(uncompressed_file_path):\n            return uncompressed_file_path\n        # check if the compressed file is there\n        elif os.path.exists(file_path):\n            return self.decompress(file_path)\n        else:\n            return None\n\n    def decompress(self, path):\n        \"\"\"\n        Decompresses the downloaded .gz archive after verifying its checksum.\n\n        Args:\n            path (str): The file path of the compressed .gz archive.\n\n        Returns:\n            (str): The path to the decompressed CSV file.\n        \"\"\"\n        verify_checksum(path, self.CHECKSUM)\n\n        # decompress downloaded file\n        decompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n        return decompress_gz(path, decompressed_file_path)\n\n    def download(self) -&gt; (str, str):\n        \"\"\"\n        Downloads the raw dataset compressed archive.\n\n        Returns:\n            (str): The local file path to the downloaded .gz archive.\n        \"\"\"\n        if not os.path.exists(self._raw_folder):\n            os.makedirs(self._raw_folder)\n            print('Raw files folder missing. Folder created at \\'{}\\''.format(self._raw_folder))\n\n        # download dataset file (compressed file)\n        file_name = os.path.basename(self.url)\n        file_path = os.path.join(self._raw_folder, file_name)\n        print('Downloading data file from {}'.format(self.url))\n        download_file(self.url, file_path, size=559019689)\n\n        return file_path\n\n    def process(self, file_path):\n        \"\"\"\n        Processes the raw data and loads it into the class.\n\n        This method reads the decompressed file into a pandas DataFrame and\n        assigns it to the `self.data` attribute.\n\n        Args:\n            file_path (str): The path to the raw data file.\n\n        Returns:\n            (None): This method assigns the processed data to `self.data` directly.\n        \"\"\"\n\n        from datarec.io import read_tabular\n\n        dataset = read_tabular(file_path, sep=',',\n                               user_col='user_id', item_col='parent_asin',\n                               rating_col='rating', timestamp_col='timestamp',\n                               header=0)\n        self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_beauty.amz_beauty_2023.AMZ_Beauty_2023.__init__","title":"<code>__init__(folder=None)</code>","text":"<p>Initializes the builder and orchestrates the data preparation workflow.</p> <p>This constructor sets up the necessary paths and automatically triggers the download, verification, and processing steps if the data is not already present in the specified cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> Source code in <code>datarec/datasets/amazon_beauty/amz_beauty_2023.py</code> <pre><code>def __init__(self, folder=None):\n    \"\"\"\n    Initializes the builder and orchestrates the data preparation workflow.\n\n    This constructor sets up the necessary paths and automatically triggers\n    the download, verification, and processing steps if the data is not\n    already present in the specified cache directory.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n    \"\"\"\n    super().__init__(None)\n\n    self.dataset_name = 'amazon_beauty'\n    self.version_name = '2023'\n\n    self._data_folder = folder if folder \\\n        else dataset_directory(self.dataset_name)\n    self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n        else dataset_raw_directory(self.dataset_name)\n\n    self.return_type = None\n\n    # check if the required files have been already downloaded\n    file_path = self.required_files()\n\n    if file_path is None:\n        file_path = self.download()\n        file_path = self.decompress(file_path)\n\n    self.process(file_path)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_beauty.amz_beauty_2023.AMZ_Beauty_2023.required_files","title":"<code>required_files()</code>","text":"<p>Checks for the presence of the required decompressed data file.</p> <p>It first looks for the final, uncompressed file. If not found, it looks for the compressed archive and decompresses it.</p> <p>Returns:</p> Type Description <code>str or None</code> <p>The path to the required data file if it exists or can be created by decompression. Otherwise, returns None.</p> Source code in <code>datarec/datasets/amazon_beauty/amz_beauty_2023.py</code> <pre><code>def required_files(self):\n    \"\"\"\n    Checks for the presence of the required decompressed data file.\n\n    It first looks for the final, uncompressed file. If not found, it\n    looks for the compressed archive and decompresses it.\n\n    Returns:\n        (str or None): The path to the required data file if it exists or can be\n            created by decompression. Otherwise, returns None.\n    \"\"\"\n    # compressed data file\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n    uncompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n\n    # check if the file is there\n    if os.path.exists(uncompressed_file_path):\n        return uncompressed_file_path\n    # check if the compressed file is there\n    elif os.path.exists(file_path):\n        return self.decompress(file_path)\n    else:\n        return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_beauty.amz_beauty_2023.AMZ_Beauty_2023.decompress","title":"<code>decompress(path)</code>","text":"<p>Decompresses the downloaded .gz archive after verifying its checksum.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path of the compressed .gz archive.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path to the decompressed CSV file.</p> Source code in <code>datarec/datasets/amazon_beauty/amz_beauty_2023.py</code> <pre><code>def decompress(self, path):\n    \"\"\"\n    Decompresses the downloaded .gz archive after verifying its checksum.\n\n    Args:\n        path (str): The file path of the compressed .gz archive.\n\n    Returns:\n        (str): The path to the decompressed CSV file.\n    \"\"\"\n    verify_checksum(path, self.CHECKSUM)\n\n    # decompress downloaded file\n    decompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n    return decompress_gz(path, decompressed_file_path)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_beauty.amz_beauty_2023.AMZ_Beauty_2023.download","title":"<code>download()</code>","text":"<p>Downloads the raw dataset compressed archive.</p> <p>Returns:</p> Type Description <code>str</code> <p>The local file path to the downloaded .gz archive.</p> Source code in <code>datarec/datasets/amazon_beauty/amz_beauty_2023.py</code> <pre><code>def download(self) -&gt; (str, str):\n    \"\"\"\n    Downloads the raw dataset compressed archive.\n\n    Returns:\n        (str): The local file path to the downloaded .gz archive.\n    \"\"\"\n    if not os.path.exists(self._raw_folder):\n        os.makedirs(self._raw_folder)\n        print('Raw files folder missing. Folder created at \\'{}\\''.format(self._raw_folder))\n\n    # download dataset file (compressed file)\n    file_name = os.path.basename(self.url)\n    file_path = os.path.join(self._raw_folder, file_name)\n    print('Downloading data file from {}'.format(self.url))\n    download_file(self.url, file_path, size=559019689)\n\n    return file_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_beauty.amz_beauty_2023.AMZ_Beauty_2023.process","title":"<code>process(file_path)</code>","text":"<p>Processes the raw data and loads it into the class.</p> <p>This method reads the decompressed file into a pandas DataFrame and assigns it to the <code>self.data</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the raw data file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This method assigns the processed data to <code>self.data</code> directly.</p> Source code in <code>datarec/datasets/amazon_beauty/amz_beauty_2023.py</code> <pre><code>def process(self, file_path):\n    \"\"\"\n    Processes the raw data and loads it into the class.\n\n    This method reads the decompressed file into a pandas DataFrame and\n    assigns it to the `self.data` attribute.\n\n    Args:\n        file_path (str): The path to the raw data file.\n\n    Returns:\n        (None): This method assigns the processed data to `self.data` directly.\n    \"\"\"\n\n    from datarec.io import read_tabular\n\n    dataset = read_tabular(file_path, sep=',',\n                           user_col='user_id', item_col='parent_asin',\n                           rating_col='rating', timestamp_col='timestamp',\n                           header=0)\n    self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#amazon-books","title":"Amazon Books","text":"<p>Entry point for loading different versions of the Amazon Books dataset.</p> <p>Builder class for the 2018 version of the Amazon Books dataset.</p> <p>Builder class for the 2023 version of the Amazon Books dataset.</p>"},{"location":"documentation/datasets/#datarec.datasets.amazon_books.amz_books.AmazonBooks","title":"<code>AmazonBooks</code>","text":"<p>Entry point class to load various versions of the Amazon Books dataset.</p> <p>This class provides a single, convenient interface for accessing the dataset. Based on the <code>version</code> parameter, it selects and returns the appropriate dataset builder for either the 2018 or 2023 version.</p> <p>The Amazon Books dataset contains product reviews and metadata from Amazon for the \"Books\" category.</p> <p>The default version is 'latest', which currently corresponds to the '2023' version.</p> <p>Examples:</p> <p>To load the latest version:</p> <pre><code>&gt;&gt;&gt; data_loader = AmazonBooks()\n</code></pre> <p>To load a specific version:</p> <pre><code>&gt;&gt;&gt; data_loader = AmazonBooks(version='2018')\n</code></pre> Source code in <code>datarec/datasets/amazon_books/amz_books.py</code> <pre><code>class AmazonBooks:\n    \"\"\"\n    Entry point class to load various versions of the Amazon Books dataset.\n\n    This class provides a single, convenient interface for accessing the dataset.\n    Based on the `version` parameter, it selects and returns the appropriate\n    dataset builder for either the 2018 or 2023 version.\n\n    The Amazon Books dataset contains product reviews and metadata from Amazon\n    for the \"Books\" category.\n\n    The default version is 'latest', which currently corresponds to the '2023' version.\n\n    Examples:\n        To load the latest version:\n        &gt;&gt;&gt; data_loader = AmazonBooks()\n\n        To load a specific version:\n        &gt;&gt;&gt; data_loader = AmazonBooks(version='2018')\n    \"\"\"\n    latest_version = '2023'\n\n    def __new__(cls, version: str = 'latest', **kwargs) -&gt; DataRec:\n        \"\"\"\n        Initializes and returns the specified version of the Amazon Books dataset builder.\n\n        This method acts as a dispatcher, instantiating the correct builder class\n        that handles the downloading, caching, and loading for a specific dataset version.\n\n        Args:\n            version (str): The version of the dataset to load. Supported versions\n                include '2023', '2018', and 'latest'. Defaults to 'latest'.\n            **kwargs: Additional keyword arguments (not currently used for this dataset).\n\n        Returns:\n            (DataRec): An instance of the appropriate dataset builder class (e.g.,\n                `AMZ_Books_2023`), populated with data.\n\n        Raises:\n            ValueError: If an unsupported version string is provided.\n        \"\"\"\n        versions = {'2023': AMZ_Books_2023,\n                    '2018': AMZ_Books_2018}\n        if version == 'latest':\n            version = cls.latest_version\n        if version in versions:\n            return versions[version]()\n        else:\n            raise ValueError(f\"Amazon Books {version}: Unsupported version \\n Supported version:\"\n                             f\"\\n \\t version \\t name \"\n                             f\"\\n \\t 2023 \\t Amazon Books 2023\"\n                             f\"\\n \\t 2018 \\t Amazon Books 2018\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_books.amz_books.AmazonBooks.__new__","title":"<code>__new__(version='latest', **kwargs)</code>","text":"<p>Initializes and returns the specified version of the Amazon Books dataset builder.</p> <p>This method acts as a dispatcher, instantiating the correct builder class that handles the downloading, caching, and loading for a specific dataset version.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>The version of the dataset to load. Supported versions include '2023', '2018', and 'latest'. Defaults to 'latest'.</p> <code>'latest'</code> <code>**kwargs</code> <p>Additional keyword arguments (not currently used for this dataset).</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataRec</code> <p>An instance of the appropriate dataset builder class (e.g., <code>AMZ_Books_2023</code>), populated with data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported version string is provided.</p> Source code in <code>datarec/datasets/amazon_books/amz_books.py</code> <pre><code>def __new__(cls, version: str = 'latest', **kwargs) -&gt; DataRec:\n    \"\"\"\n    Initializes and returns the specified version of the Amazon Books dataset builder.\n\n    This method acts as a dispatcher, instantiating the correct builder class\n    that handles the downloading, caching, and loading for a specific dataset version.\n\n    Args:\n        version (str): The version of the dataset to load. Supported versions\n            include '2023', '2018', and 'latest'. Defaults to 'latest'.\n        **kwargs: Additional keyword arguments (not currently used for this dataset).\n\n    Returns:\n        (DataRec): An instance of the appropriate dataset builder class (e.g.,\n            `AMZ_Books_2023`), populated with data.\n\n    Raises:\n        ValueError: If an unsupported version string is provided.\n    \"\"\"\n    versions = {'2023': AMZ_Books_2023,\n                '2018': AMZ_Books_2018}\n    if version == 'latest':\n        version = cls.latest_version\n    if version in versions:\n        return versions[version]()\n    else:\n        raise ValueError(f\"Amazon Books {version}: Unsupported version \\n Supported version:\"\n                         f\"\\n \\t version \\t name \"\n                         f\"\\n \\t 2023 \\t Amazon Books 2023\"\n                         f\"\\n \\t 2018 \\t Amazon Books 2018\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_books.amz_books_2018.AMZ_Books_2018","title":"<code>AMZ_Books_2018</code>","text":"<p>               Bases: <code>DataRec</code></p> <p>Builder class for the Amazon Books dataset (2018 version).</p> <p>This class handles the logic for downloading, preparing, and loading the 2018 version of the Amazon Books dataset from the Amazon Reviews V2 source. It is not typically instantiated directly but is called by the <code>AmazonBooks</code> entry point class.</p> <p>The raw data is provided as a single, uncompressed CSV file.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL from which the raw dataset is downloaded.</p> <code>CHECKSUM</code> <code>str</code> <p>The MD5 checksum to verify the integrity of the downloaded file.</p> Source code in <code>datarec/datasets/amazon_books/amz_books_2018.py</code> <pre><code>class AMZ_Books_2018(DataRec):\n    \"\"\"\n    Builder class for the Amazon Books dataset (2018 version).\n\n    This class handles the logic for downloading, preparing, and loading the\n    2018 version of the Amazon Books dataset from the Amazon Reviews V2 source.\n    It is not typically instantiated directly but is called by the `AmazonBooks`\n    entry point class.\n\n    The raw data is provided as a single, uncompressed CSV file.\n\n    Attributes:\n        url (str): The URL from which the raw dataset is downloaded.\n        CHECKSUM (str): The MD5 checksum to verify the integrity of the downloaded file.\n    \"\"\"\n    url = 'https://mcauleylab.ucsd.edu/public_datasets/data/amazon_v2/categoryFilesSmall/Books.csv'\n    data_file_name = os.path.basename(url)\n    decompressed_data_file_name = data_file_name.replace('.gz', '')\n    CHECKSUM = 'c6cb0fd6e4322d3523e9afd87d5ed9dc'\n\n    def __init__(self, folder=None):\n        \"\"\"\n        Initializes the builder and orchestrates the data preparation workflow.\n\n        This constructor sets up the necessary paths and automatically triggers\n        the download, verification, and processing steps if the data is not\n        already present in the specified cache directory.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n        \"\"\"\n        super().__init__(None)\n\n        self.dataset_name = 'amazon_books'\n        self.version_name = '2018'\n\n        self._data_folder = folder if folder \\\n            else dataset_directory(self.dataset_name)\n        self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n            else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n\n        self.return_type = None\n\n        # check if the required files have been already downloaded\n        file_path = self.required_files()\n\n        if file_path is None:\n            file_path = self.download()\n            file_path = self.decompress(file_path)\n\n        print(f'files: {file_path}')\n\n        self.process(file_path)\n\n    def required_files(self):\n        \"\"\"\n        Checks for the presence of the required data file.\n\n        Returns:\n            (str) or None: The path to the required data file if it exists,\n                otherwise returns None.\n        \"\"\"\n        # compressed data file\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n        # uncompressed data file\n        uncompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n\n        # check if the file is there\n        if os.path.exists(uncompressed_file_path):\n            return uncompressed_file_path\n        # check if the compressed file is there\n        elif os.path.exists(file_path):\n            return self.decompress(file_path)\n        else:\n            return None\n\n    def decompress(self, path):\n        \"\"\"\n        Handles the decompression step.\n\n        For this 2018 version, the source file is already decompressed, so this\n        method simply returns the path to the file.\n\n        Args:\n            path (str): The file path of the source data file.\n\n        Returns:\n            (str): The path to the data file.\n        \"\"\"\n        # file already decompressed\n        return path\n\n    def download(self) -&gt; (str, str):\n        \"\"\"\n        Downloads the raw dataset file.\n\n        Returns:\n            (str): The local file path to the downloaded file.\n        \"\"\"\n        if not os.path.exists(self._raw_folder):\n            os.makedirs(self._raw_folder)\n            print('Created folder \\'{}\\''.format(self._raw_folder))\n\n        # download dataset file (compressed file)\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n        download_file(self.url, file_path, size=2140933459)\n\n        # decompress downloaded file\n        return file_path\n\n    def process(self, file_path) -&gt; str:\n        \"\"\"\n        Processes the raw data and loads it into the class.\n\n        This method reads the raw file, which does not contain a header row.\n        Columns are identified by their integer index. The data is then assigned\n        to the `self.data` attribute.\n\n        Args:\n            file_path (str): The path to the raw data file.\n\n        Returns:\n            (str): The path to the processed data file.\n        \"\"\"\n        verify_checksum(file_path, self.CHECKSUM)\n\n        from datarec.io import read_tabular\n\n        dataset = read_tabular(file_path, sep=',',\n                               user_col=0, item_col=1,\n                               rating_col=2, timestamp_col=3,\n                               header=None)\n        self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_books.amz_books_2018.AMZ_Books_2018.__init__","title":"<code>__init__(folder=None)</code>","text":"<p>Initializes the builder and orchestrates the data preparation workflow.</p> <p>This constructor sets up the necessary paths and automatically triggers the download, verification, and processing steps if the data is not already present in the specified cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> Source code in <code>datarec/datasets/amazon_books/amz_books_2018.py</code> <pre><code>def __init__(self, folder=None):\n    \"\"\"\n    Initializes the builder and orchestrates the data preparation workflow.\n\n    This constructor sets up the necessary paths and automatically triggers\n    the download, verification, and processing steps if the data is not\n    already present in the specified cache directory.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n    \"\"\"\n    super().__init__(None)\n\n    self.dataset_name = 'amazon_books'\n    self.version_name = '2018'\n\n    self._data_folder = folder if folder \\\n        else dataset_directory(self.dataset_name)\n    self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n        else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n\n    self.return_type = None\n\n    # check if the required files have been already downloaded\n    file_path = self.required_files()\n\n    if file_path is None:\n        file_path = self.download()\n        file_path = self.decompress(file_path)\n\n    print(f'files: {file_path}')\n\n    self.process(file_path)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_books.amz_books_2018.AMZ_Books_2018.required_files","title":"<code>required_files()</code>","text":"<p>Checks for the presence of the required data file.</p> <p>Returns:</p> Type Description <p>(str) or None: The path to the required data file if it exists, otherwise returns None.</p> Source code in <code>datarec/datasets/amazon_books/amz_books_2018.py</code> <pre><code>def required_files(self):\n    \"\"\"\n    Checks for the presence of the required data file.\n\n    Returns:\n        (str) or None: The path to the required data file if it exists,\n            otherwise returns None.\n    \"\"\"\n    # compressed data file\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n    # uncompressed data file\n    uncompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n\n    # check if the file is there\n    if os.path.exists(uncompressed_file_path):\n        return uncompressed_file_path\n    # check if the compressed file is there\n    elif os.path.exists(file_path):\n        return self.decompress(file_path)\n    else:\n        return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_books.amz_books_2018.AMZ_Books_2018.decompress","title":"<code>decompress(path)</code>","text":"<p>Handles the decompression step.</p> <p>For this 2018 version, the source file is already decompressed, so this method simply returns the path to the file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path of the source data file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path to the data file.</p> Source code in <code>datarec/datasets/amazon_books/amz_books_2018.py</code> <pre><code>def decompress(self, path):\n    \"\"\"\n    Handles the decompression step.\n\n    For this 2018 version, the source file is already decompressed, so this\n    method simply returns the path to the file.\n\n    Args:\n        path (str): The file path of the source data file.\n\n    Returns:\n        (str): The path to the data file.\n    \"\"\"\n    # file already decompressed\n    return path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_books.amz_books_2018.AMZ_Books_2018.download","title":"<code>download()</code>","text":"<p>Downloads the raw dataset file.</p> <p>Returns:</p> Type Description <code>str</code> <p>The local file path to the downloaded file.</p> Source code in <code>datarec/datasets/amazon_books/amz_books_2018.py</code> <pre><code>def download(self) -&gt; (str, str):\n    \"\"\"\n    Downloads the raw dataset file.\n\n    Returns:\n        (str): The local file path to the downloaded file.\n    \"\"\"\n    if not os.path.exists(self._raw_folder):\n        os.makedirs(self._raw_folder)\n        print('Created folder \\'{}\\''.format(self._raw_folder))\n\n    # download dataset file (compressed file)\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n    download_file(self.url, file_path, size=2140933459)\n\n    # decompress downloaded file\n    return file_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_books.amz_books_2018.AMZ_Books_2018.process","title":"<code>process(file_path)</code>","text":"<p>Processes the raw data and loads it into the class.</p> <p>This method reads the raw file, which does not contain a header row. Columns are identified by their integer index. The data is then assigned to the <code>self.data</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the raw data file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path to the processed data file.</p> Source code in <code>datarec/datasets/amazon_books/amz_books_2018.py</code> <pre><code>def process(self, file_path) -&gt; str:\n    \"\"\"\n    Processes the raw data and loads it into the class.\n\n    This method reads the raw file, which does not contain a header row.\n    Columns are identified by their integer index. The data is then assigned\n    to the `self.data` attribute.\n\n    Args:\n        file_path (str): The path to the raw data file.\n\n    Returns:\n        (str): The path to the processed data file.\n    \"\"\"\n    verify_checksum(file_path, self.CHECKSUM)\n\n    from datarec.io import read_tabular\n\n    dataset = read_tabular(file_path, sep=',',\n                           user_col=0, item_col=1,\n                           rating_col=2, timestamp_col=3,\n                           header=None)\n    self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_books.amz_books_2023.AMZ_Books_2023","title":"<code>AMZ_Books_2023</code>","text":"<p>               Bases: <code>DataRec</code></p> <p>Builder class for the Amazon Books dataset (2023 version).</p> <p>This class handles the logic for downloading, preparing, and loading the 2023 version of the Amazon Books dataset. It is not typically instantiated directly but is called by the <code>AmazonBooks</code> entry point class.</p> <p>The dataset is from the \"Bridging Language and Items for Retrieval and Recommendation\" paper and contains user ratings for books.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL from which the raw dataset is downloaded.</p> <code>CHECKSUM</code> <code>str</code> <p>The MD5 checksum to verify the integrity of the downloaded file.</p> Source code in <code>datarec/datasets/amazon_books/amz_books_2023.py</code> <pre><code>class AMZ_Books_2023(DataRec):\n    \"\"\"\n    Builder class for the Amazon Books dataset (2023 version).\n\n    This class handles the logic for downloading, preparing, and loading the\n    2023 version of the Amazon Books dataset. It is not typically instantiated\n    directly but is called by the `AmazonBooks` entry point class.\n\n    The dataset is from the \"Bridging Language and Items for Retrieval and\n    Recommendation\" paper and contains user ratings for books.\n\n    Attributes:\n        url (str): The URL from which the raw dataset is downloaded.\n        CHECKSUM (str): The MD5 checksum to verify the integrity of the downloaded file.\n    \"\"\"\n    url = 'https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/benchmark/0core/rating_only/Books.csv.gz'\n    data_file_name = os.path.basename(url)\n    decompressed_data_file_name = data_file_name.replace('.gz', '')\n    CHECKSUM = 'abc9f379ac0a77860ea0792b69ad0d5d'\n\n    def __init__(self, folder=None):\n        \"\"\"\n        Initializes the builder and orchestrates the data preparation workflow.\n\n        This constructor sets up the necessary paths and automatically triggers\n        the download, verification, and processing steps if the data is not\n        already present in the specified cache directory.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n        \"\"\"\n\n        super().__init__(None)\n\n        self.dataset_name = 'amazon_books'\n        self.version_name = '2023'\n\n        self._data_folder = folder if folder \\\n            else dataset_directory(self.dataset_name)\n        self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n            else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n\n        self.return_type = None\n\n        # check if the required files have been already downloaded\n        file_path = self.required_files()\n\n        if file_path is None:\n            file_path = self.download()\n            file_path = self.decompress(file_path)\n\n        print(f'files: {file_path}')\n\n        self.process(file_path)\n\n    def required_files(self):\n        \"\"\"\n        Checks for the presence of the required decompressed data file.\n\n        It first looks for the final, uncompressed file. If not found, it\n        looks for the compressed archive and decompresses it.\n\n        Returns:\n            (str or None): The path to the required data file if it exists or can be\n                created by decompression. Otherwise, returns None.\n        \"\"\"\n        # compressed data file\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n        # uncompressed data file\n        uncompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n\n        # check if the file is there\n        if os.path.exists(uncompressed_file_path):\n            return uncompressed_file_path\n        # check if the compressed file is there\n        elif os.path.exists(file_path):\n            return self.decompress(file_path)\n        else:\n            return None\n\n    def decompress(self, path):\n        \"\"\"\n        Decompresses the downloaded archive after verifying its checksum.\n\n        Args:\n            path (str): The file path of the compressed archive.\n\n        Returns:\n            (str): The path to the decompressed file.\n        \"\"\"\n        verify_checksum(path, self.CHECKSUM)\n\n        # decompress downloaded file\n        decompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n        return decompress_gz(path, decompressed_file_path)\n\n    def download(self) -&gt; (str, str):\n        \"\"\"\n        Downloads the raw dataset compressed archive.\n\n        Returns:\n            (str): The local file path to the downloaded archive.\n        \"\"\"\n        if not os.path.exists(self._raw_folder):\n            os.makedirs(self._raw_folder)\n            print('Created folder \\'{}\\''.format(self._raw_folder))\n\n        # download dataset file (compressed file)\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n        download_file(self.url, file_path)\n\n        # decompress downloaded file\n        return file_path\n\n    def process(self, file_path):\n        \"\"\"\n        Processes the raw data and loads it into the class.\n\n        This method reads the decompressed file, which includes a header,\n        into a pandas DataFrame and assigns it to the `self.data` attribute.\n\n        Args:\n            file_path (str): The path to the raw data file.\n\n        Returns:\n            (None): This method assigns the processed data to `self.data` directly.\n        \"\"\"\n        from datarec.io import read_tabular\n\n        dataset = read_tabular(file_path, sep=',',\n                               user_col='user_id', item_col='parent_asin',\n                               rating_col='rating', timestamp_col='timestamp',\n                               header=0)\n        self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_books.amz_books_2023.AMZ_Books_2023.__init__","title":"<code>__init__(folder=None)</code>","text":"<p>Initializes the builder and orchestrates the data preparation workflow.</p> <p>This constructor sets up the necessary paths and automatically triggers the download, verification, and processing steps if the data is not already present in the specified cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> Source code in <code>datarec/datasets/amazon_books/amz_books_2023.py</code> <pre><code>def __init__(self, folder=None):\n    \"\"\"\n    Initializes the builder and orchestrates the data preparation workflow.\n\n    This constructor sets up the necessary paths and automatically triggers\n    the download, verification, and processing steps if the data is not\n    already present in the specified cache directory.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n    \"\"\"\n\n    super().__init__(None)\n\n    self.dataset_name = 'amazon_books'\n    self.version_name = '2023'\n\n    self._data_folder = folder if folder \\\n        else dataset_directory(self.dataset_name)\n    self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n        else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n\n    self.return_type = None\n\n    # check if the required files have been already downloaded\n    file_path = self.required_files()\n\n    if file_path is None:\n        file_path = self.download()\n        file_path = self.decompress(file_path)\n\n    print(f'files: {file_path}')\n\n    self.process(file_path)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_books.amz_books_2023.AMZ_Books_2023.required_files","title":"<code>required_files()</code>","text":"<p>Checks for the presence of the required decompressed data file.</p> <p>It first looks for the final, uncompressed file. If not found, it looks for the compressed archive and decompresses it.</p> <p>Returns:</p> Type Description <code>str or None</code> <p>The path to the required data file if it exists or can be created by decompression. Otherwise, returns None.</p> Source code in <code>datarec/datasets/amazon_books/amz_books_2023.py</code> <pre><code>def required_files(self):\n    \"\"\"\n    Checks for the presence of the required decompressed data file.\n\n    It first looks for the final, uncompressed file. If not found, it\n    looks for the compressed archive and decompresses it.\n\n    Returns:\n        (str or None): The path to the required data file if it exists or can be\n            created by decompression. Otherwise, returns None.\n    \"\"\"\n    # compressed data file\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n    # uncompressed data file\n    uncompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n\n    # check if the file is there\n    if os.path.exists(uncompressed_file_path):\n        return uncompressed_file_path\n    # check if the compressed file is there\n    elif os.path.exists(file_path):\n        return self.decompress(file_path)\n    else:\n        return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_books.amz_books_2023.AMZ_Books_2023.decompress","title":"<code>decompress(path)</code>","text":"<p>Decompresses the downloaded archive after verifying its checksum.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path of the compressed archive.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path to the decompressed file.</p> Source code in <code>datarec/datasets/amazon_books/amz_books_2023.py</code> <pre><code>def decompress(self, path):\n    \"\"\"\n    Decompresses the downloaded archive after verifying its checksum.\n\n    Args:\n        path (str): The file path of the compressed archive.\n\n    Returns:\n        (str): The path to the decompressed file.\n    \"\"\"\n    verify_checksum(path, self.CHECKSUM)\n\n    # decompress downloaded file\n    decompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n    return decompress_gz(path, decompressed_file_path)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_books.amz_books_2023.AMZ_Books_2023.download","title":"<code>download()</code>","text":"<p>Downloads the raw dataset compressed archive.</p> <p>Returns:</p> Type Description <code>str</code> <p>The local file path to the downloaded archive.</p> Source code in <code>datarec/datasets/amazon_books/amz_books_2023.py</code> <pre><code>def download(self) -&gt; (str, str):\n    \"\"\"\n    Downloads the raw dataset compressed archive.\n\n    Returns:\n        (str): The local file path to the downloaded archive.\n    \"\"\"\n    if not os.path.exists(self._raw_folder):\n        os.makedirs(self._raw_folder)\n        print('Created folder \\'{}\\''.format(self._raw_folder))\n\n    # download dataset file (compressed file)\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n    download_file(self.url, file_path)\n\n    # decompress downloaded file\n    return file_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_books.amz_books_2023.AMZ_Books_2023.process","title":"<code>process(file_path)</code>","text":"<p>Processes the raw data and loads it into the class.</p> <p>This method reads the decompressed file, which includes a header, into a pandas DataFrame and assigns it to the <code>self.data</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the raw data file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This method assigns the processed data to <code>self.data</code> directly.</p> Source code in <code>datarec/datasets/amazon_books/amz_books_2023.py</code> <pre><code>def process(self, file_path):\n    \"\"\"\n    Processes the raw data and loads it into the class.\n\n    This method reads the decompressed file, which includes a header,\n    into a pandas DataFrame and assigns it to the `self.data` attribute.\n\n    Args:\n        file_path (str): The path to the raw data file.\n\n    Returns:\n        (None): This method assigns the processed data to `self.data` directly.\n    \"\"\"\n    from datarec.io import read_tabular\n\n    dataset = read_tabular(file_path, sep=',',\n                           user_col='user_id', item_col='parent_asin',\n                           rating_col='rating', timestamp_col='timestamp',\n                           header=0)\n    self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#amazon-clothing","title":"Amazon Clothing","text":"<p>Entry point for loading different versions of the Amazon Clothing dataset.</p> <p>Builder class for the 2018 version of the Amazon Clothing dataset.</p> <p>Builder class for the 2023 version of the Amazon Clothing dataset.</p>"},{"location":"documentation/datasets/#datarec.datasets.amazon_clothing.amz_clothing.AmazonClothing","title":"<code>AmazonClothing</code>","text":"<p>Entry point class to load various versions of the Amazon Clothing dataset.</p> <p>This class provides a single, convenient interface for accessing the dataset. Based on the <code>version</code> parameter, it selects and returns the appropriate dataset builder for either the 2018 or 2023 version.</p> <p>The dataset contains product reviews and metadata for the category \"Clothing, Shoes and Jewelry\" from Amazon.</p> <p>The default version is 'latest', which currently corresponds to the '2023' version.</p> <p>Examples:</p> <p>To load the latest version:</p> <pre><code>&gt;&gt;&gt; data_loader = AmazonClothing()\n</code></pre> <p>To load a specific version:</p> <pre><code>&gt;&gt;&gt; data_loader = AmazonClothing(version='2018')\n</code></pre> Source code in <code>datarec/datasets/amazon_clothing/amz_clothing.py</code> <pre><code>class AmazonClothing:\n    \"\"\"\n    Entry point class to load various versions of the Amazon Clothing dataset.\n\n    This class provides a single, convenient interface for accessing the dataset.\n    Based on the `version` parameter, it selects and returns the appropriate\n    dataset builder for either the 2018 or 2023 version.\n\n    The dataset contains product reviews and metadata for the category\n    \"Clothing, Shoes and Jewelry\" from Amazon.\n\n    The default version is 'latest', which currently corresponds to the '2023' version.\n\n    Examples:\n        To load the latest version:\n        &gt;&gt;&gt; data_loader = AmazonClothing()\n\n        To load a specific version:\n        &gt;&gt;&gt; data_loader = AmazonClothing(version='2018')\n    \"\"\"\n    latest_version = '2023'\n\n    def __new__(cls, version: str = 'latest', **kwargs) -&gt; DataRec:\n        \"\"\"\n        Initializes and returns the specified version of the Amazon Clothing dataset builder.\n\n        This method acts as a dispatcher, instantiating the correct builder class\n        that handles the downloading, caching, and loading for a specific dataset version.\n\n        Args:\n            version (str): The version of the dataset to load. Supported versions\n                include '2023', '2018', and 'latest'. Defaults to 'latest'.\n            **kwargs: Additional keyword arguments (not currently used for this dataset).\n\n        Returns:\n            (DataRec): An instance of the appropriate dataset builder class\n                (e.g., `AmazonClothing_2023`), populated with data.\n\n        Raises:\n            ValueError: If an unsupported version string is provided.\n        \"\"\"\n\n        versions = {'2023': AmazonClothing_2023,\n                    '2018': AmazonClothing_2018}\n        if version == 'latest':\n            version = cls.latest_version\n        if version in versions:\n            return versions[version]()\n        else:\n            raise ValueError(f\"Amazon Clothing {version}: Unsupported version \\n Supported version:\"\n                             f\"\\n \\t version \\t name \"\n                             f\"\\n \\t 2023 \\t Amazon Clothing 2023\"\n                             f\"\\n \\t 2018 \\t Amazon Clothing 2018\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_clothing.amz_clothing.AmazonClothing.__new__","title":"<code>__new__(version='latest', **kwargs)</code>","text":"<p>Initializes and returns the specified version of the Amazon Clothing dataset builder.</p> <p>This method acts as a dispatcher, instantiating the correct builder class that handles the downloading, caching, and loading for a specific dataset version.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>The version of the dataset to load. Supported versions include '2023', '2018', and 'latest'. Defaults to 'latest'.</p> <code>'latest'</code> <code>**kwargs</code> <p>Additional keyword arguments (not currently used for this dataset).</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataRec</code> <p>An instance of the appropriate dataset builder class (e.g., <code>AmazonClothing_2023</code>), populated with data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported version string is provided.</p> Source code in <code>datarec/datasets/amazon_clothing/amz_clothing.py</code> <pre><code>def __new__(cls, version: str = 'latest', **kwargs) -&gt; DataRec:\n    \"\"\"\n    Initializes and returns the specified version of the Amazon Clothing dataset builder.\n\n    This method acts as a dispatcher, instantiating the correct builder class\n    that handles the downloading, caching, and loading for a specific dataset version.\n\n    Args:\n        version (str): The version of the dataset to load. Supported versions\n            include '2023', '2018', and 'latest'. Defaults to 'latest'.\n        **kwargs: Additional keyword arguments (not currently used for this dataset).\n\n    Returns:\n        (DataRec): An instance of the appropriate dataset builder class\n            (e.g., `AmazonClothing_2023`), populated with data.\n\n    Raises:\n        ValueError: If an unsupported version string is provided.\n    \"\"\"\n\n    versions = {'2023': AmazonClothing_2023,\n                '2018': AmazonClothing_2018}\n    if version == 'latest':\n        version = cls.latest_version\n    if version in versions:\n        return versions[version]()\n    else:\n        raise ValueError(f\"Amazon Clothing {version}: Unsupported version \\n Supported version:\"\n                         f\"\\n \\t version \\t name \"\n                         f\"\\n \\t 2023 \\t Amazon Clothing 2023\"\n                         f\"\\n \\t 2018 \\t Amazon Clothing 2018\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_clothing.amz_clothing_2018.AmazonClothing_2018","title":"<code>AmazonClothing_2018</code>","text":"<p>               Bases: <code>DataRec</code></p> <p>Builder class for the Amazon Clothing dataset (2018 version).</p> <p>This class handles the logic for downloading, preparing, and loading the 2018 version of the \"Clothing, Shoes and Jewelry\" dataset. It is not typically instantiated directly but is called by the <code>AmazonClothing</code> entry point class.</p> <p>The raw data is provided as a single, uncompressed CSV file without a header.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL from which the raw dataset is downloaded.</p> <code>CHECKSUM</code> <code>str</code> <p>The MD5 checksum to verify the integrity of the downloaded file.</p> Source code in <code>datarec/datasets/amazon_clothing/amz_clothing_2018.py</code> <pre><code>class AmazonClothing_2018(DataRec):\n    \"\"\"\n    Builder class for the Amazon Clothing dataset (2018 version).\n\n    This class handles the logic for downloading, preparing, and loading the\n    2018 version of the \"Clothing, Shoes and Jewelry\" dataset. It is not typically\n    instantiated directly but is called by the `AmazonClothing` entry point class.\n\n    The raw data is provided as a single, uncompressed CSV file without a header.\n\n    Attributes:\n        url (str): The URL from which the raw dataset is downloaded.\n        CHECKSUM (str): The MD5 checksum to verify the integrity of the downloaded file.\n    \"\"\"\n    url = 'https://mcauleylab.ucsd.edu/public_datasets/data/amazon_v2/categoryFilesSmall/Clothing_Shoes_and_Jewelry.csv'\n    data_file_name = os.path.basename(url)\n    decompressed_data_file_name = data_file_name\n    CHECKSUM = '27b4184d3d4b5e443d31dc608badf927'\n\n    def __init__(self, folder=None):\n        \"\"\"\n        Initializes the builder and orchestrates the data preparation workflow.\n\n        This constructor sets up the necessary paths and automatically triggers\n        the download, verification, and processing steps if the data is not\n        already present in the specified cache directory.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n        \"\"\"\n        super().__init__(None)\n\n        self.dataset_name = 'amazon_clothing'\n        self.version_name = '2018'\n\n        self._data_folder = folder if folder \\\n            else dataset_directory(self.dataset_name)\n        self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n            else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n\n        self.return_type = None\n\n        # check if the required files have been already downloaded\n        file_path = self.required_files()\n\n        if file_path is None:\n            file_path = self.download()\n            file_path = self.decompress(file_path)\n\n        print(f'files: {file_path}')\n\n        self.process(file_path)\n\n    def required_files(self):\n        \"\"\"\n        Checks for the presence of the required data file.\n\n        Returns:\n            (str or None): The path to the required data file if it exists,\n                otherwise returns None.\n        \"\"\"\n        # compressed data file\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n        uncompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n\n        # check if the file is there\n        if os.path.exists(uncompressed_file_path):\n            return uncompressed_file_path\n        # check if the compressed file is there\n        elif os.path.exists(file_path):\n            return self.decompress(file_path)\n        else:\n            return None\n\n    def decompress(self, path):\n        \"\"\"\n        Handles the decompression step.\n\n        For this 2018 version, the source file is already decompressed, so this\n        method simply returns the path to the file.\n\n        Args:\n            path (str): The file path of the source data file.\n\n        Returns:\n            (str): The path to the data file.\n        \"\"\"\n        # already decompressed\n        return path\n\n    def download(self) -&gt; (str, str):\n        \"\"\"\n        Downloads the raw dataset file.\n\n        Returns:\n            (str): The local file path to the downloaded file.\n        \"\"\"\n        if not os.path.exists(self._raw_folder):\n            os.makedirs(self._raw_folder)\n            print('Created folder \\'{}\\''.format(self._raw_folder))\n\n        # download dataset file (compressed file)\n        file_name = os.path.basename(self.url)\n        file_path = os.path.join(self._raw_folder, file_name)\n        download_file(self.url, file_path, size=1395554400)\n\n        return file_path\n\n    def process(self, file_path):\n        \"\"\"\n        Processes the raw data and loads it into the class.\n\n        This method reads the raw file, which has no header. Columns are\n        identified by their integer index. The data is then assigned to the\n        `self.data` attribute.\n\n        Args:\n            file_path (str): The path to the raw data file.\n\n        Returns:\n            (None): This method assigns the processed data to `self.data` directly.\n        \"\"\"\n        verify_checksum(file_path, self.CHECKSUM)\n\n        from datarec.io import read_tabular\n\n        dataset = read_tabular(file_path, sep=',',\n                               user_col=0, item_col=1,\n                               rating_col=2, timestamp_col=3,\n                               header=None)\n        self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_clothing.amz_clothing_2018.AmazonClothing_2018.__init__","title":"<code>__init__(folder=None)</code>","text":"<p>Initializes the builder and orchestrates the data preparation workflow.</p> <p>This constructor sets up the necessary paths and automatically triggers the download, verification, and processing steps if the data is not already present in the specified cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> Source code in <code>datarec/datasets/amazon_clothing/amz_clothing_2018.py</code> <pre><code>def __init__(self, folder=None):\n    \"\"\"\n    Initializes the builder and orchestrates the data preparation workflow.\n\n    This constructor sets up the necessary paths and automatically triggers\n    the download, verification, and processing steps if the data is not\n    already present in the specified cache directory.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n    \"\"\"\n    super().__init__(None)\n\n    self.dataset_name = 'amazon_clothing'\n    self.version_name = '2018'\n\n    self._data_folder = folder if folder \\\n        else dataset_directory(self.dataset_name)\n    self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n        else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n\n    self.return_type = None\n\n    # check if the required files have been already downloaded\n    file_path = self.required_files()\n\n    if file_path is None:\n        file_path = self.download()\n        file_path = self.decompress(file_path)\n\n    print(f'files: {file_path}')\n\n    self.process(file_path)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_clothing.amz_clothing_2018.AmazonClothing_2018.required_files","title":"<code>required_files()</code>","text":"<p>Checks for the presence of the required data file.</p> <p>Returns:</p> Type Description <code>str or None</code> <p>The path to the required data file if it exists, otherwise returns None.</p> Source code in <code>datarec/datasets/amazon_clothing/amz_clothing_2018.py</code> <pre><code>def required_files(self):\n    \"\"\"\n    Checks for the presence of the required data file.\n\n    Returns:\n        (str or None): The path to the required data file if it exists,\n            otherwise returns None.\n    \"\"\"\n    # compressed data file\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n    uncompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n\n    # check if the file is there\n    if os.path.exists(uncompressed_file_path):\n        return uncompressed_file_path\n    # check if the compressed file is there\n    elif os.path.exists(file_path):\n        return self.decompress(file_path)\n    else:\n        return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_clothing.amz_clothing_2018.AmazonClothing_2018.decompress","title":"<code>decompress(path)</code>","text":"<p>Handles the decompression step.</p> <p>For this 2018 version, the source file is already decompressed, so this method simply returns the path to the file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path of the source data file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path to the data file.</p> Source code in <code>datarec/datasets/amazon_clothing/amz_clothing_2018.py</code> <pre><code>def decompress(self, path):\n    \"\"\"\n    Handles the decompression step.\n\n    For this 2018 version, the source file is already decompressed, so this\n    method simply returns the path to the file.\n\n    Args:\n        path (str): The file path of the source data file.\n\n    Returns:\n        (str): The path to the data file.\n    \"\"\"\n    # already decompressed\n    return path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_clothing.amz_clothing_2018.AmazonClothing_2018.download","title":"<code>download()</code>","text":"<p>Downloads the raw dataset file.</p> <p>Returns:</p> Type Description <code>str</code> <p>The local file path to the downloaded file.</p> Source code in <code>datarec/datasets/amazon_clothing/amz_clothing_2018.py</code> <pre><code>def download(self) -&gt; (str, str):\n    \"\"\"\n    Downloads the raw dataset file.\n\n    Returns:\n        (str): The local file path to the downloaded file.\n    \"\"\"\n    if not os.path.exists(self._raw_folder):\n        os.makedirs(self._raw_folder)\n        print('Created folder \\'{}\\''.format(self._raw_folder))\n\n    # download dataset file (compressed file)\n    file_name = os.path.basename(self.url)\n    file_path = os.path.join(self._raw_folder, file_name)\n    download_file(self.url, file_path, size=1395554400)\n\n    return file_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_clothing.amz_clothing_2018.AmazonClothing_2018.process","title":"<code>process(file_path)</code>","text":"<p>Processes the raw data and loads it into the class.</p> <p>This method reads the raw file, which has no header. Columns are identified by their integer index. The data is then assigned to the <code>self.data</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the raw data file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This method assigns the processed data to <code>self.data</code> directly.</p> Source code in <code>datarec/datasets/amazon_clothing/amz_clothing_2018.py</code> <pre><code>def process(self, file_path):\n    \"\"\"\n    Processes the raw data and loads it into the class.\n\n    This method reads the raw file, which has no header. Columns are\n    identified by their integer index. The data is then assigned to the\n    `self.data` attribute.\n\n    Args:\n        file_path (str): The path to the raw data file.\n\n    Returns:\n        (None): This method assigns the processed data to `self.data` directly.\n    \"\"\"\n    verify_checksum(file_path, self.CHECKSUM)\n\n    from datarec.io import read_tabular\n\n    dataset = read_tabular(file_path, sep=',',\n                           user_col=0, item_col=1,\n                           rating_col=2, timestamp_col=3,\n                           header=None)\n    self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_clothing.amz_clothing_2023.AmazonClothing_2023","title":"<code>AmazonClothing_2023</code>","text":"<p>               Bases: <code>DataRec</code></p> <p>Builder class for the Amazon Clothing dataset (2023 version).</p> <p>This class handles the logic for downloading, preparing, and loading the 2023 version of the \"Clothing, Shoes and Jewelry\" dataset. It is not typically instantiated directly but is called by the <code>AmazonClothing</code> entry point class.</p> <p>The dataset is from the \"Bridging Language and Items for Retrieval and Recommendation\" paper and is provided as a compressed CSV file.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL from which the raw dataset is downloaded.</p> <code>CHECKSUM</code> <code>str</code> <p>The MD5 checksum to verify the integrity of the downloaded file.</p> Source code in <code>datarec/datasets/amazon_clothing/amz_clothing_2023.py</code> <pre><code>class AmazonClothing_2023(DataRec):\n    \"\"\"\n    Builder class for the Amazon Clothing dataset (2023 version).\n\n    This class handles the logic for downloading, preparing, and loading the\n    2023 version of the \"Clothing, Shoes and Jewelry\" dataset. It is not typically\n    instantiated directly but is called by the `AmazonClothing` entry point class.\n\n    The dataset is from the \"Bridging Language and Items for Retrieval and\n    Recommendation\" paper and is provided as a compressed CSV file.\n\n    Attributes:\n        url (str): The URL from which the raw dataset is downloaded.\n        CHECKSUM (str): The MD5 checksum to verify the integrity of the downloaded file.\n    \"\"\"\n    url = 'https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/benchmark/0core/rating_only/Clothing_Shoes_and_Jewelry.csv.gz'\n    data_file_name = os.path.basename(url)\n    decompressed_data_file_name = data_file_name.replace('.gz', '')\n    CHECKSUM = 'cfb9400815ce8fb6430130b7d439c203'\n\n    def __init__(self, folder=None):\n        \"\"\"\n        Initializes the builder and orchestrates the data preparation workflow.\n\n        This constructor sets up the necessary paths and automatically triggers\n        the download, verification, and processing steps if the data is not\n        already present in the specified cache directory.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n        \"\"\"\n        super().__init__(None)\n\n        self.dataset_name = 'amazon_clothing'\n        self.version_name = '2023'\n\n        self._data_folder = folder if folder \\\n            else dataset_directory(self.dataset_name)\n        self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n            else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n\n        self.return_type = None\n\n        # check if the required files have been already downloaded\n        file_path = self.required_files()\n\n        if file_path is None:\n            file_path = self.download()\n            file_path = self.decompress(file_path)\n\n        print(f'files: {file_path}')\n\n        self.process(file_path)\n\n    def required_files(self):\n        \"\"\"\n        Checks for the presence of the required decompressed data file.\n\n        It first looks for the final, uncompressed file. If not found, it\n        looks for the compressed archive and decompresses it.\n\n        Returns:\n            (str or None): The path to the required data file if it exists or can be\n                created by decompression. Otherwise, returns None.\n        \"\"\"\n        # compressed data file\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n        uncompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n\n        # check if the file is there\n        if os.path.exists(uncompressed_file_path):\n            return uncompressed_file_path\n        # check if the compressed file is there\n        elif os.path.exists(file_path):\n            return self.decompress(file_path)\n        else:\n            return None\n\n    def decompress(self, path):\n        \"\"\"\n        Decompresses the downloaded archive after verifying its checksum.\n\n        Args:\n            path (str): The file path of the compressed archive.\n\n        Returns:\n            (str): The path to the decompressed file.\n        \"\"\"\n        verify_checksum(path, self.CHECKSUM)\n\n        # decompress downloaded file\n        decompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n        return decompress_gz(path, decompressed_file_path)\n\n    def download(self) -&gt; (str, str):\n        \"\"\"\n        Downloads the raw dataset compressed archive.\n\n        Returns:\n            (str): The local file path to the downloaded archive.\n        \"\"\"\n        if not os.path.exists(self._raw_folder):\n            os.makedirs(self._raw_folder)\n            print('Created folder \\'{}\\''.format(self._raw_folder))\n\n        # download dataset file (compressed file)\n        file_name = os.path.basename(self.url)\n        file_path = os.path.join(self._raw_folder, file_name)\n        download_file(self.url, file_path, size=1395554400)\n\n        return file_path\n\n    def process(self, file_path):\n        \"\"\"\n        Processes the raw data and loads it into the class.\n\n        This method reads the decompressed file, which includes a header row,\n        into a pandas DataFrame and assigns it to the `self.data` attribute.\n\n        Args:\n            file_path (str): The path to the raw data file.\n\n        Returns:\n            (None): This method assigns the processed data to `self.data` directly.\n        \"\"\"\n\n        from datarec.io import read_tabular\n\n        dataset = read_tabular(file_path, sep=',',\n                               user_col='user_id', item_col='parent_asin',\n                               rating_col='rating', timestamp_col='timestamp',\n                               header=0)\n        self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_clothing.amz_clothing_2023.AmazonClothing_2023.__init__","title":"<code>__init__(folder=None)</code>","text":"<p>Initializes the builder and orchestrates the data preparation workflow.</p> <p>This constructor sets up the necessary paths and automatically triggers the download, verification, and processing steps if the data is not already present in the specified cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> Source code in <code>datarec/datasets/amazon_clothing/amz_clothing_2023.py</code> <pre><code>def __init__(self, folder=None):\n    \"\"\"\n    Initializes the builder and orchestrates the data preparation workflow.\n\n    This constructor sets up the necessary paths and automatically triggers\n    the download, verification, and processing steps if the data is not\n    already present in the specified cache directory.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n    \"\"\"\n    super().__init__(None)\n\n    self.dataset_name = 'amazon_clothing'\n    self.version_name = '2023'\n\n    self._data_folder = folder if folder \\\n        else dataset_directory(self.dataset_name)\n    self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n        else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n\n    self.return_type = None\n\n    # check if the required files have been already downloaded\n    file_path = self.required_files()\n\n    if file_path is None:\n        file_path = self.download()\n        file_path = self.decompress(file_path)\n\n    print(f'files: {file_path}')\n\n    self.process(file_path)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_clothing.amz_clothing_2023.AmazonClothing_2023.required_files","title":"<code>required_files()</code>","text":"<p>Checks for the presence of the required decompressed data file.</p> <p>It first looks for the final, uncompressed file. If not found, it looks for the compressed archive and decompresses it.</p> <p>Returns:</p> Type Description <code>str or None</code> <p>The path to the required data file if it exists or can be created by decompression. Otherwise, returns None.</p> Source code in <code>datarec/datasets/amazon_clothing/amz_clothing_2023.py</code> <pre><code>def required_files(self):\n    \"\"\"\n    Checks for the presence of the required decompressed data file.\n\n    It first looks for the final, uncompressed file. If not found, it\n    looks for the compressed archive and decompresses it.\n\n    Returns:\n        (str or None): The path to the required data file if it exists or can be\n            created by decompression. Otherwise, returns None.\n    \"\"\"\n    # compressed data file\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n    uncompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n\n    # check if the file is there\n    if os.path.exists(uncompressed_file_path):\n        return uncompressed_file_path\n    # check if the compressed file is there\n    elif os.path.exists(file_path):\n        return self.decompress(file_path)\n    else:\n        return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_clothing.amz_clothing_2023.AmazonClothing_2023.decompress","title":"<code>decompress(path)</code>","text":"<p>Decompresses the downloaded archive after verifying its checksum.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path of the compressed archive.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path to the decompressed file.</p> Source code in <code>datarec/datasets/amazon_clothing/amz_clothing_2023.py</code> <pre><code>def decompress(self, path):\n    \"\"\"\n    Decompresses the downloaded archive after verifying its checksum.\n\n    Args:\n        path (str): The file path of the compressed archive.\n\n    Returns:\n        (str): The path to the decompressed file.\n    \"\"\"\n    verify_checksum(path, self.CHECKSUM)\n\n    # decompress downloaded file\n    decompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n    return decompress_gz(path, decompressed_file_path)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_clothing.amz_clothing_2023.AmazonClothing_2023.download","title":"<code>download()</code>","text":"<p>Downloads the raw dataset compressed archive.</p> <p>Returns:</p> Type Description <code>str</code> <p>The local file path to the downloaded archive.</p> Source code in <code>datarec/datasets/amazon_clothing/amz_clothing_2023.py</code> <pre><code>def download(self) -&gt; (str, str):\n    \"\"\"\n    Downloads the raw dataset compressed archive.\n\n    Returns:\n        (str): The local file path to the downloaded archive.\n    \"\"\"\n    if not os.path.exists(self._raw_folder):\n        os.makedirs(self._raw_folder)\n        print('Created folder \\'{}\\''.format(self._raw_folder))\n\n    # download dataset file (compressed file)\n    file_name = os.path.basename(self.url)\n    file_path = os.path.join(self._raw_folder, file_name)\n    download_file(self.url, file_path, size=1395554400)\n\n    return file_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_clothing.amz_clothing_2023.AmazonClothing_2023.process","title":"<code>process(file_path)</code>","text":"<p>Processes the raw data and loads it into the class.</p> <p>This method reads the decompressed file, which includes a header row, into a pandas DataFrame and assigns it to the <code>self.data</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the raw data file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This method assigns the processed data to <code>self.data</code> directly.</p> Source code in <code>datarec/datasets/amazon_clothing/amz_clothing_2023.py</code> <pre><code>def process(self, file_path):\n    \"\"\"\n    Processes the raw data and loads it into the class.\n\n    This method reads the decompressed file, which includes a header row,\n    into a pandas DataFrame and assigns it to the `self.data` attribute.\n\n    Args:\n        file_path (str): The path to the raw data file.\n\n    Returns:\n        (None): This method assigns the processed data to `self.data` directly.\n    \"\"\"\n\n    from datarec.io import read_tabular\n\n    dataset = read_tabular(file_path, sep=',',\n                           user_col='user_id', item_col='parent_asin',\n                           rating_col='rating', timestamp_col='timestamp',\n                           header=0)\n    self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#amazon-sports-and-outdoors","title":"Amazon Sports and Outdoors","text":"<p>Entry point for loading different versions of the Amazon Sports and Outdoors dataset.</p> <p>Builder class for the 2018 version of the Amazon Sports and Outdoors dataset.</p> <p>Builder class for the 2023 version of the Amazon Sports and Outdoors dataset.</p>"},{"location":"documentation/datasets/#datarec.datasets.amazon_sports_and_outdoors.amz_sports.AmazonSportsOutdoors","title":"<code>AmazonSportsOutdoors</code>","text":"<p>Entry point class to load various versions of the Amazon Sports and Outdoors dataset.</p> <p>This class provides a single, convenient interface for accessing the dataset. Based on the <code>version</code> parameter, it selects and returns the appropriate dataset builder for either the 2018 or 2023 version.</p> <p>The dataset contains product reviews and metadata for the category \"Sports and Outdoors\" from Amazon.</p> <p>The default version is 'latest', which currently corresponds to the '2023' version.</p> <p>Examples:</p> <p>To load the latest version:</p> <pre><code>&gt;&gt;&gt; data_loader = AmazonSportsOutdoors()\n</code></pre> <p>To load a specific version:</p> <pre><code>&gt;&gt;&gt; data_loader = AmazonSportsOutdoors(version='2018')\n</code></pre> Source code in <code>datarec/datasets/amazon_sports_and_outdoors/amz_sports.py</code> <pre><code>class AmazonSportsOutdoors:\n    \"\"\"\n    Entry point class to load various versions of the Amazon Sports and Outdoors dataset.\n\n    This class provides a single, convenient interface for accessing the dataset.\n    Based on the `version` parameter, it selects and returns the appropriate\n    dataset builder for either the 2018 or 2023 version.\n\n    The dataset contains product reviews and metadata for the category\n    \"Sports and Outdoors\" from Amazon.\n\n    The default version is 'latest', which currently corresponds to the '2023' version.\n\n    Examples:\n        To load the latest version:\n        &gt;&gt;&gt; data_loader = AmazonSportsOutdoors()\n\n        To load a specific version:\n        &gt;&gt;&gt; data_loader = AmazonSportsOutdoors(version='2018')\n    \"\"\"\n    latest_version = '2023'\n\n    def __new__(cls, version: str = 'latest', **kwargs) -&gt; DataRec:\n        \"\"\"\n        Initializes and returns the specified version of the dataset builder.\n\n        This method acts as a dispatcher, instantiating the correct builder class\n        that handles the downloading, caching, and loading for a specific dataset version.\n\n        Args:\n            version (str): The version of the dataset to load. Supported versions\n                include '2023', '2018', and 'latest'. Defaults to 'latest'.\n            **kwargs: Additional keyword arguments (not currently used for this dataset).\n\n        Returns:\n            (DataRec): An instance of the appropriate dataset builder class\n                (e.g., `AMZ_SportsOutdoors_2023`), populated with data.\n\n        Raises:\n            ValueError: If an unsupported version string is provided.\n        \"\"\"\n        versions = {'2023': AMZ_SportsOutdoors_2023,\n                    '2018': AMZ_SportsOutdoors_2018}\n        if version == 'latest':\n            version = cls.latest_version\n        if version in versions:\n            return versions[version]()\n        else:\n            raise ValueError(f\"Amazon Sports and Outdoors {version}: Unsupported version \\n Supported version:\"\n                             f\"\\n \\t version \\t name \"\n                             f\"\\n \\t 2023 \\t Amazon Sports and Outdoors 2023\"\n                             f\"\\n \\t 2018 \\t Amazon Sports and Outdoors 2018\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_sports_and_outdoors.amz_sports.AmazonSportsOutdoors.__new__","title":"<code>__new__(version='latest', **kwargs)</code>","text":"<p>Initializes and returns the specified version of the dataset builder.</p> <p>This method acts as a dispatcher, instantiating the correct builder class that handles the downloading, caching, and loading for a specific dataset version.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>The version of the dataset to load. Supported versions include '2023', '2018', and 'latest'. Defaults to 'latest'.</p> <code>'latest'</code> <code>**kwargs</code> <p>Additional keyword arguments (not currently used for this dataset).</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataRec</code> <p>An instance of the appropriate dataset builder class (e.g., <code>AMZ_SportsOutdoors_2023</code>), populated with data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported version string is provided.</p> Source code in <code>datarec/datasets/amazon_sports_and_outdoors/amz_sports.py</code> <pre><code>def __new__(cls, version: str = 'latest', **kwargs) -&gt; DataRec:\n    \"\"\"\n    Initializes and returns the specified version of the dataset builder.\n\n    This method acts as a dispatcher, instantiating the correct builder class\n    that handles the downloading, caching, and loading for a specific dataset version.\n\n    Args:\n        version (str): The version of the dataset to load. Supported versions\n            include '2023', '2018', and 'latest'. Defaults to 'latest'.\n        **kwargs: Additional keyword arguments (not currently used for this dataset).\n\n    Returns:\n        (DataRec): An instance of the appropriate dataset builder class\n            (e.g., `AMZ_SportsOutdoors_2023`), populated with data.\n\n    Raises:\n        ValueError: If an unsupported version string is provided.\n    \"\"\"\n    versions = {'2023': AMZ_SportsOutdoors_2023,\n                '2018': AMZ_SportsOutdoors_2018}\n    if version == 'latest':\n        version = cls.latest_version\n    if version in versions:\n        return versions[version]()\n    else:\n        raise ValueError(f\"Amazon Sports and Outdoors {version}: Unsupported version \\n Supported version:\"\n                         f\"\\n \\t version \\t name \"\n                         f\"\\n \\t 2023 \\t Amazon Sports and Outdoors 2023\"\n                         f\"\\n \\t 2018 \\t Amazon Sports and Outdoors 2018\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_sports_and_outdoors.amz_sports_2018.AMZ_SportsOutdoors_2018","title":"<code>AMZ_SportsOutdoors_2018</code>","text":"<p>               Bases: <code>DataRec</code></p> <p>Builder class for the Amazon Sports and Outdoors dataset (2018 version).</p> <p>This class handles the logic for downloading, preparing, and loading the 2018 version of the \"Sports and Outdoors\" dataset. It is not typically instantiated directly but is called by the <code>AmazonSportsOutdoors</code> entry point class.</p> <p>The raw data is provided as a single, uncompressed CSV file without a header.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL from which the raw dataset is downloaded.</p> <code>CHECKSUM</code> <code>str</code> <p>The MD5 checksum to verify the integrity of the downloaded file.</p> Source code in <code>datarec/datasets/amazon_sports_and_outdoors/amz_sports_2018.py</code> <pre><code>class AMZ_SportsOutdoors_2018(DataRec):\n    \"\"\"\n    Builder class for the Amazon Sports and Outdoors dataset (2018 version).\n\n    This class handles the logic for downloading, preparing, and loading the\n    2018 version of the \"Sports and Outdoors\" dataset. It is not typically\n    instantiated directly but is called by the `AmazonSportsOutdoors` entry point class.\n\n    The raw data is provided as a single, uncompressed CSV file without a header.\n\n    Attributes:\n        url (str): The URL from which the raw dataset is downloaded.\n        CHECKSUM (str): The MD5 checksum to verify the integrity of the downloaded file.\n    \"\"\"\n    url = 'https://mcauleylab.ucsd.edu/public_datasets/data/amazon_v2/categoryFilesSmall/Sports_and_Outdoors.csv'\n    data_file_name = os.path.basename(url)\n    decompressed_data_file_name = data_file_name\n    CHECKSUM = '1ed3d6c7a89f3c78fa260b2419753785'\n\n    def __init__(self, folder=None):\n        \"\"\"\n        Initializes the builder and orchestrates the data preparation workflow.\n\n        This constructor sets up the necessary paths and automatically triggers\n        the download, verification, and processing steps if the data is not\n        already present in the specified cache directory.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n        \"\"\"\n        super().__init__(None)\n\n        self.dataset_name = 'amazon_sports_and_outdoors'\n        self.version_name = '2018'\n\n        self._data_folder = folder if folder \\\n            else dataset_directory(self.dataset_name)\n        self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n            else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n\n        self.return_type = None\n\n        # check if the required files have been already downloaded\n        file_path = self.required_files()\n\n        if file_path is None:\n            file_path = self.download()\n            file_path = self.decompress(file_path)\n\n        print(f'files: {file_path}')\n\n        self.process(file_path)\n\n    def required_files(self):\n        \"\"\"\n        Checks for the presence of the required data file.\n\n        Returns:\n            (str or None): The path to the required data file if it exists,\n                otherwise returns None.\n        \"\"\"\n        # compressed data file\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n        # uncompressed data file\n        uncompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n\n        # check if the file is there\n        if os.path.exists(uncompressed_file_path):\n            return uncompressed_file_path\n        # check if the compressed file is there\n        elif os.path.exists(file_path):\n            return self.decompress(file_path)\n        else:\n            return None\n\n    def decompress(self, path):\n        \"\"\"\n        Handles the decompression step.\n\n        For this 2018 version, the source file is already decompressed, so this\n        method simply returns the path to the file.\n\n        Args:\n            path (str): The file path of the source data file.\n\n        Returns:\n            (str): The path to the data file.\n        \"\"\"\n        # already decompressed\n        return path\n\n\n    def download(self) -&gt; (str, str):\n        \"\"\"\n        Downloads the raw dataset file.\n\n        Returns:\n            (str): The local file path to the downloaded file.\n        \"\"\"\n        if not os.path.exists(self._raw_folder):\n            os.makedirs(self._raw_folder)\n            print('Created folder \\'{}\\''.format(self._raw_folder))\n\n        # download dataset file (compressed file)\n        file_name = os.path.basename(self.url)\n        file_path = os.path.join(self._raw_folder, file_name)\n        download_file(self.url, file_path)\n\n        return file_path\n\n    def process(self, file_path):\n        \"\"\"\n        Processes the raw data and loads it into the class.\n\n        This method reads the raw file, which has no header. Columns are\n        identified by their integer index. The data is then assigned to the\n        `self.data` attribute after checksum verification.\n\n        Args:\n            file_path (str): The path to the raw data file.\n\n        Returns:\n            (None): This method assigns the processed data to `self.data` directly.\n        \"\"\"\n        verify_checksum(file_path, self.CHECKSUM)\n\n        from datarec.io import read_tabular\n\n        dataset = read_tabular(file_path, sep=',',\n                               user_col=0, item_col=1,\n                               rating_col=2, timestamp_col=3,\n                               header=None)\n        self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_sports_and_outdoors.amz_sports_2018.AMZ_SportsOutdoors_2018.__init__","title":"<code>__init__(folder=None)</code>","text":"<p>Initializes the builder and orchestrates the data preparation workflow.</p> <p>This constructor sets up the necessary paths and automatically triggers the download, verification, and processing steps if the data is not already present in the specified cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> Source code in <code>datarec/datasets/amazon_sports_and_outdoors/amz_sports_2018.py</code> <pre><code>def __init__(self, folder=None):\n    \"\"\"\n    Initializes the builder and orchestrates the data preparation workflow.\n\n    This constructor sets up the necessary paths and automatically triggers\n    the download, verification, and processing steps if the data is not\n    already present in the specified cache directory.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n    \"\"\"\n    super().__init__(None)\n\n    self.dataset_name = 'amazon_sports_and_outdoors'\n    self.version_name = '2018'\n\n    self._data_folder = folder if folder \\\n        else dataset_directory(self.dataset_name)\n    self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n        else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n\n    self.return_type = None\n\n    # check if the required files have been already downloaded\n    file_path = self.required_files()\n\n    if file_path is None:\n        file_path = self.download()\n        file_path = self.decompress(file_path)\n\n    print(f'files: {file_path}')\n\n    self.process(file_path)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_sports_and_outdoors.amz_sports_2018.AMZ_SportsOutdoors_2018.required_files","title":"<code>required_files()</code>","text":"<p>Checks for the presence of the required data file.</p> <p>Returns:</p> Type Description <code>str or None</code> <p>The path to the required data file if it exists, otherwise returns None.</p> Source code in <code>datarec/datasets/amazon_sports_and_outdoors/amz_sports_2018.py</code> <pre><code>def required_files(self):\n    \"\"\"\n    Checks for the presence of the required data file.\n\n    Returns:\n        (str or None): The path to the required data file if it exists,\n            otherwise returns None.\n    \"\"\"\n    # compressed data file\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n    # uncompressed data file\n    uncompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n\n    # check if the file is there\n    if os.path.exists(uncompressed_file_path):\n        return uncompressed_file_path\n    # check if the compressed file is there\n    elif os.path.exists(file_path):\n        return self.decompress(file_path)\n    else:\n        return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_sports_and_outdoors.amz_sports_2018.AMZ_SportsOutdoors_2018.decompress","title":"<code>decompress(path)</code>","text":"<p>Handles the decompression step.</p> <p>For this 2018 version, the source file is already decompressed, so this method simply returns the path to the file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path of the source data file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path to the data file.</p> Source code in <code>datarec/datasets/amazon_sports_and_outdoors/amz_sports_2018.py</code> <pre><code>def decompress(self, path):\n    \"\"\"\n    Handles the decompression step.\n\n    For this 2018 version, the source file is already decompressed, so this\n    method simply returns the path to the file.\n\n    Args:\n        path (str): The file path of the source data file.\n\n    Returns:\n        (str): The path to the data file.\n    \"\"\"\n    # already decompressed\n    return path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_sports_and_outdoors.amz_sports_2018.AMZ_SportsOutdoors_2018.download","title":"<code>download()</code>","text":"<p>Downloads the raw dataset file.</p> <p>Returns:</p> Type Description <code>str</code> <p>The local file path to the downloaded file.</p> Source code in <code>datarec/datasets/amazon_sports_and_outdoors/amz_sports_2018.py</code> <pre><code>def download(self) -&gt; (str, str):\n    \"\"\"\n    Downloads the raw dataset file.\n\n    Returns:\n        (str): The local file path to the downloaded file.\n    \"\"\"\n    if not os.path.exists(self._raw_folder):\n        os.makedirs(self._raw_folder)\n        print('Created folder \\'{}\\''.format(self._raw_folder))\n\n    # download dataset file (compressed file)\n    file_name = os.path.basename(self.url)\n    file_path = os.path.join(self._raw_folder, file_name)\n    download_file(self.url, file_path)\n\n    return file_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_sports_and_outdoors.amz_sports_2018.AMZ_SportsOutdoors_2018.process","title":"<code>process(file_path)</code>","text":"<p>Processes the raw data and loads it into the class.</p> <p>This method reads the raw file, which has no header. Columns are identified by their integer index. The data is then assigned to the <code>self.data</code> attribute after checksum verification.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the raw data file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This method assigns the processed data to <code>self.data</code> directly.</p> Source code in <code>datarec/datasets/amazon_sports_and_outdoors/amz_sports_2018.py</code> <pre><code>def process(self, file_path):\n    \"\"\"\n    Processes the raw data and loads it into the class.\n\n    This method reads the raw file, which has no header. Columns are\n    identified by their integer index. The data is then assigned to the\n    `self.data` attribute after checksum verification.\n\n    Args:\n        file_path (str): The path to the raw data file.\n\n    Returns:\n        (None): This method assigns the processed data to `self.data` directly.\n    \"\"\"\n    verify_checksum(file_path, self.CHECKSUM)\n\n    from datarec.io import read_tabular\n\n    dataset = read_tabular(file_path, sep=',',\n                           user_col=0, item_col=1,\n                           rating_col=2, timestamp_col=3,\n                           header=None)\n    self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_sports_and_outdoors.amz_sports_2023.AMZ_SportsOutdoors_2023","title":"<code>AMZ_SportsOutdoors_2023</code>","text":"<p>               Bases: <code>DataRec</code></p> <p>Builder class for the Amazon Sports and Outdoors dataset (2023 version).</p> <p>This class handles the logic for downloading, preparing, and loading the 2023 version of the \"Sports and Outdoors\" dataset. It is not typically instantiated directly but is called by the <code>AmazonSportsOutdoors</code> entry point class.</p> <p>The dataset is from the \"Bridging Language and Items for Retrieval and Recommendation\" paper and is provided as a compressed CSV file with a header.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL from which the raw dataset is downloaded.</p> <code>CHECKSUM</code> <code>str</code> <p>The MD5 checksum to verify the integrity of the downloaded file.</p> Source code in <code>datarec/datasets/amazon_sports_and_outdoors/amz_sports_2023.py</code> <pre><code>class AMZ_SportsOutdoors_2023(DataRec):\n    \"\"\"\n    Builder class for the Amazon Sports and Outdoors dataset (2023 version).\n\n    This class handles the logic for downloading, preparing, and loading the\n    2023 version of the \"Sports and Outdoors\" dataset. It is not typically\n    instantiated directly but is called by the `AmazonSportsOutdoors` entry point class.\n\n    The dataset is from the \"Bridging Language and Items for Retrieval and\n    Recommendation\" paper and is provided as a compressed CSV file with a header.\n\n    Attributes:\n        url (str): The URL from which the raw dataset is downloaded.\n        CHECKSUM (str): The MD5 checksum to verify the integrity of the downloaded file.\n    \"\"\"\n    url = 'https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/benchmark/0core/rating_only/Sports_and_Outdoors.csv.gz'\n    data_file_name = os.path.basename(url)\n    decompressed_data_file_name = data_file_name.replace('.gz', '')\n    CHECKSUM = '75e1dfbb3b3014fab914832b734922e6'\n\n    def __init__(self, folder=None):\n        \"\"\"\n        Initializes the builder and orchestrates the data preparation workflow.\n\n        This constructor sets up the necessary paths and automatically triggers\n        the download, verification, and processing steps if the data is not\n        already present in the specified cache directory.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n        \"\"\"\n        super().__init__(None)\n\n        self.dataset_name = 'amazon_sports_and_outdoors'\n        self.version_name = '2023'\n\n        self._data_folder = folder if folder \\\n            else dataset_directory(self.dataset_name)\n        self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n            else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n\n        self.return_type = None\n\n        # check if the required files have been already downloaded\n        file_path = self.required_files()\n\n        if file_path is None:\n            file_path = self.download()\n            file_path = self.decompress(file_path)\n\n        print(f'files: {file_path}')\n\n        self.process(file_path)\n\n    def required_files(self):\n        \"\"\"\n        Checks for the presence of the required decompressed data file.\n\n        It first looks for the final, uncompressed file. If not found, it\n        looks for the compressed archive and decompresses it.\n\n        Returns:\n            (str or None): The path to the required data file if it exists or can be\n                created by decompression. Otherwise, returns None.\n        \"\"\"\n        # compressed data file\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n        # uncompressed data file\n        uncompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n\n        # check if the file is there\n        if os.path.exists(uncompressed_file_path):\n            return uncompressed_file_path\n        # check if the compressed file is there\n        elif os.path.exists(file_path):\n            return self.decompress(file_path)\n        else:\n            return None\n\n    def decompress(self, path):\n        \"\"\"\n        Decompresses the downloaded archive after verifying its checksum.\n\n        Args:\n            path (str): The file path of the compressed archive.\n\n        Returns:\n            (str): The path to the decompressed file.\n        \"\"\"\n        verify_checksum(path, self.CHECKSUM)\n\n        # decompress downloaded file\n        decompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n        return decompress_gz(path, decompressed_file_path)\n\n\n    def download(self) -&gt; (str, str):\n        \"\"\"\n        Downloads the raw dataset compressed archive.\n\n        Returns:\n            (str): The local file path to the downloaded archive.\n        \"\"\"\n        if not os.path.exists(self._raw_folder):\n            os.makedirs(self._raw_folder)\n            print('Created folder \\'{}\\''.format(self._raw_folder))\n\n        # download dataset file (compressed file)\n        file_name = os.path.basename(self.url)\n        file_path = os.path.join(self._raw_folder, file_name)\n        download_file(self.url, file_path)\n\n        return file_path\n\n    def process(self, file_path):\n        \"\"\"\n        Processes the raw data and loads it into the class.\n\n        This method reads the decompressed file, which includes a header row,\n        into a pandas DataFrame and assigns it to the `self.data` attribute.\n\n        Args:\n            file_path (str): The path to the raw data file.\n\n        Returns:\n            (None): This method assigns the processed data to `self.data` directly.\n        \"\"\"\n\n        from datarec.io import read_tabular\n\n        dataset = read_tabular(file_path, sep=',',\n                               user_col='user_id', item_col='parent_asin',\n                               rating_col='rating', timestamp_col='timestamp',\n                               header=0)\n        self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_sports_and_outdoors.amz_sports_2023.AMZ_SportsOutdoors_2023.__init__","title":"<code>__init__(folder=None)</code>","text":"<p>Initializes the builder and orchestrates the data preparation workflow.</p> <p>This constructor sets up the necessary paths and automatically triggers the download, verification, and processing steps if the data is not already present in the specified cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> Source code in <code>datarec/datasets/amazon_sports_and_outdoors/amz_sports_2023.py</code> <pre><code>def __init__(self, folder=None):\n    \"\"\"\n    Initializes the builder and orchestrates the data preparation workflow.\n\n    This constructor sets up the necessary paths and automatically triggers\n    the download, verification, and processing steps if the data is not\n    already present in the specified cache directory.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n    \"\"\"\n    super().__init__(None)\n\n    self.dataset_name = 'amazon_sports_and_outdoors'\n    self.version_name = '2023'\n\n    self._data_folder = folder if folder \\\n        else dataset_directory(self.dataset_name)\n    self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n        else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n\n    self.return_type = None\n\n    # check if the required files have been already downloaded\n    file_path = self.required_files()\n\n    if file_path is None:\n        file_path = self.download()\n        file_path = self.decompress(file_path)\n\n    print(f'files: {file_path}')\n\n    self.process(file_path)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_sports_and_outdoors.amz_sports_2023.AMZ_SportsOutdoors_2023.required_files","title":"<code>required_files()</code>","text":"<p>Checks for the presence of the required decompressed data file.</p> <p>It first looks for the final, uncompressed file. If not found, it looks for the compressed archive and decompresses it.</p> <p>Returns:</p> Type Description <code>str or None</code> <p>The path to the required data file if it exists or can be created by decompression. Otherwise, returns None.</p> Source code in <code>datarec/datasets/amazon_sports_and_outdoors/amz_sports_2023.py</code> <pre><code>def required_files(self):\n    \"\"\"\n    Checks for the presence of the required decompressed data file.\n\n    It first looks for the final, uncompressed file. If not found, it\n    looks for the compressed archive and decompresses it.\n\n    Returns:\n        (str or None): The path to the required data file if it exists or can be\n            created by decompression. Otherwise, returns None.\n    \"\"\"\n    # compressed data file\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n    # uncompressed data file\n    uncompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n\n    # check if the file is there\n    if os.path.exists(uncompressed_file_path):\n        return uncompressed_file_path\n    # check if the compressed file is there\n    elif os.path.exists(file_path):\n        return self.decompress(file_path)\n    else:\n        return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_sports_and_outdoors.amz_sports_2023.AMZ_SportsOutdoors_2023.decompress","title":"<code>decompress(path)</code>","text":"<p>Decompresses the downloaded archive after verifying its checksum.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path of the compressed archive.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path to the decompressed file.</p> Source code in <code>datarec/datasets/amazon_sports_and_outdoors/amz_sports_2023.py</code> <pre><code>def decompress(self, path):\n    \"\"\"\n    Decompresses the downloaded archive after verifying its checksum.\n\n    Args:\n        path (str): The file path of the compressed archive.\n\n    Returns:\n        (str): The path to the decompressed file.\n    \"\"\"\n    verify_checksum(path, self.CHECKSUM)\n\n    # decompress downloaded file\n    decompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n    return decompress_gz(path, decompressed_file_path)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_sports_and_outdoors.amz_sports_2023.AMZ_SportsOutdoors_2023.download","title":"<code>download()</code>","text":"<p>Downloads the raw dataset compressed archive.</p> <p>Returns:</p> Type Description <code>str</code> <p>The local file path to the downloaded archive.</p> Source code in <code>datarec/datasets/amazon_sports_and_outdoors/amz_sports_2023.py</code> <pre><code>def download(self) -&gt; (str, str):\n    \"\"\"\n    Downloads the raw dataset compressed archive.\n\n    Returns:\n        (str): The local file path to the downloaded archive.\n    \"\"\"\n    if not os.path.exists(self._raw_folder):\n        os.makedirs(self._raw_folder)\n        print('Created folder \\'{}\\''.format(self._raw_folder))\n\n    # download dataset file (compressed file)\n    file_name = os.path.basename(self.url)\n    file_path = os.path.join(self._raw_folder, file_name)\n    download_file(self.url, file_path)\n\n    return file_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_sports_and_outdoors.amz_sports_2023.AMZ_SportsOutdoors_2023.process","title":"<code>process(file_path)</code>","text":"<p>Processes the raw data and loads it into the class.</p> <p>This method reads the decompressed file, which includes a header row, into a pandas DataFrame and assigns it to the <code>self.data</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the raw data file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This method assigns the processed data to <code>self.data</code> directly.</p> Source code in <code>datarec/datasets/amazon_sports_and_outdoors/amz_sports_2023.py</code> <pre><code>def process(self, file_path):\n    \"\"\"\n    Processes the raw data and loads it into the class.\n\n    This method reads the decompressed file, which includes a header row,\n    into a pandas DataFrame and assigns it to the `self.data` attribute.\n\n    Args:\n        file_path (str): The path to the raw data file.\n\n    Returns:\n        (None): This method assigns the processed data to `self.data` directly.\n    \"\"\"\n\n    from datarec.io import read_tabular\n\n    dataset = read_tabular(file_path, sep=',',\n                           user_col='user_id', item_col='parent_asin',\n                           rating_col='rating', timestamp_col='timestamp',\n                           header=0)\n    self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#amazon-toys-and-games","title":"Amazon Toys and Games","text":"<p>Entry point for loading different versions of the Amazon Toys and Games dataset.</p> <p>Builder class for the 2018 version of the Amazon Toys and Games dataset.</p> <p>Builder class for the 2023 version of the Amazon Toys and Games dataset.</p>"},{"location":"documentation/datasets/#datarec.datasets.amazon_toys_and_games.amz_toys.AmazonToysGames","title":"<code>AmazonToysGames</code>","text":"<p>Entry point class to load various versions of the Amazon Toys and Games dataset.</p> <p>This class provides a single, convenient interface for accessing the dataset. Based on the <code>version</code> parameter, it selects and returns the appropriate dataset builder for either the 2018 or 2023 version.</p> <p>The dataset contains product reviews and metadata for the category \"Toys and Games\" from Amazon.</p> <p>The default version is 'latest', which currently corresponds to the '2023' version.</p> <p>Examples:</p> <p>To load the latest version:</p> <pre><code>&gt;&gt;&gt; data_loader = AmazonToysGames()\n</code></pre> <p>To load a specific version:</p> <pre><code>&gt;&gt;&gt; data_loader = AmazonToysGames(version='2018')\n</code></pre> Source code in <code>datarec/datasets/amazon_toys_and_games/amz_toys.py</code> <pre><code>class AmazonToysGames:\n    \"\"\"\n    Entry point class to load various versions of the Amazon Toys and Games dataset.\n\n    This class provides a single, convenient interface for accessing the dataset.\n    Based on the `version` parameter, it selects and returns the appropriate\n    dataset builder for either the 2018 or 2023 version.\n\n    The dataset contains product reviews and metadata for the category\n    \"Toys and Games\" from Amazon.\n\n    The default version is 'latest', which currently corresponds to the '2023' version.\n\n    Examples:\n        To load the latest version:\n        &gt;&gt;&gt; data_loader = AmazonToysGames()\n\n        To load a specific version:\n        &gt;&gt;&gt; data_loader = AmazonToysGames(version='2018')\n    \"\"\"\n    latest_version = '2023'\n\n    def __new__(cls, version: str = 'latest', **kwargs) -&gt; DataRec:\n        \"\"\"\n        Initializes and returns the specified version of the dataset builder.\n\n        This method acts as a dispatcher, instantiating the correct builder class\n        that handles the downloading, caching, and loading for a specific dataset version.\n\n        Args:\n            version (str): The version of the dataset to load. Supported versions\n                include '2023', '2018', and 'latest'. Defaults to 'latest'.\n            **kwargs: Additional keyword arguments (not currently used for this dataset).\n\n        Returns:\n            (DataRec): An instance of the appropriate dataset builder class\n                (e.g., `AMZ_ToysGames_2023`), populated with data.\n\n        Raises:\n            ValueError: If an unsupported version string is provided.\n        \"\"\"\n        versions = {'2023': AMZ_ToysGames_2023,\n                    '2018': AMZ_ToysGames_2018}\n        if version == 'latest':\n            version = cls.latest_version\n        if version in versions:\n            return versions[version]()\n        else:\n            raise ValueError(f\"Amazon Toys and Games {version}: Unsupported version \\n Supported version:\"\n                             f\"\\n \\t version \\t name \"\n                             f\"\\n \\t 2023 \\t Amazon Toys and Games 2023\"\n                             f\"\\n \\t 2018 \\t Amazon Toys and Games 2018\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_toys_and_games.amz_toys.AmazonToysGames.__new__","title":"<code>__new__(version='latest', **kwargs)</code>","text":"<p>Initializes and returns the specified version of the dataset builder.</p> <p>This method acts as a dispatcher, instantiating the correct builder class that handles the downloading, caching, and loading for a specific dataset version.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>The version of the dataset to load. Supported versions include '2023', '2018', and 'latest'. Defaults to 'latest'.</p> <code>'latest'</code> <code>**kwargs</code> <p>Additional keyword arguments (not currently used for this dataset).</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataRec</code> <p>An instance of the appropriate dataset builder class (e.g., <code>AMZ_ToysGames_2023</code>), populated with data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported version string is provided.</p> Source code in <code>datarec/datasets/amazon_toys_and_games/amz_toys.py</code> <pre><code>def __new__(cls, version: str = 'latest', **kwargs) -&gt; DataRec:\n    \"\"\"\n    Initializes and returns the specified version of the dataset builder.\n\n    This method acts as a dispatcher, instantiating the correct builder class\n    that handles the downloading, caching, and loading for a specific dataset version.\n\n    Args:\n        version (str): The version of the dataset to load. Supported versions\n            include '2023', '2018', and 'latest'. Defaults to 'latest'.\n        **kwargs: Additional keyword arguments (not currently used for this dataset).\n\n    Returns:\n        (DataRec): An instance of the appropriate dataset builder class\n            (e.g., `AMZ_ToysGames_2023`), populated with data.\n\n    Raises:\n        ValueError: If an unsupported version string is provided.\n    \"\"\"\n    versions = {'2023': AMZ_ToysGames_2023,\n                '2018': AMZ_ToysGames_2018}\n    if version == 'latest':\n        version = cls.latest_version\n    if version in versions:\n        return versions[version]()\n    else:\n        raise ValueError(f\"Amazon Toys and Games {version}: Unsupported version \\n Supported version:\"\n                         f\"\\n \\t version \\t name \"\n                         f\"\\n \\t 2023 \\t Amazon Toys and Games 2023\"\n                         f\"\\n \\t 2018 \\t Amazon Toys and Games 2018\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_toys_and_games.amz_toys_2018.AMZ_ToysGames_2018","title":"<code>AMZ_ToysGames_2018</code>","text":"<p>               Bases: <code>DataRec</code></p> <p>Builder class for the Amazon Toys and Games dataset (2018 version).</p> <p>This class handles the logic for downloading, preparing, and loading the 2018 version of the \"Toys and Games\" dataset. It is not typically instantiated directly but is called by the <code>AmazonToysGames</code> entry point class.</p> <p>The raw data is provided as a single, uncompressed CSV file without a header.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL from which the raw dataset is downloaded.</p> <code>CHECKSUM</code> <code>str</code> <p>The MD5 checksum to verify the integrity of the downloaded file.</p> Source code in <code>datarec/datasets/amazon_toys_and_games/amz_toys_2018.py</code> <pre><code>class AMZ_ToysGames_2018(DataRec):\n    \"\"\"\n    Builder class for the Amazon Toys and Games dataset (2018 version).\n\n    This class handles the logic for downloading, preparing, and loading the\n    2018 version of the \"Toys and Games\" dataset. It is not typically\n    instantiated directly but is called by the `AmazonToysGames` entry point class.\n\n    The raw data is provided as a single, uncompressed CSV file without a header.\n\n    Attributes:\n        url (str): The URL from which the raw dataset is downloaded.\n        CHECKSUM (str): The MD5 checksum to verify the integrity of the downloaded file.\n    \"\"\"\n    url = 'https://mcauleylab.ucsd.edu/public_datasets/data/amazon_v2/categoryFilesSmall/Toys_and_Games.csv'\n    data_file_name = os.path.basename(url)\n    decompressed_data_file_name = data_file_name\n    CHECKSUM = '3e3f0c05d880403de6601f22398ccd78'\n\n    def __init__(self, folder=None):\n        \"\"\"\n        Initializes the builder and orchestrates the data preparation workflow.\n\n        This constructor sets up the necessary paths and automatically triggers\n        the download, verification, and processing steps if the data is not\n        already present in the specified cache directory.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n        \"\"\"\n        super().__init__(None)\n\n        self.dataset_name = 'amazon_toys_and_games'\n        self.version_name = '2018'\n\n        self._data_folder = folder if folder \\\n            else dataset_directory(self.dataset_name)\n        self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n            else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n\n        self.return_type = None\n\n        # check if the required files have been already downloaded\n        file_path = self.required_files()\n\n        if file_path is None:\n            file_path = self.download()\n            file_path = self.decompress(file_path)\n\n        print(f'files: {file_path}')\n\n        self.process(file_path)\n\n    def required_files(self):\n        \"\"\"\n        Checks for the presence of the required data file.\n\n        Returns:\n            (str or None): The path to the required data file if it exists,\n                otherwise returns None.\n        \"\"\"\n        # compressed data file\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n        uncompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n\n        # check if the file is there\n        if os.path.exists(uncompressed_file_path):\n            return uncompressed_file_path\n        # check if the compressed file is there\n        elif os.path.exists(file_path):\n            return self.decompress(file_path)\n        else:\n            return None\n\n    def decompress(self, path):\n        \"\"\"\n        Handles the decompression step.\n\n        For this 2018 version, the source file is already decompressed, so this\n        method simply returns the path to the file.\n\n        Args:\n            path (str): The file path of the source data file.\n\n        Returns:\n            (str): The path to the data file.\n        \"\"\"\n        # decompress downloaded file\n        return path\n\n    def download(self) -&gt; (str, str):\n        \"\"\"\n        Downloads the raw dataset file.\n\n        Returns:\n            (str): The local file path to the downloaded file.\n        \"\"\"\n        if not os.path.exists(self._raw_folder):\n            os.makedirs(self._raw_folder)\n            print('Created folder \\'{}\\''.format(self._raw_folder))\n\n        # download dataset file (compressed file)\n        file_name = os.path.basename(self.url)\n        file_path = os.path.join(self._raw_folder, file_name)\n        download_file(self.url, file_path, size=388191962)\n\n        return file_path\n\n    def process(self, file_path):\n        \"\"\"\n        Processes the raw data and loads it into the class.\n\n        This method reads the raw file, which has no header. Columns are\n        identified by their integer index. The data is then assigned to the\n        `self.data` attribute after checksum verification.\n\n        Args:\n            file_path (str): The path to the raw data file.\n\n        Returns:\n            (None): This method assigns the processed data to `self.data` directly.\n        \"\"\"\n        verify_checksum(file_path, self.CHECKSUM)\n\n        from datarec.io import read_tabular\n\n        dataset = read_tabular(file_path, sep=',',\n                               user_col=0, item_col=1,\n                               rating_col=2, timestamp_col=3,\n                               header=None)\n        self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_toys_and_games.amz_toys_2018.AMZ_ToysGames_2018.__init__","title":"<code>__init__(folder=None)</code>","text":"<p>Initializes the builder and orchestrates the data preparation workflow.</p> <p>This constructor sets up the necessary paths and automatically triggers the download, verification, and processing steps if the data is not already present in the specified cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> Source code in <code>datarec/datasets/amazon_toys_and_games/amz_toys_2018.py</code> <pre><code>def __init__(self, folder=None):\n    \"\"\"\n    Initializes the builder and orchestrates the data preparation workflow.\n\n    This constructor sets up the necessary paths and automatically triggers\n    the download, verification, and processing steps if the data is not\n    already present in the specified cache directory.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n    \"\"\"\n    super().__init__(None)\n\n    self.dataset_name = 'amazon_toys_and_games'\n    self.version_name = '2018'\n\n    self._data_folder = folder if folder \\\n        else dataset_directory(self.dataset_name)\n    self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n        else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n\n    self.return_type = None\n\n    # check if the required files have been already downloaded\n    file_path = self.required_files()\n\n    if file_path is None:\n        file_path = self.download()\n        file_path = self.decompress(file_path)\n\n    print(f'files: {file_path}')\n\n    self.process(file_path)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_toys_and_games.amz_toys_2018.AMZ_ToysGames_2018.required_files","title":"<code>required_files()</code>","text":"<p>Checks for the presence of the required data file.</p> <p>Returns:</p> Type Description <code>str or None</code> <p>The path to the required data file if it exists, otherwise returns None.</p> Source code in <code>datarec/datasets/amazon_toys_and_games/amz_toys_2018.py</code> <pre><code>def required_files(self):\n    \"\"\"\n    Checks for the presence of the required data file.\n\n    Returns:\n        (str or None): The path to the required data file if it exists,\n            otherwise returns None.\n    \"\"\"\n    # compressed data file\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n    uncompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n\n    # check if the file is there\n    if os.path.exists(uncompressed_file_path):\n        return uncompressed_file_path\n    # check if the compressed file is there\n    elif os.path.exists(file_path):\n        return self.decompress(file_path)\n    else:\n        return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_toys_and_games.amz_toys_2018.AMZ_ToysGames_2018.decompress","title":"<code>decompress(path)</code>","text":"<p>Handles the decompression step.</p> <p>For this 2018 version, the source file is already decompressed, so this method simply returns the path to the file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path of the source data file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path to the data file.</p> Source code in <code>datarec/datasets/amazon_toys_and_games/amz_toys_2018.py</code> <pre><code>def decompress(self, path):\n    \"\"\"\n    Handles the decompression step.\n\n    For this 2018 version, the source file is already decompressed, so this\n    method simply returns the path to the file.\n\n    Args:\n        path (str): The file path of the source data file.\n\n    Returns:\n        (str): The path to the data file.\n    \"\"\"\n    # decompress downloaded file\n    return path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_toys_and_games.amz_toys_2018.AMZ_ToysGames_2018.download","title":"<code>download()</code>","text":"<p>Downloads the raw dataset file.</p> <p>Returns:</p> Type Description <code>str</code> <p>The local file path to the downloaded file.</p> Source code in <code>datarec/datasets/amazon_toys_and_games/amz_toys_2018.py</code> <pre><code>def download(self) -&gt; (str, str):\n    \"\"\"\n    Downloads the raw dataset file.\n\n    Returns:\n        (str): The local file path to the downloaded file.\n    \"\"\"\n    if not os.path.exists(self._raw_folder):\n        os.makedirs(self._raw_folder)\n        print('Created folder \\'{}\\''.format(self._raw_folder))\n\n    # download dataset file (compressed file)\n    file_name = os.path.basename(self.url)\n    file_path = os.path.join(self._raw_folder, file_name)\n    download_file(self.url, file_path, size=388191962)\n\n    return file_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_toys_and_games.amz_toys_2018.AMZ_ToysGames_2018.process","title":"<code>process(file_path)</code>","text":"<p>Processes the raw data and loads it into the class.</p> <p>This method reads the raw file, which has no header. Columns are identified by their integer index. The data is then assigned to the <code>self.data</code> attribute after checksum verification.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the raw data file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This method assigns the processed data to <code>self.data</code> directly.</p> Source code in <code>datarec/datasets/amazon_toys_and_games/amz_toys_2018.py</code> <pre><code>def process(self, file_path):\n    \"\"\"\n    Processes the raw data and loads it into the class.\n\n    This method reads the raw file, which has no header. Columns are\n    identified by their integer index. The data is then assigned to the\n    `self.data` attribute after checksum verification.\n\n    Args:\n        file_path (str): The path to the raw data file.\n\n    Returns:\n        (None): This method assigns the processed data to `self.data` directly.\n    \"\"\"\n    verify_checksum(file_path, self.CHECKSUM)\n\n    from datarec.io import read_tabular\n\n    dataset = read_tabular(file_path, sep=',',\n                           user_col=0, item_col=1,\n                           rating_col=2, timestamp_col=3,\n                           header=None)\n    self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_toys_and_games.amz_toys_2023.AMZ_ToysGames_2023","title":"<code>AMZ_ToysGames_2023</code>","text":"<p>               Bases: <code>DataRec</code></p> <p>Builder class for the Amazon Toys and Games dataset (2023 version).</p> <p>This class handles the logic for downloading, preparing, and loading the 2023 version of the \"Toys and Games\" dataset. It is not typically instantiated directly but is called by the <code>AmazonToysGames</code> entry point class.</p> <p>The dataset is from the \"Bridging Language and Items for Retrieval and Recommendation\" paper and is provided as a compressed CSV file with a header.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL from which the raw dataset is downloaded.</p> <code>CHECKSUM</code> <code>str</code> <p>The MD5 checksum to verify the integrity of the downloaded file.</p> Source code in <code>datarec/datasets/amazon_toys_and_games/amz_toys_2023.py</code> <pre><code>class AMZ_ToysGames_2023(DataRec):\n    \"\"\"\n    Builder class for the Amazon Toys and Games dataset (2023 version).\n\n    This class handles the logic for downloading, preparing, and loading the\n    2023 version of the \"Toys and Games\" dataset. It is not typically\n    instantiated directly but is called by the `AmazonToysGames` entry point class.\n\n    The dataset is from the \"Bridging Language and Items for Retrieval and\n    Recommendation\" paper and is provided as a compressed CSV file with a header.\n\n    Attributes:\n        url (str): The URL from which the raw dataset is downloaded.\n        CHECKSUM (str): The MD5 checksum to verify the integrity of the downloaded file.\n    \"\"\"\n    url = 'https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/benchmark/0core/rating_only/Toys_and_Games.csv.gz'\n    data_file_name = os.path.basename(url)\n    decompressed_data_file_name = data_file_name.replace('.gz', '')\n    CHECKSUM = '542250672811854e9803d90b1f52cc14'\n\n    def __init__(self, folder=None):\n        \"\"\"\n        Initializes the builder and orchestrates the data preparation workflow.\n\n        This constructor sets up the necessary paths and automatically triggers\n        the download, verification, and processing steps if the data is not\n        already present in the specified cache directory.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n        \"\"\"\n        super().__init__(None)\n\n        self.dataset_name = 'amazon_toys_and_games'\n        self.version_name = '2023'\n\n        self._data_folder = folder if folder \\\n            else dataset_directory(self.dataset_name)\n        self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n            else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n\n        self.return_type = None\n\n        # check if the required files have been already downloaded\n        file_path = self.required_files()\n\n        if file_path is None:\n            file_path = self.download()\n            file_path = self.decompress(file_path)\n\n        print(f'files: {file_path}')\n\n        self.process(file_path)\n\n    def required_files(self):\n        \"\"\"\n        Checks for the presence of the required decompressed data file.\n\n        It first looks for the final, uncompressed file. If not found, it\n        looks for the compressed archive and decompresses it.\n\n        Returns:\n            (str or None): The path to the required data file if it exists or can be\n                created by decompression. Otherwise, returns None.\n        \"\"\"\n        # compressed data file\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n        uncompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n\n        # check if the file is there\n        if os.path.exists(uncompressed_file_path):\n            return uncompressed_file_path\n        # check if the compressed file is there\n        elif os.path.exists(file_path):\n            return self.decompress(file_path)\n        else:\n            return None\n\n    def decompress(self, path):\n        \"\"\"\n        Decompresses the downloaded archive after verifying its checksum.\n\n        Args:\n            path (str): The file path of the compressed archive.\n\n        Returns:\n            (str): The path to the decompressed file.\n        \"\"\"\n        verify_checksum(path, self.CHECKSUM)\n\n        # decompress downloaded file\n        decompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n        return decompress_gz(path, decompressed_file_path)\n\n\n    def download(self) -&gt; (str, str):\n        \"\"\"\n        Downloads the raw dataset compressed archive.\n\n        Returns:\n            (str): The local file path to the downloaded archive.\n        \"\"\"\n        if not os.path.exists(self._raw_folder):\n            os.makedirs(self._raw_folder)\n            print('Created folder \\'{}\\''.format(self._raw_folder))\n\n        # download dataset file (compressed file)\n        file_name = os.path.basename(self.url)\n        file_path = os.path.join(self._raw_folder, file_name)\n        download_file(self.url, file_path, size=388191962)\n\n        return file_path\n\n    def process(self, file_path):\n        \"\"\"\n        Processes the raw data and loads it into the class.\n\n        This method reads the decompressed file, which includes a header row,\n        into a pandas DataFrame and assigns it to the `self.data` attribute.\n\n        Args:\n            file_path (str): The path to the raw data file.\n\n        Returns:\n            (None): This method assigns the processed data to `self.data` directly.\n        \"\"\"\n\n        from datarec.io import read_tabular\n\n        dataset = read_tabular(file_path, sep=',',\n                               user_col='user_id', item_col='parent_asin',\n                               rating_col='rating', timestamp_col='timestamp',\n                               header=0)\n        self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_toys_and_games.amz_toys_2023.AMZ_ToysGames_2023.__init__","title":"<code>__init__(folder=None)</code>","text":"<p>Initializes the builder and orchestrates the data preparation workflow.</p> <p>This constructor sets up the necessary paths and automatically triggers the download, verification, and processing steps if the data is not already present in the specified cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> Source code in <code>datarec/datasets/amazon_toys_and_games/amz_toys_2023.py</code> <pre><code>def __init__(self, folder=None):\n    \"\"\"\n    Initializes the builder and orchestrates the data preparation workflow.\n\n    This constructor sets up the necessary paths and automatically triggers\n    the download, verification, and processing steps if the data is not\n    already present in the specified cache directory.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n    \"\"\"\n    super().__init__(None)\n\n    self.dataset_name = 'amazon_toys_and_games'\n    self.version_name = '2023'\n\n    self._data_folder = folder if folder \\\n        else dataset_directory(self.dataset_name)\n    self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n        else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n\n    self.return_type = None\n\n    # check if the required files have been already downloaded\n    file_path = self.required_files()\n\n    if file_path is None:\n        file_path = self.download()\n        file_path = self.decompress(file_path)\n\n    print(f'files: {file_path}')\n\n    self.process(file_path)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_toys_and_games.amz_toys_2023.AMZ_ToysGames_2023.required_files","title":"<code>required_files()</code>","text":"<p>Checks for the presence of the required decompressed data file.</p> <p>It first looks for the final, uncompressed file. If not found, it looks for the compressed archive and decompresses it.</p> <p>Returns:</p> Type Description <code>str or None</code> <p>The path to the required data file if it exists or can be created by decompression. Otherwise, returns None.</p> Source code in <code>datarec/datasets/amazon_toys_and_games/amz_toys_2023.py</code> <pre><code>def required_files(self):\n    \"\"\"\n    Checks for the presence of the required decompressed data file.\n\n    It first looks for the final, uncompressed file. If not found, it\n    looks for the compressed archive and decompresses it.\n\n    Returns:\n        (str or None): The path to the required data file if it exists or can be\n            created by decompression. Otherwise, returns None.\n    \"\"\"\n    # compressed data file\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n    uncompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n\n    # check if the file is there\n    if os.path.exists(uncompressed_file_path):\n        return uncompressed_file_path\n    # check if the compressed file is there\n    elif os.path.exists(file_path):\n        return self.decompress(file_path)\n    else:\n        return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_toys_and_games.amz_toys_2023.AMZ_ToysGames_2023.decompress","title":"<code>decompress(path)</code>","text":"<p>Decompresses the downloaded archive after verifying its checksum.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path of the compressed archive.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path to the decompressed file.</p> Source code in <code>datarec/datasets/amazon_toys_and_games/amz_toys_2023.py</code> <pre><code>def decompress(self, path):\n    \"\"\"\n    Decompresses the downloaded archive after verifying its checksum.\n\n    Args:\n        path (str): The file path of the compressed archive.\n\n    Returns:\n        (str): The path to the decompressed file.\n    \"\"\"\n    verify_checksum(path, self.CHECKSUM)\n\n    # decompress downloaded file\n    decompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n    return decompress_gz(path, decompressed_file_path)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_toys_and_games.amz_toys_2023.AMZ_ToysGames_2023.download","title":"<code>download()</code>","text":"<p>Downloads the raw dataset compressed archive.</p> <p>Returns:</p> Type Description <code>str</code> <p>The local file path to the downloaded archive.</p> Source code in <code>datarec/datasets/amazon_toys_and_games/amz_toys_2023.py</code> <pre><code>def download(self) -&gt; (str, str):\n    \"\"\"\n    Downloads the raw dataset compressed archive.\n\n    Returns:\n        (str): The local file path to the downloaded archive.\n    \"\"\"\n    if not os.path.exists(self._raw_folder):\n        os.makedirs(self._raw_folder)\n        print('Created folder \\'{}\\''.format(self._raw_folder))\n\n    # download dataset file (compressed file)\n    file_name = os.path.basename(self.url)\n    file_path = os.path.join(self._raw_folder, file_name)\n    download_file(self.url, file_path, size=388191962)\n\n    return file_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_toys_and_games.amz_toys_2023.AMZ_ToysGames_2023.process","title":"<code>process(file_path)</code>","text":"<p>Processes the raw data and loads it into the class.</p> <p>This method reads the decompressed file, which includes a header row, into a pandas DataFrame and assigns it to the <code>self.data</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the raw data file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This method assigns the processed data to <code>self.data</code> directly.</p> Source code in <code>datarec/datasets/amazon_toys_and_games/amz_toys_2023.py</code> <pre><code>def process(self, file_path):\n    \"\"\"\n    Processes the raw data and loads it into the class.\n\n    This method reads the decompressed file, which includes a header row,\n    into a pandas DataFrame and assigns it to the `self.data` attribute.\n\n    Args:\n        file_path (str): The path to the raw data file.\n\n    Returns:\n        (None): This method assigns the processed data to `self.data` directly.\n    \"\"\"\n\n    from datarec.io import read_tabular\n\n    dataset = read_tabular(file_path, sep=',',\n                           user_col='user_id', item_col='parent_asin',\n                           rating_col='rating', timestamp_col='timestamp',\n                           header=0)\n    self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#amazon-video-games","title":"Amazon Video Games","text":"<p>Entry point for loading different versions of the Amazon Video Games dataset.</p> <p>Builder class for the 2018 version of the Amazon Video Games dataset.</p> <p>Builder class for the 2023 version of the Amazon Video Games dataset.</p>"},{"location":"documentation/datasets/#datarec.datasets.amazon_videogames.amz_videogames.AmazonVideoGames","title":"<code>AmazonVideoGames</code>","text":"<p>Entry point class to load various versions of the Amazon Video Games dataset.</p> <p>This class provides a single, convenient interface for accessing the dataset. Based on the <code>version</code> parameter, it selects and returns the appropriate dataset builder for either the 2018 or 2023 version.</p> <p>The dataset contains product reviews and metadata for the \"Video Games\" category from Amazon.</p> <p>The default version is 'latest', which currently corresponds to the '2023' version.</p> <p>Examples:</p> <p>To load the latest version:</p> <pre><code>&gt;&gt;&gt; data_loader = AmazonVideoGames()\n</code></pre> <p>To load a specific version:</p> <pre><code>&gt;&gt;&gt; data_loader = AmazonVideoGames(version='2018')\n</code></pre> Source code in <code>datarec/datasets/amazon_videogames/amz_videogames.py</code> <pre><code>class AmazonVideoGames:\n    \"\"\"\n    Entry point class to load various versions of the Amazon Video Games dataset.\n\n    This class provides a single, convenient interface for accessing the dataset.\n    Based on the `version` parameter, it selects and returns the appropriate\n    dataset builder for either the 2018 or 2023 version.\n\n    The dataset contains product reviews and metadata for the \"Video Games\"\n    category from Amazon.\n\n    The default version is 'latest', which currently corresponds to the '2023' version.\n\n    Examples:\n        To load the latest version:\n        &gt;&gt;&gt; data_loader = AmazonVideoGames()\n\n        To load a specific version:\n        &gt;&gt;&gt; data_loader = AmazonVideoGames(version='2018')\n    \"\"\"\n    latest_version = '2023'\n\n    def __new__(cls, version: str = 'latest', **kwargs) -&gt; DataRec:\n        \"\"\"\n        Initializes and returns the specified version of the dataset builder.\n\n        This method acts as a dispatcher, instantiating the correct builder class\n        that handles the downloading, caching, and loading for a specific dataset version.\n\n        Args:\n            version (str): The version of the dataset to load. Supported versions\n                include '2023', '2018', and 'latest'. Defaults to 'latest'.\n            **kwargs: Additional keyword arguments (not currently used for this dataset).\n\n        Returns:\n            (DataRec): An instance of the appropriate dataset builder class\n                (e.g., `AMZ_VideoGames_2023`), populated with data.\n\n        Raises:\n            ValueError: If an unsupported version string is provided.\n        \"\"\"\n\n        versions = {'2023': AMZ_VideoGames_2023,\n                    '2018': AMZ_VideoGames_2018}\n        if version == 'latest':\n            version = cls.latest_version\n        if version in versions:\n            return versions[version]()\n        else:\n            raise ValueError(f\"Amazon Video Games {version}: Unsupported version \\n Supported version:\"\n                             f\"\\n \\t version \\t name \"\n                             f\"\\n \\t 2023 \\t Amazon Video Games 2023\"\n                             f\"\\n \\t 2018 \\t Amazon Video Games 2018\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_videogames.amz_videogames.AmazonVideoGames.__new__","title":"<code>__new__(version='latest', **kwargs)</code>","text":"<p>Initializes and returns the specified version of the dataset builder.</p> <p>This method acts as a dispatcher, instantiating the correct builder class that handles the downloading, caching, and loading for a specific dataset version.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>The version of the dataset to load. Supported versions include '2023', '2018', and 'latest'. Defaults to 'latest'.</p> <code>'latest'</code> <code>**kwargs</code> <p>Additional keyword arguments (not currently used for this dataset).</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataRec</code> <p>An instance of the appropriate dataset builder class (e.g., <code>AMZ_VideoGames_2023</code>), populated with data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported version string is provided.</p> Source code in <code>datarec/datasets/amazon_videogames/amz_videogames.py</code> <pre><code>def __new__(cls, version: str = 'latest', **kwargs) -&gt; DataRec:\n    \"\"\"\n    Initializes and returns the specified version of the dataset builder.\n\n    This method acts as a dispatcher, instantiating the correct builder class\n    that handles the downloading, caching, and loading for a specific dataset version.\n\n    Args:\n        version (str): The version of the dataset to load. Supported versions\n            include '2023', '2018', and 'latest'. Defaults to 'latest'.\n        **kwargs: Additional keyword arguments (not currently used for this dataset).\n\n    Returns:\n        (DataRec): An instance of the appropriate dataset builder class\n            (e.g., `AMZ_VideoGames_2023`), populated with data.\n\n    Raises:\n        ValueError: If an unsupported version string is provided.\n    \"\"\"\n\n    versions = {'2023': AMZ_VideoGames_2023,\n                '2018': AMZ_VideoGames_2018}\n    if version == 'latest':\n        version = cls.latest_version\n    if version in versions:\n        return versions[version]()\n    else:\n        raise ValueError(f\"Amazon Video Games {version}: Unsupported version \\n Supported version:\"\n                         f\"\\n \\t version \\t name \"\n                         f\"\\n \\t 2023 \\t Amazon Video Games 2023\"\n                         f\"\\n \\t 2018 \\t Amazon Video Games 2018\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_videogames.amz_videogames_2018.AMZ_VideoGames_2018","title":"<code>AMZ_VideoGames_2018</code>","text":"<p>               Bases: <code>DataRec</code></p> <p>Builder class for the Amazon Video Games dataset (2018 version).</p> <p>This class handles the logic for downloading, preparing, and loading the 2018 version of the \"Video Games\" dataset. It is not typically instantiated directly but is called by the <code>AmazonVideoGames</code> entry point class.</p> <p>The raw data is provided as a single, uncompressed CSV file without a header.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL from which the raw dataset is downloaded.</p> <code>CHECKSUM</code> <code>str</code> <p>The MD5 checksum to verify the integrity of the downloaded file.</p> Source code in <code>datarec/datasets/amazon_videogames/amz_videogames_2018.py</code> <pre><code>class AMZ_VideoGames_2018(DataRec):\n    \"\"\"\n    Builder class for the Amazon Video Games dataset (2018 version).\n\n    This class handles the logic for downloading, preparing, and loading the\n    2018 version of the \"Video Games\" dataset. It is not typically\n    instantiated directly but is called by the `AmazonVideoGames` entry point class.\n\n    The raw data is provided as a single, uncompressed CSV file without a header.\n\n    Attributes:\n        url (str): The URL from which the raw dataset is downloaded.\n        CHECKSUM (str): The MD5 checksum to verify the integrity of the downloaded file.\n    \"\"\"\n    url = 'https://mcauleylab.ucsd.edu/public_datasets/data/amazon_v2/categoryFilesSmall/Video_Games.csv'\n    data_file_name = os.path.basename(url)\n    decompressed_data_file_name = data_file_name\n    CHECKSUM = 'feecdbf6bf247e54d2a572e2be503515'\n\n    def __init__(self, folder=None):\n        \"\"\"\n        Initializes the builder and orchestrates the data preparation workflow.\n\n        This constructor sets up the necessary paths and automatically triggers\n        the download, verification, and processing steps if the data is not\n        already present in the specified cache directory.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n        \"\"\"\n        super().__init__(None)\n\n        self.dataset_name = 'amazon_video_games'\n        self.version_name = '2018'\n\n        self._data_folder = folder if folder \\\n            else dataset_directory(self.dataset_name)\n        self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n            else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n        # check if the required files have been already downloaded\n        file_path = self.required_files()\n\n        if file_path is None:\n            file_path = self.download()\n            file_path = self.decompress(file_path)\n\n        print(f'files: {file_path}')\n\n        self.process(file_path)\n\n    def required_files(self):\n        \"\"\"\n        Checks for the presence of the required data file.\n\n        Returns:\n            (str or None): The path to the required data file if it exists,\n                otherwise returns None.\n        \"\"\"\n        # compressed data file\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n        uncompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n\n        # check if the file is there\n        if os.path.exists(uncompressed_file_path):\n            return uncompressed_file_path\n        # check if the compressed file is there\n        elif os.path.exists(file_path):\n            return self.decompress(file_path)\n        else:\n            return None\n\n    def decompress(self, path):\n        \"\"\"\n        Handles the decompression step.\n\n        For this 2018 version, the source file is already decompressed, so this\n        method simply returns the path to the file.\n\n        Args:\n            path (str): The file path of the source data file.\n\n        Returns:\n            (str): The path to the data file.\n        \"\"\"\n        return path\n\n    def download(self) -&gt; (str, str):\n        \"\"\"\n        Downloads the raw dataset file.\n\n        Returns:\n            (str): The local file path to the downloaded file.\n        \"\"\"\n        if not os.path.exists(self._raw_folder):\n            os.makedirs(self._raw_folder)\n            print('Created folder \\'{}\\''.format(self._raw_folder))\n\n        # download dataset file (compressed file)\n        file_name = os.path.basename(self.url)\n        file_path = os.path.join(self._raw_folder, file_name)\n        download_file(self.url, file_path, size=115388622)\n\n        return file_path\n\n    def process(self, file_path):\n        \"\"\"\n        Processes the raw data and loads it into the class.\n\n        This method reads the raw file, which has no header. Columns are\n        identified by their integer index. The data is then assigned to the\n        `self.data` attribute after checksum verification.\n\n        Args:\n            file_path (str): The path to the raw data file.\n\n        Returns:\n            (None): This method assigns the processed data to `self.data` directly.\n        \"\"\"\n        verify_checksum(file_path, self.CHECKSUM)\n\n        from datarec.io import read_tabular\n\n        dataset = read_tabular(file_path, sep=',',\n                               user_col=0, item_col=1,\n                               rating_col=2, timestamp_col=3,\n                               header=None)\n        self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_videogames.amz_videogames_2018.AMZ_VideoGames_2018.__init__","title":"<code>__init__(folder=None)</code>","text":"<p>Initializes the builder and orchestrates the data preparation workflow.</p> <p>This constructor sets up the necessary paths and automatically triggers the download, verification, and processing steps if the data is not already present in the specified cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> Source code in <code>datarec/datasets/amazon_videogames/amz_videogames_2018.py</code> <pre><code>def __init__(self, folder=None):\n    \"\"\"\n    Initializes the builder and orchestrates the data preparation workflow.\n\n    This constructor sets up the necessary paths and automatically triggers\n    the download, verification, and processing steps if the data is not\n    already present in the specified cache directory.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n    \"\"\"\n    super().__init__(None)\n\n    self.dataset_name = 'amazon_video_games'\n    self.version_name = '2018'\n\n    self._data_folder = folder if folder \\\n        else dataset_directory(self.dataset_name)\n    self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n        else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n    # check if the required files have been already downloaded\n    file_path = self.required_files()\n\n    if file_path is None:\n        file_path = self.download()\n        file_path = self.decompress(file_path)\n\n    print(f'files: {file_path}')\n\n    self.process(file_path)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_videogames.amz_videogames_2018.AMZ_VideoGames_2018.required_files","title":"<code>required_files()</code>","text":"<p>Checks for the presence of the required data file.</p> <p>Returns:</p> Type Description <code>str or None</code> <p>The path to the required data file if it exists, otherwise returns None.</p> Source code in <code>datarec/datasets/amazon_videogames/amz_videogames_2018.py</code> <pre><code>def required_files(self):\n    \"\"\"\n    Checks for the presence of the required data file.\n\n    Returns:\n        (str or None): The path to the required data file if it exists,\n            otherwise returns None.\n    \"\"\"\n    # compressed data file\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n    uncompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n\n    # check if the file is there\n    if os.path.exists(uncompressed_file_path):\n        return uncompressed_file_path\n    # check if the compressed file is there\n    elif os.path.exists(file_path):\n        return self.decompress(file_path)\n    else:\n        return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_videogames.amz_videogames_2018.AMZ_VideoGames_2018.decompress","title":"<code>decompress(path)</code>","text":"<p>Handles the decompression step.</p> <p>For this 2018 version, the source file is already decompressed, so this method simply returns the path to the file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path of the source data file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path to the data file.</p> Source code in <code>datarec/datasets/amazon_videogames/amz_videogames_2018.py</code> <pre><code>def decompress(self, path):\n    \"\"\"\n    Handles the decompression step.\n\n    For this 2018 version, the source file is already decompressed, so this\n    method simply returns the path to the file.\n\n    Args:\n        path (str): The file path of the source data file.\n\n    Returns:\n        (str): The path to the data file.\n    \"\"\"\n    return path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_videogames.amz_videogames_2018.AMZ_VideoGames_2018.download","title":"<code>download()</code>","text":"<p>Downloads the raw dataset file.</p> <p>Returns:</p> Type Description <code>str</code> <p>The local file path to the downloaded file.</p> Source code in <code>datarec/datasets/amazon_videogames/amz_videogames_2018.py</code> <pre><code>def download(self) -&gt; (str, str):\n    \"\"\"\n    Downloads the raw dataset file.\n\n    Returns:\n        (str): The local file path to the downloaded file.\n    \"\"\"\n    if not os.path.exists(self._raw_folder):\n        os.makedirs(self._raw_folder)\n        print('Created folder \\'{}\\''.format(self._raw_folder))\n\n    # download dataset file (compressed file)\n    file_name = os.path.basename(self.url)\n    file_path = os.path.join(self._raw_folder, file_name)\n    download_file(self.url, file_path, size=115388622)\n\n    return file_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_videogames.amz_videogames_2018.AMZ_VideoGames_2018.process","title":"<code>process(file_path)</code>","text":"<p>Processes the raw data and loads it into the class.</p> <p>This method reads the raw file, which has no header. Columns are identified by their integer index. The data is then assigned to the <code>self.data</code> attribute after checksum verification.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the raw data file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This method assigns the processed data to <code>self.data</code> directly.</p> Source code in <code>datarec/datasets/amazon_videogames/amz_videogames_2018.py</code> <pre><code>def process(self, file_path):\n    \"\"\"\n    Processes the raw data and loads it into the class.\n\n    This method reads the raw file, which has no header. Columns are\n    identified by their integer index. The data is then assigned to the\n    `self.data` attribute after checksum verification.\n\n    Args:\n        file_path (str): The path to the raw data file.\n\n    Returns:\n        (None): This method assigns the processed data to `self.data` directly.\n    \"\"\"\n    verify_checksum(file_path, self.CHECKSUM)\n\n    from datarec.io import read_tabular\n\n    dataset = read_tabular(file_path, sep=',',\n                           user_col=0, item_col=1,\n                           rating_col=2, timestamp_col=3,\n                           header=None)\n    self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_videogames.amz_videogames_2023.AMZ_VideoGames_2023","title":"<code>AMZ_VideoGames_2023</code>","text":"<p>               Bases: <code>DataRec</code></p> <p>Builder class for the Amazon Video Games dataset (2023 version).</p> <p>This class handles the logic for downloading, preparing, and loading the 2023 version of the \"Video Games\" dataset. It is not typically instantiated directly but is called by the <code>AmazonVideoGames</code> entry point class.</p> <p>The dataset is from the \"Bridging Language and Items for Retrieval and Recommendation\" paper and is provided as a compressed CSV file with a header.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL from which the raw dataset is downloaded.</p> <code>CHECKSUM</code> <code>str</code> <p>The MD5 checksum to verify the integrity of the downloaded file.</p> Source code in <code>datarec/datasets/amazon_videogames/amz_videogames_2023.py</code> <pre><code>class AMZ_VideoGames_2023(DataRec):\n    \"\"\"\n    Builder class for the Amazon Video Games dataset (2023 version).\n\n    This class handles the logic for downloading, preparing, and loading the\n    2023 version of the \"Video Games\" dataset. It is not typically\n    instantiated directly but is called by the `AmazonVideoGames` entry point class.\n\n    The dataset is from the \"Bridging Language and Items for Retrieval and\n    Recommendation\" paper and is provided as a compressed CSV file with a header.\n\n    Attributes:\n        url (str): The URL from which the raw dataset is downloaded.\n        CHECKSUM (str): The MD5 checksum to verify the integrity of the downloaded file.\n    \"\"\"\n    url = 'https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/benchmark/0core/rating_only/Video_Games.csv.gz'\n    data_file_name = os.path.basename(url)\n    decompressed_data_file_name = data_file_name.replace('.gz', '')\n    CHECKSUM = '60fdc3e812de871c30d65722e9a91a0a'\n\n    def __init__(self, folder=None):\n        \"\"\"\n        Initializes the builder and orchestrates the data preparation workflow.\n\n        This constructor sets up the necessary paths and automatically triggers\n        the download, verification, and processing steps if the data is not\n        already present in the specified cache directory.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n        \"\"\"\n        super().__init__(None)\n\n        self.dataset_name = 'amazon_video_games'\n        self.version_name = '2023'\n\n        self._data_folder = folder if folder \\\n            else dataset_directory(self.dataset_name)\n        self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n            else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n        # check if the required files have been already downloaded\n        file_path = self.required_files()\n\n        if file_path is None:\n            file_path = self.download()\n            file_path = self.decompress(file_path)\n\n        print(f'files: {file_path}')\n\n        self.process(file_path)\n\n    def required_files(self):\n        \"\"\"\n        Checks for the presence of the required decompressed data file.\n\n        It first looks for the final, uncompressed file. If not found, it\n        looks for the compressed archive and decompresses it.\n\n        Returns:\n            (str or None): The path to the required data file if it exists or can be\n                created by decompression. Otherwise, returns None.\n        \"\"\"\n        # compressed data file\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n        uncompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n\n        # check if the file is there\n        if os.path.exists(uncompressed_file_path):\n            return uncompressed_file_path\n        # check if the compressed file is there\n        elif os.path.exists(file_path):\n            return self.decompress(file_path)\n        else:\n            return None\n\n    def decompress(self, path):\n        \"\"\"\n        Decompresses the downloaded archive after verifying its checksum.\n\n        Args:\n            path (str): The file path of the compressed archive.\n\n        Returns:\n            (str): The path to the decompressed file.\n        \"\"\"\n        verify_checksum(path, self.CHECKSUM)\n\n        # decompress downloaded file\n        decompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n        return decompress_gz(path, decompressed_file_path)\n\n    def download(self) -&gt; (str, str):\n        \"\"\"\n        Downloads the raw dataset compressed archive.\n\n        Returns:\n            (str): The local file path to the downloaded archive.\n        \"\"\"\n        if not os.path.exists(self._raw_folder):\n            os.makedirs(self._raw_folder)\n            print('Created folder \\'{}\\''.format(self._raw_folder))\n\n        # download dataset file (compressed file)\n        file_name = os.path.basename(self.url)\n        file_path = os.path.join(self._raw_folder, file_name)\n        download_file(self.url, file_path, size=115388622)\n\n        return file_path\n\n    def process(self, file_path):\n        \"\"\"\n        Processes the raw data and loads it into the class.\n\n        This method reads the decompressed file, which includes a header row,\n        into a pandas DataFrame and assigns it to the `self.data` attribute.\n\n        Args:\n            file_path (str): The path to the raw data file.\n\n        Returns:\n            (None): This method assigns the processed data to `self.data` directly.\n        \"\"\"\n\n        from datarec.io import read_tabular\n\n        dataset = read_tabular(file_path, sep=',',\n                               user_col='user_id', item_col='parent_asin',\n                               rating_col='rating', timestamp_col='timestamp',\n                               header=0)\n        self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_videogames.amz_videogames_2023.AMZ_VideoGames_2023.__init__","title":"<code>__init__(folder=None)</code>","text":"<p>Initializes the builder and orchestrates the data preparation workflow.</p> <p>This constructor sets up the necessary paths and automatically triggers the download, verification, and processing steps if the data is not already present in the specified cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> Source code in <code>datarec/datasets/amazon_videogames/amz_videogames_2023.py</code> <pre><code>def __init__(self, folder=None):\n    \"\"\"\n    Initializes the builder and orchestrates the data preparation workflow.\n\n    This constructor sets up the necessary paths and automatically triggers\n    the download, verification, and processing steps if the data is not\n    already present in the specified cache directory.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n    \"\"\"\n    super().__init__(None)\n\n    self.dataset_name = 'amazon_video_games'\n    self.version_name = '2023'\n\n    self._data_folder = folder if folder \\\n        else dataset_directory(self.dataset_name)\n    self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n        else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n    # check if the required files have been already downloaded\n    file_path = self.required_files()\n\n    if file_path is None:\n        file_path = self.download()\n        file_path = self.decompress(file_path)\n\n    print(f'files: {file_path}')\n\n    self.process(file_path)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_videogames.amz_videogames_2023.AMZ_VideoGames_2023.required_files","title":"<code>required_files()</code>","text":"<p>Checks for the presence of the required decompressed data file.</p> <p>It first looks for the final, uncompressed file. If not found, it looks for the compressed archive and decompresses it.</p> <p>Returns:</p> Type Description <code>str or None</code> <p>The path to the required data file if it exists or can be created by decompression. Otherwise, returns None.</p> Source code in <code>datarec/datasets/amazon_videogames/amz_videogames_2023.py</code> <pre><code>def required_files(self):\n    \"\"\"\n    Checks for the presence of the required decompressed data file.\n\n    It first looks for the final, uncompressed file. If not found, it\n    looks for the compressed archive and decompresses it.\n\n    Returns:\n        (str or None): The path to the required data file if it exists or can be\n            created by decompression. Otherwise, returns None.\n    \"\"\"\n    # compressed data file\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n    uncompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n\n    # check if the file is there\n    if os.path.exists(uncompressed_file_path):\n        return uncompressed_file_path\n    # check if the compressed file is there\n    elif os.path.exists(file_path):\n        return self.decompress(file_path)\n    else:\n        return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_videogames.amz_videogames_2023.AMZ_VideoGames_2023.decompress","title":"<code>decompress(path)</code>","text":"<p>Decompresses the downloaded archive after verifying its checksum.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path of the compressed archive.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path to the decompressed file.</p> Source code in <code>datarec/datasets/amazon_videogames/amz_videogames_2023.py</code> <pre><code>def decompress(self, path):\n    \"\"\"\n    Decompresses the downloaded archive after verifying its checksum.\n\n    Args:\n        path (str): The file path of the compressed archive.\n\n    Returns:\n        (str): The path to the decompressed file.\n    \"\"\"\n    verify_checksum(path, self.CHECKSUM)\n\n    # decompress downloaded file\n    decompressed_file_path = os.path.join(self._raw_folder, self.decompressed_data_file_name)\n    return decompress_gz(path, decompressed_file_path)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_videogames.amz_videogames_2023.AMZ_VideoGames_2023.download","title":"<code>download()</code>","text":"<p>Downloads the raw dataset compressed archive.</p> <p>Returns:</p> Type Description <code>str</code> <p>The local file path to the downloaded archive.</p> Source code in <code>datarec/datasets/amazon_videogames/amz_videogames_2023.py</code> <pre><code>def download(self) -&gt; (str, str):\n    \"\"\"\n    Downloads the raw dataset compressed archive.\n\n    Returns:\n        (str): The local file path to the downloaded archive.\n    \"\"\"\n    if not os.path.exists(self._raw_folder):\n        os.makedirs(self._raw_folder)\n        print('Created folder \\'{}\\''.format(self._raw_folder))\n\n    # download dataset file (compressed file)\n    file_name = os.path.basename(self.url)\n    file_path = os.path.join(self._raw_folder, file_name)\n    download_file(self.url, file_path, size=115388622)\n\n    return file_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.amazon_videogames.amz_videogames_2023.AMZ_VideoGames_2023.process","title":"<code>process(file_path)</code>","text":"<p>Processes the raw data and loads it into the class.</p> <p>This method reads the decompressed file, which includes a header row, into a pandas DataFrame and assigns it to the <code>self.data</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the raw data file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This method assigns the processed data to <code>self.data</code> directly.</p> Source code in <code>datarec/datasets/amazon_videogames/amz_videogames_2023.py</code> <pre><code>def process(self, file_path):\n    \"\"\"\n    Processes the raw data and loads it into the class.\n\n    This method reads the decompressed file, which includes a header row,\n    into a pandas DataFrame and assigns it to the `self.data` attribute.\n\n    Args:\n        file_path (str): The path to the raw data file.\n\n    Returns:\n        (None): This method assigns the processed data to `self.data` directly.\n    \"\"\"\n\n    from datarec.io import read_tabular\n\n    dataset = read_tabular(file_path, sep=',',\n                           user_col='user_id', item_col='parent_asin',\n                           rating_col='rating', timestamp_col='timestamp',\n                           header=0)\n    self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#ciaodvd","title":"CiaoDVD","text":"<p>Entry point for loading different versions of the CiaoDVD dataset.</p> <p>Builder class for the v1 version of the CiaoDVD dataset.</p>"},{"location":"documentation/datasets/#datarec.datasets.ciao.ciao.Ciao","title":"<code>Ciao</code>","text":"<p>Entry point class to load various versions of the CiaoDVD dataset.</p> <p>This class provides a single, convenient interface for accessing the dataset. Based on the <code>version</code> parameter, it selects and returns the appropriate dataset builder.</p> <p>CiaoDVD is a dataset for DVD recommendations, also containing social trust data. This loader focuses on the movie ratings.</p> <p>The default version is 'latest', which currently corresponds to 'v1'.</p> <p>Examples:</p> <p>To load the latest version:</p> <pre><code>&gt;&gt;&gt; data_loader = Ciao()\n</code></pre> <p>To load a specific version:</p> <pre><code>&gt;&gt;&gt; data_loader = Ciao(version='v1')\n</code></pre> Source code in <code>datarec/datasets/ciao/ciao.py</code> <pre><code>class Ciao:\n    \"\"\"\n    Entry point class to load various versions of the CiaoDVD dataset.\n\n    This class provides a single, convenient interface for accessing the dataset.\n    Based on the `version` parameter, it selects and returns the appropriate\n    dataset builder.\n\n    CiaoDVD is a dataset for DVD recommendations, also containing social trust data.\n    This loader focuses on the movie ratings.\n\n    The default version is 'latest', which currently corresponds to 'v1'.\n\n    Examples:\n        To load the latest version:\n        &gt;&gt;&gt; data_loader = Ciao()\n\n        To load a specific version:\n        &gt;&gt;&gt; data_loader = Ciao(version='v1')\n    \"\"\"\n    latest_version = 'v1'\n    def __new__(self, version: str = 'latest', **kwargs):\n        \"\"\"\n        Initializes and returns the specified version of the CiaoDVD dataset builder.\n\n        This method acts as a dispatcher, instantiating the correct builder class\n        that handles the downloading, caching, and loading for a specific dataset version.\n\n        Args:\n            version (str): The version of the dataset to load. Currently, only\n                'v1' and 'latest' are supported. Defaults to 'latest'.\n            **kwargs: Additional keyword arguments (not currently used for this dataset).\n\n        Returns:\n            (Ciao_V1): An instance of the dataset builder class, populated with data.\n\n        Raises:\n            ValueError: If an unsupported version string is provided.\n        \"\"\"\n\n        versions = {'v1': Ciao_V1}\n        if version == 'latest':\n            version = self.latest_version\n        if version in versions:\n            return versions[version]()\n        else:\n            raise ValueError(\"Ciao: Unsupported version\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.ciao.ciao.Ciao.__new__","title":"<code>__new__(version='latest', **kwargs)</code>","text":"<p>Initializes and returns the specified version of the CiaoDVD dataset builder.</p> <p>This method acts as a dispatcher, instantiating the correct builder class that handles the downloading, caching, and loading for a specific dataset version.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>The version of the dataset to load. Currently, only 'v1' and 'latest' are supported. Defaults to 'latest'.</p> <code>'latest'</code> <code>**kwargs</code> <p>Additional keyword arguments (not currently used for this dataset).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Ciao_V1</code> <p>An instance of the dataset builder class, populated with data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported version string is provided.</p> Source code in <code>datarec/datasets/ciao/ciao.py</code> <pre><code>def __new__(self, version: str = 'latest', **kwargs):\n    \"\"\"\n    Initializes and returns the specified version of the CiaoDVD dataset builder.\n\n    This method acts as a dispatcher, instantiating the correct builder class\n    that handles the downloading, caching, and loading for a specific dataset version.\n\n    Args:\n        version (str): The version of the dataset to load. Currently, only\n            'v1' and 'latest' are supported. Defaults to 'latest'.\n        **kwargs: Additional keyword arguments (not currently used for this dataset).\n\n    Returns:\n        (Ciao_V1): An instance of the dataset builder class, populated with data.\n\n    Raises:\n        ValueError: If an unsupported version string is provided.\n    \"\"\"\n\n    versions = {'v1': Ciao_V1}\n    if version == 'latest':\n        version = self.latest_version\n    if version in versions:\n        return versions[version]()\n    else:\n        raise ValueError(\"Ciao: Unsupported version\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.ciao.ciao_v1.Ciao_V1","title":"<code>Ciao_V1</code>","text":"<p>               Bases: <code>DataRec</code></p> <p>Builder class for the CiaoDVD dataset.</p> <p>This class handles the logic for downloading, preparing, and loading the CiaoDVD dataset from the LibRec repository. It is not typically instantiated directly but is called by the <code>Ciao</code> entry point class.</p> <p>The dataset was introduced in the paper \"ETAF: An Extended Trust Antecedents Framework for Trust Prediction\". The archive contains multiple files; this loader specifically processes <code>movie-ratings.txt</code>.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL from which the raw dataset is downloaded.</p> <code>CHECKSUM</code> <code>str</code> <p>The MD5 checksum to verify the integrity of the downloaded file.</p> <code>REQUIRED_FILES</code> <code>list</code> <p>A list of files expected within the decompressed archive.</p> Source code in <code>datarec/datasets/ciao/ciao_v1.py</code> <pre><code>class Ciao_V1(DataRec):\n    \"\"\"\n    Builder class for the CiaoDVD dataset.\n\n    This class handles the logic for downloading, preparing, and loading the\n    CiaoDVD dataset from the LibRec repository. It is not typically instantiated\n    directly but is called by the `Ciao` entry point class.\n\n    The dataset was introduced in the paper \"ETAF: An Extended Trust Antecedents\n    Framework for Trust Prediction\". The archive contains multiple files; this\n    loader specifically processes `movie-ratings.txt`.\n\n    Attributes:\n        url (str): The URL from which the raw dataset is downloaded.\n        CHECKSUM (str): The MD5 checksum to verify the integrity of the downloaded file.\n        REQUIRED_FILES (list): A list of files expected within the decompressed archive.\n    \"\"\"\n    url = 'https://guoguibing.github.io/librec/datasets/CiaoDVD.zip'\n    data_file_name = os.path.basename(url)\n    movie_file_name = 'movie-ratings.txt'\n    review_file_name = 'review-ratings.txt'\n    trusts_file_name = 'trusts.txt'\n    REQUIRED_FILES = [movie_file_name, review_file_name, trusts_file_name]\n    CHECKSUM = '43a39e068e3fc494a7f7f7581293e2c2'\n\n\n    def __init__(self, folder=None):\n        \"\"\"\n        Initializes the builder and orchestrates the data preparation workflow.\n\n        This constructor sets up the necessary paths and automatically triggers\n        the download, verification, and processing steps if the data is not\n        already present in the specified cache directory.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n        \"\"\"\n        super().__init__(None)\n\n        self.dataset_name = 'ciaoDVD'\n        self.version_name = 'v1'\n\n        self._data_folder = folder if folder \\\n            else dataset_directory(self.dataset_name)\n        self._raw_folder = os.path.abspath(os.path.join(self._data_folder, RAW_DATA_FOLDER)) if folder \\\n            else dataset_raw_directory(self.dataset_name)\n\n        self.return_type = None\n\n        # check if the required files have been already downloaded\n        file_path = self.required_files()\n\n        if file_path is None:\n            file_path = self.download()\n            file_path = self.decompress(file_path)\n\n        print(f'files: {file_path}')\n\n        rating_file_path = os.path.abspath(os.path.join(self._raw_folder, self.movie_file_name))\n        self.process(rating_file_path)\n\n    def required_files(self):\n        \"\"\"\n        Checks for the presence of the required decompressed data files.\n\n        It first looks for the final, uncompressed files. If not found, it\n        looks for the compressed archive and decompresses it.\n\n        Returns:\n            (list or None): A list of paths to the required data files if they\n                exist or can be created by decompression. Otherwise, returns None.\n        \"\"\"\n        # compressed data file\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n\n        # check if the file is there\n        paths = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n        if all([os.path.exists(p) for p in paths]):\n            return paths\n        # check if the compressed file is there\n        elif os.path.exists(file_path):\n            return self.decompress(file_path)\n        else:\n            return None\n\n    def decompress(self, path):\n        \"\"\"\n        Decompresses the downloaded archive after verifying its checksum.\n\n        Args:\n            path (str): The file path of the compressed archive.\n\n        Returns:\n            (list or None): A list of paths to the decompressed files if successful,\n                otherwise None.\n        \"\"\"\n        verify_checksum(path, self.CHECKSUM)\n\n        # decompress downloaded file\n        decompress_zip_file(path, self._raw_folder)\n        files = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n        if all([os.path.exists(f) for f in files]):\n            return [os.path.join(self._raw_folder, f) for f in files]\n        return None\n\n    def download(self) -&gt; (str, str):\n        \"\"\"\n        Downloads the raw dataset compressed archive.\n\n        Returns:\n            (str): The local file path to the downloaded archive.\n        \"\"\"\n        if not os.path.exists(self._raw_folder):\n            os.makedirs(self._raw_folder)\n            print('Created folder \\'{}\\''.format(self._raw_folder))\n\n        # download dataset file (compressed file)\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n        download_file(self.url, file_path, size=5814757)\n\n        return file_path\n\n    def process(self, path) -&gt; None:\n        \"\"\"\n        Processes the raw `movie-ratings.txt` data and loads it into the class.\n\n        This method reads the file, which has no header. It also parses the\n        date strings in 'YYYY-MM-DD' format and converts them to Unix timestamps.\n\n        Args:\n            path (str): The path to the raw `movie-ratings.txt` file.\n\n        Returns:\n            (None): This method assigns the processed data to `self.data` directly.\n        \"\"\"\n\n        from datarec.io import read_tabular\n\n        user_col = 0\n        item_col = 1\n        rating_col = 4\n        timestamp_col = 5\n        dataset = read_tabular(path, sep=',', user_col=user_col, item_col=item_col, rating_col=rating_col, timestamp_col=timestamp_col, header=None)\n        # timestamps = pd.Series(dataset.data[timestamp_col].apply(lambda x: x.timestamp()).values,\n        #                        index=dataset.data.index, dtype='float64')\n\n        # Convert the date strings to datetime objects using the specified format\n        dataset.data[timestamp_col] = pd.to_datetime(dataset.data[timestamp_col], format='%Y-%m-%d')\n\n        # Now extract the Unix timestamps (in seconds)\n        timestamps = pd.Series(dataset.data[timestamp_col].apply(lambda x: x.timestamp()).values,\n                               index=dataset.data.index, dtype='float64')\n        dataset.data[timestamp_col] = timestamps\n\n        self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.ciao.ciao_v1.Ciao_V1.__init__","title":"<code>__init__(folder=None)</code>","text":"<p>Initializes the builder and orchestrates the data preparation workflow.</p> <p>This constructor sets up the necessary paths and automatically triggers the download, verification, and processing steps if the data is not already present in the specified cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> Source code in <code>datarec/datasets/ciao/ciao_v1.py</code> <pre><code>def __init__(self, folder=None):\n    \"\"\"\n    Initializes the builder and orchestrates the data preparation workflow.\n\n    This constructor sets up the necessary paths and automatically triggers\n    the download, verification, and processing steps if the data is not\n    already present in the specified cache directory.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n    \"\"\"\n    super().__init__(None)\n\n    self.dataset_name = 'ciaoDVD'\n    self.version_name = 'v1'\n\n    self._data_folder = folder if folder \\\n        else dataset_directory(self.dataset_name)\n    self._raw_folder = os.path.abspath(os.path.join(self._data_folder, RAW_DATA_FOLDER)) if folder \\\n        else dataset_raw_directory(self.dataset_name)\n\n    self.return_type = None\n\n    # check if the required files have been already downloaded\n    file_path = self.required_files()\n\n    if file_path is None:\n        file_path = self.download()\n        file_path = self.decompress(file_path)\n\n    print(f'files: {file_path}')\n\n    rating_file_path = os.path.abspath(os.path.join(self._raw_folder, self.movie_file_name))\n    self.process(rating_file_path)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.ciao.ciao_v1.Ciao_V1.required_files","title":"<code>required_files()</code>","text":"<p>Checks for the presence of the required decompressed data files.</p> <p>It first looks for the final, uncompressed files. If not found, it looks for the compressed archive and decompresses it.</p> <p>Returns:</p> Type Description <code>list or None</code> <p>A list of paths to the required data files if they exist or can be created by decompression. Otherwise, returns None.</p> Source code in <code>datarec/datasets/ciao/ciao_v1.py</code> <pre><code>def required_files(self):\n    \"\"\"\n    Checks for the presence of the required decompressed data files.\n\n    It first looks for the final, uncompressed files. If not found, it\n    looks for the compressed archive and decompresses it.\n\n    Returns:\n        (list or None): A list of paths to the required data files if they\n            exist or can be created by decompression. Otherwise, returns None.\n    \"\"\"\n    # compressed data file\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n\n    # check if the file is there\n    paths = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n    if all([os.path.exists(p) for p in paths]):\n        return paths\n    # check if the compressed file is there\n    elif os.path.exists(file_path):\n        return self.decompress(file_path)\n    else:\n        return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.ciao.ciao_v1.Ciao_V1.decompress","title":"<code>decompress(path)</code>","text":"<p>Decompresses the downloaded archive after verifying its checksum.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path of the compressed archive.</p> required <p>Returns:</p> Type Description <code>list or None</code> <p>A list of paths to the decompressed files if successful, otherwise None.</p> Source code in <code>datarec/datasets/ciao/ciao_v1.py</code> <pre><code>def decompress(self, path):\n    \"\"\"\n    Decompresses the downloaded archive after verifying its checksum.\n\n    Args:\n        path (str): The file path of the compressed archive.\n\n    Returns:\n        (list or None): A list of paths to the decompressed files if successful,\n            otherwise None.\n    \"\"\"\n    verify_checksum(path, self.CHECKSUM)\n\n    # decompress downloaded file\n    decompress_zip_file(path, self._raw_folder)\n    files = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n    if all([os.path.exists(f) for f in files]):\n        return [os.path.join(self._raw_folder, f) for f in files]\n    return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.ciao.ciao_v1.Ciao_V1.download","title":"<code>download()</code>","text":"<p>Downloads the raw dataset compressed archive.</p> <p>Returns:</p> Type Description <code>str</code> <p>The local file path to the downloaded archive.</p> Source code in <code>datarec/datasets/ciao/ciao_v1.py</code> <pre><code>def download(self) -&gt; (str, str):\n    \"\"\"\n    Downloads the raw dataset compressed archive.\n\n    Returns:\n        (str): The local file path to the downloaded archive.\n    \"\"\"\n    if not os.path.exists(self._raw_folder):\n        os.makedirs(self._raw_folder)\n        print('Created folder \\'{}\\''.format(self._raw_folder))\n\n    # download dataset file (compressed file)\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n    download_file(self.url, file_path, size=5814757)\n\n    return file_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.ciao.ciao_v1.Ciao_V1.process","title":"<code>process(path)</code>","text":"<p>Processes the raw <code>movie-ratings.txt</code> data and loads it into the class.</p> <p>This method reads the file, which has no header. It also parses the date strings in 'YYYY-MM-DD' format and converts them to Unix timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the raw <code>movie-ratings.txt</code> file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This method assigns the processed data to <code>self.data</code> directly.</p> Source code in <code>datarec/datasets/ciao/ciao_v1.py</code> <pre><code>def process(self, path) -&gt; None:\n    \"\"\"\n    Processes the raw `movie-ratings.txt` data and loads it into the class.\n\n    This method reads the file, which has no header. It also parses the\n    date strings in 'YYYY-MM-DD' format and converts them to Unix timestamps.\n\n    Args:\n        path (str): The path to the raw `movie-ratings.txt` file.\n\n    Returns:\n        (None): This method assigns the processed data to `self.data` directly.\n    \"\"\"\n\n    from datarec.io import read_tabular\n\n    user_col = 0\n    item_col = 1\n    rating_col = 4\n    timestamp_col = 5\n    dataset = read_tabular(path, sep=',', user_col=user_col, item_col=item_col, rating_col=rating_col, timestamp_col=timestamp_col, header=None)\n    # timestamps = pd.Series(dataset.data[timestamp_col].apply(lambda x: x.timestamp()).values,\n    #                        index=dataset.data.index, dtype='float64')\n\n    # Convert the date strings to datetime objects using the specified format\n    dataset.data[timestamp_col] = pd.to_datetime(dataset.data[timestamp_col], format='%Y-%m-%d')\n\n    # Now extract the Unix timestamps (in seconds)\n    timestamps = pd.Series(dataset.data[timestamp_col].apply(lambda x: x.timestamp()).values,\n                           index=dataset.data.index, dtype='float64')\n    dataset.data[timestamp_col] = timestamps\n\n    self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#epinions","title":"Epinions","text":"<p>Entry point for loading different versions of the Epinions dataset.</p> <p>Builder class for the v1 version of the Epinions dataset.</p>"},{"location":"documentation/datasets/#datarec.datasets.epinions.epinions.Epinions","title":"<code>Epinions</code>","text":"<p>Entry point class to load various versions of the Epinions dataset.</p> <p>This class provides a single, convenient interface for accessing the dataset. Based on the <code>version</code> parameter, it selects and returns the appropriate dataset builder.</p> <p>Epinions is a who-trust-whom online social network from a general consumer review site. Members of the site can decide whether to \"trust\" each other.</p> <p>The default version is 'latest', which currently corresponds to 'v1'.</p> <p>Examples:</p> <p>To load the latest version:</p> <pre><code>&gt;&gt;&gt; data_loader = Epinions()\n</code></pre> <p>To load a specific version:</p> <pre><code>&gt;&gt;&gt; data_loader = Epinions(version='v1')\n</code></pre> Source code in <code>datarec/datasets/epinions/epinions.py</code> <pre><code>class Epinions:\n    \"\"\"\n    Entry point class to load various versions of the Epinions dataset.\n\n    This class provides a single, convenient interface for accessing the dataset.\n    Based on the `version` parameter, it selects and returns the appropriate\n    dataset builder.\n\n    Epinions is a who-trust-whom online social network from a general consumer\n    review site. Members of the site can decide whether to \"trust\" each other.\n\n    The default version is 'latest', which currently corresponds to 'v1'.\n\n    Examples:\n        To load the latest version:\n        &gt;&gt;&gt; data_loader = Epinions()\n\n        To load a specific version:\n        &gt;&gt;&gt; data_loader = Epinions(version='v1')\n    \"\"\"\n    latest_version = 'v1'\n    def __new__(cls, version: str = 'latest', **kwargs) -&gt; DataRec:\n        \"\"\"\n        Initializes and returns the specified version of the Epinions dataset builder.\n\n        This method acts as a dispatcher, instantiating the correct builder class\n        that handles the downloading, caching, and loading for a specific dataset version.\n\n        Args:\n            version (str): The version of the dataset to load. Currently, only\n                'v1' and 'latest' are supported. Defaults to 'latest'.\n            **kwargs: Additional keyword arguments (not currently used for this dataset).\n\n        Returns:\n            (Epinions_V1): An instance of the dataset builder class, populated with data.\n\n        Raises:\n            ValueError: If an unsupported version string is provided.\n        \"\"\"\n\n        versions = {'v1': Epinions_V1}\n        if version == 'latest':\n            version = cls.latest_version\n        if version in versions:\n            return versions[version]()\n        else:\n            raise ValueError(\"Epinions: Unsupported version\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.epinions.epinions.Epinions.__new__","title":"<code>__new__(version='latest', **kwargs)</code>","text":"<p>Initializes and returns the specified version of the Epinions dataset builder.</p> <p>This method acts as a dispatcher, instantiating the correct builder class that handles the downloading, caching, and loading for a specific dataset version.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>The version of the dataset to load. Currently, only 'v1' and 'latest' are supported. Defaults to 'latest'.</p> <code>'latest'</code> <code>**kwargs</code> <p>Additional keyword arguments (not currently used for this dataset).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Epinions_V1</code> <p>An instance of the dataset builder class, populated with data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported version string is provided.</p> Source code in <code>datarec/datasets/epinions/epinions.py</code> <pre><code>def __new__(cls, version: str = 'latest', **kwargs) -&gt; DataRec:\n    \"\"\"\n    Initializes and returns the specified version of the Epinions dataset builder.\n\n    This method acts as a dispatcher, instantiating the correct builder class\n    that handles the downloading, caching, and loading for a specific dataset version.\n\n    Args:\n        version (str): The version of the dataset to load. Currently, only\n            'v1' and 'latest' are supported. Defaults to 'latest'.\n        **kwargs: Additional keyword arguments (not currently used for this dataset).\n\n    Returns:\n        (Epinions_V1): An instance of the dataset builder class, populated with data.\n\n    Raises:\n        ValueError: If an unsupported version string is provided.\n    \"\"\"\n\n    versions = {'v1': Epinions_V1}\n    if version == 'latest':\n        version = cls.latest_version\n    if version in versions:\n        return versions[version]()\n    else:\n        raise ValueError(\"Epinions: Unsupported version\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.epinions.epinions_v1.Epinions_V1","title":"<code>Epinions_V1</code>","text":"<p>               Bases: <code>DataRec</code></p> <p>Builder class for the Epinions dataset.</p> <p>This class handles the logic for downloading, preparing, and loading the Epinions social network dataset from the Stanford SNAP repository. It is not typically instantiated directly but is called by the <code>Epinions</code> entry point class.</p> <p>The dataset was introduced in the paper \"Trust Management for the Semantic Web\". It represents a directed graph of trust relationships.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL from which the raw dataset is downloaded.</p> <code>CHECKSUM</code> <code>str</code> <p>The MD5 checksum to verify the integrity of the downloaded file.</p> <code>REQUIRED_FILES</code> <code>list</code> <p>A list of files expected after decompression.</p> Source code in <code>datarec/datasets/epinions/epinions_v1.py</code> <pre><code>class Epinions_V1(DataRec):\n    \"\"\"\n    Builder class for the Epinions dataset.\n\n    This class handles the logic for downloading, preparing, and loading the\n    Epinions social network dataset from the Stanford SNAP repository. It is not\n    typically instantiated directly but is called by the `Epinions` entry point class.\n\n    The dataset was introduced in the paper \"Trust Management for the Semantic Web\".\n    It represents a directed graph of trust relationships.\n\n    Attributes:\n        url (str): The URL from which the raw dataset is downloaded.\n        CHECKSUM (str): The MD5 checksum to verify the integrity of the downloaded file.\n        REQUIRED_FILES (list): A list of files expected after decompression.\n    \"\"\"\n    url = 'https://snap.stanford.edu/data/soc-Epinions1.txt.gz'\n    data_file_name = os.path.basename(url)\n    decompressed_file_name = data_file_name.replace('.gz', '')\n    REQUIRED_FILES = [decompressed_file_name]\n    CHECKSUM = '8df7433d4486ba68eb25e623feacff04'\n\n    def __init__(self, folder=None):\n        \"\"\"\n        Initializes the builder and orchestrates the data preparation workflow.\n\n        This constructor sets up the necessary paths and automatically triggers\n        the download, verification, and processing steps if the data is not\n        already present in the specified cache directory.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n        \"\"\"\n        super().__init__(None)\n\n        self.dataset_name = 'epinions'\n        self.version_name = 'v1'\n\n        self._data_folder = folder if folder \\\n            else dataset_directory(self.dataset_name)\n        self._raw_folder = os.path.abspath(os.path.join(self._data_folder, RAW_DATA_FOLDER)) if folder \\\n            else dataset_raw_directory(self.dataset_name)\n\n        self.return_type = None\n\n        # check if the required files have been already downloaded\n        file_path = self.required_files()\n\n        if file_path is None:\n            file_path = self.download()\n            file_path = self.decompress(file_path)\n\n        print(f'files: {file_path}')\n\n        self.process(file_path[0])\n\n    def required_files(self):\n        \"\"\"\n        Checks for the presence of the required decompressed data file.\n\n        It first looks for the final, uncompressed file. If not found, it\n        looks for the compressed archive and decompresses it.\n\n        Returns:\n            (list or None): A list containing the path to the required data file if\n                it exists or can be created by decompression. Otherwise, returns None.\n        \"\"\"\n        # compressed data file\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n\n        # check if the file is there\n        paths = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n        if all([os.path.exists(p) for p in paths]):\n            return paths\n        # check if the compressed file is there\n        elif os.path.exists(file_path):\n            return self.decompress(file_path)\n        else:\n            return None\n\n    def decompress(self, path):\n        \"\"\"\n        Decompresses the downloaded archive after verifying its checksum.\n\n        Args:\n            path (str): The file path of the compressed archive.\n\n        Returns:\n            (list or None): A list containing the path to the decompressed file\n                if successful, otherwise None.\n        \"\"\"\n        verify_checksum(path, self.CHECKSUM)\n\n        # decompress downloaded file\n        decompressed_file = os.path.join(self._raw_folder, self.decompressed_file_name)\n        decompress_gz(path, decompressed_file)\n        files = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n        if all([os.path.exists(f) for f in files]):\n            return [os.path.join(self._raw_folder, f) for f in files]\n        return None\n\n    def download(self) -&gt; (str, str):\n        \"\"\"\n        Downloads the raw dataset compressed archive.\n\n        Returns:\n            (str): The local file path to the downloaded .gz archive.\n        \"\"\"\n        if not os.path.exists(self._raw_folder):\n            os.makedirs(self._raw_folder)\n            print('Created folder \\'{}\\''.format(self._raw_folder))\n\n        # download dataset file (compressed file)\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n        download_file(self.url, file_path)\n\n        return file_path\n\n    def process(self, file_path):\n        \"\"\"\n        Processes the raw trust network data and loads it into the class.\n\n        This method reads the file, skipping the header comment\n        lines. The first column is treated as the 'user' (truster) and the\n        second column as the 'item' (trustee).\n\n        Args:\n            file_path (str): The path to the raw `soc-Epinions1.txt` file.\n\n        Returns:\n            (None): This method assigns the processed data to `self.data` directly.\n        \"\"\"\n\n        from datarec.io import read_tabular\n\n        dataset = read_tabular(file_path, sep='\\t',\n                               user_col=0, item_col=1,\n                               header=None, skiprows=4)\n\n        self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.epinions.epinions_v1.Epinions_V1.__init__","title":"<code>__init__(folder=None)</code>","text":"<p>Initializes the builder and orchestrates the data preparation workflow.</p> <p>This constructor sets up the necessary paths and automatically triggers the download, verification, and processing steps if the data is not already present in the specified cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> Source code in <code>datarec/datasets/epinions/epinions_v1.py</code> <pre><code>def __init__(self, folder=None):\n    \"\"\"\n    Initializes the builder and orchestrates the data preparation workflow.\n\n    This constructor sets up the necessary paths and automatically triggers\n    the download, verification, and processing steps if the data is not\n    already present in the specified cache directory.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n    \"\"\"\n    super().__init__(None)\n\n    self.dataset_name = 'epinions'\n    self.version_name = 'v1'\n\n    self._data_folder = folder if folder \\\n        else dataset_directory(self.dataset_name)\n    self._raw_folder = os.path.abspath(os.path.join(self._data_folder, RAW_DATA_FOLDER)) if folder \\\n        else dataset_raw_directory(self.dataset_name)\n\n    self.return_type = None\n\n    # check if the required files have been already downloaded\n    file_path = self.required_files()\n\n    if file_path is None:\n        file_path = self.download()\n        file_path = self.decompress(file_path)\n\n    print(f'files: {file_path}')\n\n    self.process(file_path[0])\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.epinions.epinions_v1.Epinions_V1.required_files","title":"<code>required_files()</code>","text":"<p>Checks for the presence of the required decompressed data file.</p> <p>It first looks for the final, uncompressed file. If not found, it looks for the compressed archive and decompresses it.</p> <p>Returns:</p> Type Description <code>list or None</code> <p>A list containing the path to the required data file if it exists or can be created by decompression. Otherwise, returns None.</p> Source code in <code>datarec/datasets/epinions/epinions_v1.py</code> <pre><code>def required_files(self):\n    \"\"\"\n    Checks for the presence of the required decompressed data file.\n\n    It first looks for the final, uncompressed file. If not found, it\n    looks for the compressed archive and decompresses it.\n\n    Returns:\n        (list or None): A list containing the path to the required data file if\n            it exists or can be created by decompression. Otherwise, returns None.\n    \"\"\"\n    # compressed data file\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n\n    # check if the file is there\n    paths = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n    if all([os.path.exists(p) for p in paths]):\n        return paths\n    # check if the compressed file is there\n    elif os.path.exists(file_path):\n        return self.decompress(file_path)\n    else:\n        return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.epinions.epinions_v1.Epinions_V1.decompress","title":"<code>decompress(path)</code>","text":"<p>Decompresses the downloaded archive after verifying its checksum.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path of the compressed archive.</p> required <p>Returns:</p> Type Description <code>list or None</code> <p>A list containing the path to the decompressed file if successful, otherwise None.</p> Source code in <code>datarec/datasets/epinions/epinions_v1.py</code> <pre><code>def decompress(self, path):\n    \"\"\"\n    Decompresses the downloaded archive after verifying its checksum.\n\n    Args:\n        path (str): The file path of the compressed archive.\n\n    Returns:\n        (list or None): A list containing the path to the decompressed file\n            if successful, otherwise None.\n    \"\"\"\n    verify_checksum(path, self.CHECKSUM)\n\n    # decompress downloaded file\n    decompressed_file = os.path.join(self._raw_folder, self.decompressed_file_name)\n    decompress_gz(path, decompressed_file)\n    files = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n    if all([os.path.exists(f) for f in files]):\n        return [os.path.join(self._raw_folder, f) for f in files]\n    return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.epinions.epinions_v1.Epinions_V1.download","title":"<code>download()</code>","text":"<p>Downloads the raw dataset compressed archive.</p> <p>Returns:</p> Type Description <code>str</code> <p>The local file path to the downloaded .gz archive.</p> Source code in <code>datarec/datasets/epinions/epinions_v1.py</code> <pre><code>def download(self) -&gt; (str, str):\n    \"\"\"\n    Downloads the raw dataset compressed archive.\n\n    Returns:\n        (str): The local file path to the downloaded .gz archive.\n    \"\"\"\n    if not os.path.exists(self._raw_folder):\n        os.makedirs(self._raw_folder)\n        print('Created folder \\'{}\\''.format(self._raw_folder))\n\n    # download dataset file (compressed file)\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n    download_file(self.url, file_path)\n\n    return file_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.epinions.epinions_v1.Epinions_V1.process","title":"<code>process(file_path)</code>","text":"<p>Processes the raw trust network data and loads it into the class.</p> <p>This method reads the file, skipping the header comment lines. The first column is treated as the 'user' (truster) and the second column as the 'item' (trustee).</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the raw <code>soc-Epinions1.txt</code> file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This method assigns the processed data to <code>self.data</code> directly.</p> Source code in <code>datarec/datasets/epinions/epinions_v1.py</code> <pre><code>def process(self, file_path):\n    \"\"\"\n    Processes the raw trust network data and loads it into the class.\n\n    This method reads the file, skipping the header comment\n    lines. The first column is treated as the 'user' (truster) and the\n    second column as the 'item' (trustee).\n\n    Args:\n        file_path (str): The path to the raw `soc-Epinions1.txt` file.\n\n    Returns:\n        (None): This method assigns the processed data to `self.data` directly.\n    \"\"\"\n\n    from datarec.io import read_tabular\n\n    dataset = read_tabular(file_path, sep='\\t',\n                           user_col=0, item_col=1,\n                           header=None, skiprows=4)\n\n    self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#gowalla","title":"Gowalla","text":"<p>Entry point for loading different versions of the Gowalla dataset.</p> <p>Builder class for the Gowalla check-ins dataset.</p> <p>Builder class for the Gowalla friendships (social network) dataset.</p>"},{"location":"documentation/datasets/#datarec.datasets.gowalla.gowalla.Gowalla","title":"<code>Gowalla</code>","text":"<p>Entry point class to load various types of data from the Gowalla dataset.</p> <p>This class provides a single, convenient interface for accessing the dataset. Based on the <code>version</code> parameter, it selects and returns the appropriate dataset builder for either the user check-ins or the social friendships graph.</p> <p>Gowalla was a location-based social network. Two types of data are available: - 'checkins': User interactions with locations (suitable for recommendation). - 'friendships': The user-user social network graph.</p> <p>The default version is 'latest', which currently corresponds to 'checkins'.</p> <p>Examples:</p> <p>To load the user check-in data (default):</p> <pre><code>&gt;&gt;&gt; data_loader = Gowalla()\n# or explicitly\n&gt;&gt;&gt; data_loader = Gowalla(version='checkins')\n</code></pre> <p>To load the social friendship graph:</p> <pre><code>&gt;&gt;&gt; data_loader = Gowalla(version='friendships')\n</code></pre> Source code in <code>datarec/datasets/gowalla/gowalla.py</code> <pre><code>class Gowalla:\n    \"\"\"\n    Entry point class to load various types of data from the Gowalla dataset.\n\n    This class provides a single, convenient interface for accessing the dataset.\n    Based on the `version` parameter, it selects and returns the appropriate\n    dataset builder for either the user check-ins or the social friendships graph.\n\n    Gowalla was a location-based social network. Two types of data are available:\n    - 'checkins': User interactions with locations (suitable for recommendation).\n    - 'friendships': The user-user social network graph.\n\n    The default version is 'latest', which currently corresponds to 'checkins'.\n\n    Examples:\n        To load the user check-in data (default):\n        &gt;&gt;&gt; data_loader = Gowalla()\n        # or explicitly\n        &gt;&gt;&gt; data_loader = Gowalla(version='checkins')\n\n        To load the social friendship graph:\n        &gt;&gt;&gt; data_loader = Gowalla(version='friendships')\n    \"\"\"\n    latest_version = 'checkins'\n\n    def __new__(self, version: str = 'latest', **kwargs):\n        \"\"\"\n        Initializes and returns the specified version of the Gowalla dataset builder.\n\n        This method acts as a dispatcher, instantiating the correct builder class\n        that handles the downloading, caching, and loading for a specific data type.\n\n        Args:\n            version (str): The type of data to load. Supported versions\n                include 'checkins', 'friendships', and 'latest'. Defaults to 'latest'.\n            **kwargs: Additional keyword arguments (not currently used).\n\n        Returns:\n            (GowallaCheckins or GowallaFriendships): An instance of the appropriate dataset\n                builder class, populated with data.\n\n        Raises:\n            ValueError: If an unsupported version string is provided.\n        \"\"\"\n\n        versions = {'friendships': GowallaFriendships,\n                    'checkins': GowallaCheckins}\n        if version == 'latest':\n            version = self.latest_version\n        if version in versions:\n            return versions[version]()\n        else:\n            raise ValueError(\"Gowalla: Unsupported version\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.gowalla.gowalla.Gowalla.__new__","title":"<code>__new__(version='latest', **kwargs)</code>","text":"<p>Initializes and returns the specified version of the Gowalla dataset builder.</p> <p>This method acts as a dispatcher, instantiating the correct builder class that handles the downloading, caching, and loading for a specific data type.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>The type of data to load. Supported versions include 'checkins', 'friendships', and 'latest'. Defaults to 'latest'.</p> <code>'latest'</code> <code>**kwargs</code> <p>Additional keyword arguments (not currently used).</p> <code>{}</code> <p>Returns:</p> Type Description <code>GowallaCheckins or GowallaFriendships</code> <p>An instance of the appropriate dataset builder class, populated with data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported version string is provided.</p> Source code in <code>datarec/datasets/gowalla/gowalla.py</code> <pre><code>def __new__(self, version: str = 'latest', **kwargs):\n    \"\"\"\n    Initializes and returns the specified version of the Gowalla dataset builder.\n\n    This method acts as a dispatcher, instantiating the correct builder class\n    that handles the downloading, caching, and loading for a specific data type.\n\n    Args:\n        version (str): The type of data to load. Supported versions\n            include 'checkins', 'friendships', and 'latest'. Defaults to 'latest'.\n        **kwargs: Additional keyword arguments (not currently used).\n\n    Returns:\n        (GowallaCheckins or GowallaFriendships): An instance of the appropriate dataset\n            builder class, populated with data.\n\n    Raises:\n        ValueError: If an unsupported version string is provided.\n    \"\"\"\n\n    versions = {'friendships': GowallaFriendships,\n                'checkins': GowallaCheckins}\n    if version == 'latest':\n        version = self.latest_version\n    if version in versions:\n        return versions[version]()\n    else:\n        raise ValueError(\"Gowalla: Unsupported version\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.gowalla.gowalla_checkins.GowallaCheckins","title":"<code>GowallaCheckins</code>","text":"<p>               Bases: <code>DataRec</code></p> <p>Builder class for the Gowalla check-ins dataset.</p> <p>This class handles the logic for downloading, preparing, and loading the user check-in data from the Stanford SNAP repository. It is not typically instantiated directly but is called by the <code>Gowalla</code> entry point class when <code>version='checkins'</code>.</p> <p>The dataset contains user check-ins at various locations, representing user-item interactions suitable for recommendation tasks. It was introduced in the paper \"Friendship and mobility: user movement in location-based social networks\".</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL from which the raw dataset is downloaded.</p> <code>CHECKSUM</code> <code>str</code> <p>The MD5 checksum to verify the integrity of the downloaded file.</p> Source code in <code>datarec/datasets/gowalla/gowalla_checkins.py</code> <pre><code>class GowallaCheckins(DataRec):\n    \"\"\"\n    Builder class for the Gowalla check-ins dataset.\n\n    This class handles the logic for downloading, preparing, and loading the\n    user check-in data from the Stanford SNAP repository. It is not typically\n    instantiated directly but is called by the `Gowalla` entry point class\n    when `version='checkins'`.\n\n    The dataset contains user check-ins at various locations, representing\n    user-item interactions suitable for recommendation tasks. It was introduced\n    in the paper \"Friendship and mobility: user movement in location-based social networks\".\n\n    Attributes:\n        url (str): The URL from which the raw dataset is downloaded.\n        CHECKSUM (str): The MD5 checksum to verify the integrity of the downloaded file.\n    \"\"\"\n    url = 'https://snap.stanford.edu/data/loc-gowalla_totalCheckins.txt.gz'\n    data_file_name = os.path.basename(url)\n    decompressed_file_name = data_file_name.replace('.gz', '')\n    REQUIRED_FILES = [decompressed_file_name]\n    CHECKSUM = '8ebd5ed2dd376d8982987c49429cb9f9'\n\n    def __init__(self, folder=None):\n        \"\"\"\n        Initializes the builder and orchestrates the data preparation workflow.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n        \"\"\"\n        super().__init__(None)\n\n        self.dataset_name = 'gowalla'\n        self.version_name = 'checkins'\n\n        self._data_folder = folder if folder \\\n            else dataset_directory(self.dataset_name)\n        self._raw_folder = os.path.abspath(os.path.join(self._data_folder, RAW_DATA_FOLDER)) if folder \\\n            else dataset_raw_directory(self.dataset_name)\n\n        self.return_type = None\n\n        # check if the required files have been already downloaded\n        file_path = self.required_files()\n\n        if file_path is None:\n            file_path = self.download()\n            file_path = self.decompress(file_path)\n\n        self.process(file_path[0])\n\n    def required_files(self):\n        \"\"\"\n        Checks for the presence of the required decompressed data file.\n\n        Returns:\n            (list or None): A list containing the path to the required data file if\n                it exists or can be created by decompression. Otherwise, returns None.\n        \"\"\"\n        # compressed data file\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n\n        # check if the file is there\n        paths = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n        if all([os.path.exists(p) for p in paths]):\n            return paths\n        # check if the compressed file is there\n        elif os.path.exists(file_path):\n            return self.decompress(file_path)\n        else:\n            return None\n\n    def decompress(self, path):\n        \"\"\"\n        Decompresses the downloaded archive after verifying its checksum.\n\n        Args:\n            path (str): The file path of the compressed archive.\n\n        Returns:\n            (list or None): A list containing the path to the decompressed file\n                if successful, otherwise None.\n        \"\"\"\n        print(path)\n        verify_checksum(path, self.CHECKSUM)\n\n        # decompress downloaded file\n        decompressed_file = os.path.join(self._raw_folder, self.decompressed_file_name)\n        decompress_gz(path, decompressed_file)\n        files = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n        if all([os.path.exists(f) for f in files]):\n            return [os.path.join(self._raw_folder, f) for f in files]\n        return None\n\n    def download(self) -&gt; (str, str):\n        \"\"\"\n        Downloads the raw dataset compressed archive.\n\n        Returns:\n            (str): The local file path to the downloaded archive.\n        \"\"\"\n        if not os.path.exists(self._raw_folder):\n            os.makedirs(self._raw_folder)\n            print('Created folder \\'{}\\''.format(self._raw_folder))\n\n        # download dataset file (compressed file)\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n        print('Downloading data from: \\'{}\\''.format(self.url))\n        download_file(self.url, file_path, size=105470044)\n\n        return file_path\n\n    def process(self, file_path):\n        \"\"\"\n        Processes the raw check-in data and loads it into the class.\n\n        This method reads the tab-separated file, which has no header. It maps\n        column 0 to 'user_id', column 4 to 'item_id' (location ID), and column 1\n        to 'timestamp'.\n\n        Args:\n            file_path (str): The path to the raw check-in data file.\n\n        Returns:\n            (None): This method assigns the processed data to `self.data` directly.\n        \"\"\"\n\n        from datarec.io import read_tabular\n\n        dataset = read_tabular(file_path, sep='\\t', user_col=0, item_col=4, timestamp_col=1, header=None)\n        self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.gowalla.gowalla_checkins.GowallaCheckins.__init__","title":"<code>__init__(folder=None)</code>","text":"<p>Initializes the builder and orchestrates the data preparation workflow.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> Source code in <code>datarec/datasets/gowalla/gowalla_checkins.py</code> <pre><code>def __init__(self, folder=None):\n    \"\"\"\n    Initializes the builder and orchestrates the data preparation workflow.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n    \"\"\"\n    super().__init__(None)\n\n    self.dataset_name = 'gowalla'\n    self.version_name = 'checkins'\n\n    self._data_folder = folder if folder \\\n        else dataset_directory(self.dataset_name)\n    self._raw_folder = os.path.abspath(os.path.join(self._data_folder, RAW_DATA_FOLDER)) if folder \\\n        else dataset_raw_directory(self.dataset_name)\n\n    self.return_type = None\n\n    # check if the required files have been already downloaded\n    file_path = self.required_files()\n\n    if file_path is None:\n        file_path = self.download()\n        file_path = self.decompress(file_path)\n\n    self.process(file_path[0])\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.gowalla.gowalla_checkins.GowallaCheckins.required_files","title":"<code>required_files()</code>","text":"<p>Checks for the presence of the required decompressed data file.</p> <p>Returns:</p> Type Description <code>list or None</code> <p>A list containing the path to the required data file if it exists or can be created by decompression. Otherwise, returns None.</p> Source code in <code>datarec/datasets/gowalla/gowalla_checkins.py</code> <pre><code>def required_files(self):\n    \"\"\"\n    Checks for the presence of the required decompressed data file.\n\n    Returns:\n        (list or None): A list containing the path to the required data file if\n            it exists or can be created by decompression. Otherwise, returns None.\n    \"\"\"\n    # compressed data file\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n\n    # check if the file is there\n    paths = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n    if all([os.path.exists(p) for p in paths]):\n        return paths\n    # check if the compressed file is there\n    elif os.path.exists(file_path):\n        return self.decompress(file_path)\n    else:\n        return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.gowalla.gowalla_checkins.GowallaCheckins.decompress","title":"<code>decompress(path)</code>","text":"<p>Decompresses the downloaded archive after verifying its checksum.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path of the compressed archive.</p> required <p>Returns:</p> Type Description <code>list or None</code> <p>A list containing the path to the decompressed file if successful, otherwise None.</p> Source code in <code>datarec/datasets/gowalla/gowalla_checkins.py</code> <pre><code>def decompress(self, path):\n    \"\"\"\n    Decompresses the downloaded archive after verifying its checksum.\n\n    Args:\n        path (str): The file path of the compressed archive.\n\n    Returns:\n        (list or None): A list containing the path to the decompressed file\n            if successful, otherwise None.\n    \"\"\"\n    print(path)\n    verify_checksum(path, self.CHECKSUM)\n\n    # decompress downloaded file\n    decompressed_file = os.path.join(self._raw_folder, self.decompressed_file_name)\n    decompress_gz(path, decompressed_file)\n    files = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n    if all([os.path.exists(f) for f in files]):\n        return [os.path.join(self._raw_folder, f) for f in files]\n    return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.gowalla.gowalla_checkins.GowallaCheckins.download","title":"<code>download()</code>","text":"<p>Downloads the raw dataset compressed archive.</p> <p>Returns:</p> Type Description <code>str</code> <p>The local file path to the downloaded archive.</p> Source code in <code>datarec/datasets/gowalla/gowalla_checkins.py</code> <pre><code>def download(self) -&gt; (str, str):\n    \"\"\"\n    Downloads the raw dataset compressed archive.\n\n    Returns:\n        (str): The local file path to the downloaded archive.\n    \"\"\"\n    if not os.path.exists(self._raw_folder):\n        os.makedirs(self._raw_folder)\n        print('Created folder \\'{}\\''.format(self._raw_folder))\n\n    # download dataset file (compressed file)\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n    print('Downloading data from: \\'{}\\''.format(self.url))\n    download_file(self.url, file_path, size=105470044)\n\n    return file_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.gowalla.gowalla_checkins.GowallaCheckins.process","title":"<code>process(file_path)</code>","text":"<p>Processes the raw check-in data and loads it into the class.</p> <p>This method reads the tab-separated file, which has no header. It maps column 0 to 'user_id', column 4 to 'item_id' (location ID), and column 1 to 'timestamp'.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the raw check-in data file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This method assigns the processed data to <code>self.data</code> directly.</p> Source code in <code>datarec/datasets/gowalla/gowalla_checkins.py</code> <pre><code>def process(self, file_path):\n    \"\"\"\n    Processes the raw check-in data and loads it into the class.\n\n    This method reads the tab-separated file, which has no header. It maps\n    column 0 to 'user_id', column 4 to 'item_id' (location ID), and column 1\n    to 'timestamp'.\n\n    Args:\n        file_path (str): The path to the raw check-in data file.\n\n    Returns:\n        (None): This method assigns the processed data to `self.data` directly.\n    \"\"\"\n\n    from datarec.io import read_tabular\n\n    dataset = read_tabular(file_path, sep='\\t', user_col=0, item_col=4, timestamp_col=1, header=None)\n    self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.gowalla.gowalla_friendships.GowallaFriendships","title":"<code>GowallaFriendships</code>","text":"<p>               Bases: <code>DataRec</code></p> <p>Builder class for the Gowalla friendships dataset.</p> <p>This class handles the logic for downloading, preparing, and loading the user-user social network graph from the Stanford SNAP repository. It is not typically instantiated directly but is called by the <code>Gowalla</code> entry point class when <code>version='friendships'</code>.</p> <p>The dataset contains the social friendship network of Gowalla users. Each row represents a directed edge from one user to another.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL from which the raw dataset is downloaded.</p> <code>CHECKSUM</code> <code>str</code> <p>The MD5 checksum to verify the integrity of the downloaded file.</p> Source code in <code>datarec/datasets/gowalla/gowalla_friendships.py</code> <pre><code>class GowallaFriendships(DataRec):\n    \"\"\"\n    Builder class for the Gowalla friendships dataset.\n\n    This class handles the logic for downloading, preparing, and loading the\n    user-user social network graph from the Stanford SNAP repository. It is not\n    typically instantiated directly but is called by the `Gowalla` entry point class\n    when `version='friendships'`.\n\n    The dataset contains the social friendship network of Gowalla users. Each row\n    represents a directed edge from one user to another.\n\n    Attributes:\n        url (str): The URL from which the raw dataset is downloaded.\n        CHECKSUM (str): The MD5 checksum to verify the integrity of the downloaded file.\n    \"\"\"\n    url = 'https://snap.stanford.edu/data/loc-gowalla_edges.txt.gz'\n    data_file_name = os.path.basename(url)\n    decompressed_file_name = data_file_name.replace('.gz', '')\n    REQUIRED_FILES = [decompressed_file_name]\n    CHECKSUM = '68bce8dc51609fe32bbd95e668aaf65e'\n\n    def __init__(self, folder=None):\n        \"\"\"\n        Initializes the builder and orchestrates the data preparation workflow.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n        \"\"\"\n        super().__init__(None)\n\n        self.dataset_name = 'gowalla'\n        self.version_name = 'friendships'\n\n        self._data_folder = folder if folder \\\n            else dataset_directory(self.dataset_name)\n        self._raw_folder = os.path.abspath(os.path.join(self._data_folder, RAW_DATA_FOLDER)) if folder \\\n            else dataset_raw_directory(self.dataset_name)\n\n        self.return_type = None\n\n        # check if the required files have been already downloaded\n        file_path = self.required_files()\n\n        if file_path is None:\n            file_path = self.download()\n            file_path = self.decompress(file_path)  # only one file\n\n        self.process(file_path[0])\n\n    def required_files(self):\n        \"\"\"\n        Checks for the presence of the required decompressed data file.\n\n        Returns:\n            (list or None): A list containing the path to the required data file if\n                it exists or can be created by decompression. Otherwise, returns None.\n        \"\"\"\n        # compressed data file\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n\n        # check if the file is there\n        paths = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n        if all([os.path.exists(p) for p in paths]):\n            return paths\n        # check if the compressed file is there\n        elif os.path.exists(file_path):\n            return self.decompress(file_path)\n        else:\n            return None\n\n    def decompress(self, path):\n        \"\"\"\n        Decompresses the downloaded archive after verifying its checksum.\n\n        Args:\n            path (str): The file path of the compressed archive.\n\n        Returns:\n            (list or None): A list containing the path to the decompressed file\n                if successful, otherwise None.\n        \"\"\"\n        verify_checksum(path, self.CHECKSUM)\n\n        # decompress downloaded file\n        decompressed_file = os.path.join(self._raw_folder, self.decompressed_file_name)\n        decompress_gz(path, decompressed_file)\n        files = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n        if all([os.path.exists(f) for f in files]):\n            return [os.path.join(self._raw_folder, f) for f in files]\n        return None\n\n    def download(self) -&gt; (str, str):\n        \"\"\"\n        Downloads the raw dataset compressed archive.\n\n        Returns:\n            (str): The local file path to the downloaded archive.\n        \"\"\"\n        if not os.path.exists(self._raw_folder):\n            os.makedirs(self._raw_folder)\n            print('Created folder \\'{}\\''.format(self._raw_folder))\n\n        # download dataset file (compressed file)\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n        download_file(self.url, file_path, size=6351523)\n\n        return file_path\n\n    def process(self, file_path):\n        \"\"\"\n        Processes the raw friendship data and loads it into the class.\n\n        This method reads the file, which has no header. Each row\n        represents a user-user link. To fit the DataRec structure, the first\n        user column is mapped to 'user_id' and the second to 'item_id'.\n\n        Args:\n            file_path (str): The path to the raw friendship data file.\n\n        Returns:\n            (None): This method assigns the processed data to `self.data` directly.\n        \"\"\"\n\n        from datarec.io import read_tabular\n\n        dataset = read_tabular(file_path, sep='\\t', user_col=0, item_col=1, header=None)\n        self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.gowalla.gowalla_friendships.GowallaFriendships.__init__","title":"<code>__init__(folder=None)</code>","text":"<p>Initializes the builder and orchestrates the data preparation workflow.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> Source code in <code>datarec/datasets/gowalla/gowalla_friendships.py</code> <pre><code>def __init__(self, folder=None):\n    \"\"\"\n    Initializes the builder and orchestrates the data preparation workflow.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n    \"\"\"\n    super().__init__(None)\n\n    self.dataset_name = 'gowalla'\n    self.version_name = 'friendships'\n\n    self._data_folder = folder if folder \\\n        else dataset_directory(self.dataset_name)\n    self._raw_folder = os.path.abspath(os.path.join(self._data_folder, RAW_DATA_FOLDER)) if folder \\\n        else dataset_raw_directory(self.dataset_name)\n\n    self.return_type = None\n\n    # check if the required files have been already downloaded\n    file_path = self.required_files()\n\n    if file_path is None:\n        file_path = self.download()\n        file_path = self.decompress(file_path)  # only one file\n\n    self.process(file_path[0])\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.gowalla.gowalla_friendships.GowallaFriendships.required_files","title":"<code>required_files()</code>","text":"<p>Checks for the presence of the required decompressed data file.</p> <p>Returns:</p> Type Description <code>list or None</code> <p>A list containing the path to the required data file if it exists or can be created by decompression. Otherwise, returns None.</p> Source code in <code>datarec/datasets/gowalla/gowalla_friendships.py</code> <pre><code>def required_files(self):\n    \"\"\"\n    Checks for the presence of the required decompressed data file.\n\n    Returns:\n        (list or None): A list containing the path to the required data file if\n            it exists or can be created by decompression. Otherwise, returns None.\n    \"\"\"\n    # compressed data file\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n\n    # check if the file is there\n    paths = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n    if all([os.path.exists(p) for p in paths]):\n        return paths\n    # check if the compressed file is there\n    elif os.path.exists(file_path):\n        return self.decompress(file_path)\n    else:\n        return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.gowalla.gowalla_friendships.GowallaFriendships.decompress","title":"<code>decompress(path)</code>","text":"<p>Decompresses the downloaded archive after verifying its checksum.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path of the compressed archive.</p> required <p>Returns:</p> Type Description <code>list or None</code> <p>A list containing the path to the decompressed file if successful, otherwise None.</p> Source code in <code>datarec/datasets/gowalla/gowalla_friendships.py</code> <pre><code>def decompress(self, path):\n    \"\"\"\n    Decompresses the downloaded archive after verifying its checksum.\n\n    Args:\n        path (str): The file path of the compressed archive.\n\n    Returns:\n        (list or None): A list containing the path to the decompressed file\n            if successful, otherwise None.\n    \"\"\"\n    verify_checksum(path, self.CHECKSUM)\n\n    # decompress downloaded file\n    decompressed_file = os.path.join(self._raw_folder, self.decompressed_file_name)\n    decompress_gz(path, decompressed_file)\n    files = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n    if all([os.path.exists(f) for f in files]):\n        return [os.path.join(self._raw_folder, f) for f in files]\n    return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.gowalla.gowalla_friendships.GowallaFriendships.download","title":"<code>download()</code>","text":"<p>Downloads the raw dataset compressed archive.</p> <p>Returns:</p> Type Description <code>str</code> <p>The local file path to the downloaded archive.</p> Source code in <code>datarec/datasets/gowalla/gowalla_friendships.py</code> <pre><code>def download(self) -&gt; (str, str):\n    \"\"\"\n    Downloads the raw dataset compressed archive.\n\n    Returns:\n        (str): The local file path to the downloaded archive.\n    \"\"\"\n    if not os.path.exists(self._raw_folder):\n        os.makedirs(self._raw_folder)\n        print('Created folder \\'{}\\''.format(self._raw_folder))\n\n    # download dataset file (compressed file)\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n    download_file(self.url, file_path, size=6351523)\n\n    return file_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.gowalla.gowalla_friendships.GowallaFriendships.process","title":"<code>process(file_path)</code>","text":"<p>Processes the raw friendship data and loads it into the class.</p> <p>This method reads the file, which has no header. Each row represents a user-user link. To fit the DataRec structure, the first user column is mapped to 'user_id' and the second to 'item_id'.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the raw friendship data file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This method assigns the processed data to <code>self.data</code> directly.</p> Source code in <code>datarec/datasets/gowalla/gowalla_friendships.py</code> <pre><code>def process(self, file_path):\n    \"\"\"\n    Processes the raw friendship data and loads it into the class.\n\n    This method reads the file, which has no header. Each row\n    represents a user-user link. To fit the DataRec structure, the first\n    user column is mapped to 'user_id' and the second to 'item_id'.\n\n    Args:\n        file_path (str): The path to the raw friendship data file.\n\n    Returns:\n        (None): This method assigns the processed data to `self.data` directly.\n    \"\"\"\n\n    from datarec.io import read_tabular\n\n    dataset = read_tabular(file_path, sep='\\t', user_col=0, item_col=1, header=None)\n    self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#lastfm","title":"Last.fm","text":"<p>Entry point for loading different versions of the Last.fm dataset.</p> <p>Builder class for the 2011 version of the Last.fm dataset (HetRec 2011).</p>"},{"location":"documentation/datasets/#datarec.datasets.lastfm.lastfm.LastFM","title":"<code>LastFM</code>","text":"<p>Entry point class to load various versions of the Last.fm dataset.</p> <p>This class provides a single, convenient interface for accessing the dataset. Based on the <code>version</code> parameter, it selects and returns the appropriate dataset builder.</p> <p>This dataset contains social networking, tagging, and music artist listening information from the Last.fm online music system. This loader focuses on the user-artist listening data. It was released during the HetRec 2011 workshop.</p> <p>The default version is 'latest', which currently corresponds to '2011'.</p> <p>Examples:</p> <p>To load the latest version:</p> <pre><code>&gt;&gt;&gt; data_loader = LastFM()\n</code></pre> <p>To load a specific version:</p> <pre><code>&gt;&gt;&gt; data_loader = LastFM(version='2011')\n</code></pre> Source code in <code>datarec/datasets/lastfm/lastfm.py</code> <pre><code>class LastFM:\n    \"\"\"\n    Entry point class to load various versions of the Last.fm dataset.\n\n    This class provides a single, convenient interface for accessing the dataset.\n    Based on the `version` parameter, it selects and returns the appropriate\n    dataset builder.\n\n    This dataset contains social networking, tagging, and music artist listening\n    information from the Last.fm online music system. This loader focuses on the\n    user-artist listening data. It was released during the HetRec 2011 workshop.\n\n    The default version is 'latest', which currently corresponds to '2011'.\n\n    Examples:\n        To load the latest version:\n        &gt;&gt;&gt; data_loader = LastFM()\n\n        To load a specific version:\n        &gt;&gt;&gt; data_loader = LastFM(version='2011')\n    \"\"\"\n    VERSIONS = {'2011': LastFM2011}\n    latest_version = '2011'\n\n    def __new__(cls, version: str = 'latest', **kwargs):\n        \"\"\"\n        Initializes and returns the specified version of the Last.fm dataset builder.\n\n        This method acts as a dispatcher, instantiating the correct builder class\n        that handles the downloading, caching, and loading for a specific dataset version.\n\n        Args:\n            version (str): The version of the dataset to load. Currently, only\n                '2011' and 'latest' are supported. Defaults to 'latest'.\n            **kwargs: Additional keyword arguments (not currently used for this dataset).\n\n        Returns:\n            (LastFM2011): An instance of the dataset builder class, populated with data.\n\n        Raises:\n            ValueError: If an unsupported version string is provided.\n        \"\"\"\n\n        if version == 'latest':\n            version = cls.latest_version\n        if version in cls.VERSIONS:\n            return cls.VERSIONS[version]()\n        else:\n            raise ValueError(f\"HetRec LastFM {version}: Unsupported version \\n Supported version:\"\n                             f\"\\n \\t version \\t name \"\n                             f\"\\n \\t 2011 \\t LastFM (HetRec) 2011\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.lastfm.lastfm.LastFM.__new__","title":"<code>__new__(version='latest', **kwargs)</code>","text":"<p>Initializes and returns the specified version of the Last.fm dataset builder.</p> <p>This method acts as a dispatcher, instantiating the correct builder class that handles the downloading, caching, and loading for a specific dataset version.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>The version of the dataset to load. Currently, only '2011' and 'latest' are supported. Defaults to 'latest'.</p> <code>'latest'</code> <code>**kwargs</code> <p>Additional keyword arguments (not currently used for this dataset).</p> <code>{}</code> <p>Returns:</p> Type Description <code>LastFM2011</code> <p>An instance of the dataset builder class, populated with data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported version string is provided.</p> Source code in <code>datarec/datasets/lastfm/lastfm.py</code> <pre><code>def __new__(cls, version: str = 'latest', **kwargs):\n    \"\"\"\n    Initializes and returns the specified version of the Last.fm dataset builder.\n\n    This method acts as a dispatcher, instantiating the correct builder class\n    that handles the downloading, caching, and loading for a specific dataset version.\n\n    Args:\n        version (str): The version of the dataset to load. Currently, only\n            '2011' and 'latest' are supported. Defaults to 'latest'.\n        **kwargs: Additional keyword arguments (not currently used for this dataset).\n\n    Returns:\n        (LastFM2011): An instance of the dataset builder class, populated with data.\n\n    Raises:\n        ValueError: If an unsupported version string is provided.\n    \"\"\"\n\n    if version == 'latest':\n        version = cls.latest_version\n    if version in cls.VERSIONS:\n        return cls.VERSIONS[version]()\n    else:\n        raise ValueError(f\"HetRec LastFM {version}: Unsupported version \\n Supported version:\"\n                         f\"\\n \\t version \\t name \"\n                         f\"\\n \\t 2011 \\t LastFM (HetRec) 2011\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.lastfm.lastfm_2011.LastFM2011","title":"<code>LastFM2011</code>","text":"<p>               Bases: <code>DataRec</code></p> <p>Builder class for the Last.fm dataset (HetRec 2011 version).</p> <p>This class handles the logic for downloading, preparing, and loading the Last.fm dataset provided for the 2nd International Workshop on Information Heterogeneity and Fusion in Recommender Systems (HetRec 2011).</p> <p>The full archive contains multiple files (user-friends, tags, etc.), but this loader specifically processes the <code>user_artists.dat</code> file, which contains artists listened to by each user and a corresponding listening count (<code>weight</code>).</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL from which the raw dataset is downloaded.</p> <code>CHECKSUM</code> <code>str</code> <p>The MD5 checksum to verify the integrity of the downloaded file.</p> <code>REQUIRED_FILES</code> <code>list</code> <p>A list of all files expected within the decompressed archive.</p> Source code in <code>datarec/datasets/lastfm/lastfm_2011.py</code> <pre><code>class LastFM2011(DataRec):\n    \"\"\"\n    Builder class for the Last.fm dataset (HetRec 2011 version).\n\n    This class handles the logic for downloading, preparing, and loading the\n    Last.fm dataset provided for the 2nd International Workshop on Information\n    Heterogeneity and Fusion in Recommender Systems (HetRec 2011).\n\n    The full archive contains multiple files (user-friends, tags, etc.), but this\n    loader specifically processes the `user_artists.dat` file, which contains\n    artists listened to by each user and a corresponding listening count (`weight`).\n\n    Attributes:\n        url (str): The URL from which the raw dataset is downloaded.\n        CHECKSUM (str): The MD5 checksum to verify the integrity of the downloaded file.\n        REQUIRED_FILES (list): A list of all files expected within the decompressed archive.\n    \"\"\"\n\n    url = 'https://files.grouplens.org/datasets/hetrec2011/hetrec2011-lastfm-2k.zip'\n    data_file_name = os.path.basename(url)\n    decompressed_data_file_name = data_file_name.replace('.zip', '')\n\n    user_artists_file_name = 'user_artists.dat'\n    tags_file_name = 'tags.dat'\n    artists_file_name = 'artists.dat'\n    user_taggedartists_file_name = 'user_taggedartists.dat'\n    user_taggedartists_timestamp_file_name = 'user_taggedartists-timestamps.dat'\n    user_friends_file_name = 'user_friends.dat'\n\n    REQUIRED_FILES = [p for p in\n                      [user_friends_file_name, user_taggedartists_file_name, user_taggedartists_timestamp_file_name,\n                       artists_file_name, tags_file_name, user_artists_file_name]]\n    # REQUIRED_FILES = [os.path.join('ml-1m', p) for p in [movies_file_name, ratings_file_name, users_file_name]]\n    CHECKSUM = '296d61afe4e8632b173fc2dd3be20ce2'\n\n    def __init__(self, folder=None):\n        \"\"\"\n        Initializes the builder and orchestrates the data preparation workflow.\n\n        This constructor sets up the necessary paths and automatically triggers\n        the download, verification, and processing steps if the data is not\n        already present in the specified cache directory.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n        \"\"\"\n        super().__init__(None)\n\n        self.dataset_name = 'lastfm'\n        self.version_name = '2011'\n\n        self._data_folder = folder if folder \\\n            else dataset_directory(self.dataset_name)\n        self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n            else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n\n        self.return_type = None\n\n        # check if the required files have been already downloaded\n        file_path = self.required_files()\n\n        if file_path is None:\n            file_path = self.download()\n            file_path = self.decompress(file_path)\n\n        user_friends_file_path, user_taggedartists_file_path, user_taggedartists_timestamp_file_path, artists_file_path, tags_file_path, user_artists_file_path = file_path\n        self.process(user_artists_file_path)\n\n    def required_files(self):\n        \"\"\"\n        Checks for the presence of all required decompressed data files.\n\n        Returns:\n            (list or None): A list of paths to the required data files if they\n                exist or can be created by decompression. Otherwise, returns None.\n        \"\"\"\n        # compressed data file\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n        paths = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n\n        if all([os.path.exists(p) for p in paths]):\n            return paths\n        elif os.path.exists(file_path):\n            return self.decompress(file_path)\n        else:\n            return None\n\n    def decompress(self, path):\n        \"\"\"\n        Decompresses the downloaded archive after verifying its checksum.\n\n        Args:\n            path (str): The file path of the compressed archive.\n\n        Returns:\n            (list or None): A list of paths to the decompressed files if successful,\n                otherwise None.\n        \"\"\"\n        verify_checksum(path, self.CHECKSUM)\n\n        # decompress downloaded file\n        decompress_zip_file(path, self._raw_folder)\n        files = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n        if all([os.path.exists(f) for f in files]):\n            return [os.path.join(self._raw_folder, f) for f in files]\n        return None\n\n    def download(self) -&gt; (str, str):\n        \"\"\"\n        Downloads the raw dataset compressed archive.\n\n        Returns:\n            (str): The local file path to the downloaded archive.\n        \"\"\"\n        if not os.path.exists(self._raw_folder):\n            os.makedirs(self._raw_folder)\n            print('Created folder \\'{}\\''.format(self._raw_folder))\n\n        # download dataset file (compressed file)\n        file_name = os.path.basename(self.url)\n        file_path = os.path.join(self._raw_folder, file_name)\n        download_file(self.url, file_path)\n\n        return file_path\n\n    def process(self, file_path):\n        \"\"\"\n        Processes the raw `user_artists.dat` file and loads it into the class.\n\n        This method reads the tab-separated file, which includes a header. It maps\n        the 'userID', 'artistID', and 'weight' columns to the standard user, item,\n        and rating columns, respectively. Note that timestamp information is not\n        available in this specific file.\n\n        Args:\n            file_path (str): The path to the raw `user_artists.dat` file.\n\n        Returns:\n            (None): This method assigns the processed data to `self.data` directly.\n        \"\"\"\n\n        from datarec.io import read_tabular\n        dataset = read_tabular(file_path, sep='\\t',\n                               user_col='userID', item_col='artistID',\n                               rating_col='weight',\n                               header=0)\n        self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.lastfm.lastfm_2011.LastFM2011.__init__","title":"<code>__init__(folder=None)</code>","text":"<p>Initializes the builder and orchestrates the data preparation workflow.</p> <p>This constructor sets up the necessary paths and automatically triggers the download, verification, and processing steps if the data is not already present in the specified cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> Source code in <code>datarec/datasets/lastfm/lastfm_2011.py</code> <pre><code>def __init__(self, folder=None):\n    \"\"\"\n    Initializes the builder and orchestrates the data preparation workflow.\n\n    This constructor sets up the necessary paths and automatically triggers\n    the download, verification, and processing steps if the data is not\n    already present in the specified cache directory.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n    \"\"\"\n    super().__init__(None)\n\n    self.dataset_name = 'lastfm'\n    self.version_name = '2011'\n\n    self._data_folder = folder if folder \\\n        else dataset_directory(self.dataset_name)\n    self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n        else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n\n    self.return_type = None\n\n    # check if the required files have been already downloaded\n    file_path = self.required_files()\n\n    if file_path is None:\n        file_path = self.download()\n        file_path = self.decompress(file_path)\n\n    user_friends_file_path, user_taggedartists_file_path, user_taggedartists_timestamp_file_path, artists_file_path, tags_file_path, user_artists_file_path = file_path\n    self.process(user_artists_file_path)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.lastfm.lastfm_2011.LastFM2011.required_files","title":"<code>required_files()</code>","text":"<p>Checks for the presence of all required decompressed data files.</p> <p>Returns:</p> Type Description <code>list or None</code> <p>A list of paths to the required data files if they exist or can be created by decompression. Otherwise, returns None.</p> Source code in <code>datarec/datasets/lastfm/lastfm_2011.py</code> <pre><code>def required_files(self):\n    \"\"\"\n    Checks for the presence of all required decompressed data files.\n\n    Returns:\n        (list or None): A list of paths to the required data files if they\n            exist or can be created by decompression. Otherwise, returns None.\n    \"\"\"\n    # compressed data file\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n    paths = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n\n    if all([os.path.exists(p) for p in paths]):\n        return paths\n    elif os.path.exists(file_path):\n        return self.decompress(file_path)\n    else:\n        return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.lastfm.lastfm_2011.LastFM2011.decompress","title":"<code>decompress(path)</code>","text":"<p>Decompresses the downloaded archive after verifying its checksum.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path of the compressed archive.</p> required <p>Returns:</p> Type Description <code>list or None</code> <p>A list of paths to the decompressed files if successful, otherwise None.</p> Source code in <code>datarec/datasets/lastfm/lastfm_2011.py</code> <pre><code>def decompress(self, path):\n    \"\"\"\n    Decompresses the downloaded archive after verifying its checksum.\n\n    Args:\n        path (str): The file path of the compressed archive.\n\n    Returns:\n        (list or None): A list of paths to the decompressed files if successful,\n            otherwise None.\n    \"\"\"\n    verify_checksum(path, self.CHECKSUM)\n\n    # decompress downloaded file\n    decompress_zip_file(path, self._raw_folder)\n    files = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n    if all([os.path.exists(f) for f in files]):\n        return [os.path.join(self._raw_folder, f) for f in files]\n    return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.lastfm.lastfm_2011.LastFM2011.download","title":"<code>download()</code>","text":"<p>Downloads the raw dataset compressed archive.</p> <p>Returns:</p> Type Description <code>str</code> <p>The local file path to the downloaded archive.</p> Source code in <code>datarec/datasets/lastfm/lastfm_2011.py</code> <pre><code>def download(self) -&gt; (str, str):\n    \"\"\"\n    Downloads the raw dataset compressed archive.\n\n    Returns:\n        (str): The local file path to the downloaded archive.\n    \"\"\"\n    if not os.path.exists(self._raw_folder):\n        os.makedirs(self._raw_folder)\n        print('Created folder \\'{}\\''.format(self._raw_folder))\n\n    # download dataset file (compressed file)\n    file_name = os.path.basename(self.url)\n    file_path = os.path.join(self._raw_folder, file_name)\n    download_file(self.url, file_path)\n\n    return file_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.lastfm.lastfm_2011.LastFM2011.process","title":"<code>process(file_path)</code>","text":"<p>Processes the raw <code>user_artists.dat</code> file and loads it into the class.</p> <p>This method reads the tab-separated file, which includes a header. It maps the 'userID', 'artistID', and 'weight' columns to the standard user, item, and rating columns, respectively. Note that timestamp information is not available in this specific file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the raw <code>user_artists.dat</code> file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This method assigns the processed data to <code>self.data</code> directly.</p> Source code in <code>datarec/datasets/lastfm/lastfm_2011.py</code> <pre><code>def process(self, file_path):\n    \"\"\"\n    Processes the raw `user_artists.dat` file and loads it into the class.\n\n    This method reads the tab-separated file, which includes a header. It maps\n    the 'userID', 'artistID', and 'weight' columns to the standard user, item,\n    and rating columns, respectively. Note that timestamp information is not\n    available in this specific file.\n\n    Args:\n        file_path (str): The path to the raw `user_artists.dat` file.\n\n    Returns:\n        (None): This method assigns the processed data to `self.data` directly.\n    \"\"\"\n\n    from datarec.io import read_tabular\n    dataset = read_tabular(file_path, sep='\\t',\n                           user_col='userID', item_col='artistID',\n                           rating_col='weight',\n                           header=0)\n    self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#mind-microsoft-news-dataset","title":"MIND (Microsoft News Dataset)","text":"<p>Entry point for loading different versions of the MIND dataset.</p> <p>Builder class for the large version of the MIND dataset.</p> <p>Builder class for the small version of the MIND dataset.</p>"},{"location":"documentation/datasets/#datarec.datasets.mind.mind.Mind","title":"<code>Mind</code>","text":"<p>Entry point class to load various versions of the MIND dataset.</p> <p>This class provides a single, convenient interface for accessing the Microsoft News Dataset (MIND). Based on the <code>version</code> parameter, it selects and returns the appropriate dataset builder for either the 'small' or 'large' version.</p> <p>MIND is a large-scale dataset for news recommendation research. It contains user click histories on a news website.</p> <p>Note: This dataset requires manual download from the official source.</p> <p>The default version is 'latest', which currently corresponds to the 'large' version.</p> <p>Examples:</p> <p>To load the training split of the large version (default):</p> <pre><code>&gt;&gt;&gt; data_loader = Mind()\n</code></pre> <p>To load the validation split of the small version:</p> <pre><code>&gt;&gt;&gt; data_loader = Mind(version='small', split='validation')\n</code></pre> Source code in <code>datarec/datasets/mind/mind.py</code> <pre><code>class Mind:\n    \"\"\"\n    Entry point class to load various versions of the MIND dataset.\n\n    This class provides a single, convenient interface for accessing the Microsoft\n    News Dataset (MIND). Based on the `version` parameter, it selects and returns\n    the appropriate dataset builder for either the 'small' or 'large' version.\n\n    MIND is a large-scale dataset for news recommendation research. It contains user\n    click histories on a news website.\n\n    **Note:** This dataset requires manual download from the official source.\n\n    The default version is 'latest', which currently corresponds to the 'large' version.\n\n    Examples:\n        To load the training split of the large version (default):\n        &gt;&gt;&gt; data_loader = Mind()\n\n        To load the validation split of the small version:\n        &gt;&gt;&gt; data_loader = Mind(version='small', split='validation')\n    \"\"\"\n    latest_version = 'large'\n    versions = {'large': MindLarge,\n                'small': MindSmall}\n\n\n    def __new__(cls, version: str = 'latest', split: str = 'train', **kwargs):\n        \"\"\"\n        Initializes and returns the specified version of the MIND dataset builder.\n\n        This method acts as a dispatcher, instantiating the correct builder class\n        that handles the preparation and loading for a specific dataset version and split.\n\n        Args:\n            version (str): The version of the dataset to load. Supported versions\n                include 'large', 'small', and 'latest'. Defaults to 'latest'.\n            split (str): The data split to load. For 'large', options are 'train',\n                'validation', 'test'. For 'small', options are 'train', 'validation'.\n                Defaults to 'train'.\n            **kwargs: Additional keyword arguments (not currently used).\n\n        Returns:\n            (MindLarge or MindSmall): An instance of the dataset builder class,\n                populated with data from the specified split.\n\n        Raises:\n            ValueError: If an unsupported version string is provided.\n        \"\"\"\n\n        if version == 'latest':\n            version = cls.latest_version\n        if version in cls.versions:\n            return cls.versions[version](split=split)\n        else:\n            raise ValueError(\"Mind dataset: Unsupported version\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.mind.mind.Mind.__new__","title":"<code>__new__(version='latest', split='train', **kwargs)</code>","text":"<p>Initializes and returns the specified version of the MIND dataset builder.</p> <p>This method acts as a dispatcher, instantiating the correct builder class that handles the preparation and loading for a specific dataset version and split.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>The version of the dataset to load. Supported versions include 'large', 'small', and 'latest'. Defaults to 'latest'.</p> <code>'latest'</code> <code>split</code> <code>str</code> <p>The data split to load. For 'large', options are 'train', 'validation', 'test'. For 'small', options are 'train', 'validation'. Defaults to 'train'.</p> <code>'train'</code> <code>**kwargs</code> <p>Additional keyword arguments (not currently used).</p> <code>{}</code> <p>Returns:</p> Type Description <code>MindLarge or MindSmall</code> <p>An instance of the dataset builder class, populated with data from the specified split.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported version string is provided.</p> Source code in <code>datarec/datasets/mind/mind.py</code> <pre><code>def __new__(cls, version: str = 'latest', split: str = 'train', **kwargs):\n    \"\"\"\n    Initializes and returns the specified version of the MIND dataset builder.\n\n    This method acts as a dispatcher, instantiating the correct builder class\n    that handles the preparation and loading for a specific dataset version and split.\n\n    Args:\n        version (str): The version of the dataset to load. Supported versions\n            include 'large', 'small', and 'latest'. Defaults to 'latest'.\n        split (str): The data split to load. For 'large', options are 'train',\n            'validation', 'test'. For 'small', options are 'train', 'validation'.\n            Defaults to 'train'.\n        **kwargs: Additional keyword arguments (not currently used).\n\n    Returns:\n        (MindLarge or MindSmall): An instance of the dataset builder class,\n            populated with data from the specified split.\n\n    Raises:\n        ValueError: If an unsupported version string is provided.\n    \"\"\"\n\n    if version == 'latest':\n        version = cls.latest_version\n    if version in cls.versions:\n        return cls.versions[version](split=split)\n    else:\n        raise ValueError(\"Mind dataset: Unsupported version\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.mind.mindLarge.MindLarge","title":"<code>MindLarge</code>","text":"<p>               Bases: <code>DataRec</code></p> <p>Builder class for the large version of the MIND dataset.</p> <p>This class handles the logic for preparing and loading the MINDlarge dataset. It is not typically instantiated directly but is called by the <code>Mind</code> entry point class.</p> <p>Note on usage: The MIND dataset must be downloaded manually. This class will prompt the user to download the required zip files and place them in the correct cache directory before proceeding with decompression and processing.</p> <p>The dataset is pre-split into train, validation, and test sets, which can be loaded individually.</p> <p>Attributes:</p> Name Type Description <code>source</code> <code>str</code> <p>The official website for the dataset.</p> <code>REQUIRED</code> <code>dict</code> <p>A dictionary detailing the filenames and checksums for each data split (train, validation, test).</p> Source code in <code>datarec/datasets/mind/mindLarge.py</code> <pre><code>class MindLarge(DataRec):\n    \"\"\"\n    Builder class for the large version of the MIND dataset.\n\n    This class handles the logic for preparing and loading the MINDlarge dataset.\n    It is not typically instantiated directly but is called by the `Mind` entry\n    point class.\n\n    **Note on usage:** The MIND dataset must be downloaded manually. This class will\n    prompt the user to download the required zip files and place them in the\n    correct cache directory before proceeding with decompression and processing.\n\n    The dataset is pre-split into train, validation, and test sets, which can be\n    loaded individually.\n\n    Attributes:\n        source (str): The official website for the dataset.\n        REQUIRED (dict): A dictionary detailing the filenames and checksums for\n            each data split (train, validation, test).\n    \"\"\"\n    source = 'https://msnews.github.io/#Download'\n\n    REQUIRED = {\n        'train': {\n            'compressed': 'MINDlarge_train.zip',\n            'decompressed': ['behaviors.tsv', 'entity_embedding.vec', 'relation_embedding.vec', 'news.tsv'],\n            'interactions': 'behaviors.tsv',\n            'checksum': '5be1c8f9a6809092db5fc0ac23d60f72'\n        },\n        'validation': {\n            'compressed': 'MINDlarge_dev.zip',\n            'decompressed': ['behaviors.tsv', 'entity_embedding.vec', 'relation_embedding.vec', 'news.tsv'],\n            'interactions': 'behaviors.tsv',\n            'checksum': '8f3dd8923172048b0e5980e7ee40841b'\n        },\n        'test': {\n            'compressed': 'MINDlarge_test.zip',\n            'decompressed': ['behaviors.tsv', 'entity_embedding.vec', 'relation_embedding.vec', 'news.tsv'],\n            'interactions': 'behaviors.tsv',\n            'checksum': '50406027c032898d9eddf9c8a8ecbc17'\n        }\n    }\n\n    SPLITS = ('train', 'validation', 'test')\n\n    NAME = 'MIND'\n    VERSION = 'small'\n\n    def __init__(self, folder=None, split='train'):\n        \"\"\"\n        Initializes the builder and orchestrates the data preparation workflow.\n\n        This constructor sets up paths and checks for the required files. If the\n        compressed archives are missing, it provides instructions for manual\n        download. It then proceeds to decompress and process the specified split.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n            split (str, optional): The data split to load, one of 'train',\n                'validation', or 'test'. Defaults to 'train'.\n        \"\"\"\n        super().__init__(None)\n\n        self.dataset_name = self.NAME\n        self.version_name = self.VERSION\n\n        # set data folder and raw folder\n        self._data_folder = folder if folder \\\n            else dataset_directory(self.dataset_name)\n        self._raw_folder = os.path.abspath(os.path.join(\n            self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n            else os.path.join(dataset_directory(self.dataset_name), self.version_name, RAW_DATA_FOLDER)\n\n        if not os.path.exists(self._data_folder):\n            os.makedirs(self._data_folder)\n            print('Created data folder \\'{}\\''.format(self._data_folder))\n\n        if not os.path.exists(self._raw_folder):\n            os.makedirs(self._raw_folder)\n            print('Created raw folder \\'{}\\''.format(self._raw_folder))\n\n        # check if the required files have been already downloaded\n        found, missing = self.required_files()\n\n        for file_type in missing:\n            file_path = self.download(file_type=file_type)\n            self.decompress(file_type, file_path)\n\n        self.process(split=split)\n\n    def required_files(self):\n        \"\"\"\n        Checks for the presence of the required compressed and decompressed files.\n\n        Returns:\n            (tuple[list, list]): A tuple where the first element is a list of\n                found splits and the second is a list of missing splits.\n        \"\"\"\n        found, missing = [], []\n\n        # check each required file\n        for rn, rf in self.REQUIRED.items():\n            # check if decompressed file is there\n            comp, dec = rf['compressed'], rf['decompressed']\n            decompressed_path = [os.path.join(self._raw_folder, rn, dec_name) for dec_name in dec]\n            # all the decompressed files should be there, otherwise it needs to decompress the compressed file again\n            if all([os.path.exists(p) for p in decompressed_path]):\n                # files found!\n                found.append(rn)\n            else:\n                # check if compressed file is there\n                compressed_path = os.path.join(self._raw_folder, comp)\n                if os.path.exists(compressed_path):\n                    # decompress compressed file\n                    self.decompress(file_type=rn, path=compressed_path)\n                    # files ready!\n                    found.append(rn)\n                else:\n                    # add missing file to missing list\n                    missing.append(rn)\n        return found, missing\n\n    def decompress(self, file_type, path):\n        \"\"\"\n        Decompresses the specified zip archive after verifying its checksum.\n\n        Args:\n            file_type (str): The split type ('train', 'validation', 'test').\n            path (str): The file path of the compressed .zip archive.\n\n        Returns:\n            (list): A list of paths to the decompressed files.\n\n        Raises:\n            FileNotFoundError: If decompression fails to produce the expected files.\n        \"\"\"\n        assert file_type in ('train', 'validation', 'test'), 'Invalid required file type'\n\n        verify_checksum(path, self.REQUIRED[file_type]['checksum'])\n\n        # decompress downloaded file\n        output_folder = os.path.join(self._raw_folder, file_type)\n        if not os.path.exists(output_folder):\n            os.makedirs(output_folder)\n        decompress_zip_file(path, output_folder)\n\n        # check that all the files in the compressed file exist\n        files = [os.path.join(output_folder, f) for f in self.REQUIRED[file_type]['decompressed']]\n        if all([os.path.exists(f) for f in files]):\n            return files\n        else:\n            raise FileNotFoundError(f'Error decompressing file \\'{path}\\'')\n\n    def download(self, file_type) -&gt; str:\n        \"\"\"\n        Guides the user to manually download the dataset archive.\n\n        This method does not download automatically. Instead, it prints instructions\n        and waits for the user to place the required file in the cache directory.\n\n        Args:\n            file_type (str): The split type ('train', 'validation', 'test') to download.\n\n        Returns:\n            (str): The local file path to the user-provided archive.\n\n        Raises:\n            FileNotFoundError: If the file is not found after the user confirms download.\n        \"\"\"\n        print(f'\\'{file_type}\\' MIND file not found.')\n\n        print('Microsoft News Dataset (MIND) requires manual download from the source.\\n'\n              'Please download the dataset from the following link:\\n'\n              'https://msnews.github.io/#Download\\n'\n              'Then, place the downloaded files in the following directory:\\n'\n              f'{self._raw_folder}')\n\n        # press continue after downloading the dataset\n        input('Press Enter to continue after downloading the dataset...')\n\n        file_path = os.path.join(self._raw_folder, self.REQUIRED[file_type]['compressed'])\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(f'MIND \\'{file_type}\\' file not found.')\n        return file_path\n\n    def process_split(self, split) -&gt; RawData:\n        \"\"\"\n        Processes a single split from the decompressed files.\n\n        This method reads the `behaviors.tsv` file for a given split, which\n        is in an 'inline' format, and parses it.\n\n        Args:\n            split (str): The data split to process ('train', 'validation', or 'test').\n\n        Returns:\n            (RawData): A RawData object containing the user-item interactions.\n\n        Raises:\n            ValueError: If an invalid split name is provided.\n        \"\"\"\n\n        if split not in self.SPLITS:\n            raise ValueError(f'Invalid split type: {split}')\n\n        # read the dataset\n        file_path = os.path.join(self._raw_folder, split, self.REQUIRED[split]['interactions'])\n        print('Reading file:', file_path)\n        return read_inline(file_path, cols=['impression_id', 'user', 'time', 'item', 'impressions'],\n                           col_sep='\\t', history_sep=' ')\n\n    def process(self, split) -&gt; None:\n        \"\"\"\n        Loads the processed data for the specified split into the class.\n\n        Args:\n            split (str): The data split to load ('train', 'validation', or 'test').\n\n        Returns:\n            (None): This method assigns the processed data to `self.data` directly.\n\n        Raises:\n            ValueError: If an invalid split name is provided.\n        \"\"\"\n\n        if split not in self.SPLITS and split != 'merge':\n            raise ValueError(f'Invalid split type: {split}')\n\n        if split == 'merge':\n            # merge all the splits\n            data = None\n            for s in self.SPLITS:\n                print('Processing split:', s)\n                if data is None:\n                    data = self.process_split(split=s)\n                else:\n                    data = data + self.process_split(split=s)\n            print('Split merged successfully')\n            print('Sorting data...')\n            data.data = data.data.sort_values(by=[data.user, data.item])\n            print('Reindexing data...')\n            data.data = data.data.reset_index(drop=True)\n            print(f'{split} split processed successfully')\n            self.data = data\n        else:\n            self.data = self.process_split(split=split)\n            print(f'{split} split processed successfully')\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.mind.mindLarge.MindLarge.__init__","title":"<code>__init__(folder=None, split='train')</code>","text":"<p>Initializes the builder and orchestrates the data preparation workflow.</p> <p>This constructor sets up paths and checks for the required files. If the compressed archives are missing, it provides instructions for manual download. It then proceeds to decompress and process the specified split.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> <code>split</code> <code>str</code> <p>The data split to load, one of 'train', 'validation', or 'test'. Defaults to 'train'.</p> <code>'train'</code> Source code in <code>datarec/datasets/mind/mindLarge.py</code> <pre><code>def __init__(self, folder=None, split='train'):\n    \"\"\"\n    Initializes the builder and orchestrates the data preparation workflow.\n\n    This constructor sets up paths and checks for the required files. If the\n    compressed archives are missing, it provides instructions for manual\n    download. It then proceeds to decompress and process the specified split.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n        split (str, optional): The data split to load, one of 'train',\n            'validation', or 'test'. Defaults to 'train'.\n    \"\"\"\n    super().__init__(None)\n\n    self.dataset_name = self.NAME\n    self.version_name = self.VERSION\n\n    # set data folder and raw folder\n    self._data_folder = folder if folder \\\n        else dataset_directory(self.dataset_name)\n    self._raw_folder = os.path.abspath(os.path.join(\n        self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n        else os.path.join(dataset_directory(self.dataset_name), self.version_name, RAW_DATA_FOLDER)\n\n    if not os.path.exists(self._data_folder):\n        os.makedirs(self._data_folder)\n        print('Created data folder \\'{}\\''.format(self._data_folder))\n\n    if not os.path.exists(self._raw_folder):\n        os.makedirs(self._raw_folder)\n        print('Created raw folder \\'{}\\''.format(self._raw_folder))\n\n    # check if the required files have been already downloaded\n    found, missing = self.required_files()\n\n    for file_type in missing:\n        file_path = self.download(file_type=file_type)\n        self.decompress(file_type, file_path)\n\n    self.process(split=split)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.mind.mindLarge.MindLarge.required_files","title":"<code>required_files()</code>","text":"<p>Checks for the presence of the required compressed and decompressed files.</p> <p>Returns:</p> Type Description <code>tuple[list, list]</code> <p>A tuple where the first element is a list of found splits and the second is a list of missing splits.</p> Source code in <code>datarec/datasets/mind/mindLarge.py</code> <pre><code>def required_files(self):\n    \"\"\"\n    Checks for the presence of the required compressed and decompressed files.\n\n    Returns:\n        (tuple[list, list]): A tuple where the first element is a list of\n            found splits and the second is a list of missing splits.\n    \"\"\"\n    found, missing = [], []\n\n    # check each required file\n    for rn, rf in self.REQUIRED.items():\n        # check if decompressed file is there\n        comp, dec = rf['compressed'], rf['decompressed']\n        decompressed_path = [os.path.join(self._raw_folder, rn, dec_name) for dec_name in dec]\n        # all the decompressed files should be there, otherwise it needs to decompress the compressed file again\n        if all([os.path.exists(p) for p in decompressed_path]):\n            # files found!\n            found.append(rn)\n        else:\n            # check if compressed file is there\n            compressed_path = os.path.join(self._raw_folder, comp)\n            if os.path.exists(compressed_path):\n                # decompress compressed file\n                self.decompress(file_type=rn, path=compressed_path)\n                # files ready!\n                found.append(rn)\n            else:\n                # add missing file to missing list\n                missing.append(rn)\n    return found, missing\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.mind.mindLarge.MindLarge.decompress","title":"<code>decompress(file_type, path)</code>","text":"<p>Decompresses the specified zip archive after verifying its checksum.</p> <p>Parameters:</p> Name Type Description Default <code>file_type</code> <code>str</code> <p>The split type ('train', 'validation', 'test').</p> required <code>path</code> <code>str</code> <p>The file path of the compressed .zip archive.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of paths to the decompressed files.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If decompression fails to produce the expected files.</p> Source code in <code>datarec/datasets/mind/mindLarge.py</code> <pre><code>def decompress(self, file_type, path):\n    \"\"\"\n    Decompresses the specified zip archive after verifying its checksum.\n\n    Args:\n        file_type (str): The split type ('train', 'validation', 'test').\n        path (str): The file path of the compressed .zip archive.\n\n    Returns:\n        (list): A list of paths to the decompressed files.\n\n    Raises:\n        FileNotFoundError: If decompression fails to produce the expected files.\n    \"\"\"\n    assert file_type in ('train', 'validation', 'test'), 'Invalid required file type'\n\n    verify_checksum(path, self.REQUIRED[file_type]['checksum'])\n\n    # decompress downloaded file\n    output_folder = os.path.join(self._raw_folder, file_type)\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n    decompress_zip_file(path, output_folder)\n\n    # check that all the files in the compressed file exist\n    files = [os.path.join(output_folder, f) for f in self.REQUIRED[file_type]['decompressed']]\n    if all([os.path.exists(f) for f in files]):\n        return files\n    else:\n        raise FileNotFoundError(f'Error decompressing file \\'{path}\\'')\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.mind.mindLarge.MindLarge.download","title":"<code>download(file_type)</code>","text":"<p>Guides the user to manually download the dataset archive.</p> <p>This method does not download automatically. Instead, it prints instructions and waits for the user to place the required file in the cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>file_type</code> <code>str</code> <p>The split type ('train', 'validation', 'test') to download.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The local file path to the user-provided archive.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file is not found after the user confirms download.</p> Source code in <code>datarec/datasets/mind/mindLarge.py</code> <pre><code>def download(self, file_type) -&gt; str:\n    \"\"\"\n    Guides the user to manually download the dataset archive.\n\n    This method does not download automatically. Instead, it prints instructions\n    and waits for the user to place the required file in the cache directory.\n\n    Args:\n        file_type (str): The split type ('train', 'validation', 'test') to download.\n\n    Returns:\n        (str): The local file path to the user-provided archive.\n\n    Raises:\n        FileNotFoundError: If the file is not found after the user confirms download.\n    \"\"\"\n    print(f'\\'{file_type}\\' MIND file not found.')\n\n    print('Microsoft News Dataset (MIND) requires manual download from the source.\\n'\n          'Please download the dataset from the following link:\\n'\n          'https://msnews.github.io/#Download\\n'\n          'Then, place the downloaded files in the following directory:\\n'\n          f'{self._raw_folder}')\n\n    # press continue after downloading the dataset\n    input('Press Enter to continue after downloading the dataset...')\n\n    file_path = os.path.join(self._raw_folder, self.REQUIRED[file_type]['compressed'])\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f'MIND \\'{file_type}\\' file not found.')\n    return file_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.mind.mindLarge.MindLarge.process_split","title":"<code>process_split(split)</code>","text":"<p>Processes a single split from the decompressed files.</p> <p>This method reads the <code>behaviors.tsv</code> file for a given split, which is in an 'inline' format, and parses it.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>str</code> <p>The data split to process ('train', 'validation', or 'test').</p> required <p>Returns:</p> Type Description <code>RawData</code> <p>A RawData object containing the user-item interactions.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid split name is provided.</p> Source code in <code>datarec/datasets/mind/mindLarge.py</code> <pre><code>def process_split(self, split) -&gt; RawData:\n    \"\"\"\n    Processes a single split from the decompressed files.\n\n    This method reads the `behaviors.tsv` file for a given split, which\n    is in an 'inline' format, and parses it.\n\n    Args:\n        split (str): The data split to process ('train', 'validation', or 'test').\n\n    Returns:\n        (RawData): A RawData object containing the user-item interactions.\n\n    Raises:\n        ValueError: If an invalid split name is provided.\n    \"\"\"\n\n    if split not in self.SPLITS:\n        raise ValueError(f'Invalid split type: {split}')\n\n    # read the dataset\n    file_path = os.path.join(self._raw_folder, split, self.REQUIRED[split]['interactions'])\n    print('Reading file:', file_path)\n    return read_inline(file_path, cols=['impression_id', 'user', 'time', 'item', 'impressions'],\n                       col_sep='\\t', history_sep=' ')\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.mind.mindLarge.MindLarge.process","title":"<code>process(split)</code>","text":"<p>Loads the processed data for the specified split into the class.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>str</code> <p>The data split to load ('train', 'validation', or 'test').</p> required <p>Returns:</p> Type Description <code>None</code> <p>This method assigns the processed data to <code>self.data</code> directly.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid split name is provided.</p> Source code in <code>datarec/datasets/mind/mindLarge.py</code> <pre><code>def process(self, split) -&gt; None:\n    \"\"\"\n    Loads the processed data for the specified split into the class.\n\n    Args:\n        split (str): The data split to load ('train', 'validation', or 'test').\n\n    Returns:\n        (None): This method assigns the processed data to `self.data` directly.\n\n    Raises:\n        ValueError: If an invalid split name is provided.\n    \"\"\"\n\n    if split not in self.SPLITS and split != 'merge':\n        raise ValueError(f'Invalid split type: {split}')\n\n    if split == 'merge':\n        # merge all the splits\n        data = None\n        for s in self.SPLITS:\n            print('Processing split:', s)\n            if data is None:\n                data = self.process_split(split=s)\n            else:\n                data = data + self.process_split(split=s)\n        print('Split merged successfully')\n        print('Sorting data...')\n        data.data = data.data.sort_values(by=[data.user, data.item])\n        print('Reindexing data...')\n        data.data = data.data.reset_index(drop=True)\n        print(f'{split} split processed successfully')\n        self.data = data\n    else:\n        self.data = self.process_split(split=split)\n        print(f'{split} split processed successfully')\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.mind.mindSmall.MindSmall","title":"<code>MindSmall</code>","text":"<p>               Bases: <code>MindLarge</code></p> <p>Builder class for the small version of the MIND dataset.</p> <p>This class handles the logic for preparing and loading the MINDsmall dataset. It inherits most of its functionality from the <code>MindLarge</code> class but overrides the required file configurations for the smaller version.</p> <p>MINDsmall is a smaller version of the MIND dataset, suitable for rapid prototyping. It contains only <code>train</code> and <code>validation</code> splits.</p> <p>Note on usage: Like the large version, this dataset requires manual download.</p> <p>Attributes:</p> Name Type Description <code>REQUIRED</code> <code>dict</code> <p>A dictionary detailing the filenames and checksums for each data split (train, validation).</p> <code>SPLITS</code> <code>tuple</code> <p>The available splits for this version.</p> Source code in <code>datarec/datasets/mind/mindSmall.py</code> <pre><code>class MindSmall(MindLarge):\n    \"\"\"\n    Builder class for the small version of the MIND dataset.\n\n    This class handles the logic for preparing and loading the MINDsmall dataset.\n    It inherits most of its functionality from the `MindLarge` class but overrides\n    the required file configurations for the smaller version.\n\n    MINDsmall is a smaller version of the MIND dataset, suitable for rapid\n    prototyping. It contains only `train` and `validation` splits.\n\n    **Note on usage:** Like the large version, this dataset requires manual download.\n\n    Attributes:\n        REQUIRED (dict): A dictionary detailing the filenames and checksums for\n            each data split (train, validation).\n        SPLITS (tuple): The available splits for this version.\n    \"\"\"\n\n    REQUIRED = {\n        'train': {\n            'compressed': 'MINDsmall_train.zip',\n            'decompressed': ['behaviors.tsv', 'entity_embedding.vec', 'relation_embedding.vec', 'news.tsv'],\n            'interactions': 'behaviors.tsv',\n            'checksum': '8ab752c7d11564622d93132be05dcf6b'\n        },\n        'validation': {\n            'compressed': 'MINDsmall_dev.zip',\n            'decompressed': ['behaviors.tsv', 'entity_embedding.vec', 'relation_embedding.vec', 'news.tsv'],\n            'interactions': 'behaviors.tsv',\n            'checksum': 'e3bac5485be8fc7a9934e85e3b78615f'\n        }\n    }\n\n    SPLITS = ('train', 'validation')\n\n    VERSION = 'small'\n\n    def __init__(self, folder=None, split='train'):\n        \"\"\"\n        Initializes the builder for the MINDsmall dataset.\n\n        This constructor calls the parent `MindLarge` constructor but will use the\n        overridden `REQUIRED`, `SPLITS`, and `VERSION` attributes specific to\n        the small version of the dataset.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n            split (str, optional): The data split to load, one of 'train' or\n                'validation'. Defaults to 'train'.\n        \"\"\"\n\n        super().__init__(folder=folder, split=split)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.mind.mindSmall.MindSmall.__init__","title":"<code>__init__(folder=None, split='train')</code>","text":"<p>Initializes the builder for the MINDsmall dataset.</p> <p>This constructor calls the parent <code>MindLarge</code> constructor but will use the overridden <code>REQUIRED</code>, <code>SPLITS</code>, and <code>VERSION</code> attributes specific to the small version of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> <code>split</code> <code>str</code> <p>The data split to load, one of 'train' or 'validation'. Defaults to 'train'.</p> <code>'train'</code> Source code in <code>datarec/datasets/mind/mindSmall.py</code> <pre><code>def __init__(self, folder=None, split='train'):\n    \"\"\"\n    Initializes the builder for the MINDsmall dataset.\n\n    This constructor calls the parent `MindLarge` constructor but will use the\n    overridden `REQUIRED`, `SPLITS`, and `VERSION` attributes specific to\n    the small version of the dataset.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n        split (str, optional): The data split to load, one of 'train' or\n            'validation'. Defaults to 'train'.\n    \"\"\"\n\n    super().__init__(folder=folder, split=split)\n</code></pre>"},{"location":"documentation/datasets/#movielens","title":"MovieLens","text":"<p>Entry point for loading different versions of the MovieLens dataset.</p> <p>Builder class for the MovieLens 100k dataset.</p> <p>Builder class for the MovieLens 20M dataset.</p>"},{"location":"documentation/datasets/#datarec.datasets.movielens.movielens.MovieLens","title":"<code>MovieLens</code>","text":"<p>Entry point class to load various versions of the MovieLens dataset.</p> <p>This class provides a single, convenient interface for accessing the dataset. Based on the <code>version</code> parameter, it selects and returns the appropriate dataset builder.</p> <p>The MovieLens datasets are a collection of movie ratings data collected by the GroupLens Research project at the University of Minnesota.</p> <p>The default version is 'latest', which currently corresponds to the '1m' version.</p> <p>Examples:</p> <p>To load the latest version (1M):</p> <pre><code>&gt;&gt;&gt; ml_1m = MovieLens().prepare_and_load()\n</code></pre> <p>To load a specific version (e.g., 100k):</p> <pre><code>&gt;&gt;&gt; ml_100k = MovieLens(version='100k').prepare_and_load()\n</code></pre> Source code in <code>datarec/datasets/movielens/movielens.py</code> <pre><code>class MovieLens:\n    \"\"\"Entry point class to load various versions of the MovieLens dataset.\n\n    This class provides a single, convenient interface for accessing the dataset.\n    Based on the `version` parameter, it selects and returns the appropriate\n    dataset builder.\n\n    The MovieLens datasets are a collection of movie ratings data collected by the\n    GroupLens Research project at the University of Minnesota.\n\n    The default version is 'latest', which currently corresponds to the '1m' version.\n\n    Examples:\n        To load the latest version (1M):\n        &gt;&gt;&gt; ml_1m = MovieLens().prepare_and_load()\n\n        To load a specific version (e.g., 100k):\n        &gt;&gt;&gt; ml_100k = MovieLens(version='100k').prepare_and_load()\n    \"\"\"\n    latest_version = '1m'\n\n    def __new__(cls, version: str = 'latest', **kwargs) -&gt; BaseDataRecBuilder:\n        \"\"\"\n        Initializes and returns the specified version of the MovieLens dataset builder.\n\n        This method acts as a dispatcher, instantiating the correct builder class\n        that handles the downloading, caching, and loading for a specific dataset version.\n\n        Note: The returned object is a builder. You must call `.prepare_and_load()`\n        on it to get a populated `DataRec` object.\n\n        Args:\n            version (str): The version of the dataset to load. Supported versions\n                include '1m', '20m', '100k', and 'latest'. Defaults to 'latest'.\n            **kwargs: Additional keyword arguments (not currently used).\n\n        Returns:\n            (BaseDataRecBuilder): An instance of the appropriate dataset builder class\n                (e.g., `MovieLens1M`), ready to prepare and load data.\n\n        Raises:\n            ValueError: If an unsupported version string is provided.\n        \"\"\"\n        versions = {'1m': MovieLens1M,\n                    '20m': MovieLens20M,\n                    '100k': MovieLens100k}\n        if version == 'latest':\n            version = cls.latest_version\n        if version in versions:\n            return versions[version]()\n        else:\n            raise ValueError(f\"MovieLens {version}: Unsupported version \\n Supported version:\"\n                             f\"\\n \\t version \\t name \"\n                             f\"\\n \\t 1m \\t Movielens 1 Million \"\n                             f\"\\n \\t 20m \\t Movielens 20 Million\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.movielens.movielens.MovieLens.__new__","title":"<code>__new__(version='latest', **kwargs)</code>","text":"<p>Initializes and returns the specified version of the MovieLens dataset builder.</p> <p>This method acts as a dispatcher, instantiating the correct builder class that handles the downloading, caching, and loading for a specific dataset version.</p> <p>Note: The returned object is a builder. You must call <code>.prepare_and_load()</code> on it to get a populated <code>DataRec</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>The version of the dataset to load. Supported versions include '1m', '20m', '100k', and 'latest'. Defaults to 'latest'.</p> <code>'latest'</code> <code>**kwargs</code> <p>Additional keyword arguments (not currently used).</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseDataRecBuilder</code> <p>An instance of the appropriate dataset builder class (e.g., <code>MovieLens1M</code>), ready to prepare and load data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported version string is provided.</p> Source code in <code>datarec/datasets/movielens/movielens.py</code> <pre><code>def __new__(cls, version: str = 'latest', **kwargs) -&gt; BaseDataRecBuilder:\n    \"\"\"\n    Initializes and returns the specified version of the MovieLens dataset builder.\n\n    This method acts as a dispatcher, instantiating the correct builder class\n    that handles the downloading, caching, and loading for a specific dataset version.\n\n    Note: The returned object is a builder. You must call `.prepare_and_load()`\n    on it to get a populated `DataRec` object.\n\n    Args:\n        version (str): The version of the dataset to load. Supported versions\n            include '1m', '20m', '100k', and 'latest'. Defaults to 'latest'.\n        **kwargs: Additional keyword arguments (not currently used).\n\n    Returns:\n        (BaseDataRecBuilder): An instance of the appropriate dataset builder class\n            (e.g., `MovieLens1M`), ready to prepare and load data.\n\n    Raises:\n        ValueError: If an unsupported version string is provided.\n    \"\"\"\n    versions = {'1m': MovieLens1M,\n                '20m': MovieLens20M,\n                '100k': MovieLens100k}\n    if version == 'latest':\n        version = cls.latest_version\n    if version in versions:\n        return versions[version]()\n    else:\n        raise ValueError(f\"MovieLens {version}: Unsupported version \\n Supported version:\"\n                         f\"\\n \\t version \\t name \"\n                         f\"\\n \\t 1m \\t Movielens 1 Million \"\n                         f\"\\n \\t 20m \\t Movielens 20 Million\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.movielens.movielens100k.MovieLens100k","title":"<code>MovieLens100k</code>","text":"<p>               Bases: <code>BaseDataRecBuilder</code></p> <p>Builder class for the MovieLens 100k dataset.</p> <p>This dataset contains 100,000 ratings. It is not typically instantiated directly but is called by the <code>MovieLens</code> entry point class.</p> <p>The raw data is provided in a tab-separated file (<code>u.data</code>).</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL from which the raw dataset is downloaded.</p> <code>CHECKSUM</code> <code>str</code> <p>The MD5 checksum to verify the integrity of the downloaded file.</p> <code>REQUIRED_FILES</code> <code>list</code> <p>A list of file paths expected after decompression.</p> Source code in <code>datarec/datasets/movielens/movielens100k.py</code> <pre><code>class MovieLens100k(BaseDataRecBuilder):\n    \"\"\"\n    Builder class for the MovieLens 100k dataset.\n\n    This dataset contains 100,000 ratings. It is not typically instantiated\n    directly but is called by the `MovieLens` entry point class.\n\n    The raw data is provided in a tab-separated file (`u.data`).\n\n    Attributes:\n        url (str): The URL from which the raw dataset is downloaded.\n        CHECKSUM (str): The MD5 checksum to verify the integrity of the downloaded file.\n        REQUIRED_FILES (list): A list of file paths expected after decompression.\n    \"\"\"\n    url = 'https://files.grouplens.org/datasets/movielens/ml-100k.zip'\n    data_file_name = os.path.basename(url)\n    ratings_file_name = 'u.data'\n    REQUIRED_FILES = [os.path.join('ml-100k', p) for p in [ratings_file_name]]\n    CHECKSUM = \"0e33842e24a9c977be4e0107933c0723\"\n\n    def __init__(self, folder=None):\n        \"\"\"\n        Initializes the builder.\n\n        This constructor sets up the necessary paths for caching the dataset.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n        \"\"\"\n        self.dataset_name = 'MovieLens'\n        self.version_name = '100k'\n\n        self._data_folder = folder if folder \\\n            else dataset_directory(self.dataset_name)\n        self._raw_folder = os.path.abspath(\n            os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n            else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n\n    def prepare(self):\n        \"\"\"\n        Ensures all required raw files are downloaded and decompressed.\n\n        This method checks for the existence of the required files. If they are\n        not found, it triggers the download and decompression process.\n        \"\"\"\n        if self.required_files() is not None:\n            # All required files are already available\n            return\n\n        file_path = self.download()\n        verify_checksum(file_path, self.CHECKSUM)\n        self.decompress(file_path)\n\n    def load(self):\n        \"\"\"\n        Loads the prepared `u.data` file into a DataRec object.\n\n        Returns:\n            (DataRec): A DataRec object containing the user-item interactions.\n        \"\"\"\n        from datarec.io import read_tabular\n\n        ratings_file_path = self.required_files()[0]\n        dataset = read_tabular(ratings_file_path, sep='\\t', user_col=0, item_col=1, rating_col=2, timestamp_col=3,\n                               header=None)\n        return DataRec(rawdata=dataset,\n                       dataset_name=self.dataset_name,\n                       version_name=self.version_name)\n\n    def required_files(self):\n        \"\"\"\n        Check whether the required dataset files exist.\n\n        Returns:\n            (list[str]): Paths to required files if they exist, or None.\n        \"\"\"\n        # compressed data file\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n\n        # check if the file is there\n        paths = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n        if all([os.path.exists(p) for p in paths]):\n            return paths\n        # check if the compressed file is there\n        elif os.path.exists(file_path):\n            return self.decompress(file_path)\n        else:\n            return None\n\n    def decompress(self, path):\n        \"\"\"\n        Decompress the downloaded zip file and verify required files.\n\n        Args:\n            path (str): Path to the zip file.\n\n        Returns:\n            (list[str]): Paths to the extracted files if successful, or None.\n        \"\"\"\n        decompress_zip_file(path, self._raw_folder)\n        files = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n        if all([os.path.exists(f) for f in files]):\n            return [os.path.join(self._raw_folder, f) for f in files]\n        return None\n\n    def download(self) -&gt; str:\n        \"\"\"\n        Download the raw dataset zip file to the raw folder.\n\n        Returns:\n            (str): Path to the downloaded zip file.\n        \"\"\"\n        if not os.path.exists(self._raw_folder):\n            os.makedirs(self._raw_folder)\n            print('Created folder \\'{}\\''.format(self._raw_folder))\n\n        # download dataset file (compressed file)\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n        download_file(self.url, file_path)\n\n        return file_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.movielens.movielens100k.MovieLens100k.__init__","title":"<code>__init__(folder=None)</code>","text":"<p>Initializes the builder.</p> <p>This constructor sets up the necessary paths for caching the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> Source code in <code>datarec/datasets/movielens/movielens100k.py</code> <pre><code>def __init__(self, folder=None):\n    \"\"\"\n    Initializes the builder.\n\n    This constructor sets up the necessary paths for caching the dataset.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n    \"\"\"\n    self.dataset_name = 'MovieLens'\n    self.version_name = '100k'\n\n    self._data_folder = folder if folder \\\n        else dataset_directory(self.dataset_name)\n    self._raw_folder = os.path.abspath(\n        os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n        else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.movielens.movielens100k.MovieLens100k.prepare","title":"<code>prepare()</code>","text":"<p>Ensures all required raw files are downloaded and decompressed.</p> <p>This method checks for the existence of the required files. If they are not found, it triggers the download and decompression process.</p> Source code in <code>datarec/datasets/movielens/movielens100k.py</code> <pre><code>def prepare(self):\n    \"\"\"\n    Ensures all required raw files are downloaded and decompressed.\n\n    This method checks for the existence of the required files. If they are\n    not found, it triggers the download and decompression process.\n    \"\"\"\n    if self.required_files() is not None:\n        # All required files are already available\n        return\n\n    file_path = self.download()\n    verify_checksum(file_path, self.CHECKSUM)\n    self.decompress(file_path)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.movielens.movielens100k.MovieLens100k.load","title":"<code>load()</code>","text":"<p>Loads the prepared <code>u.data</code> file into a DataRec object.</p> <p>Returns:</p> Type Description <code>DataRec</code> <p>A DataRec object containing the user-item interactions.</p> Source code in <code>datarec/datasets/movielens/movielens100k.py</code> <pre><code>def load(self):\n    \"\"\"\n    Loads the prepared `u.data` file into a DataRec object.\n\n    Returns:\n        (DataRec): A DataRec object containing the user-item interactions.\n    \"\"\"\n    from datarec.io import read_tabular\n\n    ratings_file_path = self.required_files()[0]\n    dataset = read_tabular(ratings_file_path, sep='\\t', user_col=0, item_col=1, rating_col=2, timestamp_col=3,\n                           header=None)\n    return DataRec(rawdata=dataset,\n                   dataset_name=self.dataset_name,\n                   version_name=self.version_name)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.movielens.movielens100k.MovieLens100k.required_files","title":"<code>required_files()</code>","text":"<p>Check whether the required dataset files exist.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>Paths to required files if they exist, or None.</p> Source code in <code>datarec/datasets/movielens/movielens100k.py</code> <pre><code>def required_files(self):\n    \"\"\"\n    Check whether the required dataset files exist.\n\n    Returns:\n        (list[str]): Paths to required files if they exist, or None.\n    \"\"\"\n    # compressed data file\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n\n    # check if the file is there\n    paths = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n    if all([os.path.exists(p) for p in paths]):\n        return paths\n    # check if the compressed file is there\n    elif os.path.exists(file_path):\n        return self.decompress(file_path)\n    else:\n        return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.movielens.movielens100k.MovieLens100k.decompress","title":"<code>decompress(path)</code>","text":"<p>Decompress the downloaded zip file and verify required files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the zip file.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>Paths to the extracted files if successful, or None.</p> Source code in <code>datarec/datasets/movielens/movielens100k.py</code> <pre><code>def decompress(self, path):\n    \"\"\"\n    Decompress the downloaded zip file and verify required files.\n\n    Args:\n        path (str): Path to the zip file.\n\n    Returns:\n        (list[str]): Paths to the extracted files if successful, or None.\n    \"\"\"\n    decompress_zip_file(path, self._raw_folder)\n    files = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n    if all([os.path.exists(f) for f in files]):\n        return [os.path.join(self._raw_folder, f) for f in files]\n    return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.movielens.movielens100k.MovieLens100k.download","title":"<code>download()</code>","text":"<p>Download the raw dataset zip file to the raw folder.</p> <p>Returns:</p> Type Description <code>str</code> <p>Path to the downloaded zip file.</p> Source code in <code>datarec/datasets/movielens/movielens100k.py</code> <pre><code>def download(self) -&gt; str:\n    \"\"\"\n    Download the raw dataset zip file to the raw folder.\n\n    Returns:\n        (str): Path to the downloaded zip file.\n    \"\"\"\n    if not os.path.exists(self._raw_folder):\n        os.makedirs(self._raw_folder)\n        print('Created folder \\'{}\\''.format(self._raw_folder))\n\n    # download dataset file (compressed file)\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n    download_file(self.url, file_path)\n\n    return file_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.movielens.movielens1m.MovieLens1M","title":"<code>MovieLens1M</code>","text":"<p>               Bases: <code>BaseDataRecBuilder</code></p> <p>Builder class for the MovieLens 1M dataset.</p> <p>This dataset contains 1 million ratings. It is not typically instantiated directly but is  called by the <code>MovieLens</code> entry point class.</p> <p>The raw ratings data is provided in <code>ratings.dat</code> with a <code>::</code> separator.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL from which the raw dataset is downloaded.</p> <code>CHECKSUM</code> <code>str</code> <p>The MD5 checksum to verify the integrity of the downloaded file.</p> <code>REQUIRED_FILES</code> <code>list</code> <p>A list of file paths expected after decompression.</p> Source code in <code>datarec/datasets/movielens/movielens1m.py</code> <pre><code>class MovieLens1M(BaseDataRecBuilder):\n    \"\"\"\n    Builder class for the MovieLens 1M dataset.\n\n    This dataset contains 1 million ratings. It is not typically instantiated directly but is\n    called by the `MovieLens` entry point class.\n\n    The raw ratings data is provided in `ratings.dat` with a `::` separator.\n\n   Attributes:\n       url (str): The URL from which the raw dataset is downloaded.\n       CHECKSUM (str): The MD5 checksum to verify the integrity of the downloaded file.\n       REQUIRED_FILES (list): A list of file paths expected after decompression.\n   \"\"\"\n    url = 'https://files.grouplens.org/datasets/movielens/ml-1m.zip'\n    data_file_name = os.path.basename(url)\n    movies_file_name = 'movies.dat'\n    ratings_file_name = 'ratings.dat'\n    users_file_name = 'users.dat'\n    REQUIRED_FILES = [os.path.join('ml-1m', p) for p in [movies_file_name, ratings_file_name, users_file_name]]\n    CHECKSUM = \"c4d9eecfca2ab87c1945afe126590906\"\n\n    def __init__(self, folder=None):\n        \"\"\"\n        Initializes the builder.\n\n        This constructor sets up the necessary paths for caching the dataset.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n        \"\"\"\n        self.dataset_name = 'MovieLens'\n        self.version_name = '1m'\n\n        self._data_folder = folder if folder else dataset_directory(self.dataset_name)\n        self._raw_folder = (\n            os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER))\n            if folder\n            else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n        )\n\n    def download(self) -&gt; str:\n        \"\"\"\n        Downloads the raw dataset archive file.\n\n        Returns:\n            (str): The local file path to the downloaded zip file.\n        \"\"\"\n        if not os.path.exists(self._raw_folder):\n            os.makedirs(self._raw_folder)\n            print(f\"Created folder '{self._raw_folder}'\")\n\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n        if not os.path.exists(file_path):\n            download_file(self.url, file_path)\n        return file_path\n\n    def prepare(self):\n        \"\"\"\n        Ensures all required raw files are downloaded and decompressed.\n\n        This method checks for the existence of the required files. If they are\n        not found, it triggers the download and decompression process.\n        \"\"\"\n        raw_paths = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n        if all(os.path.exists(p) for p in raw_paths):\n            return\n\n        archive_path = os.path.join(self._raw_folder, self.data_file_name)\n        if not os.path.exists(archive_path):\n            archive_path = self.download()\n\n        verify_checksum(archive_path, self.CHECKSUM)\n        decompress_zip_file(archive_path, self._raw_folder)\n\n    def load(self) -&gt; DataRec:\n        \"\"\"\n        Loads the prepared `ratings.dat` file into a DataRec object.\n\n        Returns:\n            (DataRec): A DataRec object containing the user-item interactions.\n        \"\"\"\n        ratings_path = os.path.join(self._raw_folder, 'ml-1m', self.ratings_file_name)\n        dataset = read_tabular(ratings_path, sep='::', user_col=0, item_col=1, rating_col=2, timestamp_col=3, header=None)\n\n        dr = DataRec(dataset_name=self.dataset_name, version_name=self.version_name)\n        dr.data = dataset\n        return dr\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.movielens.movielens1m.MovieLens1M.__init__","title":"<code>__init__(folder=None)</code>","text":"<p>Initializes the builder.</p> <p>This constructor sets up the necessary paths for caching the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> Source code in <code>datarec/datasets/movielens/movielens1m.py</code> <pre><code>def __init__(self, folder=None):\n    \"\"\"\n    Initializes the builder.\n\n    This constructor sets up the necessary paths for caching the dataset.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n    \"\"\"\n    self.dataset_name = 'MovieLens'\n    self.version_name = '1m'\n\n    self._data_folder = folder if folder else dataset_directory(self.dataset_name)\n    self._raw_folder = (\n        os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER))\n        if folder\n        else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n    )\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.movielens.movielens1m.MovieLens1M.download","title":"<code>download()</code>","text":"<p>Downloads the raw dataset archive file.</p> <p>Returns:</p> Type Description <code>str</code> <p>The local file path to the downloaded zip file.</p> Source code in <code>datarec/datasets/movielens/movielens1m.py</code> <pre><code>def download(self) -&gt; str:\n    \"\"\"\n    Downloads the raw dataset archive file.\n\n    Returns:\n        (str): The local file path to the downloaded zip file.\n    \"\"\"\n    if not os.path.exists(self._raw_folder):\n        os.makedirs(self._raw_folder)\n        print(f\"Created folder '{self._raw_folder}'\")\n\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n    if not os.path.exists(file_path):\n        download_file(self.url, file_path)\n    return file_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.movielens.movielens1m.MovieLens1M.prepare","title":"<code>prepare()</code>","text":"<p>Ensures all required raw files are downloaded and decompressed.</p> <p>This method checks for the existence of the required files. If they are not found, it triggers the download and decompression process.</p> Source code in <code>datarec/datasets/movielens/movielens1m.py</code> <pre><code>def prepare(self):\n    \"\"\"\n    Ensures all required raw files are downloaded and decompressed.\n\n    This method checks for the existence of the required files. If they are\n    not found, it triggers the download and decompression process.\n    \"\"\"\n    raw_paths = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n    if all(os.path.exists(p) for p in raw_paths):\n        return\n\n    archive_path = os.path.join(self._raw_folder, self.data_file_name)\n    if not os.path.exists(archive_path):\n        archive_path = self.download()\n\n    verify_checksum(archive_path, self.CHECKSUM)\n    decompress_zip_file(archive_path, self._raw_folder)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.movielens.movielens1m.MovieLens1M.load","title":"<code>load()</code>","text":"<p>Loads the prepared <code>ratings.dat</code> file into a DataRec object.</p> <p>Returns:</p> Type Description <code>DataRec</code> <p>A DataRec object containing the user-item interactions.</p> Source code in <code>datarec/datasets/movielens/movielens1m.py</code> <pre><code>def load(self) -&gt; DataRec:\n    \"\"\"\n    Loads the prepared `ratings.dat` file into a DataRec object.\n\n    Returns:\n        (DataRec): A DataRec object containing the user-item interactions.\n    \"\"\"\n    ratings_path = os.path.join(self._raw_folder, 'ml-1m', self.ratings_file_name)\n    dataset = read_tabular(ratings_path, sep='::', user_col=0, item_col=1, rating_col=2, timestamp_col=3, header=None)\n\n    dr = DataRec(dataset_name=self.dataset_name, version_name=self.version_name)\n    dr.data = dataset\n    return dr\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.movielens.movielens20m.MovieLens20M","title":"<code>MovieLens20M</code>","text":"<p>               Bases: <code>DataRec</code></p> <p>Builder class for the MovieLens 20M dataset.</p> <p>This dataset contains 20 million ratings. It is not typically instantiated directly but is called by the <code>MovieLens</code> entry point class.</p> <p>This loader specifically processes the <code>ratings.csv</code> file from the archive.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL from which the raw dataset is downloaded.</p> <code>CHECKSUM</code> <code>str</code> <p>The MD5 checksum to verify the integrity of the downloaded file.</p> <code>REQUIRED_FILES</code> <code>list</code> <p>A list of all files expected after decompression.</p> Source code in <code>datarec/datasets/movielens/movielens20m.py</code> <pre><code>class MovieLens20M(DataRec):\n    \"\"\"\n    Builder class for the MovieLens 20M dataset.\n\n    This dataset contains 20 million ratings. It is not typically instantiated directly\n    but is called by the `MovieLens` entry point class.\n\n    This loader specifically processes the `ratings.csv` file from the archive.\n\n    Attributes:\n        url (str): The URL from which the raw dataset is downloaded.\n        CHECKSUM (str): The MD5 checksum to verify the integrity of the downloaded file.\n        REQUIRED_FILES (list): A list of all files expected after decompression.\n    \"\"\"\n    url = 'https://files.grouplens.org/datasets/movielens/ml-20m.zip'\n    data_file_name = os.path.basename(url)\n    genome_scores_file_name = 'genome-scores.csv'\n    genome_tags_file_name = 'genome-tags.csv'\n    links_file_name = 'links.csv'\n    movies_file_name = 'movies.csv'\n    ratings_file_name = 'ratings.csv'\n    tags_file_name = 'tags.csv'\n    REQUIRED_FILES = [os.path.join('ml-20m', p) for p in [genome_scores_file_name,\n                                                          genome_tags_file_name,\n                                                          links_file_name,\n                                                          movies_file_name,\n                                                          ratings_file_name,\n                                                          tags_file_name]]\n    CHECKSUM = \"cd245b17a1ae2cc31bb14903e1204af3\"\n\n    def __init__(self, folder=None):\n        \"\"\"\n        Initializes the builder and orchestrates the data preparation workflow.\n\n        This constructor sets up the necessary paths and automatically triggers\n        the download, verification, and processing steps if the data is not\n        already present in the specified cache directory.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n        \"\"\"\n        super().__init__(None)\n\n        self.dataset_name = 'MovieLens'\n        self.version_name = '20m'\n\n        self._data_folder = folder if folder \\\n            else dataset_directory(self.dataset_name)\n        self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n            else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n\n        self.return_type = None\n\n        # check if the required files have been already downloaded\n        file_path = self.required_files()\n\n        if file_path is None:\n            file_path = self.download()\n            file_path = self.decompress(file_path)\n\n        print(f'files: {file_path}')\n\n        # the order of tha paths is the same of REQUIRED_PATHS\n        genome_scores_file_path, genome_tags_file_path, links_file_path, movies_file_path, ratings_file_path,\\\n            tags_file_path = file_path\n\n        self.process(ratings_file_path)\n\n\n    def required_files(self):\n        \"\"\"\n        Checks for the presence of all required decompressed data files.\n\n        Returns:\n            (list or None): A list of paths to the required data files if they\n                exist or can be created by decompression. Otherwise, returns None.\n        \"\"\"\n        # compressed data file\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n\n        # check if the file is there\n        paths = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n        if all([os.path.exists(p) for p in paths]):\n            return paths\n        # check if the compressed file is there\n        elif os.path.exists(file_path):\n            return self.decompress(file_path)\n        else:\n            return None\n\n    def decompress(self, path):\n        \"\"\"\n        Decompresses the downloaded archive after verifying its checksum.\n\n        Args:\n            path (str): The file path of the compressed archive.\n\n        Returns:\n            (list or None): A list of paths to the decompressed files if successful,\n                otherwise None.\n        \"\"\"\n        verify_checksum(path, self.CHECKSUM)\n\n        # decompress downloaded file\n        decompress_zip_file(path, self._raw_folder)\n        files = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n        if all([os.path.exists(f) for f in files]):\n            return [os.path.join(self._raw_folder, f) for f in files]\n        return None\n\n    def download(self) -&gt; str:\n        \"\"\"\n        Downloads the raw dataset compressed archive.\n\n        Returns:\n            (str): The local file path to the downloaded archive.\n        \"\"\"\n        if not os.path.exists(self._raw_folder):\n            os.makedirs(self._raw_folder)\n            print('Created folder \\'{}\\''.format(self._raw_folder))\n\n        # download dataset file (compressed file)\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n        download_file(self.url, file_path)\n\n        return file_path\n\n    def process(self, file_path):\n        \"\"\"\n        Processes the raw  file and loads it into the class.\n\n        This method reads the file, which includes a header row, and maps\n        the columns to the standard user, item, rating, and timestamp fields.\n\n        Args:\n            file_path (str): The path to the raw  file.\n\n        Returns:\n            (None): This method assigns the processed data to `self.data` directly.\n        \"\"\"\n\n        from datarec.io import read_tabular\n\n        dataset = read_tabular(file_path, sep=',',\n                               user_col='userId', item_col='movieId', rating_col='rating', timestamp_col='timestamp',\n                               header=0)\n        self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.movielens.movielens20m.MovieLens20M.__init__","title":"<code>__init__(folder=None)</code>","text":"<p>Initializes the builder and orchestrates the data preparation workflow.</p> <p>This constructor sets up the necessary paths and automatically triggers the download, verification, and processing steps if the data is not already present in the specified cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> Source code in <code>datarec/datasets/movielens/movielens20m.py</code> <pre><code>def __init__(self, folder=None):\n    \"\"\"\n    Initializes the builder and orchestrates the data preparation workflow.\n\n    This constructor sets up the necessary paths and automatically triggers\n    the download, verification, and processing steps if the data is not\n    already present in the specified cache directory.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n    \"\"\"\n    super().__init__(None)\n\n    self.dataset_name = 'MovieLens'\n    self.version_name = '20m'\n\n    self._data_folder = folder if folder \\\n        else dataset_directory(self.dataset_name)\n    self._raw_folder = os.path.abspath(os.path.join(self._data_folder, self.version_name, RAW_DATA_FOLDER)) if folder \\\n        else os.path.join(dataset_raw_directory(self.dataset_name), self.version_name)\n\n    self.return_type = None\n\n    # check if the required files have been already downloaded\n    file_path = self.required_files()\n\n    if file_path is None:\n        file_path = self.download()\n        file_path = self.decompress(file_path)\n\n    print(f'files: {file_path}')\n\n    # the order of tha paths is the same of REQUIRED_PATHS\n    genome_scores_file_path, genome_tags_file_path, links_file_path, movies_file_path, ratings_file_path,\\\n        tags_file_path = file_path\n\n    self.process(ratings_file_path)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.movielens.movielens20m.MovieLens20M.required_files","title":"<code>required_files()</code>","text":"<p>Checks for the presence of all required decompressed data files.</p> <p>Returns:</p> Type Description <code>list or None</code> <p>A list of paths to the required data files if they exist or can be created by decompression. Otherwise, returns None.</p> Source code in <code>datarec/datasets/movielens/movielens20m.py</code> <pre><code>def required_files(self):\n    \"\"\"\n    Checks for the presence of all required decompressed data files.\n\n    Returns:\n        (list or None): A list of paths to the required data files if they\n            exist or can be created by decompression. Otherwise, returns None.\n    \"\"\"\n    # compressed data file\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n\n    # check if the file is there\n    paths = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n    if all([os.path.exists(p) for p in paths]):\n        return paths\n    # check if the compressed file is there\n    elif os.path.exists(file_path):\n        return self.decompress(file_path)\n    else:\n        return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.movielens.movielens20m.MovieLens20M.decompress","title":"<code>decompress(path)</code>","text":"<p>Decompresses the downloaded archive after verifying its checksum.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path of the compressed archive.</p> required <p>Returns:</p> Type Description <code>list or None</code> <p>A list of paths to the decompressed files if successful, otherwise None.</p> Source code in <code>datarec/datasets/movielens/movielens20m.py</code> <pre><code>def decompress(self, path):\n    \"\"\"\n    Decompresses the downloaded archive after verifying its checksum.\n\n    Args:\n        path (str): The file path of the compressed archive.\n\n    Returns:\n        (list or None): A list of paths to the decompressed files if successful,\n            otherwise None.\n    \"\"\"\n    verify_checksum(path, self.CHECKSUM)\n\n    # decompress downloaded file\n    decompress_zip_file(path, self._raw_folder)\n    files = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n    if all([os.path.exists(f) for f in files]):\n        return [os.path.join(self._raw_folder, f) for f in files]\n    return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.movielens.movielens20m.MovieLens20M.download","title":"<code>download()</code>","text":"<p>Downloads the raw dataset compressed archive.</p> <p>Returns:</p> Type Description <code>str</code> <p>The local file path to the downloaded archive.</p> Source code in <code>datarec/datasets/movielens/movielens20m.py</code> <pre><code>def download(self) -&gt; str:\n    \"\"\"\n    Downloads the raw dataset compressed archive.\n\n    Returns:\n        (str): The local file path to the downloaded archive.\n    \"\"\"\n    if not os.path.exists(self._raw_folder):\n        os.makedirs(self._raw_folder)\n        print('Created folder \\'{}\\''.format(self._raw_folder))\n\n    # download dataset file (compressed file)\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n    download_file(self.url, file_path)\n\n    return file_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.movielens.movielens20m.MovieLens20M.process","title":"<code>process(file_path)</code>","text":"<p>Processes the raw  file and loads it into the class.</p> <p>This method reads the file, which includes a header row, and maps the columns to the standard user, item, rating, and timestamp fields.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the raw  file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This method assigns the processed data to <code>self.data</code> directly.</p> Source code in <code>datarec/datasets/movielens/movielens20m.py</code> <pre><code>def process(self, file_path):\n    \"\"\"\n    Processes the raw  file and loads it into the class.\n\n    This method reads the file, which includes a header row, and maps\n    the columns to the standard user, item, rating, and timestamp fields.\n\n    Args:\n        file_path (str): The path to the raw  file.\n\n    Returns:\n        (None): This method assigns the processed data to `self.data` directly.\n    \"\"\"\n\n    from datarec.io import read_tabular\n\n    dataset = read_tabular(file_path, sep=',',\n                           user_col='userId', item_col='movieId', rating_col='rating', timestamp_col='timestamp',\n                           header=0)\n    self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#tmall","title":"Tmall","text":"<p>Entry point for loading different versions of the Tmall dataset.</p> <p>Builder class for the v1 version of the Tmall IJCAI-16 dataset.</p>"},{"location":"documentation/datasets/#datarec.datasets.tmall.tmall.Tmall","title":"<code>Tmall</code>","text":"<p>Entry point class to load various versions of the Tmall dataset.</p> <p>This class provides a single, convenient interface for accessing the dataset. Based on the <code>version</code> parameter, it selects and returns the appropriate dataset builder.</p> <p>This dataset was released for the IJCAI-16 Contest and contains user interactions from the Tmall.com platform for a nearby store recommendation task.</p> <p>Note: This dataset requires manual download from the official source.</p> <p>The default version is 'latest', which currently corresponds to 'v1'.</p> <p>Examples:</p> <p>To load the latest version:</p> <pre><code>&gt;&gt;&gt; data_loader = Tmall()\n</code></pre> Source code in <code>datarec/datasets/tmall/tmall.py</code> <pre><code>class Tmall:\n    \"\"\"\n    Entry point class to load various versions of the Tmall dataset.\n\n    This class provides a single, convenient interface for accessing the dataset.\n    Based on the `version` parameter, it selects and returns the appropriate\n    dataset builder.\n\n    This dataset was released for the IJCAI-16 Contest and contains user\n    interactions from the Tmall.com platform for a nearby store recommendation task.\n\n    **Note:** This dataset requires manual download from the official source.\n\n    The default version is 'latest', which currently corresponds to 'v1'.\n\n    Examples:\n        To load the latest version:\n        &gt;&gt;&gt; data_loader = Tmall()\n    \"\"\"\n    latest_version = 'v1'\n\n    def __new__(self, version: str = 'latest', **kwargs):\n        \"\"\"\n        Initializes and returns the specified version of the Tmall dataset builder.\n\n        This method acts as a dispatcher, instantiating the correct builder class\n        that handles the preparation and loading for a specific dataset version.\n\n        Args:\n            version (str): The version of the dataset to load. Currently, only\n                'v1' and 'latest' are supported. Defaults to 'latest'.\n            **kwargs: Additional keyword arguments (not currently used for this dataset).\n\n        Returns:\n            (Tmall_v1): An instance of the dataset builder class, populated with data.\n\n        Raises:\n            ValueError: If an unsupported version string is provided.\n        \"\"\"\n\n        versions = {'v1': Tmall_v1}\n        if version == 'latest':\n            version = self.latest_version\n        if version in versions:\n            return versions[version]()\n        else:\n            raise ValueError(\"Tmall dataset: Unsupported version\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.tmall.tmall.Tmall.__new__","title":"<code>__new__(version='latest', **kwargs)</code>","text":"<p>Initializes and returns the specified version of the Tmall dataset builder.</p> <p>This method acts as a dispatcher, instantiating the correct builder class that handles the preparation and loading for a specific dataset version.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>The version of the dataset to load. Currently, only 'v1' and 'latest' are supported. Defaults to 'latest'.</p> <code>'latest'</code> <code>**kwargs</code> <p>Additional keyword arguments (not currently used for this dataset).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tmall_v1</code> <p>An instance of the dataset builder class, populated with data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported version string is provided.</p> Source code in <code>datarec/datasets/tmall/tmall.py</code> <pre><code>def __new__(self, version: str = 'latest', **kwargs):\n    \"\"\"\n    Initializes and returns the specified version of the Tmall dataset builder.\n\n    This method acts as a dispatcher, instantiating the correct builder class\n    that handles the preparation and loading for a specific dataset version.\n\n    Args:\n        version (str): The version of the dataset to load. Currently, only\n            'v1' and 'latest' are supported. Defaults to 'latest'.\n        **kwargs: Additional keyword arguments (not currently used for this dataset).\n\n    Returns:\n        (Tmall_v1): An instance of the dataset builder class, populated with data.\n\n    Raises:\n        ValueError: If an unsupported version string is provided.\n    \"\"\"\n\n    versions = {'v1': Tmall_v1}\n    if version == 'latest':\n        version = self.latest_version\n    if version in versions:\n        return versions[version]()\n    else:\n        raise ValueError(\"Tmall dataset: Unsupported version\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.tmall.tmall_v1.Tmall_v1","title":"<code>Tmall_v1</code>","text":"<p>               Bases: <code>DataRec</code></p> <p>Builder class for the Tmall dataset (IJCAI-16 Contest version).</p> <p>This class handles the logic for preparing and loading the Tmall dataset. It is not typically instantiated directly but is called by the <code>Tmall</code> entry point class.</p> <p>Note on usage: The Tmall dataset must be downloaded manually after registering on the Tianchi Aliyun website. This class will prompt the user with instructions to download the required zip file and place it in the correct cache directory before proceeding.</p> <p>This loader processes the <code>ijcai2016_taobao.csv</code> file from the archive.</p> <p>Attributes:</p> Name Type Description <code>website_url</code> <code>str</code> <p>The official website where the dataset can be downloaded.</p> <code>CHECKSUM</code> <code>str</code> <p>The MD5 checksum to verify the integrity of the user-provided file.</p> Source code in <code>datarec/datasets/tmall/tmall_v1.py</code> <pre><code>class Tmall_v1(DataRec):\n    \"\"\"\n    Builder class for the Tmall dataset (IJCAI-16 Contest version).\n\n    This class handles the logic for preparing and loading the Tmall dataset. It is\n    not typically instantiated directly but is called by the `Tmall` entry point class.\n\n    **Note on usage:** The Tmall dataset must be downloaded manually after\n    registering on the Tianchi Aliyun website. This class will prompt the user\n    with instructions to download the required zip file and place it in the\n    correct cache directory before proceeding.\n\n    This loader processes the `ijcai2016_taobao.csv` file from the archive.\n\n    Attributes:\n        website_url (str): The official website where the dataset can be downloaded.\n        CHECKSUM (str): The MD5 checksum to verify the integrity of the user-provided file.\n    \"\"\"\n    website_url = 'https://tianchi.aliyun.com/dataset/dataDetail?dataId=53'\n    data_file_name = 'IJCAI16_data.zip'\n    uncompressed_user_item_file_name = 'ijcai2016_taobao.csv'\n    REQUIRED_FILES = [uncompressed_user_item_file_name]\n    CHECKSUM = 'c4f4f0b8860984723652d2e91bcddc01'\n\n    def __init__(self, folder=None):\n        \"\"\"\n        Initializes the builder and orchestrates the data preparation workflow.\n\n        This constructor sets up paths and checks for the required files. If the\n        compressed archive is missing, it provides instructions for manual\n        download. It then proceeds to decompress and process the data.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n        \"\"\"\n        super().__init__(None)\n\n        self.dataset_name = 'tmall'\n        self.version_name = 'v1'\n\n        self._data_folder = folder if folder \\\n            else dataset_directory(self.dataset_name)\n        self._raw_folder = os.path.abspath(os.path.join(self._data_folder, RAW_DATA_FOLDER)) if folder \\\n            else dataset_raw_directory(self.dataset_name)\n\n        self.return_type = None\n\n        # check if the required files have been already downloaded\n        file_path = self.required_files()\n\n        if file_path is None:\n            file_path = self.download()\n            file_path = self.decompress(file_path)\n\n        print(f'found {file_path}')\n        self.process(file_path[0])\n\n    def required_files(self):\n        \"\"\"\n        Checks for the presence of the required decompressed data file.\n\n        Returns:\n            (list or None): A list containing the path to the required data file if\n                it exists or can be created by decompression. Otherwise, returns None.\n        \"\"\"\n        # compressed data file\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n\n        # check if the file is there\n        paths = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n        if all([os.path.exists(p) for p in paths]):\n            return paths\n        # check if the compressed file is there\n        elif os.path.exists(file_path):\n            return self.decompress(file_path)\n        else:\n            return None\n\n    def decompress(self, path):\n        \"\"\"\n        Decompresses the downloaded archive after verifying its checksum.\n\n        Args:\n            path (str): The file path of the compressed archive.\n\n        Returns:\n            (list or None): A list of paths to the decompressed files if successful,\n                otherwise None.\n        \"\"\"\n        verify_checksum(path, self.CHECKSUM)\n\n        # decompress downloaded file\n        decompress_zip_file(input_file=path, output_dir=self._raw_folder)\n        files = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n        if all([os.path.exists(f) for f in files]):\n            return [os.path.join(self._raw_folder, f) for f in files]\n        return None\n\n    def download(self) -&gt; (str, str):\n        \"\"\"\n        Guides the user to manually download the dataset archive.\n\n        This method does not download automatically. Instead, it prints instructions\n        for the user to visit the Tianchi Aliyun website, register, download the\n        `IJCAI16_data.zip` file, and place it in the correct cache directory.\n\n        Returns:\n            (str): The local file path to the user-provided archive.\n        \"\"\"\n        if not os.path.exists(self._raw_folder):\n            os.makedirs(self._raw_folder)\n            print('Created folder \\'{}\\''.format(self._raw_folder))\n\n        print(f'\\nThis version of Tmall dataset requires the user to manually download it.\\n'\n              f'Please, go to {self.website_url} on your browser, register, and click on the download button.\\n'\n              f'Then, move or copy \\'{self.data_file_name}\\' in the following directory:\\n'\n              f'\\'{self._raw_folder}\\'\\n'\n              f'Please, do not change the original file name and try again.')\n        file_path = os.path.join(self._raw_folder, self.data_file_name)\n        return file_path\n\n    def process(self, file_path):\n        \"\"\"\n        Processes the raw file and loads it.\n\n        This method reads the CSV file, which has a header, and maps the specific\n        column names (`use_ID`, `ite_ID`, `act_ID`, `time`) to the standard\n        user, item, rating, and timestamp fields.\n\n        Args:\n            file_path (str): The path to the raw data file.\n\n        Returns:\n            (None): This method assigns the processed data to `self.data` directly.\n        \"\"\"\n\n        from datarec.io import read_tabular\n\n        dataset = read_tabular(file_path, sep=',', user_col='use_ID', item_col='ite_ID', rating_col='act_ID', timestamp_col='time', header=0)\n        self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.tmall.tmall_v1.Tmall_v1.__init__","title":"<code>__init__(folder=None)</code>","text":"<p>Initializes the builder and orchestrates the data preparation workflow.</p> <p>This constructor sets up paths and checks for the required files. If the compressed archive is missing, it provides instructions for manual download. It then proceeds to decompress and process the data.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> Source code in <code>datarec/datasets/tmall/tmall_v1.py</code> <pre><code>def __init__(self, folder=None):\n    \"\"\"\n    Initializes the builder and orchestrates the data preparation workflow.\n\n    This constructor sets up paths and checks for the required files. If the\n    compressed archive is missing, it provides instructions for manual\n    download. It then proceeds to decompress and process the data.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n    \"\"\"\n    super().__init__(None)\n\n    self.dataset_name = 'tmall'\n    self.version_name = 'v1'\n\n    self._data_folder = folder if folder \\\n        else dataset_directory(self.dataset_name)\n    self._raw_folder = os.path.abspath(os.path.join(self._data_folder, RAW_DATA_FOLDER)) if folder \\\n        else dataset_raw_directory(self.dataset_name)\n\n    self.return_type = None\n\n    # check if the required files have been already downloaded\n    file_path = self.required_files()\n\n    if file_path is None:\n        file_path = self.download()\n        file_path = self.decompress(file_path)\n\n    print(f'found {file_path}')\n    self.process(file_path[0])\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.tmall.tmall_v1.Tmall_v1.required_files","title":"<code>required_files()</code>","text":"<p>Checks for the presence of the required decompressed data file.</p> <p>Returns:</p> Type Description <code>list or None</code> <p>A list containing the path to the required data file if it exists or can be created by decompression. Otherwise, returns None.</p> Source code in <code>datarec/datasets/tmall/tmall_v1.py</code> <pre><code>def required_files(self):\n    \"\"\"\n    Checks for the presence of the required decompressed data file.\n\n    Returns:\n        (list or None): A list containing the path to the required data file if\n            it exists or can be created by decompression. Otherwise, returns None.\n    \"\"\"\n    # compressed data file\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n\n    # check if the file is there\n    paths = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n    if all([os.path.exists(p) for p in paths]):\n        return paths\n    # check if the compressed file is there\n    elif os.path.exists(file_path):\n        return self.decompress(file_path)\n    else:\n        return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.tmall.tmall_v1.Tmall_v1.decompress","title":"<code>decompress(path)</code>","text":"<p>Decompresses the downloaded archive after verifying its checksum.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path of the compressed archive.</p> required <p>Returns:</p> Type Description <code>list or None</code> <p>A list of paths to the decompressed files if successful, otherwise None.</p> Source code in <code>datarec/datasets/tmall/tmall_v1.py</code> <pre><code>def decompress(self, path):\n    \"\"\"\n    Decompresses the downloaded archive after verifying its checksum.\n\n    Args:\n        path (str): The file path of the compressed archive.\n\n    Returns:\n        (list or None): A list of paths to the decompressed files if successful,\n            otherwise None.\n    \"\"\"\n    verify_checksum(path, self.CHECKSUM)\n\n    # decompress downloaded file\n    decompress_zip_file(input_file=path, output_dir=self._raw_folder)\n    files = [os.path.join(self._raw_folder, f) for f in self.REQUIRED_FILES]\n    if all([os.path.exists(f) for f in files]):\n        return [os.path.join(self._raw_folder, f) for f in files]\n    return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.tmall.tmall_v1.Tmall_v1.download","title":"<code>download()</code>","text":"<p>Guides the user to manually download the dataset archive.</p> <p>This method does not download automatically. Instead, it prints instructions for the user to visit the Tianchi Aliyun website, register, download the <code>IJCAI16_data.zip</code> file, and place it in the correct cache directory.</p> <p>Returns:</p> Type Description <code>str</code> <p>The local file path to the user-provided archive.</p> Source code in <code>datarec/datasets/tmall/tmall_v1.py</code> <pre><code>def download(self) -&gt; (str, str):\n    \"\"\"\n    Guides the user to manually download the dataset archive.\n\n    This method does not download automatically. Instead, it prints instructions\n    for the user to visit the Tianchi Aliyun website, register, download the\n    `IJCAI16_data.zip` file, and place it in the correct cache directory.\n\n    Returns:\n        (str): The local file path to the user-provided archive.\n    \"\"\"\n    if not os.path.exists(self._raw_folder):\n        os.makedirs(self._raw_folder)\n        print('Created folder \\'{}\\''.format(self._raw_folder))\n\n    print(f'\\nThis version of Tmall dataset requires the user to manually download it.\\n'\n          f'Please, go to {self.website_url} on your browser, register, and click on the download button.\\n'\n          f'Then, move or copy \\'{self.data_file_name}\\' in the following directory:\\n'\n          f'\\'{self._raw_folder}\\'\\n'\n          f'Please, do not change the original file name and try again.')\n    file_path = os.path.join(self._raw_folder, self.data_file_name)\n    return file_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.tmall.tmall_v1.Tmall_v1.process","title":"<code>process(file_path)</code>","text":"<p>Processes the raw file and loads it.</p> <p>This method reads the CSV file, which has a header, and maps the specific column names (<code>use_ID</code>, <code>ite_ID</code>, <code>act_ID</code>, <code>time</code>) to the standard user, item, rating, and timestamp fields.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the raw data file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This method assigns the processed data to <code>self.data</code> directly.</p> Source code in <code>datarec/datasets/tmall/tmall_v1.py</code> <pre><code>def process(self, file_path):\n    \"\"\"\n    Processes the raw file and loads it.\n\n    This method reads the CSV file, which has a header, and maps the specific\n    column names (`use_ID`, `ite_ID`, `act_ID`, `time`) to the standard\n    user, item, rating, and timestamp fields.\n\n    Args:\n        file_path (str): The path to the raw data file.\n\n    Returns:\n        (None): This method assigns the processed data to `self.data` directly.\n    \"\"\"\n\n    from datarec.io import read_tabular\n\n    dataset = read_tabular(file_path, sep=',', user_col='use_ID', item_col='ite_ID', rating_col='act_ID', timestamp_col='time', header=0)\n    self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#yelp","title":"Yelp","text":"<p>Entry point for loading different versions of the Yelp dataset.</p> <p>Builder class for the v1 version of the Yelp Dataset.</p>"},{"location":"documentation/datasets/#datarec.datasets.yelp.yelp.Yelp","title":"<code>Yelp</code>","text":"<p>Entry point class to load various versions of the Yelp Dataset.</p> <p>This class provides a single, convenient interface for accessing the dataset. Based on the <code>version</code> parameter, it selects and returns the appropriate dataset builder.</p> <p>The default version is 'latest', which currently corresponds to 'v1'.</p> <p>Examples:</p> <p>To load the latest version:</p> <pre><code>&gt;&gt;&gt; data_loader = Yelp()\n</code></pre> <p>To load a specific version:</p> <pre><code>&gt;&gt;&gt; data_loader = Yelp(version='v1')\n</code></pre> Source code in <code>datarec/datasets/yelp/yelp.py</code> <pre><code>class Yelp:\n    \"\"\"\n    Entry point class to load various versions of the Yelp Dataset.\n\n    This class provides a single, convenient interface for accessing the dataset.\n    Based on the `version` parameter, it selects and returns the appropriate\n    dataset builder.\n\n    The default version is 'latest', which currently corresponds to 'v1'.\n\n    Examples:\n        To load the latest version:\n        &gt;&gt;&gt; data_loader = Yelp()\n\n        To load a specific version:\n        &gt;&gt;&gt; data_loader = Yelp(version='v1')\n    \"\"\"\n    latest_version = 'v1'\n\n    def __new__(self, version: str = 'latest', **kwargs):\n        \"\"\"\n        Initializes and returns the specified version of the Yelp dataset builder.\n\n        This method acts as a dispatcher, instantiating the correct builder class\n        that handles the downloading, caching, and loading for a specific dataset version.\n\n        Args:\n            version (str): The version of the dataset to load. Currently, only\n                'v1' and 'latest' are supported. Defaults to 'latest'.\n            **kwargs: Additional keyword arguments (not currently used for this dataset).\n\n        Returns:\n            (Yelp_v1): An instance of the dataset builder class, populated with data.\n\n        Raises:\n            ValueError: If an unsupported version string is provided.\n        \"\"\"\n\n        versions = {'v1': Yelp_v1}\n        if version == 'latest':\n            version = self.latest_version\n        if version in versions:\n            return versions[version]()\n        else:\n            raise ValueError(\"Yelp dataset: Unsupported version\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.yelp.yelp.Yelp.__new__","title":"<code>__new__(version='latest', **kwargs)</code>","text":"<p>Initializes and returns the specified version of the Yelp dataset builder.</p> <p>This method acts as a dispatcher, instantiating the correct builder class that handles the downloading, caching, and loading for a specific dataset version.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>The version of the dataset to load. Currently, only 'v1' and 'latest' are supported. Defaults to 'latest'.</p> <code>'latest'</code> <code>**kwargs</code> <p>Additional keyword arguments (not currently used for this dataset).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Yelp_v1</code> <p>An instance of the dataset builder class, populated with data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported version string is provided.</p> Source code in <code>datarec/datasets/yelp/yelp.py</code> <pre><code>def __new__(self, version: str = 'latest', **kwargs):\n    \"\"\"\n    Initializes and returns the specified version of the Yelp dataset builder.\n\n    This method acts as a dispatcher, instantiating the correct builder class\n    that handles the downloading, caching, and loading for a specific dataset version.\n\n    Args:\n        version (str): The version of the dataset to load. Currently, only\n            'v1' and 'latest' are supported. Defaults to 'latest'.\n        **kwargs: Additional keyword arguments (not currently used for this dataset).\n\n    Returns:\n        (Yelp_v1): An instance of the dataset builder class, populated with data.\n\n    Raises:\n        ValueError: If an unsupported version string is provided.\n    \"\"\"\n\n    versions = {'v1': Yelp_v1}\n    if version == 'latest':\n        version = self.latest_version\n    if version in versions:\n        return versions[version]()\n    else:\n        raise ValueError(\"Yelp dataset: Unsupported version\")\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.yelp.yelp_v1.Yelp_v1","title":"<code>Yelp_v1</code>","text":"<p>               Bases: <code>DataRec</code></p> <p>Builder class for the Yelp Dataset.</p> <p>This class handles the logic for downloading, preparing, and loading the Yelp dataset. It is not typically instantiated directly but is called by the <code>Yelp</code> entry point class.</p> <p>The download and preparation process is multi-step: 1. A <code>.zip</code> archive is downloaded. 2. The zip is extracted, revealing a <code>.tar</code> archive. 3. The tar is extracted, revealing several <code>.json</code> files. This loader specifically processes <code>yelp_academic_dataset_review.json</code> to extract user-business interactions (ratings).</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>The URL from which the raw dataset is downloaded.</p> <code>CHECKSUM_ZIP</code> <code>str</code> <p>MD5 checksum for the initial downloaded .zip file.</p> <code>CHECKSUM_TAR</code> <code>str</code> <p>MD5 checksum for the intermediate .tar file.</p> Source code in <code>datarec/datasets/yelp/yelp_v1.py</code> <pre><code>class Yelp_v1(DataRec):\n    \"\"\"\n    Builder class for the Yelp Dataset.\n\n    This class handles the logic for downloading, preparing, and loading the\n    Yelp dataset. It is not typically instantiated directly but is called by\n    the `Yelp` entry point class.\n\n    The download and preparation process is multi-step:\n    1. A `.zip` archive is downloaded.\n    2. The zip is extracted, revealing a `.tar` archive.\n    3. The tar is extracted, revealing several `.json` files.\n    This loader specifically processes `yelp_academic_dataset_review.json` to extract\n    user-business interactions (ratings).\n\n    Attributes:\n        url (str): The URL from which the raw dataset is downloaded.\n        CHECKSUM_ZIP (str): MD5 checksum for the initial downloaded .zip file.\n        CHECKSUM_TAR (str): MD5 checksum for the intermediate .tar file.\n    \"\"\"\n    website_url = 'https://www.yelp.com/dataset'\n    url = 'https://business.yelp.com/external-assets/files/Yelp-JSON.zip'\n    data_file_name = 'Yelp-JSON.zip'\n    data_tar_file_name = 'yelp_dataset.tar'\n    subdirectory_name = 'Yelp JSON'  # once extracted the zip file\n    uncompressed_business_file_name = 'yelp_academic_dataset_business.json'\n    uncompressed_checkin_file_name = 'yelp_academic_dataset_checkin.json'\n    uncompressed_review_file_name = 'yelp_academic_dataset_review.json'\n    uncompressed_tip_file_name = 'yelp_academic_dataset_tip.json'\n    uncompressed_user_file_name = 'yelp_academic_dataset_user.json'\n    REQUIRED_FILES = [uncompressed_business_file_name,\n                      uncompressed_checkin_file_name,\n                      uncompressed_review_file_name,\n                      uncompressed_tip_file_name,\n                      uncompressed_user_file_name]\n    CHECKSUM_ZIP = 'b0c36fe2d00a52d8de44fa3b2513c9d2'\n    CHECKSUM_TAR = '0bc8cc1481ccbbd140d2aba2909a928a'\n\n    def __init__(self, folder=None):\n        \"\"\"\n        Initializes the builder and orchestrates the data preparation workflow.\n\n        This constructor sets up the necessary paths and automatically triggers\n        the download, verification, and processing steps if the data is not\n        already present in the specified cache directory.\n\n        Args:\n            folder (str, optional): A custom directory to store the dataset files.\n                If None, a default user cache directory is used. Defaults to None.\n        \"\"\"\n        super().__init__(None)\n\n        self.dataset_name = 'yelp'\n        self.version_name = 'v1'\n\n        self._data_folder = folder if folder \\\n            else dataset_directory(self.dataset_name)\n        self._raw_folder = os.path.abspath(os.path.join(self._data_folder, RAW_DATA_FOLDER)) if folder \\\n            else dataset_raw_directory(self.dataset_name)\n\n        self.return_type = None\n\n        # check if the required files have been already downloaded\n        file_path = self.required_files()\n\n        if file_path is None:\n            file_path = self.download()\n            file_path = self.decompress(file_path) ## all files\n\n        business_file_path, checkin_file_path, review_file_path, tip_file_path, user_file_path = file_path\n\n        print(f'found {file_path}')\n        self.process(review_file_path)\n\n    def required_files(self):\n        \"\"\"\n        Checks for the presence of the final required decompressed JSON files.\n\n        Returns:\n            (list or None): A list of paths to the required data files if they\n                exist. Otherwise, returns None.\n        \"\"\"\n        # compressed data file\n        file_path = os.path.join(self._raw_folder, self.subdirectory_name)\n\n        # check if the file is there\n        paths = [os.path.join(self._raw_folder, self.subdirectory_name, f) for f in self.REQUIRED_FILES]\n        if all([os.path.exists(p) for p in paths]):\n            return paths\n        # check if the compressed file is there\n        elif os.path.exists(file_path):\n            return self.decompress(file_path)\n        else:\n            return None\n\n    def decompress(self, path):\n        \"\"\"\n        Decompresses the dataset via a two-step process (zip then tar).\n\n        Args:\n            path (str): The file path of the initial downloaded .zip archive.\n\n        Returns:\n            (list or None): A list of paths to the final decompressed files if\n                successful, otherwise None.\n        \"\"\"\n        verify_checksum(path, self.CHECKSUM_ZIP)\n        decompress_zip_file(path, self._raw_folder)\n\n\n        tar_file_path = os.path.join(self._raw_folder, self.subdirectory_name, self.data_tar_file_name)\n        verify_checksum(tar_file_path, self.CHECKSUM_TAR)\n\n        decompress_tar_file(tar_file_path, os.path.join(self._raw_folder, self.subdirectory_name))\n        files = [os.path.join(self._raw_folder, self.subdirectory_name, f) for f in self.REQUIRED_FILES]\n        if all([os.path.exists(f) for f in files]):\n            return [os.path.join(self._raw_folder, f) for f in files]\n        return None\n\n    def download(self) -&gt; (str, str):\n        \"\"\"\n        Downloads the raw dataset compressed archive.\n\n        Returns:\n            (str): The local file path to the downloaded .zip archive.\n        \"\"\"\n        if not os.path.exists(self._raw_folder):\n            os.makedirs(self._raw_folder)\n            print('Created folder \\'{}\\''.format(self._raw_folder))\n\n        file_name = os.path.basename(self.url)\n        file_path = os.path.join(self._raw_folder, file_name)\n        if not os.path.exists(file_path):\n            download_browser(self.url, file_path)\n        return file_path\n\n    def process(self, path):\n        \"\"\"\n        Processes the raw file and loads it into the class.\n\n        This method reads the JSON file line by line. It extracts the user ID,\n        business ID (as the item), star rating, and date. The date strings are\n        then converted to Unix timestamps.\n\n        Args:\n            path (str): The path to the raw file.\n\n        Returns:\n            (None): This method assigns the processed data to `self.data` directly.\n        \"\"\"\n        from datarec.io import read_json\n\n        user_field = 'user_id'\n        item_field = 'business_id'\n        rating_field = 'stars'\n        date_field = 'date'  # format: YYYY-MM-DD , e.g.: 2016-03-09\n        dataset = read_json(path, user_field=user_field, item_field=item_field, rating_field=rating_field, timestamp_field=date_field)\n        timestamps = pd.Series(dataset.data[date_field].apply(lambda x: x.timestamp()).values,\n                               index=dataset.data.index, dtype='float64')\n        dataset.data.loc[:, date_field] = timestamps\n        self.data = dataset\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.yelp.yelp_v1.Yelp_v1.__init__","title":"<code>__init__(folder=None)</code>","text":"<p>Initializes the builder and orchestrates the data preparation workflow.</p> <p>This constructor sets up the necessary paths and automatically triggers the download, verification, and processing steps if the data is not already present in the specified cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>A custom directory to store the dataset files. If None, a default user cache directory is used. Defaults to None.</p> <code>None</code> Source code in <code>datarec/datasets/yelp/yelp_v1.py</code> <pre><code>def __init__(self, folder=None):\n    \"\"\"\n    Initializes the builder and orchestrates the data preparation workflow.\n\n    This constructor sets up the necessary paths and automatically triggers\n    the download, verification, and processing steps if the data is not\n    already present in the specified cache directory.\n\n    Args:\n        folder (str, optional): A custom directory to store the dataset files.\n            If None, a default user cache directory is used. Defaults to None.\n    \"\"\"\n    super().__init__(None)\n\n    self.dataset_name = 'yelp'\n    self.version_name = 'v1'\n\n    self._data_folder = folder if folder \\\n        else dataset_directory(self.dataset_name)\n    self._raw_folder = os.path.abspath(os.path.join(self._data_folder, RAW_DATA_FOLDER)) if folder \\\n        else dataset_raw_directory(self.dataset_name)\n\n    self.return_type = None\n\n    # check if the required files have been already downloaded\n    file_path = self.required_files()\n\n    if file_path is None:\n        file_path = self.download()\n        file_path = self.decompress(file_path) ## all files\n\n    business_file_path, checkin_file_path, review_file_path, tip_file_path, user_file_path = file_path\n\n    print(f'found {file_path}')\n    self.process(review_file_path)\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.yelp.yelp_v1.Yelp_v1.required_files","title":"<code>required_files()</code>","text":"<p>Checks for the presence of the final required decompressed JSON files.</p> <p>Returns:</p> Type Description <code>list or None</code> <p>A list of paths to the required data files if they exist. Otherwise, returns None.</p> Source code in <code>datarec/datasets/yelp/yelp_v1.py</code> <pre><code>def required_files(self):\n    \"\"\"\n    Checks for the presence of the final required decompressed JSON files.\n\n    Returns:\n        (list or None): A list of paths to the required data files if they\n            exist. Otherwise, returns None.\n    \"\"\"\n    # compressed data file\n    file_path = os.path.join(self._raw_folder, self.subdirectory_name)\n\n    # check if the file is there\n    paths = [os.path.join(self._raw_folder, self.subdirectory_name, f) for f in self.REQUIRED_FILES]\n    if all([os.path.exists(p) for p in paths]):\n        return paths\n    # check if the compressed file is there\n    elif os.path.exists(file_path):\n        return self.decompress(file_path)\n    else:\n        return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.yelp.yelp_v1.Yelp_v1.decompress","title":"<code>decompress(path)</code>","text":"<p>Decompresses the dataset via a two-step process (zip then tar).</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path of the initial downloaded .zip archive.</p> required <p>Returns:</p> Type Description <code>list or None</code> <p>A list of paths to the final decompressed files if successful, otherwise None.</p> Source code in <code>datarec/datasets/yelp/yelp_v1.py</code> <pre><code>def decompress(self, path):\n    \"\"\"\n    Decompresses the dataset via a two-step process (zip then tar).\n\n    Args:\n        path (str): The file path of the initial downloaded .zip archive.\n\n    Returns:\n        (list or None): A list of paths to the final decompressed files if\n            successful, otherwise None.\n    \"\"\"\n    verify_checksum(path, self.CHECKSUM_ZIP)\n    decompress_zip_file(path, self._raw_folder)\n\n\n    tar_file_path = os.path.join(self._raw_folder, self.subdirectory_name, self.data_tar_file_name)\n    verify_checksum(tar_file_path, self.CHECKSUM_TAR)\n\n    decompress_tar_file(tar_file_path, os.path.join(self._raw_folder, self.subdirectory_name))\n    files = [os.path.join(self._raw_folder, self.subdirectory_name, f) for f in self.REQUIRED_FILES]\n    if all([os.path.exists(f) for f in files]):\n        return [os.path.join(self._raw_folder, f) for f in files]\n    return None\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.yelp.yelp_v1.Yelp_v1.download","title":"<code>download()</code>","text":"<p>Downloads the raw dataset compressed archive.</p> <p>Returns:</p> Type Description <code>str</code> <p>The local file path to the downloaded .zip archive.</p> Source code in <code>datarec/datasets/yelp/yelp_v1.py</code> <pre><code>def download(self) -&gt; (str, str):\n    \"\"\"\n    Downloads the raw dataset compressed archive.\n\n    Returns:\n        (str): The local file path to the downloaded .zip archive.\n    \"\"\"\n    if not os.path.exists(self._raw_folder):\n        os.makedirs(self._raw_folder)\n        print('Created folder \\'{}\\''.format(self._raw_folder))\n\n    file_name = os.path.basename(self.url)\n    file_path = os.path.join(self._raw_folder, file_name)\n    if not os.path.exists(file_path):\n        download_browser(self.url, file_path)\n    return file_path\n</code></pre>"},{"location":"documentation/datasets/#datarec.datasets.yelp.yelp_v1.Yelp_v1.process","title":"<code>process(path)</code>","text":"<p>Processes the raw file and loads it into the class.</p> <p>This method reads the JSON file line by line. It extracts the user ID, business ID (as the item), star rating, and date. The date strings are then converted to Unix timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the raw file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This method assigns the processed data to <code>self.data</code> directly.</p> Source code in <code>datarec/datasets/yelp/yelp_v1.py</code> <pre><code>def process(self, path):\n    \"\"\"\n    Processes the raw file and loads it into the class.\n\n    This method reads the JSON file line by line. It extracts the user ID,\n    business ID (as the item), star rating, and date. The date strings are\n    then converted to Unix timestamps.\n\n    Args:\n        path (str): The path to the raw file.\n\n    Returns:\n        (None): This method assigns the processed data to `self.data` directly.\n    \"\"\"\n    from datarec.io import read_json\n\n    user_field = 'user_id'\n    item_field = 'business_id'\n    rating_field = 'stars'\n    date_field = 'date'  # format: YYYY-MM-DD , e.g.: 2016-03-09\n    dataset = read_json(path, user_field=user_field, item_field=item_field, rating_field=rating_field, timestamp_field=date_field)\n    timestamps = pd.Series(dataset.data[date_field].apply(lambda x: x.timestamp()).values,\n                           index=dataset.data.index, dtype='float64')\n    dataset.data.loc[:, date_field] = timestamps\n    self.data = dataset\n</code></pre>"},{"location":"documentation/io/","title":"Input/Output Reference","text":"<p>This section provides a detailed API reference for all modules related to data input, output, and framework interoperability in the <code>datarec</code> library.</p>"},{"location":"documentation/io/#core-io-modules","title":"Core I/O Modules","text":"<p>These modules handle the fundamental tasks of reading, writing, and representing raw data.</p>"},{"location":"documentation/io/#datarec.io.rawdata.RawData","title":"<code>RawData</code>","text":"<p>Container for raw datasets in DataRec.</p> <p>Wraps a <code>pandas.DataFrame</code> and stores metadata about user, item, rating, and timestamp columns. Provides lightweight methods for slicing, copying, and merging data.</p> Source code in <code>datarec/io/rawdata.py</code> <pre><code>class RawData:\n    \"\"\"\n    Container for raw datasets in DataRec.\n\n    Wraps a `pandas.DataFrame` and stores metadata about user, item, rating, and timestamp columns.\n    Provides lightweight methods for slicing, copying, and merging data.\n    \"\"\"\n    def __init__(self, data=None, header=False, user=None, item=None, rating=None, timestamp=None):\n        \"\"\"\n        Initialize a RawData object.\n\n        Args:\n            data (pd.DataFrame): DataFrame of the dataset. Defaults to None.\n            header (bool): Whether the file has a header. Defaults to False.\n            user (str): Column name for user IDs.\n            item (str): Column name for item IDs.\n            rating (str): Column name for ratings.\n            timestamp (str): Column name for timestamps.\n        \"\"\"\n        self.data = data\n        self.header = header\n        if data is None:\n            self.data = pd.DataFrame\n            self.header = header\n        self.path = None\n\n        self.user = user\n        self.item = item\n        self.rating = rating\n        self.timestamp = timestamp\n\n    def append(self, new_data):\n        \"\"\"\n        Append new rows to the dataset.\n\n        Args:\n            new_data (pd.DataFrame): DataFrame to append.\n\n        Returns:\n            None\n        \"\"\"\n        self.data.append(new_data)\n\n    def copy(self, deep=True):\n        \"\"\"\n        Make a copy of the dataset.\n\n        Args:\n            deep (bool): If True, return a deep copy of the dataset.\n\n        Returns:\n            (RawData): A copy of the dataset.\n\n        \"\"\"\n        self.data.copy(deep=deep)\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation of the dataset.\n        \"\"\"\n        return repr(self.data)\n\n    def __len__(self):\n        \"\"\"\n        Return the length of the dataset.\n        \"\"\"\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Return the item at the given index.\n        Args:\n            idx: index of the item to return.\n\n        Returns:\n            (RawData): the sample at the given index.\n\n        \"\"\"\n        return self.data[idx]\n\n    def __add__(self, other):\n        \"\"\"\n        Concatenate two RawData objects.\n        Args:\n            other (RawData): the other RawData to concatenate.\n\n        Returns:\n            (RawData): the concatenated RawData object.\n\n        \"\"\"\n        self.__check_rawdata_compatibility__(other)\n        new_data = pd.concat([self.data, other.data])\n        new_rawdata = RawData(new_data, user=self.user, item=self.item, rating=self.rating,\n                              timestamp=self.timestamp, header=self.header)\n        return new_rawdata\n\n    def __iter__(self):\n        \"\"\"\n        Iterate over dataset rows.\n\n        Returns:\n            (pd.Series): Each row in the dataset.\n\n        \"\"\"\n        return iter(self.data)\n\n    def __check_rawdata_compatibility__(self, rawdata):\n        \"\"\"\n        Check compatibility between RawData objects.\n        Args:\n            rawdata (RawData): RawData object to check.\n\n        Returns:\n            (bool): True if compatibility is verified.\n\n        \"\"\"\n        return __check_rawdata_compatibility__(self, rawdata)\n</code></pre>"},{"location":"documentation/io/#datarec.io.rawdata.RawData.__init__","title":"<code>__init__(data=None, header=False, user=None, item=None, rating=None, timestamp=None)</code>","text":"<p>Initialize a RawData object.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame of the dataset. Defaults to None.</p> <code>None</code> <code>header</code> <code>bool</code> <p>Whether the file has a header. Defaults to False.</p> <code>False</code> <code>user</code> <code>str</code> <p>Column name for user IDs.</p> <code>None</code> <code>item</code> <code>str</code> <p>Column name for item IDs.</p> <code>None</code> <code>rating</code> <code>str</code> <p>Column name for ratings.</p> <code>None</code> <code>timestamp</code> <code>str</code> <p>Column name for timestamps.</p> <code>None</code> Source code in <code>datarec/io/rawdata.py</code> <pre><code>def __init__(self, data=None, header=False, user=None, item=None, rating=None, timestamp=None):\n    \"\"\"\n    Initialize a RawData object.\n\n    Args:\n        data (pd.DataFrame): DataFrame of the dataset. Defaults to None.\n        header (bool): Whether the file has a header. Defaults to False.\n        user (str): Column name for user IDs.\n        item (str): Column name for item IDs.\n        rating (str): Column name for ratings.\n        timestamp (str): Column name for timestamps.\n    \"\"\"\n    self.data = data\n    self.header = header\n    if data is None:\n        self.data = pd.DataFrame\n        self.header = header\n    self.path = None\n\n    self.user = user\n    self.item = item\n    self.rating = rating\n    self.timestamp = timestamp\n</code></pre>"},{"location":"documentation/io/#datarec.io.rawdata.RawData.append","title":"<code>append(new_data)</code>","text":"<p>Append new rows to the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>new_data</code> <code>DataFrame</code> <p>DataFrame to append.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>datarec/io/rawdata.py</code> <pre><code>def append(self, new_data):\n    \"\"\"\n    Append new rows to the dataset.\n\n    Args:\n        new_data (pd.DataFrame): DataFrame to append.\n\n    Returns:\n        None\n    \"\"\"\n    self.data.append(new_data)\n</code></pre>"},{"location":"documentation/io/#datarec.io.rawdata.RawData.copy","title":"<code>copy(deep=True)</code>","text":"<p>Make a copy of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>If True, return a deep copy of the dataset.</p> <code>True</code> <p>Returns:</p> Type Description <code>RawData</code> <p>A copy of the dataset.</p> Source code in <code>datarec/io/rawdata.py</code> <pre><code>def copy(self, deep=True):\n    \"\"\"\n    Make a copy of the dataset.\n\n    Args:\n        deep (bool): If True, return a deep copy of the dataset.\n\n    Returns:\n        (RawData): A copy of the dataset.\n\n    \"\"\"\n    self.data.copy(deep=deep)\n</code></pre>"},{"location":"documentation/io/#datarec.io.rawdata.RawData.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the dataset.</p> Source code in <code>datarec/io/rawdata.py</code> <pre><code>def __repr__(self):\n    \"\"\"\n    Return a string representation of the dataset.\n    \"\"\"\n    return repr(self.data)\n</code></pre>"},{"location":"documentation/io/#datarec.io.rawdata.RawData.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the dataset.</p> Source code in <code>datarec/io/rawdata.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Return the length of the dataset.\n    \"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"documentation/io/#datarec.io.rawdata.RawData.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Return the item at the given index. Args:     idx: index of the item to return.</p> <p>Returns:</p> Type Description <code>RawData</code> <p>the sample at the given index.</p> Source code in <code>datarec/io/rawdata.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Return the item at the given index.\n    Args:\n        idx: index of the item to return.\n\n    Returns:\n        (RawData): the sample at the given index.\n\n    \"\"\"\n    return self.data[idx]\n</code></pre>"},{"location":"documentation/io/#datarec.io.rawdata.RawData.__add__","title":"<code>__add__(other)</code>","text":"<p>Concatenate two RawData objects. Args:     other (RawData): the other RawData to concatenate.</p> <p>Returns:</p> Type Description <code>RawData</code> <p>the concatenated RawData object.</p> Source code in <code>datarec/io/rawdata.py</code> <pre><code>def __add__(self, other):\n    \"\"\"\n    Concatenate two RawData objects.\n    Args:\n        other (RawData): the other RawData to concatenate.\n\n    Returns:\n        (RawData): the concatenated RawData object.\n\n    \"\"\"\n    self.__check_rawdata_compatibility__(other)\n    new_data = pd.concat([self.data, other.data])\n    new_rawdata = RawData(new_data, user=self.user, item=self.item, rating=self.rating,\n                          timestamp=self.timestamp, header=self.header)\n    return new_rawdata\n</code></pre>"},{"location":"documentation/io/#datarec.io.rawdata.RawData.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over dataset rows.</p> <p>Returns:</p> Type Description <code>Series</code> <p>Each row in the dataset.</p> Source code in <code>datarec/io/rawdata.py</code> <pre><code>def __iter__(self):\n    \"\"\"\n    Iterate over dataset rows.\n\n    Returns:\n        (pd.Series): Each row in the dataset.\n\n    \"\"\"\n    return iter(self.data)\n</code></pre>"},{"location":"documentation/io/#datarec.io.rawdata.RawData.__check_rawdata_compatibility__","title":"<code>__check_rawdata_compatibility__(rawdata)</code>","text":"<p>Check compatibility between RawData objects. Args:     rawdata (RawData): RawData object to check.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if compatibility is verified.</p> Source code in <code>datarec/io/rawdata.py</code> <pre><code>def __check_rawdata_compatibility__(self, rawdata):\n    \"\"\"\n    Check compatibility between RawData objects.\n    Args:\n        rawdata (RawData): RawData object to check.\n\n    Returns:\n        (bool): True if compatibility is verified.\n\n    \"\"\"\n    return __check_rawdata_compatibility__(self, rawdata)\n</code></pre>"},{"location":"documentation/io/#datarec.io.rawdata.__check_rawdata_compatibility__","title":"<code>__check_rawdata_compatibility__(rawdata1, rawdata2)</code>","text":"<p>Check compatibility between two RawData objects. Args:     rawdata1 (RawData): First RawData object to check.     rawdata2 (RawData): Second RawData object to check.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if compatibility is verified.</p> Source code in <code>datarec/io/rawdata.py</code> <pre><code>def __check_rawdata_compatibility__(rawdata1: RawData, rawdata2: RawData):\n    \"\"\"\n    Check compatibility between two RawData objects.\n    Args:\n        rawdata1 (RawData): First RawData object to check.\n        rawdata2 (RawData): Second RawData object to check.\n\n    Returns:\n        (bool): True if compatibility is verified.\n\n    \"\"\"\n    if rawdata1.user != rawdata2.user:\n        raise ValueError('User columns are not compatible')\n    if rawdata1.item != rawdata2.item:\n        raise ValueError('Item columns are not compatible')\n    if rawdata1.rating != rawdata2.rating:\n        raise ValueError('Rating columns are not compatible')\n    if rawdata1.timestamp != rawdata2.timestamp:\n        raise ValueError('Timestamp columns are not compatible')\n    if rawdata1.header != rawdata2.header:\n        raise ValueError('Header is not compatible')\n    return True\n</code></pre>"},{"location":"documentation/io/#datarec.io.readers.fill_rawdata","title":"<code>fill_rawdata(data, user=None, item=None, rating=None, timestamp=None, path=None)</code>","text":"<p>Create a RawData object from raw data and assign column names to RawData object attributes.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Data to create RawData object from.</p> required <code>user</code> <code>str</code> <p>Column name for user field.</p> <code>None</code> <code>item</code> <code>str</code> <p>Column name for item field.</p> <code>None</code> <code>rating</code> <code>str</code> <p>Column name for rating field.</p> <code>None</code> <code>timestamp</code> <code>str</code> <p>Column name for timestamp field.</p> <code>None</code> <code>path</code> <code>str</code> <p>Path where the original file is stored.</p> <code>None</code> Source code in <code>datarec/io/readers.py</code> <pre><code>def fill_rawdata(data, user=None, item=None, rating=None, timestamp=None, path=None):\n    \"\"\"\n    Create a RawData object from raw data and assign column names to RawData object attributes.\n\n    Args:\n        data (pd.DataFrame): Data to create RawData object from.\n        user (str): Column name for user field.\n        item (str): Column name for item field.\n        rating (str): Column name for rating field.\n        timestamp (str): Column name for timestamp field.\n        path (str): Path where the original file is stored.\n\n\n    \"\"\"\n    rawdata = RawData(data)\n\n    # set columns\n    rawdata.user = user\n    rawdata.item = item\n    rawdata.rating = rating\n    rawdata.timestamp = timestamp\n\n    # set file path\n    rawdata.path = path\n</code></pre>"},{"location":"documentation/io/#datarec.io.readers.read_json","title":"<code>read_json(filepath, user_field=None, item_field=None, rating_field=None, timestamp_field=None, lines=True)</code>","text":"<p>Reads a JSON file and returns it as a RawData object. Args:     filepath (str): path to JSON file.     user_field (str): JSON key for user field.     item_field (str): JSON key for item field.     rating_field (str): JSON key for rating field.     timestamp_field (str): JSON key for timestamp field.     lines (bool): Read the file as a JSON object per line.</p> <p>Returns:</p> Type Description <code>RawData</code> <p>RawData object</p> Source code in <code>datarec/io/readers.py</code> <pre><code>def read_json(filepath, user_field=None, item_field=None, rating_field=None, timestamp_field=None, lines=True):\n    \"\"\"\n    Reads a JSON file and returns it as a RawData object.\n    Args:\n        filepath (str): path to JSON file.\n        user_field (str): JSON key for user field.\n        item_field (str): JSON key for item field.\n        rating_field (str): JSON key for rating field.\n        timestamp_field (str): JSON key for timestamp field.\n        lines (bool): Read the file as a JSON object per line.\n\n    Returns:\n        (RawData): RawData object\n\n    \"\"\"\n    # check that file exists\n    if os.path.exists(filepath) is False:\n        raise FileNotFoundError\n\n    std_fields = [user_field, item_field, rating_field, timestamp_field]\n    assigned_fields = [c for c in std_fields if c is not None]\n\n    # at least one column given check\n    if len(assigned_fields) == 0:\n        raise AttributeError('Fields are missing. At least one should be assigned')\n\n    # read data\n    data = pd.read_json(filepath, lines=lines)\n\n    # check that columns are aligned\n    for c in assigned_fields:\n        if c not in data.columns:\n            raise ValueError(f'Field {c} not found in the dataset. Please, check the value and retry')\n\n    rawdata = RawData(data[assigned_fields])\n\n    # set columns\n    rawdata.user = user_field if user_field is not None else None\n    rawdata.item = item_field if item_field is not None else None\n    rawdata.rating = rating_field if rating_field is not None else None\n    rawdata.timestamp = timestamp_field if timestamp_field is not None else None\n    return rawdata\n</code></pre>"},{"location":"documentation/io/#datarec.io.readers.read_tabular","title":"<code>read_tabular(filepath, sep, user_col=None, item_col=None, rating_col=None, timestamp_col=None, header='infer', skiprows=0)</code>","text":"<p>Reads a tabular data file and returns it as a pandas DataFrame. Args:     filepath (str): Path to tabular data file.     sep (str): Separator to use.     user_col (str): Column name for user field.     item_col (str): Column name for item field.     rating_col (str): Column name for rating field.     timestamp_col (str): Column name for timestamp field.     header (nt, Sequence of int, \u2018infer\u2019 or None): Row number(s) containing column labels and marking the start of the data (zero-indexed). Default behavior is to infer the column names.     skiprows (int, list of int or Callable): Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file.</p> <p>Returns:</p> Type Description <code>RawData</code> <p>RawData object.</p> Source code in <code>datarec/io/readers.py</code> <pre><code>def read_tabular(filepath: str, sep: str, user_col=None, item_col=None, rating_col=None, timestamp_col=None,\n                 header=\"infer\", skiprows=0):\n    \"\"\"\n    Reads a tabular data file and returns it as a pandas DataFrame.\n    Args:\n        filepath (str): Path to tabular data file.\n        sep (str): Separator to use.\n        user_col (str): Column name for user field.\n        item_col (str): Column name for item field.\n        rating_col (str): Column name for rating field.\n        timestamp_col (str): Column name for timestamp field.\n        header (nt, Sequence of int, \u2018infer\u2019 or None): Row number(s) containing column labels and marking the start of the data (zero-indexed). Default behavior is to infer the column names.\n        skiprows (int, list of int or Callable): Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file.\n\n    Returns:\n        (RawData): RawData object.\n\n    \"\"\"\n    # check that file exists\n    if os.path.exists(filepath) is False:\n        raise FileNotFoundError\n\n    std_columns = [user_col, item_col, rating_col, timestamp_col]\n    assigned_columns = [c for c in std_columns if c is not None]\n\n    # at least one column given check\n    if len(assigned_columns) == 0:\n        raise AttributeError('Columns are missing. At least one should be assigned')\n\n    # read data\n    data = pd.read_table(filepath_or_buffer=filepath, sep=sep, header=header, skiprows=skiprows, engine='python')\n\n    # check that columns are aligned\n    for c in assigned_columns:\n        if c not in data.columns:\n            raise ValueError(f'Column {c} not found in the dataset. Please, check the value and retry')\n\n    rawdata = RawData(data=data[assigned_columns])\n\n    # set columns\n    rawdata.user = user_col if (user_col is not None) else None\n    rawdata.item = item_col if item_col is not None else None\n    rawdata.rating = rating_col if rating_col is not None else None\n    rawdata.timestamp = timestamp_col if timestamp_col is not None else None\n\n    return rawdata\n</code></pre>"},{"location":"documentation/io/#datarec.io.readers.read_inline","title":"<code>read_inline(filepath, cols=None, user_col='user', item_col='item', col_sep=',', history_sep=';')</code>","text":"<p>Read a CSV file and return a RawData object. Args:     filepath (str): Path to CVS file.     cols (list[str]): List of column names.     user_col (str): Column name for user field.:     item_col (str): Column name for item field.     col_sep (str): Separator to use.     history_sep (str): Separator for multiple items.</p> <p>Returns:</p> Type Description <code>RawData</code> <p>RawData object.</p> Source code in <code>datarec/io/readers.py</code> <pre><code>def read_inline(filepath: str, cols=None, user_col='user', item_col='item', col_sep=',', history_sep=';'):\n    \"\"\"\n    Read a CSV file and return a RawData object.\n    Args:\n        filepath (str): Path to CVS file.\n        cols (list[str]): List of column names.\n        user_col (str): Column name for user field.:\n        item_col (str): Column name for item field.\n        col_sep (str): Separator to use.\n        history_sep (str): Separator for multiple items.\n\n    Returns:\n        (RawData): RawData object.\n\n    \"\"\"\n    if cols is None:\n        cols = ['user', 'item']\n    assert os.path.exists(filepath), f'File not found at {filepath}'\n    to_drop_cols = [c for c in cols if c not in (user_col, item_col)]\n\n    data = pd.read_csv(filepath, sep=col_sep, header=None, names=cols)\n    data = data.dropna(subset=['user', 'item'])\n    data = data.drop(columns=to_drop_cols)\n    data[item_col] = data[item_col].apply(lambda x: [item.strip() for item in x.split(history_sep)])\n    data = data.explode('item')\n    data = data.reset_index(drop=True)\n    return RawData(data, user='user', item='item')\n</code></pre>"},{"location":"documentation/io/#datarec.io.readers.read_inline_chunk","title":"<code>read_inline_chunk(filepath, cols=None, user_col='user', item_col='item')</code>","text":"<p>Read a CSV file a chunk of rows at a time and return a RawData object. Args:     filepath (str): Path to CSV file.     cols (list[str]): List of column names.     user_col (str): Column name for user field.     item_col (str): Column name for item field.</p> <p>Returns:</p> Type Description <code>RawData</code> <p>RawData object.</p> Source code in <code>datarec/io/readers.py</code> <pre><code>def read_inline_chunk(filepath: str, cols=None, user_col='user', item_col='item'):\n    \"\"\"\n    Read a CSV file a chunk of rows at a time and return a RawData object.\n    Args:\n        filepath (str): Path to CSV file.\n        cols (list[str]): List of column names.\n        user_col (str): Column name for user field.\n        item_col (str): Column name for item field.\n\n    Returns:\n        (RawData): RawData object.\n\n    \"\"\"\n    if cols is None:\n        cols = ['user', 'item']\n    assert os.path.exists(filepath), f'File not found at {filepath}'\n    to_drop_cols = [c for c in cols if c not in (user_col, item_col)]\n\n    data_chunks = pd.read_csv(filepath, sep=',', header=None, names=cols, chunksize=100000)\n    data = None\n\n    for chunk in tqdm.tqdm(data_chunks):\n        chunk = chunk.drop(columns=to_drop_cols)\n        chunk[item_col] = chunk[item_col].apply(lambda x: [item.strip() for item in x.split(';')])\n        chunk = chunk.explode('item')\n        if data is not None:\n            data = pd.concat([data, chunk])\n        else:\n            data = chunk\n\n    data = data.reset_index(drop=True)\n    return RawData(data, user='user', item='item')\n</code></pre>"},{"location":"documentation/io/#datarec.io.writers.write_tabular","title":"<code>write_tabular(rawdata, path, sep='\\t', header=True, decimal='.', user=True, item=True, rating=True, timestamp=True, verbose=True)</code>","text":"<p>Write a RawData dataset to a CSV/TSV file.</p> <p>Parameters:</p> Name Type Description Default <code>rawdata</code> <code>RawData</code> <p>RawData instance.</p> required <code>path</code> <code>str</code> <p>Path to the CSV/TSV file.</p> required <code>sep</code> <code>str</code> <p>Separator to use.</p> <code>'\\t'</code> <code>header</code> <code>bool or list[str]</code> <p>Write out the column names. If a list of strings is given it is assumed to be aliases for the column names.</p> <code>True</code> <code>decimal</code> <code>str</code> <p>Character recognized as decimal separator.</p> <code>'.'</code> <code>user</code> <code>bool</code> <p>Whether to write the user information. If True, the user information will be written in the file.</p> <code>True</code> <code>item</code> <code>bool</code> <p>Whether to write the item information. If True, the item information will be written in the file.</p> <code>True</code> <code>rating</code> <code>bool</code> <p>Whether to write the rating information. If True, the rating information will be written in the file.</p> <code>True</code> <code>timestamp</code> <code>bool</code> <p>Whether to write the timestamp information. If True, the timestamp information will be written in the file.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Print out additional information.</p> <code>True</code> <p>Returns:</p> Type Description <p>(CSV/TSV file)</p> Source code in <code>datarec/io/writers.py</code> <pre><code>def write_tabular(rawdata: RawData, path, sep='\\t', header=True, decimal='.',\n                  user=True, item=True, rating=True, timestamp=True, verbose=True):\n    \"\"\"\n    Write a RawData dataset to a CSV/TSV file.\n\n    Args:\n        rawdata (RawData): RawData instance.\n        path (str): Path to the CSV/TSV file.\n        sep (str): Separator to use.\n        header (bool or list[str]): Write out the column names. If a list of strings is given it is assumed to be aliases for the column names.\n        decimal (str): Character recognized as decimal separator.\n        user (bool): Whether to write the user information. If True, the user information will be written in the file.\n        item (bool): Whether to write the item information. If True, the item information will be written in the file.\n        rating (bool): Whether to write the rating information. If True, the rating information will be written in the file.\n        timestamp (bool): Whether to write the timestamp information. If True, the timestamp information will be written in the file.\n        verbose (bool): Print out additional information.\n\n    Returns:\n        (CSV/TSV file)\n\n    \"\"\"\n    cols = []\n    if user:\n        if rawdata.user:\n            cols.append(rawdata.user)\n        else:\n            raise ValueError('User column not defined in the DataRec.')\n    if item:\n        if rawdata.item:\n            cols.append(rawdata.item)\n        else:\n            raise ValueError('Item column not defined in the DataRec.')\n    if rating:\n        if rawdata.rating:\n            cols.append(rawdata.rating)\n        else:\n            raise ValueError('Rating column not defined in the DataRec.')\n    if timestamp:\n        if rawdata.timestamp:\n            cols.append(rawdata.timestamp)\n        else:\n            raise ValueError('Timestamp column not defined in the DataRec.')\n\n    data: pd.DataFrame = rawdata.data[cols]\n\n    if sep in ACCEPTED_TAB_DELIMITERS:\n        if sep == \"::\":\n            file = data.to_csv(sep='*', header=header, index=False, decimal=decimal)\n            file.replace('*', '::')\n            with open(file, 'w') as f:\n                f.write(file)\n        else:\n            data.to_csv(path, sep=sep, header=header, index=False, decimal=decimal)\n            if verbose:\n                print(f'A dataset has been stored at \\'{path}\\'')\n    else:\n        raise ValueError\n</code></pre>"},{"location":"documentation/io/#datarec.io.writers.write_json","title":"<code>write_json(rawdata, path, user=True, item=True, rating=True, timestamp=True)</code>","text":"<p>Write a RawData dataset to a JSON file. Args:     rawdata (RawData): RawData instance.     path (str): Path to the JSON file.     user (bool): Whether to write the user information. If True, the user information will be written in the file.     item (bool): Whether to write the item information. If True, the item information will be written in the file.     rating (bool): Whether to write the rating information. If True, the rating information will be written in the file.     timestamp (bool): Whether to write the timestamp information. If True, the timestamp information will be written in the file.</p> <p>Returns:</p> Type Description <p>(JSON file)</p> Source code in <code>datarec/io/writers.py</code> <pre><code>def write_json(rawdata: RawData, path, user=True, item=True, rating=True, timestamp=True):\n    \"\"\"\n    Write a RawData dataset to a JSON file.\n    Args:\n        rawdata (RawData): RawData instance.\n        path (str): Path to the JSON file.\n        user (bool): Whether to write the user information. If True, the user information will be written in the file.\n        item (bool): Whether to write the item information. If True, the item information will be written in the file.\n        rating (bool): Whether to write the rating information. If True, the rating information will be written in the file.\n        timestamp (bool): Whether to write the timestamp information. If True, the timestamp information will be written in the file.\n\n    Returns:\n        (JSON file)\n\n    \"\"\"\n\n    cols = []\n    if user:\n        cols.append(rawdata.user)\n    if item:\n        cols.append(rawdata.item)\n    if rating:\n        cols.append(rawdata.rating)\n    if timestamp:\n        cols.append(rawdata.timestamp)\n\n    data: pd.DataFrame = rawdata.data[cols]\n\n    data.to_json(path, orient='records', lines=True)\n</code></pre>"},{"location":"documentation/io/#datarec.io.paths.get_cache_dir","title":"<code>get_cache_dir(app_name='datarec', app_author='sisinflab')</code>","text":"<p>Returns the appropriate cache directory for the library, creating it if it doesn't exist. Respects the DATAREC_CACHE_DIR environment variable if set.</p> <p>Returns:</p> Name Type Description <code>Path</code> <p>The absolute path to the cache directory.</p> Source code in <code>datarec/io/paths.py</code> <pre><code>def get_cache_dir(app_name=\"datarec\", app_author=\"sisinflab\"):\n    \"\"\"\n    Returns the appropriate cache directory for the library, creating it if it doesn't exist.\n    Respects the DATAREC_CACHE_DIR environment variable if set.\n\n    Returns:\n        Path: The absolute path to the cache directory.\n    \"\"\"\n    env_override = os.getenv(\"DATAREC_CACHE_DIR\")\n    path = Path(env_override) if env_override else Path(user_cache_dir(app_name, app_author))\n    path.mkdir(parents=True, exist_ok=True)\n    return path\n</code></pre>"},{"location":"documentation/io/#datarec.io.paths.dataset_directory","title":"<code>dataset_directory(dataset_name, must_exist=False)</code>","text":"<p>Given the dataset name returns the dataset directory Args:     dataset_name (str): name of the dataset     must_exist (bool): flag for forcing to check if the folder exists</p> <p>Returns:</p> Type Description <code>str</code> <p>the path of the directory containing the dataset data</p> Source code in <code>datarec/io/paths.py</code> <pre><code>def dataset_directory(dataset_name: str, must_exist=False) -&gt; str:\n    \"\"\"\n    Given the dataset name returns the dataset directory\n    Args:\n        dataset_name (str): name of the dataset\n        must_exist (bool): flag for forcing to check if the folder exists\n\n    Returns:\n        (str): the path of the directory containing the dataset data\n    \"\"\"\n    dataset_dir = os.path.join(DATA_DIR, dataset_name)\n    if must_exist and not os.path.exists(dataset_dir):\n        raise FileNotFoundError(f'Directory at {dataset_dir} not found. Please, check that dataset directory exists')\n    return os.path.abspath(dataset_dir)\n</code></pre>"},{"location":"documentation/io/#datarec.io.paths.dataset_raw_directory","title":"<code>dataset_raw_directory(dataset_name)</code>","text":"<p>Given the dataset name returns the directory containing the raw data of the dataset Args:     dataset_name (str): name of the dataset</p> <p>Returns:</p> Type Description <code>str</code> <p>the path of the directory containing the raw data of the dataset</p> Source code in <code>datarec/io/paths.py</code> <pre><code>def dataset_raw_directory(dataset_name: str) -&gt; str:\n    \"\"\"\n    Given the dataset name returns the directory containing the raw data of the dataset\n    Args:\n        dataset_name (str): name of the dataset\n\n    Returns:\n        (str): the path of the directory containing the raw data of the dataset\n    \"\"\"\n    return os.path.join(dataset_directory(dataset_name), RAW_DATA_FOLDER)\n</code></pre>"},{"location":"documentation/io/#datarec.io.paths.dataset_processed_directory","title":"<code>dataset_processed_directory(dataset_name)</code>","text":"<p>Given the dataset name returns the directory containing the processed data of the dataset Args:     dataset_name (str): name of the dataset</p> <p>Returns:</p> Type Description <code>str</code> <p>the path of the directory containing the processed data of the dataset</p> Source code in <code>datarec/io/paths.py</code> <pre><code>def dataset_processed_directory(dataset_name: str) -&gt; str:\n    \"\"\"\n    Given the dataset name returns the directory containing the processed data of the dataset\n    Args:\n        dataset_name (str): name of the dataset\n\n    Returns:\n        (str): the path of the directory containing the processed data of the dataset\n    \"\"\"\n    return os.path.join(dataset_directory(dataset_name), PROCESSED_DATA_FOLDER)\n</code></pre>"},{"location":"documentation/io/#datarec.io.paths.dataset_filepath","title":"<code>dataset_filepath(dataset_name)</code>","text":"<p>Given the dataset name returns the path of the dataset data Args:     dataset_name (str): name of the dataset</p> <p>Returns:</p> Type Description <code>str</code> <p>the path of the dataset data</p> Source code in <code>datarec/io/paths.py</code> <pre><code>def dataset_filepath(dataset_name: str) -&gt; str:\n    \"\"\"\n    Given the dataset name returns the path of the dataset data\n    Args:\n        dataset_name (str): name of the dataset\n\n    Returns:\n        (str): the path of the dataset data\n    \"\"\"\n    return os.path.join(dataset_directory(dataset_name), DATASET_NAME)\n</code></pre>"},{"location":"documentation/io/#framework-interoperability","title":"Framework Interoperability","text":"<p>This section covers the tools used to export <code>DataRec</code> datasets into formats compatible with other popular recommender systems libraries.</p>"},{"location":"documentation/io/#datarec.io.frameworks.exporter.FrameworkExporter","title":"<code>FrameworkExporter</code>","text":"<p>Exporter for converting RawData datasets to external recommender system frameworks.</p> <p>Provides methods to format a <code>RawData</code> object according to the expected schema of supported libraries (e.g., Cornac, RecBole).</p> Source code in <code>datarec/io/frameworks/exporter.py</code> <pre><code>class FrameworkExporter:\n    \"\"\"\n    Exporter for converting RawData datasets to external recommender system frameworks.\n\n    Provides methods to format a `RawData` object according to\n    the expected schema of supported libraries (e.g., Cornac, RecBole).\n\n    \"\"\"\n\n    def __init__(self, output_path, user=True, item=True, rating=True, timestamp=False):\n        \"\"\"\n        Initialize a FrameworkExporter object.\n        Args:\n            output_path (str): Path where to save the output file.\n            user (bool): Whether to write the user information. If True, the user information will be written in the file.\n            item (bool): Whether to write the item information. If True, the item information will be written in the file.\n            rating (bool): Whether to write the rating information. If True, the rating information will be written in the file.\n            timestamp (bool): Whether to write the timestamp information. If True, the timestamp information will be written in the file.\n        \"\"\"\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.path = output_path\n        self.user = user\n        self.item = item\n        self.rating = rating\n        self.timestamp = timestamp\n\n    def to_clayrs(self, data: RawData):\n        \"\"\"\n        Export to ClayRS format.\n        Args:\n            data (RawData): RawData object to convert to ClayRS format.\n        \"\"\"\n        write_tabular(rawdata=data, path=self.path, sep=',', header=False,\n                      user=self.user, item=self.item, rating=self.rating, timestamp=self.timestamp)\n\n        ClayRS(timestamp=self.timestamp, path=self.path).info()\n\n    def to_cornac(self, data: RawData):\n        \"\"\"\n        Export to Cornac format.\n        Args:\n            data (RawData): RawData object to convert to Cornac format.\n        \"\"\"\n        write_tabular(rawdata=data, path=self.path, sep='\\t', header=False,\n                      user=self.user, item=self.item, rating=self.rating, timestamp=self.timestamp)\n\n        Cornac(timestamp=self.timestamp, path=self.path).info()\n\n    def to_daisyrec(self, data: RawData):\n        \"\"\"\n        Export to DaisyRec format.\n        Args:\n            data (RawData): RawData object to convert to DaisyRec format.\n        \"\"\"\n        write_tabular(rawdata=data, path=self.path, sep='\\t', header=False,\n                      user=self.user, item=self.item, rating=self.rating, timestamp=self.timestamp)\n\n        DaisyRec(timestamp=self.timestamp, path=self.path).info()\n\n    def to_lenskit(self, data: RawData):\n        \"\"\"\n        Export to LensKit format.\n        Args:\n            data (RawData): RawData object to convert to LensKit format.\n        \"\"\"\n        data.data.rename(columns={data.user: \"user\", data.item: \"item\",\n                                  data.rating: \"rating\"}, inplace=True)\n        data.user = \"user\"\n        data.item = \"item\"\n        data.rating = \"rating\"\n\n        if self.timestamp:\n            data.data.rename(columns={data.timestamp: \"timestamp\"}, inplace=True)\n            data.timestamp = \"timestamp\"\n            data.rating = \"rating\"\n\n        write_tabular(rawdata=data, path=self.path, sep='\\t', header=False,\n                      user=self.user, item=self.item, rating=self.rating, timestamp=self.timestamp)\n\n        LensKit(timestamp=self.timestamp, path=self.path).info()\n\n    def to_recbole(self, data: RawData):\n        \"\"\"\n        Export to RecBole format.\n        Args:\n            data (RawData): RawData object to convert to RecBole format.\n        \"\"\"\n\n        data.data.rename(columns={data.user: \"user: token\", data.item: \"item: token\",\n                                  data.rating: \"rating: float\"}, inplace=True)\n        data.user = \"user: token\"\n        data.item = \"item: token\"\n        data.rating = \"rating: float\"\n\n        if self.timestamp:\n            data.data.rename(columns={data.timestamp: \"timestamp\"}, inplace=True)\n            data.timestamp = \"timestamp:float\"\n\n        frmk = RecBole(timestamp=self.timestamp, path=self.path)\n        frmk.info()\n\n        write_tabular(rawdata=data, path=frmk.path, sep='\\t', header=True,\n                      user=self.user, item=self.item, rating=self.rating, timestamp=self.timestamp)\n\n    def to_rechorus(self, train_data: RawData, test_data: RawData, val_data: RawData):\n        \"\"\"\n        Export to Rechus format.\n        Args:\n            train_data (RawData): Training data as RawData object to convert to Rechus format.\n            test_data (RawData): Test data as RawData object to convert to Rechus format.\n            val_data (RawData): Validation data as RawData object to convert to Rechus format.\n        \"\"\"\n        # user_id\titem_id\ttime\n        if self.rating:\n            print('Ratings will be interpreted as implicit interactions.')\n            self.rating = False\n\n        frmk = ReChorus(timestamp=self.timestamp, path=self.path)\n\n        for data, name in zip([train_data, test_data, val_data], ['train.csv', 'dev.csv', 'test.csv']):\n            data.data.rename(columns={data.user: \"user_id\", data.item: \"item_id\"}, inplace=True)\n            data.user = \"user_id\"\n            data.item = \"item_id\"\n\n            if self.timestamp:\n                data.data.rename(columns={data.timestamp: \"time\"}, inplace=True)\n                data.timestamp = \"time\"\n\n            path = os.path.join(frmk.directory, name)\n            write_tabular(rawdata=data, path=path, sep='\\t', header=True,\n                          user=self.user, item=self.item, rating=self.rating, timestamp=self.timestamp)\n\n        frmk.info()\n\n    def to_recpack(self, data: RawData):\n        \"\"\"\n        Export to RecPack format.\n        Args:\n            data (RawData): RawData object to convert to RecPack format.\n        \"\"\"\n\n        if self.rating:\n            print('Ratings will be interpreted as implicit interactions.')\n            self.rating = False\n\n        frmk = RecPack(timestamp=self.timestamp, path=self.path)\n\n        data.data.rename(columns={data.user: \"userId\", data.item: \"itemId\"}, inplace=True)\n        data.user = \"userId\"\n        data.item = \"itemId\"\n        if self.timestamp:\n            data.data.rename(columns={data.timestamp: \"timestamp\"}, inplace=True)\n            data.timestamp = \"timestamp\"\n\n        write_tabular(rawdata=data, path=frmk.file_path, sep='\\t', header=True,\n                      user=self.user, item=self.item, rating=self.rating, timestamp=self.timestamp)\n\n        frmk.info()\n\n    def to_recommenders(self, data: RawData):\n        \"\"\"\n        Export to Recommenders format.\n        Args:\n            data (RawData): RawData object to convert to Recommenders format.\n        \"\"\"\n\n        frmk = Recommenders(timestamp=self.timestamp, path=self.path)\n\n        data.data.rename(columns={data.user: \"user\", data.item: \"item\", data.rating: \"rating\"}, inplace=True)\n        data.user = \"item\"\n        data.item = \"rating\"\n        data.rating = 'rating'\n        if self.timestamp:\n            data.data.rename(columns={data.timestamp: \"timestamp\"}, inplace=True)\n            data.timestamp = \"timestamp\"\n\n        write_tabular(rawdata=data, path=frmk.file_path, sep='\\t', header=True,\n                      user=self.user, item=self.item, rating=self.rating, timestamp=self.timestamp)\n\n        frmk.info()\n\n    def to_elliot(self, train_data: DataRec, test_data: DataRec, val_data: DataRec):\n        \"\"\"\n        Export to Elliot format.\n        Args:\n            train_data (DataRec): Training data as DataRec object to convert to Elliot format.\n            test_data (DataRec): Test data as DataRec object to convert to Elliot format.\n            val_data (DataRec): Validation data as DataRec object to convert to Elliot format.\n        \"\"\"\n\n        frmk = Elliot(timestamp=self.timestamp, path=self.path)\n\n        for data, name in zip([train_data.to_rawdata(), test_data.to_rawdata(), val_data.to_rawdata()],\n                              [frmk.train_path, frmk.test_path, frmk.val_path]):\n            columns_order = [data.user, data.item, data.rating]\n            if self.timestamp:\n                columns_order.append(data.timestamp)\n\n            write_tabular(rawdata=data, path=name, sep='\\t', header=False,\n                          user=self.user, item=self.item, rating=self.rating, timestamp=self.timestamp)\n\n        frmk.info()\n        train_data.pipeline.add_step(\"export\", \"Elliot\", self.params)\n        test_data.pipeline.add_step(\"export\", \"Elliot\", self.params)\n        val_data.pipeline.add_step(\"export\", \"Elliot\", self.params)\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.exporter.FrameworkExporter.__init__","title":"<code>__init__(output_path, user=True, item=True, rating=True, timestamp=False)</code>","text":"<p>Initialize a FrameworkExporter object. Args:     output_path (str): Path where to save the output file.     user (bool): Whether to write the user information. If True, the user information will be written in the file.     item (bool): Whether to write the item information. If True, the item information will be written in the file.     rating (bool): Whether to write the rating information. If True, the rating information will be written in the file.     timestamp (bool): Whether to write the timestamp information. If True, the timestamp information will be written in the file.</p> Source code in <code>datarec/io/frameworks/exporter.py</code> <pre><code>def __init__(self, output_path, user=True, item=True, rating=True, timestamp=False):\n    \"\"\"\n    Initialize a FrameworkExporter object.\n    Args:\n        output_path (str): Path where to save the output file.\n        user (bool): Whether to write the user information. If True, the user information will be written in the file.\n        item (bool): Whether to write the item information. If True, the item information will be written in the file.\n        rating (bool): Whether to write the rating information. If True, the rating information will be written in the file.\n        timestamp (bool): Whether to write the timestamp information. If True, the timestamp information will be written in the file.\n    \"\"\"\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.path = output_path\n    self.user = user\n    self.item = item\n    self.rating = rating\n    self.timestamp = timestamp\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.exporter.FrameworkExporter.to_clayrs","title":"<code>to_clayrs(data)</code>","text":"<p>Export to ClayRS format. Args:     data (RawData): RawData object to convert to ClayRS format.</p> Source code in <code>datarec/io/frameworks/exporter.py</code> <pre><code>def to_clayrs(self, data: RawData):\n    \"\"\"\n    Export to ClayRS format.\n    Args:\n        data (RawData): RawData object to convert to ClayRS format.\n    \"\"\"\n    write_tabular(rawdata=data, path=self.path, sep=',', header=False,\n                  user=self.user, item=self.item, rating=self.rating, timestamp=self.timestamp)\n\n    ClayRS(timestamp=self.timestamp, path=self.path).info()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.exporter.FrameworkExporter.to_cornac","title":"<code>to_cornac(data)</code>","text":"<p>Export to Cornac format. Args:     data (RawData): RawData object to convert to Cornac format.</p> Source code in <code>datarec/io/frameworks/exporter.py</code> <pre><code>def to_cornac(self, data: RawData):\n    \"\"\"\n    Export to Cornac format.\n    Args:\n        data (RawData): RawData object to convert to Cornac format.\n    \"\"\"\n    write_tabular(rawdata=data, path=self.path, sep='\\t', header=False,\n                  user=self.user, item=self.item, rating=self.rating, timestamp=self.timestamp)\n\n    Cornac(timestamp=self.timestamp, path=self.path).info()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.exporter.FrameworkExporter.to_daisyrec","title":"<code>to_daisyrec(data)</code>","text":"<p>Export to DaisyRec format. Args:     data (RawData): RawData object to convert to DaisyRec format.</p> Source code in <code>datarec/io/frameworks/exporter.py</code> <pre><code>def to_daisyrec(self, data: RawData):\n    \"\"\"\n    Export to DaisyRec format.\n    Args:\n        data (RawData): RawData object to convert to DaisyRec format.\n    \"\"\"\n    write_tabular(rawdata=data, path=self.path, sep='\\t', header=False,\n                  user=self.user, item=self.item, rating=self.rating, timestamp=self.timestamp)\n\n    DaisyRec(timestamp=self.timestamp, path=self.path).info()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.exporter.FrameworkExporter.to_lenskit","title":"<code>to_lenskit(data)</code>","text":"<p>Export to LensKit format. Args:     data (RawData): RawData object to convert to LensKit format.</p> Source code in <code>datarec/io/frameworks/exporter.py</code> <pre><code>def to_lenskit(self, data: RawData):\n    \"\"\"\n    Export to LensKit format.\n    Args:\n        data (RawData): RawData object to convert to LensKit format.\n    \"\"\"\n    data.data.rename(columns={data.user: \"user\", data.item: \"item\",\n                              data.rating: \"rating\"}, inplace=True)\n    data.user = \"user\"\n    data.item = \"item\"\n    data.rating = \"rating\"\n\n    if self.timestamp:\n        data.data.rename(columns={data.timestamp: \"timestamp\"}, inplace=True)\n        data.timestamp = \"timestamp\"\n        data.rating = \"rating\"\n\n    write_tabular(rawdata=data, path=self.path, sep='\\t', header=False,\n                  user=self.user, item=self.item, rating=self.rating, timestamp=self.timestamp)\n\n    LensKit(timestamp=self.timestamp, path=self.path).info()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.exporter.FrameworkExporter.to_recbole","title":"<code>to_recbole(data)</code>","text":"<p>Export to RecBole format. Args:     data (RawData): RawData object to convert to RecBole format.</p> Source code in <code>datarec/io/frameworks/exporter.py</code> <pre><code>def to_recbole(self, data: RawData):\n    \"\"\"\n    Export to RecBole format.\n    Args:\n        data (RawData): RawData object to convert to RecBole format.\n    \"\"\"\n\n    data.data.rename(columns={data.user: \"user: token\", data.item: \"item: token\",\n                              data.rating: \"rating: float\"}, inplace=True)\n    data.user = \"user: token\"\n    data.item = \"item: token\"\n    data.rating = \"rating: float\"\n\n    if self.timestamp:\n        data.data.rename(columns={data.timestamp: \"timestamp\"}, inplace=True)\n        data.timestamp = \"timestamp:float\"\n\n    frmk = RecBole(timestamp=self.timestamp, path=self.path)\n    frmk.info()\n\n    write_tabular(rawdata=data, path=frmk.path, sep='\\t', header=True,\n                  user=self.user, item=self.item, rating=self.rating, timestamp=self.timestamp)\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.exporter.FrameworkExporter.to_rechorus","title":"<code>to_rechorus(train_data, test_data, val_data)</code>","text":"<p>Export to Rechus format. Args:     train_data (RawData): Training data as RawData object to convert to Rechus format.     test_data (RawData): Test data as RawData object to convert to Rechus format.     val_data (RawData): Validation data as RawData object to convert to Rechus format.</p> Source code in <code>datarec/io/frameworks/exporter.py</code> <pre><code>def to_rechorus(self, train_data: RawData, test_data: RawData, val_data: RawData):\n    \"\"\"\n    Export to Rechus format.\n    Args:\n        train_data (RawData): Training data as RawData object to convert to Rechus format.\n        test_data (RawData): Test data as RawData object to convert to Rechus format.\n        val_data (RawData): Validation data as RawData object to convert to Rechus format.\n    \"\"\"\n    # user_id\titem_id\ttime\n    if self.rating:\n        print('Ratings will be interpreted as implicit interactions.')\n        self.rating = False\n\n    frmk = ReChorus(timestamp=self.timestamp, path=self.path)\n\n    for data, name in zip([train_data, test_data, val_data], ['train.csv', 'dev.csv', 'test.csv']):\n        data.data.rename(columns={data.user: \"user_id\", data.item: \"item_id\"}, inplace=True)\n        data.user = \"user_id\"\n        data.item = \"item_id\"\n\n        if self.timestamp:\n            data.data.rename(columns={data.timestamp: \"time\"}, inplace=True)\n            data.timestamp = \"time\"\n\n        path = os.path.join(frmk.directory, name)\n        write_tabular(rawdata=data, path=path, sep='\\t', header=True,\n                      user=self.user, item=self.item, rating=self.rating, timestamp=self.timestamp)\n\n    frmk.info()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.exporter.FrameworkExporter.to_recpack","title":"<code>to_recpack(data)</code>","text":"<p>Export to RecPack format. Args:     data (RawData): RawData object to convert to RecPack format.</p> Source code in <code>datarec/io/frameworks/exporter.py</code> <pre><code>def to_recpack(self, data: RawData):\n    \"\"\"\n    Export to RecPack format.\n    Args:\n        data (RawData): RawData object to convert to RecPack format.\n    \"\"\"\n\n    if self.rating:\n        print('Ratings will be interpreted as implicit interactions.')\n        self.rating = False\n\n    frmk = RecPack(timestamp=self.timestamp, path=self.path)\n\n    data.data.rename(columns={data.user: \"userId\", data.item: \"itemId\"}, inplace=True)\n    data.user = \"userId\"\n    data.item = \"itemId\"\n    if self.timestamp:\n        data.data.rename(columns={data.timestamp: \"timestamp\"}, inplace=True)\n        data.timestamp = \"timestamp\"\n\n    write_tabular(rawdata=data, path=frmk.file_path, sep='\\t', header=True,\n                  user=self.user, item=self.item, rating=self.rating, timestamp=self.timestamp)\n\n    frmk.info()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.exporter.FrameworkExporter.to_recommenders","title":"<code>to_recommenders(data)</code>","text":"<p>Export to Recommenders format. Args:     data (RawData): RawData object to convert to Recommenders format.</p> Source code in <code>datarec/io/frameworks/exporter.py</code> <pre><code>def to_recommenders(self, data: RawData):\n    \"\"\"\n    Export to Recommenders format.\n    Args:\n        data (RawData): RawData object to convert to Recommenders format.\n    \"\"\"\n\n    frmk = Recommenders(timestamp=self.timestamp, path=self.path)\n\n    data.data.rename(columns={data.user: \"user\", data.item: \"item\", data.rating: \"rating\"}, inplace=True)\n    data.user = \"item\"\n    data.item = \"rating\"\n    data.rating = 'rating'\n    if self.timestamp:\n        data.data.rename(columns={data.timestamp: \"timestamp\"}, inplace=True)\n        data.timestamp = \"timestamp\"\n\n    write_tabular(rawdata=data, path=frmk.file_path, sep='\\t', header=True,\n                  user=self.user, item=self.item, rating=self.rating, timestamp=self.timestamp)\n\n    frmk.info()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.exporter.FrameworkExporter.to_elliot","title":"<code>to_elliot(train_data, test_data, val_data)</code>","text":"<p>Export to Elliot format. Args:     train_data (DataRec): Training data as DataRec object to convert to Elliot format.     test_data (DataRec): Test data as DataRec object to convert to Elliot format.     val_data (DataRec): Validation data as DataRec object to convert to Elliot format.</p> Source code in <code>datarec/io/frameworks/exporter.py</code> <pre><code>def to_elliot(self, train_data: DataRec, test_data: DataRec, val_data: DataRec):\n    \"\"\"\n    Export to Elliot format.\n    Args:\n        train_data (DataRec): Training data as DataRec object to convert to Elliot format.\n        test_data (DataRec): Test data as DataRec object to convert to Elliot format.\n        val_data (DataRec): Validation data as DataRec object to convert to Elliot format.\n    \"\"\"\n\n    frmk = Elliot(timestamp=self.timestamp, path=self.path)\n\n    for data, name in zip([train_data.to_rawdata(), test_data.to_rawdata(), val_data.to_rawdata()],\n                          [frmk.train_path, frmk.test_path, frmk.val_path]):\n        columns_order = [data.user, data.item, data.rating]\n        if self.timestamp:\n            columns_order.append(data.timestamp)\n\n        write_tabular(rawdata=data, path=name, sep='\\t', header=False,\n                      user=self.user, item=self.item, rating=self.rating, timestamp=self.timestamp)\n\n    frmk.info()\n    train_data.pipeline.add_step(\"export\", \"Elliot\", self.params)\n    test_data.pipeline.add_step(\"export\", \"Elliot\", self.params)\n    val_data.pipeline.add_step(\"export\", \"Elliot\", self.params)\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.manager.Framework","title":"<code>Framework</code>","text":"<p>Base class for all framework exporters.</p> Source code in <code>datarec/io/frameworks/manager.py</code> <pre><code>class Framework:\n    \"\"\"\n    Base class for all framework exporters.\n    \"\"\"\n    FRAMEWORK_NAME = None\n\n    PAPER = None\n\n    DOI = None\n\n    CITATION = None\n\n    CODE = None\n\n    REPOSITORY = None\n\n    DOC = None\n\n    def info_code(self):\n        \"\"\"\n        Print example code for integrating this framework with DataRec.\n        \"\"\"\n        print(f\"How to use {self.FRAMEWORK_NAME} with DataRec:\\n\" + self.CODE)\n\n    def info(self):\n        \"\"\"\n        Print citation information for the framework including: paper name, DOI and bibtex citation.\n        Print additional information such as: example code for integrating this framework with DataRec,\n        repository URL and framework documentation URL.\n        Returns:\n\n        \"\"\"\n        if self.FRAMEWORK_NAME is None:\n            raise AttributeError\n\n        print(f\"If you are going to use {self.FRAMEWORK_NAME} don't forget to cite the paper!\")\n\n        if self.PAPER:\n            print(f'Paper: \\'{self.PAPER}\\'')\n        if self.DOI:\n            print(f'DOI: {self.DOI}')\n        if self.CITATION:\n            print(f'Bib text from dblp.org:\\n {self.CITATION}')\n\n        if self.CODE:\n            print(\n                '\\n================================================ CODE EXAMPLE ================================================\\n')\n            self.info_code()\n            print(\n                '==============================================================================================================\\n')\n\n        if self.REPOSITORY:\n            print(f'For more information check {self.FRAMEWORK_NAME} repository: \\'{self.REPOSITORY}\\'')\n\n        if self.DOC:\n            print(f'More documentation on how to use {self.FRAMEWORK_NAME} at \\'{self.DOC}\\'')\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.manager.Framework.info_code","title":"<code>info_code()</code>","text":"<p>Print example code for integrating this framework with DataRec.</p> Source code in <code>datarec/io/frameworks/manager.py</code> <pre><code>def info_code(self):\n    \"\"\"\n    Print example code for integrating this framework with DataRec.\n    \"\"\"\n    print(f\"How to use {self.FRAMEWORK_NAME} with DataRec:\\n\" + self.CODE)\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.manager.Framework.info","title":"<code>info()</code>","text":"<p>Print citation information for the framework including: paper name, DOI and bibtex citation. Print additional information such as: example code for integrating this framework with DataRec, repository URL and framework documentation URL. Returns:</p> Source code in <code>datarec/io/frameworks/manager.py</code> <pre><code>def info(self):\n    \"\"\"\n    Print citation information for the framework including: paper name, DOI and bibtex citation.\n    Print additional information such as: example code for integrating this framework with DataRec,\n    repository URL and framework documentation URL.\n    Returns:\n\n    \"\"\"\n    if self.FRAMEWORK_NAME is None:\n        raise AttributeError\n\n    print(f\"If you are going to use {self.FRAMEWORK_NAME} don't forget to cite the paper!\")\n\n    if self.PAPER:\n        print(f'Paper: \\'{self.PAPER}\\'')\n    if self.DOI:\n        print(f'DOI: {self.DOI}')\n    if self.CITATION:\n        print(f'Bib text from dblp.org:\\n {self.CITATION}')\n\n    if self.CODE:\n        print(\n            '\\n================================================ CODE EXAMPLE ================================================\\n')\n        self.info_code()\n        print(\n            '==============================================================================================================\\n')\n\n    if self.REPOSITORY:\n        print(f'For more information check {self.FRAMEWORK_NAME} repository: \\'{self.REPOSITORY}\\'')\n\n    if self.DOC:\n        print(f'More documentation on how to use {self.FRAMEWORK_NAME} at \\'{self.DOC}\\'')\n</code></pre>"},{"location":"documentation/io/#clayrs","title":"ClayRS","text":""},{"location":"documentation/io/#datarec.io.frameworks.clayrs.clayrs.ClayRS","title":"<code>ClayRS</code>","text":"<p>               Bases: <code>Framework</code></p> <p>ClayRS framework adapter.</p> <p>Provide metadata, citation, and usage examples for ClayRS framework.</p> Source code in <code>datarec/io/frameworks/clayrs/clayrs.py</code> <pre><code>class ClayRS(Framework):\n    \"\"\"\n    ClayRS framework adapter.\n\n    Provide metadata, citation, and usage examples for ClayRS framework.\n    \"\"\"\n\n    def __init__(self, timestamp, path):\n        \"\"\"\n        Initialize ClayRS adapter.\n        Args:\n            timestamp (bool): Whether timestamps are included.\n            path (str): Path where the ClayRS-compatible dataset is stored.\n        \"\"\"\n        self.timestamp = timestamp\n        self.path = path\n\n    FRAMEWORK_NAME = 'ClayRS'\n\n    REPOSITORY = 'https://github.com/swapUniba/ClayRS/tree/master'\n\n    PAPER = \"\"\"ClayRS: An end-to-end framework for reproducible knowledge-aware recommender systems\"\"\"\n\n    DOI = \"https://doi.org/10.1016/j.is.2023.102273\"\n\n    CITATION = \"\"\"\n            @article{DBLP:journals/is/LopsPMSS23,\n              author       = {Pasquale Lops and\n                              Marco Polignano and\n                              Cataldo Musto and\n                              Antonio Silletti and\n                              Giovanni Semeraro},\n              title        = {ClayRS: An end-to-end framework for reproducible knowledge-aware recommender\n                              systems},\n              journal      = {Inf. Syst.},\n              volume       = {119},\n              pages        = {102273},\n              year         = {2023},\n              url          = {https://doi.org/10.1016/j.is.2023.102273},\n              doi          = {10.1016/J.IS.2023.102273},\n              timestamp    = {Mon, 05 Feb 2024 20:19:36 +0100},\n              biburl       = {https://dblp.org/rec/journals/is/LopsPMSS23.bib},\n              bibsource    = {dblp computer science bibliography, https://dblp.org}\n            }\"\"\"\n\n    CODE = \"\"\"\n    from clayrs import content_analyzer \n\n    ratings = content_analyzer.Ratings(content_analyzer.CSVFile(YOUR_PATH_HERE), timestamp_column=3)\n    \"\"\"\n\n    DOC = 'https://swapuniba.github.io/ClayRS/'\n\n    def info_code(self):\n        \"\"\"\n        Provide the code to use in ClayRS to run experiments.\n        \"\"\"\n        if self.timestamp:\n            self.CODE = \"\"\"\n    from clayrs import content_analyzer \n\n    ratings = content_analyzer.Ratings(content_analyzer.CSVFile('{path}'), timestamp_column=3)\n    \"\"\".format(path=self.path)\n        else:\n            self.CODE = \"\"\"\n    from clayrs import content_analyzer \n\n    ratings = content_analyzer.Ratings(content_analyzer.CSVFile('{path}'))\n    \"\"\".format(path=self.path)\n        super().info_code()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.clayrs.clayrs.ClayRS.__init__","title":"<code>__init__(timestamp, path)</code>","text":"<p>Initialize ClayRS adapter. Args:     timestamp (bool): Whether timestamps are included.     path (str): Path where the ClayRS-compatible dataset is stored.</p> Source code in <code>datarec/io/frameworks/clayrs/clayrs.py</code> <pre><code>def __init__(self, timestamp, path):\n    \"\"\"\n    Initialize ClayRS adapter.\n    Args:\n        timestamp (bool): Whether timestamps are included.\n        path (str): Path where the ClayRS-compatible dataset is stored.\n    \"\"\"\n    self.timestamp = timestamp\n    self.path = path\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.clayrs.clayrs.ClayRS.info_code","title":"<code>info_code()</code>","text":"<p>Provide the code to use in ClayRS to run experiments.</p> Source code in <code>datarec/io/frameworks/clayrs/clayrs.py</code> <pre><code>def info_code(self):\n    \"\"\"\n    Provide the code to use in ClayRS to run experiments.\n    \"\"\"\n    if self.timestamp:\n        self.CODE = \"\"\"\nfrom clayrs import content_analyzer \n\nratings = content_analyzer.Ratings(content_analyzer.CSVFile('{path}'), timestamp_column=3)\n\"\"\".format(path=self.path)\n    else:\n        self.CODE = \"\"\"\nfrom clayrs import content_analyzer \n\nratings = content_analyzer.Ratings(content_analyzer.CSVFile('{path}'))\n\"\"\".format(path=self.path)\n    super().info_code()\n</code></pre>"},{"location":"documentation/io/#cornac","title":"Cornac","text":""},{"location":"documentation/io/#datarec.io.frameworks.cornac.cornac.Cornac","title":"<code>Cornac</code>","text":"<p>               Bases: <code>Framework</code></p> <p>Cornac framework adapter.</p> <p>Provide metadata, citation, and usage examples for Cornac framework.</p> Source code in <code>datarec/io/frameworks/cornac/cornac.py</code> <pre><code>class Cornac(Framework):\n    \"\"\"\n    Cornac framework adapter.\n\n    Provide metadata, citation, and usage examples for Cornac framework.\n    \"\"\"\n\n    def __init__(self, timestamp, path):\n        \"\"\"\n        Initialize Cornac adapter.\n        Args:\n            timestamp (bool): Whether timestamps are included.\n            path (str): Path where the Cornac-compatible dataset is stored.\n        \"\"\"\n        self.timestamp = timestamp\n        self.path = path\n\n    FRAMEWORK_NAME = 'Cornac'\n\n    REPOSITORY = 'https://github.com/PreferredAI/cornac/tree/master'\n\n    PAPER = \"\"\"Cornac: A Comparative Framework for Multimodal Recommender Systems\"\"\"\n\n    DOI = None\n\n    CITATION = \"\"\"\n            @article{DBLP:journals/jmlr/SalahTL20,\n              author       = {Aghiles Salah and\n                              Quoc{-}Tuan Truong and\n                              Hady W. Lauw},\n              title        = {Cornac: {A} Comparative Framework for Multimodal Recommender Systems},\n              journal      = {J. Mach. Learn. Res.},\n              volume       = {21},\n              pages        = {95:1--95:5},\n              year         = {2020},\n              url          = {http://jmlr.org/papers/v21/19-805.html},\n              timestamp    = {Wed, 18 Nov 2020 15:58:12 +0100},\n              biburl       = {https://dblp.org/rec/journals/jmlr/SalahTL20.bib},\n              bibsource    = {dblp computer science bibliography, https://dblp.org}\n            }\"\"\"\n\n    CODE = \"\"\"\n        from cornac.data import Reader\n\n        reader = Reader()\n        train_data = reader.read(fpath='{path}', fmt=\"{frmt}\")\n    \"\"\"\n\n    DOC = 'https://cornac.preferred.ai/'\n\n    def info_code(self):\n        \"\"\"\n        Provide the code to use in Cornac to run experiments.\n        \"\"\"\n        if self.timestamp:\n            self.CODE = \"\"\"\n        from cornac.data import Reader\n\n        reader = Reader()\n        train_data = reader.read(fpath='{path}', fmt=\"{frmt}\")\n    \"\"\".format(path=self.path, frmt='UIRT')\n        else:\n            self.CODE = \"\"\"\n                from cornac.data import Reader\n\n                reader = Reader()\n                train_data = reader.read(fpath='{path}', fmt=\"{frmt}\")\n            \"\"\".format(path=self.path, frmt='UIR')\n        super().info_code()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.cornac.cornac.Cornac.__init__","title":"<code>__init__(timestamp, path)</code>","text":"<p>Initialize Cornac adapter. Args:     timestamp (bool): Whether timestamps are included.     path (str): Path where the Cornac-compatible dataset is stored.</p> Source code in <code>datarec/io/frameworks/cornac/cornac.py</code> <pre><code>def __init__(self, timestamp, path):\n    \"\"\"\n    Initialize Cornac adapter.\n    Args:\n        timestamp (bool): Whether timestamps are included.\n        path (str): Path where the Cornac-compatible dataset is stored.\n    \"\"\"\n    self.timestamp = timestamp\n    self.path = path\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.cornac.cornac.Cornac.info_code","title":"<code>info_code()</code>","text":"<p>Provide the code to use in Cornac to run experiments.</p> Source code in <code>datarec/io/frameworks/cornac/cornac.py</code> <pre><code>def info_code(self):\n    \"\"\"\n    Provide the code to use in Cornac to run experiments.\n    \"\"\"\n    if self.timestamp:\n        self.CODE = \"\"\"\n    from cornac.data import Reader\n\n    reader = Reader()\n    train_data = reader.read(fpath='{path}', fmt=\"{frmt}\")\n\"\"\".format(path=self.path, frmt='UIRT')\n    else:\n        self.CODE = \"\"\"\n            from cornac.data import Reader\n\n            reader = Reader()\n            train_data = reader.read(fpath='{path}', fmt=\"{frmt}\")\n        \"\"\".format(path=self.path, frmt='UIR')\n    super().info_code()\n</code></pre>"},{"location":"documentation/io/#daisyrec","title":"DaisyRec","text":""},{"location":"documentation/io/#datarec.io.frameworks.daisyrec.daisyrec.DaisyRec","title":"<code>DaisyRec</code>","text":"<p>               Bases: <code>Framework</code></p> <p>DaisyRec framework adapter.</p> <p>Provide metadata, citation, and usage examples for DaisyRec framework.</p> Source code in <code>datarec/io/frameworks/daisyrec/daisyrec.py</code> <pre><code>class DaisyRec(Framework):\n    \"\"\"\n    DaisyRec framework adapter.\n\n    Provide metadata, citation, and usage examples for DaisyRec framework.\n    \"\"\"\n\n    def __init__(self, timestamp, path):\n        \"\"\"\n        Initialize DaisyRec adapter.\n        Args:\n            timestamp (bool): Whether timestamps are included.\n            path (str): Path where the DaisyRec-compatible dataset is stored.\n        \"\"\"\n        self.timestamp = timestamp\n        self.path = path\n\n    FRAMEWORK_NAME = 'DaisyRec'\n\n    REPOSITORY = 'https://github.com/recsys-benchmark/DaisyRec-v2.0'\n\n    PAPER = \"\"\"DaisyRec 2.0: Benchmarking Recommendation for Rigorous Evaluation\"\"\"\n\n    DOI = \"https://doi.org/10.1109/TPAMI.2022.3231891\"\n\n    CITATION = \"\"\"\n            @inproceedings{DBLP:conf/recsys/SunY00Q0G20,\n              author       = {Zhu Sun and\n                              Di Yu and\n                              Hui Fang and\n                              Jie Yang and\n                              Xinghua Qu and\n                              Jie Zhang and\n                              Cong Geng},\n              editor       = {Rodrygo L. T. Santos and\n                              Leandro Balby Marinho and\n                              Elizabeth M. Daly and\n                              Li Chen and\n                              Kim Falk and\n                              Noam Koenigstein and\n                              Edleno Silva de Moura},\n              title        = {Are We Evaluating Rigorously? Benchmarking Recommendation for Reproducible\n                              Evaluation and Fair Comparison},\n              booktitle    = {RecSys 2020: Fourteenth {ACM} Conference on Recommender Systems, Virtual\n                              Event, Brazil, September 22-26, 2020},\n              pages        = {23--32},\n              publisher    = {{ACM}},\n              year         = {2020},\n              url          = {https://doi.org/10.1145/3383313.3412489},\n              doi          = {10.1145/3383313.3412489},\n              timestamp    = {Tue, 21 Mar 2023 20:57:01 +0100},\n              biburl       = {https://dblp.org/rec/conf/recsys/SunY00Q0G20.bib},\n              bibsource    = {dblp computer science bibliography, https://dblp.org}\n            }\n\n            @article{DBLP:journals/pami/SunFYQLYOZ23,\n              author       = {Zhu Sun and\n                              Hui Fang and\n                              Jie Yang and\n                              Xinghua Qu and\n                              Hongyang Liu and\n                              Di Yu and\n                              Yew{-}Soon Ong and\n                              Jie Zhang},\n              title        = {DaisyRec 2.0: Benchmarking Recommendation for Rigorous Evaluation},\n              journal      = {{IEEE} Trans. Pattern Anal. Mach. Intell.},\n              volume       = {45},\n              number       = {7},\n              pages        = {8206--8226},\n              year         = {2023},\n              url          = {https://doi.org/10.1109/TPAMI.2022.3231891},\n              doi          = {10.1109/TPAMI.2022.3231891},\n              timestamp    = {Fri, 07 Jul 2023 23:32:20 +0200},\n              biburl       = {https://dblp.org/rec/journals/pami/SunFYQLYOZ23.bib},\n              bibsource    = {dblp computer science bibliography, https://dblp.org}\n            }\"\"\"\n\n    CODE = \"\"\"\n\n    \"\"\"\n\n    DOC = 'https://daisyrec.readthedocs.io/en/latest/'\n\n    def info_code(self):\n        \"\"\"\n        Provide the code to use in DaisyRec to run experiments.\n        \"\"\"\n        if self.timestamp:\n            self.CODE = f\"\"\"\n            In DaisyRec you need to replace the file at \n            \\'daisy/utils/loader.py\\'\n            with the file at\n            \\'datarec/io/frameworks/daisyrec/loader.py\\'\n            Then you need to open the file, go to line 36 and change \\'YOUR_PATH_HERE\\' with\n            \\'{self.path}\\'\n            \"\"\"\n        else:\n            self.CODE = f\"\"\"\n            In DaisyRec you need to replace the file at \n            \\'daisy/utils/loader.py\\'\n            with the file at\n            \\'datarec/io/frameworks/daisyrec/loader.py\\'\n            Then you need to open the file, go to line 36 and change \\'YOUR_PATH_HERE\\' with\n            \\'{self.path}\\'\n            Morover, from the attribute \\'names\\' you have to remove the timestamp.\n            \"\"\"\n        super().info_code()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.daisyrec.daisyrec.DaisyRec.__init__","title":"<code>__init__(timestamp, path)</code>","text":"<p>Initialize DaisyRec adapter. Args:     timestamp (bool): Whether timestamps are included.     path (str): Path where the DaisyRec-compatible dataset is stored.</p> Source code in <code>datarec/io/frameworks/daisyrec/daisyrec.py</code> <pre><code>def __init__(self, timestamp, path):\n    \"\"\"\n    Initialize DaisyRec adapter.\n    Args:\n        timestamp (bool): Whether timestamps are included.\n        path (str): Path where the DaisyRec-compatible dataset is stored.\n    \"\"\"\n    self.timestamp = timestamp\n    self.path = path\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.daisyrec.daisyrec.DaisyRec.info_code","title":"<code>info_code()</code>","text":"<p>Provide the code to use in DaisyRec to run experiments.</p> Source code in <code>datarec/io/frameworks/daisyrec/daisyrec.py</code> <pre><code>def info_code(self):\n    \"\"\"\n    Provide the code to use in DaisyRec to run experiments.\n    \"\"\"\n    if self.timestamp:\n        self.CODE = f\"\"\"\n        In DaisyRec you need to replace the file at \n        \\'daisy/utils/loader.py\\'\n        with the file at\n        \\'datarec/io/frameworks/daisyrec/loader.py\\'\n        Then you need to open the file, go to line 36 and change \\'YOUR_PATH_HERE\\' with\n        \\'{self.path}\\'\n        \"\"\"\n    else:\n        self.CODE = f\"\"\"\n        In DaisyRec you need to replace the file at \n        \\'daisy/utils/loader.py\\'\n        with the file at\n        \\'datarec/io/frameworks/daisyrec/loader.py\\'\n        Then you need to open the file, go to line 36 and change \\'YOUR_PATH_HERE\\' with\n        \\'{self.path}\\'\n        Morover, from the attribute \\'names\\' you have to remove the timestamp.\n        \"\"\"\n    super().info_code()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.daisyrec.loader.load_rate","title":"<code>load_rate(src='ml-100k', prepro='origin', binary=True, pos_threshold=None, level='ui')</code>","text":"<p>Load certain raw data. Args:     src (str): Name of dataset.     prepro (str): Way to pre-process raw data input, expect 'origin', f'{N}core', f'{N}filter', N is integer value.     binary (boolean): Whether to transform rating to binary label as CTR or not as Regression.     pos_threshold (float): If not None, treat rating larger than this threshold as positive sample.     level (str): which level to do with f'{N}core' or f'{N}filter' operation (it only works when prepro contains 'core' or 'filter').</p> <p>Returns:</p> Type Description <code>Dataframe</code> <p>Rating information with columns: user, item, rating, (options: timestamp).</p> <code>int</code> <p>The number of users in the dataset.</p> <code>int</code> <p>The number of items in the dataset.</p> Source code in <code>datarec/io/frameworks/daisyrec/loader.py</code> <pre><code>def load_rate(src='ml-100k', prepro='origin', binary=True, pos_threshold=None, level='ui'):\n    \"\"\"\n    Load certain raw data.\n    Args:\n        src (str): Name of dataset.\n        prepro (str): Way to pre-process raw data input, expect 'origin', f'{N}core', f'{N}filter', N is integer value.\n        binary (boolean): Whether to transform rating to binary label as CTR or not as Regression.\n        pos_threshold (float): If not None, treat rating larger than this threshold as positive sample.\n        level (str): which level to do with f'{N}core' or f'{N}filter' operation (it only works when prepro contains 'core' or 'filter').\n\n    Returns:\n        (pd.Dataframe): Rating information with columns: user, item, rating, (options: timestamp).\n        (int): The number of users in the dataset.\n        (int): The number of items in the dataset.\n\n    \"\"\"\n    df = pd.DataFrame()\n    # which dataset will use\n    if src == 'ml-100k':\n        df = pd.read_csv(f'./data/{src}/u.data', sep='\\t', header=None,\n                         names=['user', 'item', 'rating', 'timestamp'], engine='python')\n    elif src == 'datarec':\n        df = pd.read_csv('YOUR_PATH_HERE', sep='\\t', header=None,\n                         names=['user', 'item', 'rating', 'timestamp'], engine='python')\n    elif src == 'ml-1m':\n        df = pd.read_csv(f'./data/{src}/ratings.dat', sep='::', header=None,\n                         names=['user', 'item', 'rating', 'timestamp'], engine='python')\n        # only consider rating &gt;=4 for data density\n        df = df.query('rating &gt;= 4').reset_index(drop=True).copy()\n\n    elif src == 'ml-10m':\n        df = pd.read_csv(f'./data/{src}/ratings.dat', sep='::', header=None,\n                         names=['user', 'item', 'rating', 'timestamp'], engine='python')\n        df = df.query('rating &gt;= 4').reset_index(drop=True).copy()\n\n    elif src == 'ml-20m':\n        df = pd.read_csv(f'./data/{src}/ratings.csv')\n        df.rename(columns={'userId': 'user', 'movieId': 'item'}, inplace=True)\n        df = df.query('rating &gt;= 4').reset_index(drop=True)\n\n    elif src == 'netflix':\n        cnt = 0\n        tmp_file = open(f'./data/{src}/training_data.csv', 'w')\n        tmp_file.write('user,item,rating,timestamp' + '\\n')\n        for f in os.listdir(f'./data/{src}/training_set/'):\n            cnt += 1\n            if cnt % 5000 == 0:\n                print(f'Finish Process {cnt} file......')\n            txt_file = open(f'./data/{src}/training_set/{f}', 'r')\n            contents = txt_file.readlines()\n            item = contents[0].strip().split(':')[0]\n            for val in contents[1:]:\n                user, rating, timestamp = val.strip().split(',')\n                tmp_file.write(','.join([user, item, rating, timestamp]) + '\\n')\n            txt_file.close()\n\n        tmp_file.close()\n\n        df = pd.read_csv(f'./data/{src}/training_data.csv')\n        df['rating'] = df.rating.astype(float)\n        df['timestamp'] = pd.to_datetime(df['timestamp'])\n\n    elif src == 'lastfm':\n        # user_artists.dat\n        df = pd.read_csv(f'./data/{src}/user_artists.dat', sep='\\t')\n        df.rename(columns={'userID': 'user', 'artistID': 'item', 'weight': 'rating'}, inplace=True)\n        # treat weight as interaction, as 1\n        df['rating'] = 1.0\n        # fake timestamp column\n        df['timestamp'] = 1\n\n    elif src == 'book-x':\n        df = pd.read_csv(f'./data/{src}/BX-Book-Ratings.csv', delimiter=\";\", encoding=\"latin1\")\n        df.rename(columns={'User-ID': 'user', 'ISBN': 'item', 'Book-Rating': 'rating'}, inplace=True)\n        # fake timestamp column\n        df['timestamp'] = 1\n\n    elif src == 'pinterest':\n        # TODO this dataset has wrong source URL, we will figure out in future\n        pass\n\n    elif src == 'amazon-cloth':\n        df = pd.read_csv(f'./data/{src}/ratings_Clothing_Shoes_and_Jewelry.csv',\n                         names=['user', 'item', 'rating', 'timestamp'])\n\n    elif src == 'amazon-electronic':\n        df = pd.read_csv(f'./data/{src}/ratings_Electronics.csv',\n                         names=['user', 'item', 'rating', 'timestamp'])\n\n    elif src == 'amazon-book':\n        df = pd.read_csv(f'./data/{src}/ratings_Books.csv',\n                         names=['user', 'item', 'rating', 'timestamp'], low_memory=False)\n        df = df[df['timestamp'].str.isnumeric()].copy()\n        df['timestamp'] = df['timestamp'].astype(int)\n\n    elif src == 'amazon-music':\n        df = pd.read_csv(f'./data/{src}/ratings_Digital_Music.csv',\n                         names=['user', 'item', 'rating', 'timestamp'])\n\n    elif src == 'epinions':\n        d = sio.loadmat(f'./data/{src}/rating_with_timestamp.mat')\n        prime = []\n        for val in d['rating_with_timestamp']:\n            user, item, rating, timestamp = val[0], val[1], val[3], val[5]\n            prime.append([user, item, rating, timestamp])\n        df = pd.DataFrame(prime, columns=['user', 'item', 'rating', 'timestamp'])\n        del prime\n        gc.collect()\n\n    elif src == 'yelp':\n        json_file_path = f'./data/{src}/yelp_academic_dataset_review.json'\n        prime = []\n        for line in open(json_file_path, 'r', encoding='UTF-8'):\n            val = json.loads(line)\n            prime.append([val['user_id'], val['business_id'], val['stars'], val['date']])\n        df = pd.DataFrame(prime, columns=['user', 'item', 'rating', 'timestamp'])\n        df['timestamp'] = pd.to_datetime(df.timestamp)\n        del prime\n        gc.collect()\n\n    elif src == 'citeulike':\n        user = 0\n        dt = []\n        for line in open(f'./data/{src}/users.dat', 'r'):\n            val = line.split()\n            for item in val:\n                dt.append([user, item])\n            user += 1\n        df = pd.DataFrame(dt, columns=['user', 'item'])\n        # fake timestamp column\n        df['timestamp'] = 1\n\n    else:\n        raise ValueError('Invalid Dataset Error')\n\n    # set rating &gt;= threshold as positive samples\n    if pos_threshold is not None:\n        df = df.query(f'rating &gt;= {pos_threshold}').reset_index(drop=True)\n\n    # reset rating to interaction, here just treat all rating as 1\n    if binary:\n        df['rating'] = 1.0\n\n    # which type of pre-dataset will use\n    if prepro == 'origin':\n        pass\n\n    elif prepro.endswith('filter'):\n        pattern = re.compile(r'\\d+')\n        filter_num = int(pattern.findall(prepro)[0])\n\n        tmp1 = df.groupby(['user'], as_index=False)['item'].count()\n        tmp1.rename(columns={'item': 'cnt_item'}, inplace=True)\n        tmp2 = df.groupby(['item'], as_index=False)['user'].count()\n        tmp2.rename(columns={'user': 'cnt_user'}, inplace=True)\n        df = df.merge(tmp1, on=['user']).merge(tmp2, on=['item'])\n        if level == 'ui':\n            df = df.query(f'cnt_item &gt;= {filter_num} and cnt_user &gt;= {filter_num}').reset_index(drop=True).copy()\n        elif level == 'u':\n            df = df.query(f'cnt_item &gt;= {filter_num}').reset_index(drop=True).copy()\n        elif level == 'i':\n            df = df.query(f'cnt_user &gt;= {filter_num}').reset_index(drop=True).copy()\n        else:\n            raise ValueError(f'Invalid level value: {level}')\n\n        df.drop(['cnt_item', 'cnt_user'], axis=1, inplace=True)\n        del tmp1, tmp2\n        gc.collect()\n\n    elif prepro.endswith('core'):\n        pattern = re.compile(r'\\d+')\n        core_num = int(pattern.findall(prepro)[0])\n\n        def filter_user(df):\n            tmp = df.groupby(['user'], as_index=False)['item'].count()\n            tmp.rename(columns={'item': 'cnt_item'}, inplace=True)\n            df = df.merge(tmp, on=['user'])\n            df = df.query(f'cnt_item &gt;= {core_num}').reset_index(drop=True).copy()\n            df.drop(['cnt_item'], axis=1, inplace=True)\n\n            return df\n\n        def filter_item(df):\n            tmp = df.groupby(['item'], as_index=False)['user'].count()\n            tmp.rename(columns={'user': 'cnt_user'}, inplace=True)\n            df = df.merge(tmp, on=['item'])\n            df = df.query(f'cnt_user &gt;= {core_num}').reset_index(drop=True).copy()\n            df.drop(['cnt_user'], axis=1, inplace=True)\n\n            return df\n\n        if level == 'ui':\n            while 1:\n                df = filter_user(df)\n                df = filter_item(df)\n                chk_u = df.groupby('user')['item'].count()\n                chk_i = df.groupby('item')['user'].count()\n                if len(chk_i[chk_i &lt; core_num]) &lt;= 0 and len(chk_u[chk_u &lt; core_num]) &lt;= 0:\n                    break\n        elif level == 'u':\n            df = filter_user(df)\n        elif level == 'i':\n            df = filter_item(df)\n        else:\n            raise ValueError(f'Invalid level value: {level}')\n\n        gc.collect()\n\n    else:\n        raise ValueError('Invalid dataset preprocess type, origin/Ncore/Nfilter (N is int number) expected')\n\n    # encoding user_id and item_id\n    df['user'] = pd.Categorical(df['user']).codes\n    df['item'] = pd.Categorical(df['item']).codes\n\n    user_num = df['user'].nunique()\n    item_num = df['item'].nunique()\n\n    print(f'Finish loading [{src}]-[{prepro}] dataset')\n\n    return df, user_num, item_num\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.daisyrec.loader.get_ur","title":"<code>get_ur(df)</code>","text":"<p>Get user-rating pairs. Args:     df (pd.DataFrame): Rating dataframe.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary which stores user-items interactions.</p> Source code in <code>datarec/io/frameworks/daisyrec/loader.py</code> <pre><code>def get_ur(df):\n    \"\"\"\n    Get user-rating pairs.\n    Args:\n        df (pd.DataFrame): Rating dataframe.\n\n    Returns:\n        (dict): Dictionary which stores user-items interactions.\n\n    \"\"\"\n    ur = defaultdict(set)\n    for _, row in df.iterrows():\n        ur[int(row['user'])].add(int(row['item']))\n\n    return ur\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.daisyrec.loader.get_ir","title":"<code>get_ir(df)</code>","text":"<p>Get item-rating pairs. Args:     df (pd.DataFrame): Rating dataframe.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary which stores item-items interactions.</p> Source code in <code>datarec/io/frameworks/daisyrec/loader.py</code> <pre><code>def get_ir(df):\n    \"\"\"\n    Get item-rating pairs.\n    Args:\n        df (pd.DataFrame): Rating dataframe.\n\n    Returns:\n        (dict): Dictionary which stores item-items interactions.\n\n    \"\"\"\n    ir = defaultdict(set)\n    for _, row in df.iterrows():\n        ir[int(row['item'])].add(int(row['user']))\n\n    return ir\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.daisyrec.loader.build_feat_idx_dict","title":"<code>build_feat_idx_dict(df, cat_cols=['user', 'item'], num_cols=[])</code>","text":"<p>Encode feature mapping for FM. Args:     df (pd.DataFrame): Feature dataframe.     cat_cols (list): List of categorical column names.     num_cols (list): List of numerical column names.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with index-feature column mapping information.</p> <code>int</code> <p>The number of features.</p> Source code in <code>datarec/io/frameworks/daisyrec/loader.py</code> <pre><code>def build_feat_idx_dict(df: pd.DataFrame,\n                        cat_cols: list = ['user', 'item'],\n                        num_cols: list = []):\n    \"\"\"\n    Encode feature mapping for FM.\n    Args:\n        df (pd.DataFrame): Feature dataframe.\n        cat_cols (list): List of categorical column names.\n        num_cols (list): List of numerical column names.\n\n    Returns:\n        (dict): Dictionary with index-feature column mapping information.\n        (int): The number of features.\n\n    \"\"\"\n    feat_idx_dict = {}\n    idx = 0\n    for col in cat_cols:\n        feat_idx_dict[col] = idx\n        idx = idx + df[col].max() + 1\n    for col in num_cols:\n        feat_idx_dict[col] = idx\n        idx += 1\n    print('Finish build feature index dictionary......')\n\n    cnt = 0\n    for col in cat_cols:\n        for _ in df[col].unique():\n            cnt += 1\n    for _ in num_cols:\n        cnt += 1\n    print(f'Number of features: {cnt}')\n\n    return feat_idx_dict, cnt\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.daisyrec.loader.convert_npy_mat","title":"<code>convert_npy_mat(user_num, item_num, df)</code>","text":"<p>Convert pd.Dataframe to numpy matrix. Args:     user_num(int): Number of users.     item_num (int): Number of items.     df (pd.DataFrame): Rating dataframe.</p> <p>Returns:</p> Type Description <code>array</code> <p>Rating matrix.</p> Source code in <code>datarec/io/frameworks/daisyrec/loader.py</code> <pre><code>def convert_npy_mat(user_num, item_num, df):\n    \"\"\"\n    Convert pd.Dataframe to numpy matrix.\n    Args:\n        user_num(int): Number of users.\n        item_num (int): Number of items.\n        df (pd.DataFrame): Rating dataframe.\n\n    Returns:\n        (np.array): Rating matrix.\n    \"\"\"\n    mat = np.zeros((user_num, item_num))\n    for _, row in df.iterrows():\n        u, i, r = row['user'], row['item'], row['rating']\n        mat[int(u), int(i)] = float(r)\n    return mat\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.daisyrec.loader.build_candidates_set","title":"<code>build_candidates_set(test_ur, train_ur, item_pool, candidates_num=1000)</code>","text":"<p>Build candidate  items for ranking. Args:     test_ur (dict): Ground truth that represents the relationship of user and item in the test set.     train_ur (dict): The relationship of user and item in the train set.     item_pool (list or set): Set of all items.     candidates_num (int): Number of candidates.:</p> <p>Returns:</p> Name Type Description <code>test_ucands</code> <code>dict</code> <p>Dictionary storing candidates for each user in test set.</p> Source code in <code>datarec/io/frameworks/daisyrec/loader.py</code> <pre><code>def build_candidates_set(test_ur, train_ur, item_pool, candidates_num=1000):\n    \"\"\"\n    Build candidate  items for ranking.\n    Args:\n        test_ur (dict): Ground truth that represents the relationship of user and item in the test set.\n        train_ur (dict): The relationship of user and item in the train set.\n        item_pool (list or set): Set of all items.\n        candidates_num (int): Number of candidates.:\n\n    Returns:\n        test_ucands (dict): Dictionary storing candidates for each user in test set.\n\n    \"\"\"\n    test_ucands = defaultdict(list)\n    for k, v in test_ur.items():\n        sample_num = candidates_num - len(v) if len(v) &lt; candidates_num else 0\n        sub_item_pool = item_pool - v - train_ur[k]  # remove GT &amp; interacted\n        sample_num = min(len(sub_item_pool), sample_num)\n        if sample_num == 0:\n            samples = random.sample(v, candidates_num)\n            test_ucands[k] = list(set(samples))\n        else:\n            samples = random.sample(sub_item_pool, sample_num)\n            test_ucands[k] = list(v | set(samples))\n\n    return test_ucands\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.daisyrec.loader.get_adj_mat","title":"<code>get_adj_mat(n_users, n_items)</code>","text":"<p>Get adjacency matrix. Args:     n_users (int): Number of users.     n_items (int): Number of items.</p> <p>Returns:</p> Name Type Description <code>adj_mat</code> <code>csr_matrix</code> <p>Adjacency matrix.</p> <code>norm_adj_mat</code> <code>csr_matrix</code> <p>Normalized adjacency matrix.</p> <code>mean_adj_mat</code> <code>csr_matrix</code> <p>Mean adjacency matrix.</p> Source code in <code>datarec/io/frameworks/daisyrec/loader.py</code> <pre><code>def get_adj_mat(n_users, n_items):\n    \"\"\"\n    Get adjacency matrix.\n    Args:\n        n_users (int): Number of users.\n        n_items (int): Number of items.\n\n    Returns:\n        adj_mat (sp.csr_matrix): Adjacency matrix.\n        norm_adj_mat (sp.csr_matrix): Normalized adjacency matrix.\n        mean_adj_mat(sp.csr_matrix): Mean adjacency matrix.\n\n    \"\"\"\n    R = sp.dok_matrix((n_users, n_items), dtype=np.float32)\n    adj_mat = sp.dok_matrix((n_users + n_items, n_users + n_items), dtype=np.float32)\n    adj_mat = adj_mat.tolil()\n    R = R.tolil()\n\n    adj_mat[:n_users, n_users:] = R\n    adj_mat[n_users:, :n_users] = R.T\n    adj_mat = adj_mat.todok()\n    print('already create adjacency matrix', adj_mat.shape)\n\n    def mean_adj_single(adj):\n        \"\"\"\n        Compute row-normalized adjacency matrix (D\u207b\u00b9A).\n        Args:\n            adj (sp.spmatrix): Sparse adjacency matrix.\n\n        Returns:\n            (sp.coo_matrix): Row-normalized adjacency matrix in COO format.\n        \"\"\"\n        # D^-1 * A\n        rowsum = np.array(adj.sum(1))\n\n        d_inv = np.power(rowsum, -1).flatten()\n        d_inv[np.isinf(d_inv)] = 0.\n        d_mat_inv = sp.diags(d_inv)\n\n        norm_adj = d_mat_inv.dot(adj)\n        # norm_adj = adj.dot(d_mat_inv)\n        print('generate single-normalized adjacency matrix.')\n        return norm_adj.tocoo()\n\n    def normalized_adj_single(adj):\n        \"\"\"\n        Compute symmetric normalized adjacency matrix (D\u207b\u00b9/\u00b2 A D\u207b\u00b9/\u00b2).\n        Args:\n            adj (sp.spmatrix): Sparse adjacency matrix.\n\n        Returns:\n            (sp.coo_matrix): Symmetric normalized adjacency matrix in COO format.\n        \"\"\"\n        # D^-1/2 * A * D^-1/2\n        rowsum = np.array(adj.sum(1))\n\n        d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n        d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n        d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n\n        # bi_lap = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt)\n        bi_lap = d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt)\n        return bi_lap.tocoo()\n\n    def check_adj_if_equal(adj):\n        \"\"\"\n        Check if normalized adjacency is equivalent to Laplacian-based transformation.\n        Args:\n            adj (sp.spmatrix): Sparse adjacency matrix.\n\n        Returns:\n            (np.ndarray): Dense matrix representing the normalized adjacency for verification\n\n        \"\"\"\n        dense_A = np.array(adj.todense())\n        degree = np.sum(dense_A, axis=1, keepdims=False)\n\n        temp = np.dot(np.diag(np.power(degree, -1)), dense_A)\n        print('check normalized adjacency matrix whether equal to this laplacian matrix.')\n        return temp\n\n    norm_adj_mat = mean_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))\n    # norm_adj_mat = normalized_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))\n    mean_adj_mat = mean_adj_single(adj_mat)\n\n    print('already normalize adjacency matrix')\n    return adj_mat.tocsr(), norm_adj_mat.tocsr(), mean_adj_mat.tocsr()\n</code></pre>"},{"location":"documentation/io/#elliot","title":"Elliot","text":""},{"location":"documentation/io/#datarec.io.frameworks.elliot.elliot.Elliot","title":"<code>Elliot</code>","text":"<p>               Bases: <code>Framework</code></p> <p>Elliot framework adapter.</p> <p>Provide metadata, citation, and usage examples for Elliot framework.</p> Source code in <code>datarec/io/frameworks/elliot/elliot.py</code> <pre><code>class Elliot(Framework):\n    \"\"\"\n    Elliot framework adapter.\n\n    Provide metadata, citation, and usage examples for Elliot framework.\n    \"\"\"\n\n    def __init__(self, timestamp, path):\n        \"\"\"\n        Initialize Elliot adapter.\n        Args:\n            timestamp (bool): Whether timestamps are included.\n            path (str): Path where the Elliot-compatible dataset is stored.\n        \"\"\"\n        self.timestamp = timestamp\n\n        self.directory = os.path.abspath(os.path.dirname(path))\n        if os.path.exists(self.directory) is False:\n            os.makedirs(self.directory)\n\n        self.train_path, self.test_path, self.val_path = \\\n            os.path.join(self.directory, 'train.tsv'), \\\n                os.path.join(self.directory, 'test.tsv'), \\\n                os.path.join(self.directory, 'validation.tsv')\n\n        self.file = os.path.basename(path)\n        self.file_path = os.path.join(self.directory, self.file)\n\n        # create configuration file\n        config_file = \\\n            CONF.format(path=self.file_path,\n                        dataset='datarec2elliot',\n                        train=self.train_path,\n                        test=self.test_path,\n                        val=self.val_path)\n\n        self.config_path = os.path.join(self.directory, 'datarec_config.yml')\n        with open(self.config_path, 'w') as file:\n            file.write(config_file)\n\n    FRAMEWORK_NAME = 'Elliot'\n\n    REPOSITORY = 'https://github.com/sisinflab/elliot'\n\n    PAPER = \"\"\"Elliot: a Comprehensive and Rigorous Framework for Reproducible Recommender Systems Evaluation\"\"\"\n\n    DOI = \"https://doi.org/10.1145/3404835.3463245\"\n\n    CITATION = \"\"\"\n            @inproceedings{DBLP:conf/sigir/AnelliBFMMPDN21,\n              author       = {Vito Walter Anelli and\n                              Alejandro Bellog{\\'{\\i}}n and\n                              Antonio Ferrara and\n                              Daniele Malitesta and\n                              Felice Antonio Merra and\n                              Claudio Pomo and\n                              Francesco Maria Donini and\n                              Tommaso Di Noia},\n              editor       = {Fernando Diaz and\n                              Chirag Shah and\n                              Torsten Suel and\n                              Pablo Castells and\n                              Rosie Jones and\n                              Tetsuya Sakai},\n              title        = {Elliot: {A} Comprehensive and Rigorous Framework for Reproducible\n                              Recommender Systems Evaluation},\n              booktitle    = {{SIGIR} '21: The 44th International {ACM} {SIGIR} Conference on Research\n                              and Development in Information Retrieval, Virtual Event, Canada, July\n                              11-15, 2021},\n              pages        = {2405--2414},\n              publisher    = {{ACM}},\n              year         = {2021},\n              url          = {https://doi.org/10.1145/3404835.3463245},\n              doi          = {10.1145/3404835.3463245},\n              timestamp    = {Sun, 12 Nov 2023 02:10:04 +0100},\n              biburl       = {https://dblp.org/rec/conf/sigir/AnelliBFMMPDN21.bib},\n              bibsource    = {dblp computer science bibliography, https://dblp.org}\n            }\"\"\"\n\n    CODE = \"  \"\n\n    DOC = 'https://elliot.readthedocs.io/en/latest/'\n\n    def info_code(self):\n        \"\"\"\n        Provide the code to use in Elliot to run experiments.\n        \"\"\"\n        self.CODE = \"\"\"\n            A configuration file for Elliot has been created here:\n            \\'{config_path}\\'\n            You can now run the script.\n             If you move the configuration file remember to change the path in the script below.\n\n            Elliot script:\n            python start_experiments.py --config {config_path}\n\n            This script contains a basic recommendation example. Change it if you need.\n            \"\"\".format(config_path=self.config_path)\n\n        super().info_code()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.elliot.elliot.Elliot.__init__","title":"<code>__init__(timestamp, path)</code>","text":"<p>Initialize Elliot adapter. Args:     timestamp (bool): Whether timestamps are included.     path (str): Path where the Elliot-compatible dataset is stored.</p> Source code in <code>datarec/io/frameworks/elliot/elliot.py</code> <pre><code>def __init__(self, timestamp, path):\n    \"\"\"\n    Initialize Elliot adapter.\n    Args:\n        timestamp (bool): Whether timestamps are included.\n        path (str): Path where the Elliot-compatible dataset is stored.\n    \"\"\"\n    self.timestamp = timestamp\n\n    self.directory = os.path.abspath(os.path.dirname(path))\n    if os.path.exists(self.directory) is False:\n        os.makedirs(self.directory)\n\n    self.train_path, self.test_path, self.val_path = \\\n        os.path.join(self.directory, 'train.tsv'), \\\n            os.path.join(self.directory, 'test.tsv'), \\\n            os.path.join(self.directory, 'validation.tsv')\n\n    self.file = os.path.basename(path)\n    self.file_path = os.path.join(self.directory, self.file)\n\n    # create configuration file\n    config_file = \\\n        CONF.format(path=self.file_path,\n                    dataset='datarec2elliot',\n                    train=self.train_path,\n                    test=self.test_path,\n                    val=self.val_path)\n\n    self.config_path = os.path.join(self.directory, 'datarec_config.yml')\n    with open(self.config_path, 'w') as file:\n        file.write(config_file)\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.elliot.elliot.Elliot.info_code","title":"<code>info_code()</code>","text":"<p>Provide the code to use in Elliot to run experiments.</p> Source code in <code>datarec/io/frameworks/elliot/elliot.py</code> <pre><code>def info_code(self):\n    \"\"\"\n    Provide the code to use in Elliot to run experiments.\n    \"\"\"\n    self.CODE = \"\"\"\n        A configuration file for Elliot has been created here:\n        \\'{config_path}\\'\n        You can now run the script.\n         If you move the configuration file remember to change the path in the script below.\n\n        Elliot script:\n        python start_experiments.py --config {config_path}\n\n        This script contains a basic recommendation example. Change it if you need.\n        \"\"\".format(config_path=self.config_path)\n\n    super().info_code()\n</code></pre>"},{"location":"documentation/io/#lenskit","title":"LensKit","text":""},{"location":"documentation/io/#datarec.io.frameworks.lenskit.lenskit.LensKit","title":"<code>LensKit</code>","text":"<p>               Bases: <code>Framework</code></p> <p>LensKit framework adapter.</p> <p>Provide metadata, citation, and usage examples for LensKit framework.</p> Source code in <code>datarec/io/frameworks/lenskit/lenskit.py</code> <pre><code>class LensKit(Framework):\n    \"\"\"\n    LensKit framework adapter.\n\n    Provide metadata, citation, and usage examples for LensKit framework.\n    \"\"\"\n\n    def __init__(self, timestamp, path):\n        \"\"\"\n        Initialize LensKit adapter.\n        Args:\n            timestamp (bool): Whether timestamps are included.\n            path (str): Path where the LensKit-compatible dataset is stored.\n        \"\"\"\n        self.timestamp = timestamp\n        self.path = path\n\n    FRAMEWORK_NAME = 'LensKit'\n\n    REPOSITORY = 'https://github.com/lenskit/lkpy'\n\n    PAPER = \"\"\"LensKit for Python: Next-Generation Software for Recommender Systems Experiments\"\"\"\n\n    DOI = \"https://doi.org/10.1145/3340531.3412778\"\n\n    CITATION = \"\"\"\n            @inproceedings{DBLP:conf/cikm/Ekstrand20,\n              author       = {Michael D. Ekstrand},\n              editor       = {Mathieu d'Aquin and\n                              Stefan Dietze and\n                              Claudia Hauff and\n                              Edward Curry and\n                              Philippe Cudr{\\'{e}}{-}Mauroux},\n              title        = {LensKit for Python: Next-Generation Software for Recommender Systems\n                              Experiments},\n              booktitle    = {{CIKM} '20: The 29th {ACM} International Conference on Information\n                              and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020},\n              pages        = {2999--3006},\n              publisher    = {{ACM}},\n              year         = {2020},\n              url          = {https://doi.org/10.1145/3340531.3412778},\n              doi          = {10.1145/3340531.3412778},\n              timestamp    = {Tue, 29 Dec 2020 18:42:41 +0100},\n              biburl       = {https://dblp.org/rec/conf/cikm/Ekstrand20.bib},\n              bibsource    = {dblp computer science bibliography, https://dblp.org}\n            }\"\"\"\n\n    CODE = \"\"\"\n\n    \"\"\"\n\n    DOC = 'https://lkpy.lenskit.org/en/stable/'\n\n    def info_code(self):\n        \"\"\"\n        Provide the code to use in LensKit to run experiments.\n        \"\"\"\n        self.CODE = \"\"\"\n        LensKit accepts pandas DataFrames with specific column naming. DataRec will do that for you!\n\n        import pandas as pd\n\n        ratings = pd.read_csv({path}, sep='\\\\t', header=False)\n        \"\"\".format(path=self.path)\n\n        super().info_code()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.lenskit.lenskit.LensKit.__init__","title":"<code>__init__(timestamp, path)</code>","text":"<p>Initialize LensKit adapter. Args:     timestamp (bool): Whether timestamps are included.     path (str): Path where the LensKit-compatible dataset is stored.</p> Source code in <code>datarec/io/frameworks/lenskit/lenskit.py</code> <pre><code>def __init__(self, timestamp, path):\n    \"\"\"\n    Initialize LensKit adapter.\n    Args:\n        timestamp (bool): Whether timestamps are included.\n        path (str): Path where the LensKit-compatible dataset is stored.\n    \"\"\"\n    self.timestamp = timestamp\n    self.path = path\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.lenskit.lenskit.LensKit.info_code","title":"<code>info_code()</code>","text":"<p>Provide the code to use in LensKit to run experiments.</p> Source code in <code>datarec/io/frameworks/lenskit/lenskit.py</code> <pre><code>def info_code(self):\n    \"\"\"\n    Provide the code to use in LensKit to run experiments.\n    \"\"\"\n    self.CODE = \"\"\"\n    LensKit accepts pandas DataFrames with specific column naming. DataRec will do that for you!\n\n    import pandas as pd\n\n    ratings = pd.read_csv({path}, sep='\\\\t', header=False)\n    \"\"\".format(path=self.path)\n\n    super().info_code()\n</code></pre>"},{"location":"documentation/io/#recbole","title":"RecBole","text":""},{"location":"documentation/io/#datarec.io.frameworks.recbole.recbole.RecBole","title":"<code>RecBole</code>","text":"<p>               Bases: <code>Framework</code></p> <p>RecBole framework adapter.</p> <p>Provide metadata, citation, and usage examples for RecBole framework.</p> Source code in <code>datarec/io/frameworks/recbole/recbole.py</code> <pre><code>class RecBole(Framework):\n    \"\"\"\n    RecBole framework adapter.\n\n    Provide metadata, citation, and usage examples for RecBole framework.\n    \"\"\"\n\n    def __init__(self, timestamp, path):\n        \"\"\"\n        Initialize RecBole adapter.\n        Args:\n            timestamp (bool): Whether timestamps are included.\n            path (str): Path where the RecBole-compatible dataset is stored.\n        \"\"\"\n        self.timestamp = timestamp\n        directory = os.path.dirname(path)\n        self.directory = os.path.join(directory, 'DataRec2RecBole')\n        print('RecBole requires a directory named as the the dataset.\\n'\n              f'Based on your path the directory that will be used is \\'{self.directory}\\'')\n        if os.path.exists(self.directory) is False:\n            os.makedirs(self.directory)\n        self.path = os.path.join(self.directory, path)\n\n    FRAMEWORK_NAME = 'RecBole'\n\n    REPOSITORY = 'https://github.com/RUCAIBox/RecBole2.0'\n\n    PAPER = \"\"\"RecBole 2.0: Towards a More Up-to-Date Recommendation Library\"\"\"\n\n    DOI = \"https://doi.org/10.1145/3511808.3557680\"\n\n    CITATION = \"\"\"\n            @inproceedings{DBLP:conf/cikm/ZhaoMHLCPLLWTMF21,\n              author       = {Wayne Xin Zhao and\n                              Shanlei Mu and\n                              Yupeng Hou and\n                              Zihan Lin and\n                              Yushuo Chen and\n                              Xingyu Pan and\n                              Kaiyuan Li and\n                              Yujie Lu and\n                              Hui Wang and\n                              Changxin Tian and\n                              Yingqian Min and\n                              Zhichao Feng and\n                              Xinyan Fan and\n                              Xu Chen and\n                              Pengfei Wang and\n                              Wendi Ji and\n                              Yaliang Li and\n                              Xiaoling Wang and\n                              Ji{-}Rong Wen},\n              editor       = {Gianluca Demartini and\n                              Guido Zuccon and\n                              J. Shane Culpepper and\n                              Zi Huang and\n                              Hanghang Tong},\n              title        = {RecBole: Towards a Unified, Comprehensive and Efficient Framework\n                              for Recommendation Algorithms},\n              booktitle    = {{CIKM} '21: The 30th {ACM} International Conference on Information\n                              and Knowledge Management, Virtual Event, Queensland, Australia, November\n                              1 - 5, 2021},\n              pages        = {4653--4664},\n              publisher    = {{ACM}},\n              year         = {2021},\n              url          = {https://doi.org/10.1145/3459637.3482016},\n              doi          = {10.1145/3459637.3482016},\n              timestamp    = {Tue, 07 May 2024 20:05:19 +0200},\n              biburl       = {https://dblp.org/rec/conf/cikm/ZhaoMHLCPLLWTMF21.bib},\n              bibsource    = {dblp computer science bibliography, https://dblp.org}\n            }\n            @inproceedings{DBLP:conf/cikm/ZhaoHPYZLZBTSCX22,\n              author       = {Wayne Xin Zhao and\n                              Yupeng Hou and\n                              Xingyu Pan and\n                              Chen Yang and\n                              Zeyu Zhang and\n                              Zihan Lin and\n                              Jingsen Zhang and\n                              Shuqing Bian and\n                              Jiakai Tang and\n                              Wenqi Sun and\n                              Yushuo Chen and\n                              Lanling Xu and\n                              Gaowei Zhang and\n                              Zhen Tian and\n                              Changxin Tian and\n                              Shanlei Mu and\n                              Xinyan Fan and\n                              Xu Chen and\n                              Ji{-}Rong Wen},\n              editor       = {Mohammad Al Hasan and\n                              Li Xiong},\n              title        = {RecBole 2.0: Towards a More Up-to-Date Recommendation Library},\n              booktitle    = {Proceedings of the 31st {ACM} International Conference on Information\n                              {\\&amp;} Knowledge Management, Atlanta, GA, USA, October 17-21, 2022},\n              pages        = {4722--4726},\n              publisher    = {{ACM}},\n              year         = {2022},\n              url          = {https://doi.org/10.1145/3511808.3557680},\n              doi          = {10.1145/3511808.3557680},\n              timestamp    = {Sun, 20 Aug 2023 12:23:03 +0200},\n              biburl       = {https://dblp.org/rec/conf/cikm/ZhaoHPYZLZBTSCX22.bib},\n              bibsource    = {dblp computer science bibliography, https://dblp.org}\n            }\"\"\"\n\n    CODE = \"\"\"\n\n    \"\"\"\n\n    DOC = 'https://recbole.io/'\n\n    def info_code(self):\n        \"\"\"\n        Provide the code to use in RecBole to run experiments.\n        \"\"\"\n        self.CODE = \"\"\"\n            from recbole.data import create_dataset\n            from recbole.config import Config\n\n            config_dict = {{\n                \"dataset\": \"datarec\",\n                \"data_path\": {path},\n            }}\n            config = Config(config_dict=config_dict, config_file_list=config_file_list)\n            dataset = create_dataset(config)\n        \"\"\".format(path=self.path)\n\n        super().info_code()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.recbole.recbole.RecBole.__init__","title":"<code>__init__(timestamp, path)</code>","text":"<p>Initialize RecBole adapter. Args:     timestamp (bool): Whether timestamps are included.     path (str): Path where the RecBole-compatible dataset is stored.</p> Source code in <code>datarec/io/frameworks/recbole/recbole.py</code> <pre><code>def __init__(self, timestamp, path):\n    \"\"\"\n    Initialize RecBole adapter.\n    Args:\n        timestamp (bool): Whether timestamps are included.\n        path (str): Path where the RecBole-compatible dataset is stored.\n    \"\"\"\n    self.timestamp = timestamp\n    directory = os.path.dirname(path)\n    self.directory = os.path.join(directory, 'DataRec2RecBole')\n    print('RecBole requires a directory named as the the dataset.\\n'\n          f'Based on your path the directory that will be used is \\'{self.directory}\\'')\n    if os.path.exists(self.directory) is False:\n        os.makedirs(self.directory)\n    self.path = os.path.join(self.directory, path)\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.recbole.recbole.RecBole.info_code","title":"<code>info_code()</code>","text":"<p>Provide the code to use in RecBole to run experiments.</p> Source code in <code>datarec/io/frameworks/recbole/recbole.py</code> <pre><code>def info_code(self):\n    \"\"\"\n    Provide the code to use in RecBole to run experiments.\n    \"\"\"\n    self.CODE = \"\"\"\n        from recbole.data import create_dataset\n        from recbole.config import Config\n\n        config_dict = {{\n            \"dataset\": \"datarec\",\n            \"data_path\": {path},\n        }}\n        config = Config(config_dict=config_dict, config_file_list=config_file_list)\n        dataset = create_dataset(config)\n    \"\"\".format(path=self.path)\n\n    super().info_code()\n</code></pre>"},{"location":"documentation/io/#rechorus","title":"ReChorus","text":""},{"location":"documentation/io/#datarec.io.frameworks.rechorus.rechorus.ReChorus","title":"<code>ReChorus</code>","text":"<p>               Bases: <code>Framework</code></p> <p>ReChorus framework adapter.</p> <p>Provide metadata, citation, and usage examples for ReChorus framework.</p> Source code in <code>datarec/io/frameworks/rechorus/rechorus.py</code> <pre><code>class ReChorus(Framework):\n    \"\"\"\n    ReChorus framework adapter.\n\n    Provide metadata, citation, and usage examples for ReChorus framework.\n    \"\"\"\n\n    def __init__(self, timestamp, path):\n        \"\"\"\n        Initialize ReChorus adapter.\n        Args:\n            timestamp (bool): Whether timestamps are included.\n            path (str): Path where the ReChorus-compatible dataset is stored.\n        \"\"\"\n        self.timestamp = timestamp\n        directory = os.path.dirname(path)\n        self.directory = os.path.abspath(os.path.join(directory, 'DataRec2ReChorus'))\n        print('RecBole requires a directory named as the the dataset.\\n'\n              f'Based on your path the directory that will be used is \\'{self.directory}\\'')\n        if os.path.exists(self.directory) is False:\n            os.makedirs(self.directory)\n\n    FRAMEWORK_NAME = 'ReChorus'\n\n    REPOSITORY = 'https://github.com/THUwangcy/ReChorus'\n\n    PAPER = \"\"\"Make It a Chorus: Knowledge- and Time-aware Item Modeling for Sequential Recommendation\"\"\"\n\n    DOI = \"https://doi.org/10.1145/3397271.3401131\"\n\n    CITATION = \"\"\"\n            @inproceedings{DBLP:conf/sigir/WangZMLM20,\n              author       = {Chenyang Wang and\n                              Min Zhang and\n                              Weizhi Ma and\n                              Yiqun Liu and\n                              Shaoping Ma},\n              editor       = {Jimmy X. Huang and\n                              Yi Chang and\n                              Xueqi Cheng and\n                              Jaap Kamps and\n                              Vanessa Murdock and\n                              Ji{-}Rong Wen and\n                              Yiqun Liu},\n              title        = {Make It a Chorus: Knowledge- and Time-aware Item Modeling for Sequential\n                              Recommendation},\n              booktitle    = {Proceedings of the 43rd International {ACM} {SIGIR} conference on\n                              research and development in Information Retrieval, {SIGIR} 2020, Virtual\n                              Event, China, July 25-30, 2020},\n              pages        = {109--118},\n              publisher    = {{ACM}},\n              year         = {2020},\n              url          = {https://doi.org/10.1145/3397271.3401131},\n              doi          = {10.1145/3397271.3401131},\n              timestamp    = {Mon, 31 Oct 2022 08:39:18 +0100},\n              biburl       = {https://dblp.org/rec/conf/sigir/WangZMLM20.bib},\n              bibsource    = {dblp computer science bibliography, https://dblp.org}\n            }\"\"\"\n\n    CODE = \"\"\"\n\n    \"\"\"\n\n    DOC = None\n\n    def info_code(self):\n        \"\"\"\n        Provide the code to use in RecBole to run experiments.\n        \"\"\"\n        self.CODE = \"\"\"\n            Dataset must be split and provided in a single folder within the \\'data\\' folder of the project.\\n\n            This data will be supported by ReChorus models that adopt a dataset \\'BaseModel.Dataset\\' \\n\n            DataRec created this directory here \\'{directory}\\'.\n        \"\"\".format(directory=self.directory)\n\n        super().info_code()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.rechorus.rechorus.ReChorus.__init__","title":"<code>__init__(timestamp, path)</code>","text":"<p>Initialize ReChorus adapter. Args:     timestamp (bool): Whether timestamps are included.     path (str): Path where the ReChorus-compatible dataset is stored.</p> Source code in <code>datarec/io/frameworks/rechorus/rechorus.py</code> <pre><code>def __init__(self, timestamp, path):\n    \"\"\"\n    Initialize ReChorus adapter.\n    Args:\n        timestamp (bool): Whether timestamps are included.\n        path (str): Path where the ReChorus-compatible dataset is stored.\n    \"\"\"\n    self.timestamp = timestamp\n    directory = os.path.dirname(path)\n    self.directory = os.path.abspath(os.path.join(directory, 'DataRec2ReChorus'))\n    print('RecBole requires a directory named as the the dataset.\\n'\n          f'Based on your path the directory that will be used is \\'{self.directory}\\'')\n    if os.path.exists(self.directory) is False:\n        os.makedirs(self.directory)\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.rechorus.rechorus.ReChorus.info_code","title":"<code>info_code()</code>","text":"<p>Provide the code to use in RecBole to run experiments.</p> Source code in <code>datarec/io/frameworks/rechorus/rechorus.py</code> <pre><code>def info_code(self):\n    \"\"\"\n    Provide the code to use in RecBole to run experiments.\n    \"\"\"\n    self.CODE = \"\"\"\n        Dataset must be split and provided in a single folder within the \\'data\\' folder of the project.\\n\n        This data will be supported by ReChorus models that adopt a dataset \\'BaseModel.Dataset\\' \\n\n        DataRec created this directory here \\'{directory}\\'.\n    \"\"\".format(directory=self.directory)\n\n    super().info_code()\n</code></pre>"},{"location":"documentation/io/#recommenders","title":"Recommenders","text":""},{"location":"documentation/io/#datarec.io.frameworks.recommenders.recommenders.Recommenders","title":"<code>Recommenders</code>","text":"<p>               Bases: <code>Framework</code></p> <p>Recommenders framework adapter.</p> <p>Provide metadata, citation, and usage examples for Recommenders framework.</p> Source code in <code>datarec/io/frameworks/recommenders/recommenders.py</code> <pre><code>class Recommenders(Framework):\n    \"\"\"\n    Recommenders framework adapter.\n\n    Provide metadata, citation, and usage examples for Recommenders framework.\n    \"\"\"\n\n    def __init__(self, timestamp, path):\n        \"\"\"\n        Initialize Recommenders adapter.\n        Args:\n            timestamp (bool): Whether timestamps are included.\n            path (str): Path where the Recommenders-compatible dataset is stored.\n        \"\"\"\n        self.timestamp = timestamp\n        self.directory = os.path.abspath(os.path.dirname(path))\n        if os.path.exists(self.directory) is False:\n            os.makedirs(self.directory)\n        self.file = os.path.basename(path)\n        self.file_path = os.path.join(self.directory, self.file)\n\n    FRAMEWORK_NAME = 'Recommenders'\n\n    REPOSITORY = 'https://github.com/recommenders-team/recommenders?tab=readme-ov-file'\n\n    PAPER = \"\"\"Microsoft recommenders: tools to accelerate developing recommender systems\"\"\"\n\n    DOI = \"https://doi.org/10.1145/3298689.3346967\"\n\n    CITATION = \"\"\"\n            @inproceedings{DBLP:conf/recsys/GrahamMW19,\n              author       = {Scott Graham and\n                              Jun{-}Ki Min and\n                              Tao Wu},\n              editor       = {Toine Bogers and\n                              Alan Said and\n                              Peter Brusilovsky and\n                              Domonkos Tikk},\n              title        = {Microsoft recommenders: tools to accelerate developing recommender\n                              systems},\n              booktitle    = {Proceedings of the 13th {ACM} Conference on Recommender Systems, RecSys\n                              2019, Copenhagen, Denmark, September 16-20, 2019},\n              pages        = {542--543},\n              publisher    = {{ACM}},\n              year         = {2019},\n              url          = {https://doi.org/10.1145/3298689.3346967},\n              doi          = {10.1145/3298689.3346967},\n              timestamp    = {Wed, 09 Oct 2019 14:20:04 +0200},\n              biburl       = {https://dblp.org/rec/conf/recsys/GrahamMW19.bib},\n              bibsource    = {dblp computer science bibliography, https://dblp.org}\n            }\"\"\"\n\n    CODE = \"\"\"\n\n    \"\"\"\n\n    DOC = 'https://recommenders-team.github.io/recommenders'\n\n    def info_code(self):\n        \"\"\"\n        Provide the code to use in Recommenders to run experiments.\n        \"\"\"\n        if self.timestamp:\n            self.CODE = \"\"\"\n                import pandas as pd\n\n                data = pd.read_csv({file}, sep=\"\\\\t\", names=['user', 'item', 'rating', 'timestamp'])\n                \"\"\".format(file=self.file_path)\n        else:\n            self.CODE = \"\"\"\n                import pandas as pd\n\n                data = pd.read_csv({file}, sep=\"\\\\t\", names=['user', 'item', 'rating'])\n                \"\"\".format(file=self.file_path)\n\n        super().info_code()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.recommenders.recommenders.Recommenders.__init__","title":"<code>__init__(timestamp, path)</code>","text":"<p>Initialize Recommenders adapter. Args:     timestamp (bool): Whether timestamps are included.     path (str): Path where the Recommenders-compatible dataset is stored.</p> Source code in <code>datarec/io/frameworks/recommenders/recommenders.py</code> <pre><code>def __init__(self, timestamp, path):\n    \"\"\"\n    Initialize Recommenders adapter.\n    Args:\n        timestamp (bool): Whether timestamps are included.\n        path (str): Path where the Recommenders-compatible dataset is stored.\n    \"\"\"\n    self.timestamp = timestamp\n    self.directory = os.path.abspath(os.path.dirname(path))\n    if os.path.exists(self.directory) is False:\n        os.makedirs(self.directory)\n    self.file = os.path.basename(path)\n    self.file_path = os.path.join(self.directory, self.file)\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.recommenders.recommenders.Recommenders.info_code","title":"<code>info_code()</code>","text":"<p>Provide the code to use in Recommenders to run experiments.</p> Source code in <code>datarec/io/frameworks/recommenders/recommenders.py</code> <pre><code>def info_code(self):\n    \"\"\"\n    Provide the code to use in Recommenders to run experiments.\n    \"\"\"\n    if self.timestamp:\n        self.CODE = \"\"\"\n            import pandas as pd\n\n            data = pd.read_csv({file}, sep=\"\\\\t\", names=['user', 'item', 'rating', 'timestamp'])\n            \"\"\".format(file=self.file_path)\n    else:\n        self.CODE = \"\"\"\n            import pandas as pd\n\n            data = pd.read_csv({file}, sep=\"\\\\t\", names=['user', 'item', 'rating'])\n            \"\"\".format(file=self.file_path)\n\n    super().info_code()\n</code></pre>"},{"location":"documentation/io/#recpack","title":"RecPack","text":""},{"location":"documentation/io/#datarec.io.frameworks.recpack.recpack.RecPack","title":"<code>RecPack</code>","text":"<p>               Bases: <code>Framework</code></p> <p>RecPack framework adapter.</p> <p>Provide metadata, citation, and usage examples for RecPack framework.</p> Source code in <code>datarec/io/frameworks/recpack/recpack.py</code> <pre><code>class RecPack(Framework):\n    \"\"\"\n    RecPack framework adapter.\n\n    Provide metadata, citation, and usage examples for RecPack framework.\n    \"\"\"\n\n    def __init__(self, timestamp, path):\n        \"\"\"\n        Initialize RecPack adapter.\n        Args:\n            timestamp (bool): Whether timestamps are included.\n            path (str): Path where the RecPack-compatible dataset is stored.\n        \"\"\"\n        self.timestamp = timestamp\n        self.directory = os.path.abspath(os.path.dirname(path))\n        if os.path.exists(self.directory) is False:\n            os.makedirs(self.directory)\n        self.file = os.path.basename(path)\n        self.file_path = os.path.join(self.directory, self.file)\n\n    FRAMEWORK_NAME = 'RecPack'\n\n    REPOSITORY = 'https://github.com/LienM/recpack'\n\n    PAPER = \"\"\"RecPack: An(other) Experimentation Toolkit for Top-N Recommendation using Implicit Feedback Data\"\"\"\n\n    DOI = \"https://doi.org/10.1145/3523227.3551472\"\n\n    CITATION = \"\"\"\n            @inproceedings{DBLP:conf/recsys/MichielsVG22,\n              author       = {Lien Michiels and\n                              Robin Verachtert and\n                              Bart Goethals},\n              editor       = {Jennifer Golbeck and\n                              F. Maxwell Harper and\n                              Vanessa Murdock and\n                              Michael D. Ekstrand and\n                              Bracha Shapira and\n                              Justin Basilico and\n                              Keld T. Lundgaard and\n                              Even Oldridge},\n              title        = {RecPack: An(other) Experimentation Toolkit for Top-N Recommendation\n                              using Implicit Feedback Data},\n              booktitle    = {RecSys '22: Sixteenth {ACM} Conference on Recommender Systems, Seattle,\n                              WA, USA, September 18 - 23, 2022},\n              pages        = {648--651},\n              publisher    = {{ACM}},\n              year         = {2022},\n              url          = {https://doi.org/10.1145/3523227.3551472},\n              doi          = {10.1145/3523227.3551472},\n              timestamp    = {Mon, 01 May 2023 13:01:24 +0200},\n              biburl       = {https://dblp.org/rec/conf/recsys/MichielsVG22.bib},\n              bibsource    = {dblp computer science bibliography, https://dblp.org}\n            }\"\"\"\n\n    CODE = \"\"\"\n\n    \"\"\"\n\n    DOC = 'https://recpack.froomle.ai/'\n\n    def info_code(self):\n        \"\"\"\n        Provide the code to use in RecPack to run experiments.\n        \"\"\"\n        self.CODE = \"\"\"\n            For using a dataset from DataRec you need to:\n            1) copy/move the file \n            \\'datarec/io/frameworks/recpack/datarec.py\\'\n            at \\'recpack/datasets/datarec.py\\'\n            2) replace the content of the init file in RecPack\n            \\'datarec/io/frameworks/recpack/__init__.py\\'\n            with the content of\n            \\'datarec/io/frameworks/recpack/copy_me_in__init__.py\\'\n            Then you can use this code\n\n            from recpack.datasets import DummyDataset\n            dataset = (path={file}, filename={directory}, use_default_filters=False)\n        \"\"\".format(file=self.file, directory=self.directory)\n\n        super().info_code()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.recpack.recpack.RecPack.__init__","title":"<code>__init__(timestamp, path)</code>","text":"<p>Initialize RecPack adapter. Args:     timestamp (bool): Whether timestamps are included.     path (str): Path where the RecPack-compatible dataset is stored.</p> Source code in <code>datarec/io/frameworks/recpack/recpack.py</code> <pre><code>def __init__(self, timestamp, path):\n    \"\"\"\n    Initialize RecPack adapter.\n    Args:\n        timestamp (bool): Whether timestamps are included.\n        path (str): Path where the RecPack-compatible dataset is stored.\n    \"\"\"\n    self.timestamp = timestamp\n    self.directory = os.path.abspath(os.path.dirname(path))\n    if os.path.exists(self.directory) is False:\n        os.makedirs(self.directory)\n    self.file = os.path.basename(path)\n    self.file_path = os.path.join(self.directory, self.file)\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.recpack.recpack.RecPack.info_code","title":"<code>info_code()</code>","text":"<p>Provide the code to use in RecPack to run experiments.</p> Source code in <code>datarec/io/frameworks/recpack/recpack.py</code> <pre><code>def info_code(self):\n    \"\"\"\n    Provide the code to use in RecPack to run experiments.\n    \"\"\"\n    self.CODE = \"\"\"\n        For using a dataset from DataRec you need to:\n        1) copy/move the file \n        \\'datarec/io/frameworks/recpack/datarec.py\\'\n        at \\'recpack/datasets/datarec.py\\'\n        2) replace the content of the init file in RecPack\n        \\'datarec/io/frameworks/recpack/__init__.py\\'\n        with the content of\n        \\'datarec/io/frameworks/recpack/copy_me_in__init__.py\\'\n        Then you can use this code\n\n        from recpack.datasets import DummyDataset\n        dataset = (path={file}, filename={directory}, use_default_filters=False)\n    \"\"\".format(file=self.file, directory=self.directory)\n\n    super().info_code()\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.recpack.datarec.DataRec","title":"<code>DataRec</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Base class for DataRec Datasets</p> Source code in <code>datarec/io/frameworks/recpack/datarec.py</code> <pre><code>class DataRec(Dataset):\n    \"\"\"\n    Base class for DataRec Datasets\n    \"\"\"\n    USER_IX = \"userId\"\n    \"\"\"Name of the column in the DataFrame that contains user identifiers.\"\"\"\n    ITEM_IX = \"itemId\"\n    \"\"\"Name of the column in the DataFrame that contains item identifiers.\"\"\"\n    TIMESTAMP_IX = \"timestamp\"\n    \"\"\"Name of the column in the DataFrame that contains time of interaction in seconds since epoch.\"\"\"\n\n    @property\n    def DEFAULT_FILENAME(self) -&gt; str:\n        \"\"\"\n        Default filename that will be used if it is not specified by the user.\n        \"\"\"\n        return f\"datarec.tsv\"\n\n    def _load_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Dataset from DataRec will be loaded as a pandas DataFrame\n\n        Warning:: This does not apply any preprocessing, and returns the raw dataset.\n\n        Returns:\n            (pd.DataFrame): The interaction data as a DataFrame with a row per interaction.\n\n        \"\"\"\n        df = pd.read_csv(os.path.join(self.path, self.filename), sep='\\t', header=True, dtype={\n                self.USER_IX: str,\n                self.TIMESTAMP_IX: np.int64,\n                self.ITEM_IX: str,\n            })\n        return df\n</code></pre>"},{"location":"documentation/io/#datarec.io.frameworks.recpack.datarec.DataRec.USER_IX","title":"<code>USER_IX = 'userId'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name of the column in the DataFrame that contains user identifiers.</p>"},{"location":"documentation/io/#datarec.io.frameworks.recpack.datarec.DataRec.ITEM_IX","title":"<code>ITEM_IX = 'itemId'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name of the column in the DataFrame that contains item identifiers.</p>"},{"location":"documentation/io/#datarec.io.frameworks.recpack.datarec.DataRec.TIMESTAMP_IX","title":"<code>TIMESTAMP_IX = 'timestamp'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name of the column in the DataFrame that contains time of interaction in seconds since epoch.</p>"},{"location":"documentation/io/#datarec.io.frameworks.recpack.datarec.DataRec.DEFAULT_FILENAME","title":"<code>DEFAULT_FILENAME</code>  <code>property</code>","text":"<p>Default filename that will be used if it is not specified by the user.</p>"},{"location":"documentation/pipe/","title":"Pipe Module Reference","text":"<p>This section provides the API reference for the <code>pipe</code> module.</p> <p>Note: This module is a placeholder for future functionality and is currently empty.</p>"},{"location":"documentation/pipeline/","title":"Pipeline Module Reference","text":"<p>This section provides the API reference for modules that handle the creation, management, and execution of reproducible data processing workflows.</p>"},{"location":"documentation/processing/","title":"Processing Module Reference","text":"<p>This section provides a detailed API reference for all modules related to processing datasets.</p>"},{"location":"documentation/processing/#datarec.processing.binarizer.Binarize","title":"<code>Binarize</code>","text":"<p>               Bases: <code>Processor</code></p> <p>A class for binarizing rating values in a dataset based on a given threshold. </p> <p>This class processes a dataset wrapped in a DataRec object and modifies the rating column based on the specified threshold. If <code>implicit</code> is set to True, rows with ratings below the threshold are removed, and the rating column is dropped. Otherwise, ratings are binarized to either <code>over_threshold</code> or <code>under_threshold</code> values.</p> Source code in <code>datarec/processing/binarizer.py</code> <pre><code>class Binarize(Processor):\n\n    \"\"\"\n    A class for binarizing rating values in a dataset based on a given threshold. \n\n    This class processes a dataset wrapped in a DataRec object and modifies the rating column\n    based on the specified threshold. If `implicit` is set to True, rows with ratings below\n    the threshold are removed, and the rating column is dropped. Otherwise, ratings are binarized\n    to either `over_threshold` or `under_threshold` values.\n    \"\"\"\n\n    def __init__(self, threshold: float, implicit: bool = False,\n                 over_threshold: float = 1, under_threshold: float = 0):\n        \"\"\"\n        Initializes the Binarize object.\n\n        Args:\n            threshold (float): The threshold for binarization.\n            implicit (bool): If True, removes rows below the threshold and drops the rating column.\n            over_threshold (int, float): The value assigned to ratings equal to or above the threshold.\n            under_threshold (int, float): The value assigned to ratings below the threshold.\n        \"\"\"\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self._threshold = threshold\n        self._over_threshold = over_threshold\n        self._under_threshold = under_threshold\n        self._implicit = implicit\n\n    def run(self, datarec: DataRec) -&gt; DataRec:\n\n        \"\"\"\n        Binarizes the rating values in the given dataset based on a threshold.\n\n        If `implicit` is True, removes rows where the rating is below the threshold\n        and drops the rating column. If `implicit` is False, replaces the rating\n        values with binary values (over_threshold if &gt;= threshold, under_threshold otherwise).\n\n        Args:\n            datarec (DataRec): The input dataset wrapped in a DataRec object.\n\n        Returns:\n            (DataRec): A new DataRec object with the processed dataset.\n        \"\"\"\n\n        dataset = datarec.data.copy()\n        column = datarec.rating_col\n\n        positive = dataset[column] &gt;= self._threshold\n\n        if self._implicit:\n            dataset = dataset[positive].copy()\n            dataset.drop(columns=[column], inplace=True)\n        else:\n            dataset[column] = self._over_threshold\n            dataset.loc[~positive, column] = self._under_threshold\n\n        result = self.output(datarec, dataset,\n                             step_info={'operation': self.__class__.__name__, 'params': self.params})\n\n        return result\n\n    @property\n    def binary_threshold(self) -&gt; float:\n        \"\"\"\n        Returns the rating threshold used to distinguish positive interactions.\n        \"\"\"\n        return self._threshold\n\n    @property\n    def over_threshold(self) -&gt; float:\n        \"\"\"\n        Returns the value assigned to ratings at or above the threshold.\n        \"\"\"\n        return self._over_threshold\n\n    @property\n    def under_threshold(self) -&gt; float:\n        \"\"\"\n        Returns the value assigned to ratings below the threshold.\n        \"\"\"\n        return self._under_threshold\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.binarizer.Binarize.binary_threshold","title":"<code>binary_threshold</code>  <code>property</code>","text":"<p>Returns the rating threshold used to distinguish positive interactions.</p>"},{"location":"documentation/processing/#datarec.processing.binarizer.Binarize.over_threshold","title":"<code>over_threshold</code>  <code>property</code>","text":"<p>Returns the value assigned to ratings at or above the threshold.</p>"},{"location":"documentation/processing/#datarec.processing.binarizer.Binarize.under_threshold","title":"<code>under_threshold</code>  <code>property</code>","text":"<p>Returns the value assigned to ratings below the threshold.</p>"},{"location":"documentation/processing/#datarec.processing.binarizer.Binarize.__init__","title":"<code>__init__(threshold, implicit=False, over_threshold=1, under_threshold=0)</code>","text":"<p>Initializes the Binarize object.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The threshold for binarization.</p> required <code>implicit</code> <code>bool</code> <p>If True, removes rows below the threshold and drops the rating column.</p> <code>False</code> <code>over_threshold</code> <code>(int, float)</code> <p>The value assigned to ratings equal to or above the threshold.</p> <code>1</code> <code>under_threshold</code> <code>(int, float)</code> <p>The value assigned to ratings below the threshold.</p> <code>0</code> Source code in <code>datarec/processing/binarizer.py</code> <pre><code>def __init__(self, threshold: float, implicit: bool = False,\n             over_threshold: float = 1, under_threshold: float = 0):\n    \"\"\"\n    Initializes the Binarize object.\n\n    Args:\n        threshold (float): The threshold for binarization.\n        implicit (bool): If True, removes rows below the threshold and drops the rating column.\n        over_threshold (int, float): The value assigned to ratings equal to or above the threshold.\n        under_threshold (int, float): The value assigned to ratings below the threshold.\n    \"\"\"\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self._threshold = threshold\n    self._over_threshold = over_threshold\n    self._under_threshold = under_threshold\n    self._implicit = implicit\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.binarizer.Binarize.run","title":"<code>run(datarec)</code>","text":"<p>Binarizes the rating values in the given dataset based on a threshold.</p> <p>If <code>implicit</code> is True, removes rows where the rating is below the threshold and drops the rating column. If <code>implicit</code> is False, replaces the rating values with binary values (over_threshold if &gt;= threshold, under_threshold otherwise).</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The input dataset wrapped in a DataRec object.</p> required <p>Returns:</p> Type Description <code>DataRec</code> <p>A new DataRec object with the processed dataset.</p> Source code in <code>datarec/processing/binarizer.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; DataRec:\n\n    \"\"\"\n    Binarizes the rating values in the given dataset based on a threshold.\n\n    If `implicit` is True, removes rows where the rating is below the threshold\n    and drops the rating column. If `implicit` is False, replaces the rating\n    values with binary values (over_threshold if &gt;= threshold, under_threshold otherwise).\n\n    Args:\n        datarec (DataRec): The input dataset wrapped in a DataRec object.\n\n    Returns:\n        (DataRec): A new DataRec object with the processed dataset.\n    \"\"\"\n\n    dataset = datarec.data.copy()\n    column = datarec.rating_col\n\n    positive = dataset[column] &gt;= self._threshold\n\n    if self._implicit:\n        dataset = dataset[positive].copy()\n        dataset.drop(columns=[column], inplace=True)\n    else:\n        dataset[column] = self._over_threshold\n        dataset.loc[~positive, column] = self._under_threshold\n\n    result = self.output(datarec, dataset,\n                         step_info={'operation': self.__class__.__name__, 'params': self.params})\n\n    return result\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.cold.ColdFilter","title":"<code>ColdFilter</code>","text":"<p>               Bases: <code>Processor</code></p> <p>A filtering class to retain only cold users or cold items, i.e., those with at most <code>interactions</code> interactions in the original DataRec dataset.</p> Source code in <code>datarec/processing/cold.py</code> <pre><code>class ColdFilter(Processor):\n    \"\"\"\n    A filtering class to retain only cold users or cold items, i.e., those with at most `interactions` interactions\n    in the original DataRec dataset.\n    \"\"\"\n\n    def __init__(self, interactions: int, mode: str = \"user\"):\n        \"\"\"\n        Initializes the ColdFilter object.\n\n        Args:\n            interactions (int): The maximum number of interactions a user or item can have to be retained.\n            mode (str): Filtering mode, either \"user\" for cold users or \"item\" for cold items.\n\n        Raises:\n            TypeError: If `interactions` is not an integer.\n            ValueError: If `mode` is not \"user\" or \"item\".\n        \"\"\"\n        if not isinstance(interactions, int):\n            raise TypeError('Interactions must be an integer.')\n\n        if mode not in {\"user\", \"item\"}:\n            raise ValueError('Mode must be \"user\" or \"item\".')\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.interactions = interactions\n        self.mode = mode\n\n    def run(self, datarec: DataRec) -&gt; DataRec:\n        \"\"\"\n        Filters the dataset to keep only cold users or cold items with at most `self.interactions` interactions.\n\n        Args:\n            datarec (DataRec): The input dataset wrapped in a DataRec object.\n\n        Returns:\n            (DataRec): A new DataRec object containing only the filtered users or items.\n        \"\"\"\n\n        dataset = datarec.data.copy()\n        group_col = datarec.user_col if self.mode == \"user\" else datarec.item_col\n        groups = dataset.groupby(group_col)\n        result = groups.filter(lambda x: len(x) &lt;= self.interactions).reset_index(drop=True)\n\n        return self.output(datarec, result, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.cold.ColdFilter.__init__","title":"<code>__init__(interactions, mode='user')</code>","text":"<p>Initializes the ColdFilter object.</p> <p>Parameters:</p> Name Type Description Default <code>interactions</code> <code>int</code> <p>The maximum number of interactions a user or item can have to be retained.</p> required <code>mode</code> <code>str</code> <p>Filtering mode, either \"user\" for cold users or \"item\" for cold items.</p> <code>'user'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>interactions</code> is not an integer.</p> <code>ValueError</code> <p>If <code>mode</code> is not \"user\" or \"item\".</p> Source code in <code>datarec/processing/cold.py</code> <pre><code>def __init__(self, interactions: int, mode: str = \"user\"):\n    \"\"\"\n    Initializes the ColdFilter object.\n\n    Args:\n        interactions (int): The maximum number of interactions a user or item can have to be retained.\n        mode (str): Filtering mode, either \"user\" for cold users or \"item\" for cold items.\n\n    Raises:\n        TypeError: If `interactions` is not an integer.\n        ValueError: If `mode` is not \"user\" or \"item\".\n    \"\"\"\n    if not isinstance(interactions, int):\n        raise TypeError('Interactions must be an integer.')\n\n    if mode not in {\"user\", \"item\"}:\n        raise ValueError('Mode must be \"user\" or \"item\".')\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.interactions = interactions\n    self.mode = mode\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.cold.ColdFilter.run","title":"<code>run(datarec)</code>","text":"<p>Filters the dataset to keep only cold users or cold items with at most <code>self.interactions</code> interactions.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The input dataset wrapped in a DataRec object.</p> required <p>Returns:</p> Type Description <code>DataRec</code> <p>A new DataRec object containing only the filtered users or items.</p> Source code in <code>datarec/processing/cold.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; DataRec:\n    \"\"\"\n    Filters the dataset to keep only cold users or cold items with at most `self.interactions` interactions.\n\n    Args:\n        datarec (DataRec): The input dataset wrapped in a DataRec object.\n\n    Returns:\n        (DataRec): A new DataRec object containing only the filtered users or items.\n    \"\"\"\n\n    dataset = datarec.data.copy()\n    group_col = datarec.user_col if self.mode == \"user\" else datarec.item_col\n    groups = dataset.groupby(group_col)\n    result = groups.filter(lambda x: len(x) &lt;= self.interactions).reset_index(drop=True)\n\n    return self.output(datarec, result, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.KCore","title":"<code>KCore</code>","text":"<p>This class filters a dataset based on a minimum number of records (core) for each group defined by a specific column.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>class KCore:\n\n    \"\"\"\n    This class filters a dataset based on a minimum number of records (core) for each group\n    defined by a specific column.\n    \"\"\"\n\n    def __init__(self, column: str, core: int):\n        \"\"\"\n        Initializes the KCore object.\n\n        Args:\n            column (str): The column name used to group the data (e.g., user or item).\n            core (int): The minimum number of records required for each group to be kept.\n\n        Raises:\n            TypeError: If 'core' is not an integer.\n        \"\"\"\n\n        if not isinstance(core, int):\n            raise TypeError('Core must be an integer.')\n\n        self._column = column\n        self._core = core\n\n    def run(self, dataset: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Filters the dataset by keeping only groups with at least the specified number of records.\n\n        Args:\n            dataset (pd.DataFrame): The dataset to be filtered.\n\n        Returns:\n            (pd.DataFrame): A new dataframe with groups filtered by the core condition.\n\n        Raises: \n            ValueError: If 'self._column' is not in the dataset.\n\n        \"\"\"\n\n        if self._column not in dataset.columns:\n            raise ValueError(f'Column \"{self._column}\" not in the dataset.')\n\n        dataset = dataset.copy()\n        groups = dataset.groupby([self._column])\n        dataset = groups.filter(lambda x: len(x) &gt;= self._core)\n        return dataset\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.KCore.__init__","title":"<code>__init__(column, core)</code>","text":"<p>Initializes the KCore object.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>str</code> <p>The column name used to group the data (e.g., user or item).</p> required <code>core</code> <code>int</code> <p>The minimum number of records required for each group to be kept.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If 'core' is not an integer.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def __init__(self, column: str, core: int):\n    \"\"\"\n    Initializes the KCore object.\n\n    Args:\n        column (str): The column name used to group the data (e.g., user or item).\n        core (int): The minimum number of records required for each group to be kept.\n\n    Raises:\n        TypeError: If 'core' is not an integer.\n    \"\"\"\n\n    if not isinstance(core, int):\n        raise TypeError('Core must be an integer.')\n\n    self._column = column\n    self._core = core\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.KCore.run","title":"<code>run(dataset)</code>","text":"<p>Filters the dataset by keeping only groups with at least the specified number of records.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataFrame</code> <p>The dataset to be filtered.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new dataframe with groups filtered by the core condition.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'self._column' is not in the dataset.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def run(self, dataset: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Filters the dataset by keeping only groups with at least the specified number of records.\n\n    Args:\n        dataset (pd.DataFrame): The dataset to be filtered.\n\n    Returns:\n        (pd.DataFrame): A new dataframe with groups filtered by the core condition.\n\n    Raises: \n        ValueError: If 'self._column' is not in the dataset.\n\n    \"\"\"\n\n    if self._column not in dataset.columns:\n        raise ValueError(f'Column \"{self._column}\" not in the dataset.')\n\n    dataset = dataset.copy()\n    groups = dataset.groupby([self._column])\n    dataset = groups.filter(lambda x: len(x) &gt;= self._core)\n    return dataset\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.UserKCore","title":"<code>UserKCore</code>","text":"<p>               Bases: <code>Processor</code></p> <p>Filters a dataset based on a minimum number of records (core) for each user.</p> <p>This class applies a KCore filter on the user column of the dataset.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>class UserKCore(Processor):\n    \"\"\"\n    Filters a dataset based on a minimum number of records (core) for each user.\n\n    This class applies a KCore filter on the user column of the dataset.\n    \"\"\"\n    def __init__(self, core: int):\n        \"\"\"\n        Initializes the UserKCore object.\n\n        Args:\n            core (int): The minimum number of records required for each user to be kept.\n\n        Raises: \n            TypeErrore: If 'core' is not an integer.\n        \"\"\"\n        if not isinstance(core, int):\n            raise TypeError('Core must be an integer.')\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.core = core\n\n    def run(self, datarec: DataRec) -&gt; DataRec:\n        \"\"\"\n        Filters the dataset by user, applying the KCore filter, and returns a new DataRec object\n        containing the filtered data.\n\n        Args:\n            datarec (DataRec): The DataRec object containing the dataset to be filtered.\n\n        Returns:\n            (DataRec): A new DataRec object with the filtered data.\n\n        \"\"\"\n\n        core_obj = KCore(column=datarec.user_col, core=self.core)\n        result = core_obj.run(datarec.data)\n\n        return self.output(datarec, result, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.UserKCore.__init__","title":"<code>__init__(core)</code>","text":"<p>Initializes the UserKCore object.</p> <p>Parameters:</p> Name Type Description Default <code>core</code> <code>int</code> <p>The minimum number of records required for each user to be kept.</p> required <p>Raises:</p> Type Description <code>TypeErrore</code> <p>If 'core' is not an integer.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def __init__(self, core: int):\n    \"\"\"\n    Initializes the UserKCore object.\n\n    Args:\n        core (int): The minimum number of records required for each user to be kept.\n\n    Raises: \n        TypeErrore: If 'core' is not an integer.\n    \"\"\"\n    if not isinstance(core, int):\n        raise TypeError('Core must be an integer.')\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.core = core\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.UserKCore.run","title":"<code>run(datarec)</code>","text":"<p>Filters the dataset by user, applying the KCore filter, and returns a new DataRec object containing the filtered data.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The DataRec object containing the dataset to be filtered.</p> required <p>Returns:</p> Type Description <code>DataRec</code> <p>A new DataRec object with the filtered data.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; DataRec:\n    \"\"\"\n    Filters the dataset by user, applying the KCore filter, and returns a new DataRec object\n    containing the filtered data.\n\n    Args:\n        datarec (DataRec): The DataRec object containing the dataset to be filtered.\n\n    Returns:\n        (DataRec): A new DataRec object with the filtered data.\n\n    \"\"\"\n\n    core_obj = KCore(column=datarec.user_col, core=self.core)\n    result = core_obj.run(datarec.data)\n\n    return self.output(datarec, result, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.ItemKCore","title":"<code>ItemKCore</code>","text":"<p>               Bases: <code>Processor</code></p> <p>Filters a dataset based on a minimum number of records (core) for each item.</p> <p>This class applies a KCore filter on the item column of the dataset.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>class ItemKCore(Processor):\n    \"\"\"\n    Filters a dataset based on a minimum number of records (core) for each item.\n\n    This class applies a KCore filter on the item column of the dataset.\n    \"\"\"\n    def __init__(self, core: int):\n        \"\"\"\n        Initializes the ItemKCore object.\n\n        Args:\n            core (int): The minimum number of records required for each item to be kept.\n\n        Raises:\n            TypeError: If \"core\" is not an integer.\n        \"\"\"\n\n        if not isinstance(core, int):\n            raise TypeError('Core must be an integer.')\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.core = core\n\n    def run(self, datarec: DataRec) -&gt; DataRec:\n        \"\"\"\n        Filters the dataset by item, applying the KCore filter, and returns a new DataRec object\n        containing the filtered data.\n\n        Args:\n            datarec (DataRec): The DataRec object containing the dataset to be filtered.\n\n        Returns:\n            (DataRec): A new DataRec object with the filtered data.\n        \"\"\"\n\n        core_obj = KCore(column=datarec.item_col, core=self.core)\n        result = core_obj.run(datarec.data)\n\n        return self.output(datarec, result, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.ItemKCore.__init__","title":"<code>__init__(core)</code>","text":"<p>Initializes the ItemKCore object.</p> <p>Parameters:</p> Name Type Description Default <code>core</code> <code>int</code> <p>The minimum number of records required for each item to be kept.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If \"core\" is not an integer.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def __init__(self, core: int):\n    \"\"\"\n    Initializes the ItemKCore object.\n\n    Args:\n        core (int): The minimum number of records required for each item to be kept.\n\n    Raises:\n        TypeError: If \"core\" is not an integer.\n    \"\"\"\n\n    if not isinstance(core, int):\n        raise TypeError('Core must be an integer.')\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.core = core\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.ItemKCore.run","title":"<code>run(datarec)</code>","text":"<p>Filters the dataset by item, applying the KCore filter, and returns a new DataRec object containing the filtered data.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The DataRec object containing the dataset to be filtered.</p> required <p>Returns:</p> Type Description <code>DataRec</code> <p>A new DataRec object with the filtered data.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; DataRec:\n    \"\"\"\n    Filters the dataset by item, applying the KCore filter, and returns a new DataRec object\n    containing the filtered data.\n\n    Args:\n        datarec (DataRec): The DataRec object containing the dataset to be filtered.\n\n    Returns:\n        (DataRec): A new DataRec object with the filtered data.\n    \"\"\"\n\n    core_obj = KCore(column=datarec.item_col, core=self.core)\n    result = core_obj.run(datarec.data)\n\n    return self.output(datarec, result, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.IterativeKCore","title":"<code>IterativeKCore</code>","text":"<p>Iteratively filters a dataset based on a set of columns and minimum core values.</p> <p>This class applies KCore filters to multiple columns and iteratively removes groups that do not meet the core requirement until no further changes occur.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>class IterativeKCore:\n    \"\"\"        \n    Iteratively filters a dataset based on a set of columns and minimum core values.\n\n    This class applies KCore filters to multiple columns and iteratively removes groups\n    that do not meet the core requirement until no further changes occur.\n    \"\"\"\n    def __init__(self, columns: list, cores: Union[int, list]):\n        \"\"\"\n        Initializes the IterativeKCore object.\n\n        Args:\n            columns (list): A list of column names to apply the KCore filter on.\n            cores (list of int or int): The minimum number of records required for each column to be kept.\n\n        Raises:\n            TypeError: If 'cores' in not a list or an integer.\n        \"\"\"\n\n        self._columns = columns\n\n        if isinstance(cores, list):\n            self._cores = list(zip(columns, cores))\n        elif isinstance(cores, int):\n            self._cores = [(c, cores) for c in columns]\n        else:\n            raise TypeError('Cores must be a list or an integer.')\n\n    def run(self, dataset: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Iteratively applies the KCore filters on the dataset until no changes occur, then returns the filtered dataset.\n\n        Args:\n            dataset (pd.DataFrame): The dataset to be iteratively filtered.\n\n        Returns:\n            (pd.DataFrame): The filtered dataset after all iterations.\n        \"\"\"\n\n        data = dataset.copy()\n\n        filters = {c: KCore(column=c, core=k) for c, k in self._cores}\n        checks = [False for _ in self._columns]\n        prev_len = len(data)\n\n        while not all(checks):\n            checks = []\n            for c, f in filters.items():\n                data = f.run(data)\n                checks.append((prev_len - len(data)) == 0)\n                prev_len = len(data)\n\n        return data\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.IterativeKCore.__init__","title":"<code>__init__(columns, cores)</code>","text":"<p>Initializes the IterativeKCore object.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list</code> <p>A list of column names to apply the KCore filter on.</p> required <code>cores</code> <code>list of int or int</code> <p>The minimum number of records required for each column to be kept.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If 'cores' in not a list or an integer.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def __init__(self, columns: list, cores: Union[int, list]):\n    \"\"\"\n    Initializes the IterativeKCore object.\n\n    Args:\n        columns (list): A list of column names to apply the KCore filter on.\n        cores (list of int or int): The minimum number of records required for each column to be kept.\n\n    Raises:\n        TypeError: If 'cores' in not a list or an integer.\n    \"\"\"\n\n    self._columns = columns\n\n    if isinstance(cores, list):\n        self._cores = list(zip(columns, cores))\n    elif isinstance(cores, int):\n        self._cores = [(c, cores) for c in columns]\n    else:\n        raise TypeError('Cores must be a list or an integer.')\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.IterativeKCore.run","title":"<code>run(dataset)</code>","text":"<p>Iteratively applies the KCore filters on the dataset until no changes occur, then returns the filtered dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataFrame</code> <p>The dataset to be iteratively filtered.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The filtered dataset after all iterations.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def run(self, dataset: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Iteratively applies the KCore filters on the dataset until no changes occur, then returns the filtered dataset.\n\n    Args:\n        dataset (pd.DataFrame): The dataset to be iteratively filtered.\n\n    Returns:\n        (pd.DataFrame): The filtered dataset after all iterations.\n    \"\"\"\n\n    data = dataset.copy()\n\n    filters = {c: KCore(column=c, core=k) for c, k in self._cores}\n    checks = [False for _ in self._columns]\n    prev_len = len(data)\n\n    while not all(checks):\n        checks = []\n        for c, f in filters.items():\n            data = f.run(data)\n            checks.append((prev_len - len(data)) == 0)\n            prev_len = len(data)\n\n    return data\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.UserItemIterativeKCore","title":"<code>UserItemIterativeKCore</code>","text":"<p>               Bases: <code>Processor</code></p> <p>Iteratively filters a dataset based on both user and item columns with specified core values.</p> <p>This class applies the IterativeKCore filter to both the user and item columns of the dataset.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>class UserItemIterativeKCore(Processor):\n\n    \"\"\"\n    Iteratively filters a dataset based on both user and item columns with specified core values.\n\n    This class applies the IterativeKCore filter to both the user and item columns of the dataset.\n    \"\"\"\n    def __init__(self, cores: Union[int, list]):\n        \"\"\"\n        Initializes the UserItemIterativeKCore object.\n\n        Args:\n            cores (list or int): A list of core values for the user and item columns.\n\n        Raises:\n            TypeError: If \"cores\" is not a list or an integer.\n        \"\"\"\n\n        if not isinstance(cores, (list, int)):\n            raise TypeError('Cores must be a list or an integer.')\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self._cores = cores\n\n    def run(self, datarec: DataRec) -&gt; DataRec:\n        \"\"\"\n        Applies the iterative KCore filter to both user and item columns, and returns a new DataRec object\n        containing the filtered data.\n\n        Args:\n            datarec (DataRec): The DataRec object containing the dataset to be filtered.\n\n        Returns:\n            (DataRec): A new DataRec object with the filtered data.\n        \"\"\"\n\n        core_obj = IterativeKCore(columns=[datarec.user_col, datarec.item_col],\n                                  cores=self._cores)\n        result = core_obj.run(datarec.data)\n\n        return self.output(datarec, result, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.UserItemIterativeKCore.__init__","title":"<code>__init__(cores)</code>","text":"<p>Initializes the UserItemIterativeKCore object.</p> <p>Parameters:</p> Name Type Description Default <code>cores</code> <code>list or int</code> <p>A list of core values for the user and item columns.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If \"cores\" is not a list or an integer.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def __init__(self, cores: Union[int, list]):\n    \"\"\"\n    Initializes the UserItemIterativeKCore object.\n\n    Args:\n        cores (list or int): A list of core values for the user and item columns.\n\n    Raises:\n        TypeError: If \"cores\" is not a list or an integer.\n    \"\"\"\n\n    if not isinstance(cores, (list, int)):\n        raise TypeError('Cores must be a list or an integer.')\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self._cores = cores\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.UserItemIterativeKCore.run","title":"<code>run(datarec)</code>","text":"<p>Applies the iterative KCore filter to both user and item columns, and returns a new DataRec object containing the filtered data.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The DataRec object containing the dataset to be filtered.</p> required <p>Returns:</p> Type Description <code>DataRec</code> <p>A new DataRec object with the filtered data.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; DataRec:\n    \"\"\"\n    Applies the iterative KCore filter to both user and item columns, and returns a new DataRec object\n    containing the filtered data.\n\n    Args:\n        datarec (DataRec): The DataRec object containing the dataset to be filtered.\n\n    Returns:\n        (DataRec): A new DataRec object with the filtered data.\n    \"\"\"\n\n    core_obj = IterativeKCore(columns=[datarec.user_col, datarec.item_col],\n                              cores=self._cores)\n    result = core_obj.run(datarec.data)\n\n    return self.output(datarec, result, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.NRoundsKCore","title":"<code>NRoundsKCore</code>","text":"<p>Filters a dataset based on a minimum number of records (core) for each column over multiple rounds.</p> <p>This class applies KCore filters iteratively over a specified number of rounds.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>class NRoundsKCore:\n    \"\"\"\n    Filters a dataset based on a minimum number of records (core) for each column over multiple rounds.\n\n    This class applies KCore filters iteratively over a specified number of rounds.\n    \"\"\"\n\n    def __init__(self, columns: list, cores: Union[int, list], rounds: int):\n        \"\"\"\n        Initializes the NRoundsKCore object.\n\n        Args:\n            columns (list): A list of column names to apply the KCore filter on.\n            cores (list of int or int): The minimum number of records required for each column to be kept.\n            rounds (int): The number of rounds to apply the filtering process.\n\n        Raises:\n            TypeError: If 'cores' is not a list or an integer.\n            TypeError: If 'rounds' is not an integer.\n        \"\"\"\n\n        self._columns = columns\n\n        if isinstance(cores, list):\n            self._cores = list(zip(columns, cores))\n        elif isinstance(cores, int):\n            self._cores = [(c, cores) for c in columns]\n        else:\n            raise TypeError('Cores must be a list or an integer.')\n\n        if not isinstance(rounds, int):\n            raise TypeError('Rounds must be an integer.')\n\n        self._rounds = rounds\n\n    def run(self, dataset: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Applies the KCore filters over the specified number of rounds and returns the filtered dataset.\n\n        Args:\n            dataset (pd.DataFrame): The dataset to be filtered.\n\n        Returns:\n            (pd.DataFrame): The dataset after filtering over the specified number of rounds.\n        \"\"\"\n\n        data = dataset.copy()\n\n        filters = {c: KCore(column=c, core=k) for c, k in self._cores}\n        checks = [False for _ in self._columns]\n        prev_len = len(data)\n\n        for _ in range(self._rounds) or all(checks):\n            checks = []\n            for c, f in filters.items():\n                data = f.run(data)\n                checks.append((prev_len - len(data)) == 0)\n                prev_len = len(data)\n        return data\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.NRoundsKCore.__init__","title":"<code>__init__(columns, cores, rounds)</code>","text":"<p>Initializes the NRoundsKCore object.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list</code> <p>A list of column names to apply the KCore filter on.</p> required <code>cores</code> <code>list of int or int</code> <p>The minimum number of records required for each column to be kept.</p> required <code>rounds</code> <code>int</code> <p>The number of rounds to apply the filtering process.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If 'cores' is not a list or an integer.</p> <code>TypeError</code> <p>If 'rounds' is not an integer.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def __init__(self, columns: list, cores: Union[int, list], rounds: int):\n    \"\"\"\n    Initializes the NRoundsKCore object.\n\n    Args:\n        columns (list): A list of column names to apply the KCore filter on.\n        cores (list of int or int): The minimum number of records required for each column to be kept.\n        rounds (int): The number of rounds to apply the filtering process.\n\n    Raises:\n        TypeError: If 'cores' is not a list or an integer.\n        TypeError: If 'rounds' is not an integer.\n    \"\"\"\n\n    self._columns = columns\n\n    if isinstance(cores, list):\n        self._cores = list(zip(columns, cores))\n    elif isinstance(cores, int):\n        self._cores = [(c, cores) for c in columns]\n    else:\n        raise TypeError('Cores must be a list or an integer.')\n\n    if not isinstance(rounds, int):\n        raise TypeError('Rounds must be an integer.')\n\n    self._rounds = rounds\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.NRoundsKCore.run","title":"<code>run(dataset)</code>","text":"<p>Applies the KCore filters over the specified number of rounds and returns the filtered dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataFrame</code> <p>The dataset to be filtered.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The dataset after filtering over the specified number of rounds.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def run(self, dataset: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Applies the KCore filters over the specified number of rounds and returns the filtered dataset.\n\n    Args:\n        dataset (pd.DataFrame): The dataset to be filtered.\n\n    Returns:\n        (pd.DataFrame): The dataset after filtering over the specified number of rounds.\n    \"\"\"\n\n    data = dataset.copy()\n\n    filters = {c: KCore(column=c, core=k) for c, k in self._cores}\n    checks = [False for _ in self._columns]\n    prev_len = len(data)\n\n    for _ in range(self._rounds) or all(checks):\n        checks = []\n        for c, f in filters.items():\n            data = f.run(data)\n            checks.append((prev_len - len(data)) == 0)\n            prev_len = len(data)\n    return data\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.UserItemNRoundsKCore","title":"<code>UserItemNRoundsKCore</code>","text":"<p>               Bases: <code>Processor</code></p> <p>Filters a dataset based on both user and item columns with specified core values over multiple rounds.</p> <p>This class applies the NRoundsKCore filter to both the user and item columns of the dataset.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>class UserItemNRoundsKCore(Processor):\n\n    \"\"\"\n    Filters a dataset based on both user and item columns with specified core values over multiple rounds.\n\n    This class applies the NRoundsKCore filter to both the user and item columns of the dataset.\n    \"\"\"\n\n    def __init__(self, cores: Union[int, list], rounds: int):\n        \"\"\"\n        Initializes the UserItemNRoundsKCore object.\n\n        Args:\n            cores (int, list): A list of core values for the user and item columns.\n            rounds (int): The number of rounds to apply the filtering process.\n\n        Raises:\n            TypeError: If 'cores' is not a list or an integer.\n            TypeError: If 'rounds' is not an integer.\n        \"\"\"\n\n        if not isinstance(cores, (list, int)):\n            raise TypeError('Cores must be a list or an integer.')\n\n        if not isinstance(rounds, int):\n            raise TypeError('Rounds must be an integer.')\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self._cores = cores\n        self._rounds = rounds\n\n    def run(self, datarec: DataRec) -&gt; DataRec:\n        \"\"\"\n        Applies the NRoundsKCore filter to both user and item columns over multiple rounds, and returns a new DataRec object\n        containing the filtered data.\n\n        Args:\n            datarec (DataRec): The DataRec object containing the dataset to be filtered.\n\n        Returns:\n            (DataRec): A new DataRec object with the filtered data.\n        \"\"\"\n\n        core_obj = NRoundsKCore(columns=[datarec.user_col, datarec.item_col],\n                                cores=self._cores, rounds=self._rounds)\n        result = core_obj.run(datarec.data)\n\n        return self.output(datarec, result, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.UserItemNRoundsKCore.__init__","title":"<code>__init__(cores, rounds)</code>","text":"<p>Initializes the UserItemNRoundsKCore object.</p> <p>Parameters:</p> Name Type Description Default <code>cores</code> <code>(int, list)</code> <p>A list of core values for the user and item columns.</p> required <code>rounds</code> <code>int</code> <p>The number of rounds to apply the filtering process.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If 'cores' is not a list or an integer.</p> <code>TypeError</code> <p>If 'rounds' is not an integer.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def __init__(self, cores: Union[int, list], rounds: int):\n    \"\"\"\n    Initializes the UserItemNRoundsKCore object.\n\n    Args:\n        cores (int, list): A list of core values for the user and item columns.\n        rounds (int): The number of rounds to apply the filtering process.\n\n    Raises:\n        TypeError: If 'cores' is not a list or an integer.\n        TypeError: If 'rounds' is not an integer.\n    \"\"\"\n\n    if not isinstance(cores, (list, int)):\n        raise TypeError('Cores must be a list or an integer.')\n\n    if not isinstance(rounds, int):\n        raise TypeError('Rounds must be an integer.')\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self._cores = cores\n    self._rounds = rounds\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.kcore.UserItemNRoundsKCore.run","title":"<code>run(datarec)</code>","text":"<p>Applies the NRoundsKCore filter to both user and item columns over multiple rounds, and returns a new DataRec object containing the filtered data.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The DataRec object containing the dataset to be filtered.</p> required <p>Returns:</p> Type Description <code>DataRec</code> <p>A new DataRec object with the filtered data.</p> Source code in <code>datarec/processing/kcore.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; DataRec:\n    \"\"\"\n    Applies the NRoundsKCore filter to both user and item columns over multiple rounds, and returns a new DataRec object\n    containing the filtered data.\n\n    Args:\n        datarec (DataRec): The DataRec object containing the dataset to be filtered.\n\n    Returns:\n        (DataRec): A new DataRec object with the filtered data.\n    \"\"\"\n\n    core_obj = NRoundsKCore(columns=[datarec.user_col, datarec.item_col],\n                            cores=self._cores, rounds=self._rounds)\n    result = core_obj.run(datarec.data)\n\n    return self.output(datarec, result, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.processor.Processor","title":"<code>Processor</code>","text":"<p>Utility class for handling the output of preprocessing steps on <code>DataRec</code>  objects.</p> <p>This class provides functionality to build a new <code>DataRec</code> from  transformation results while updating the processing pipeline accordingly.</p> Source code in <code>datarec/processing/processor.py</code> <pre><code>class Processor:\n    \"\"\"\n    Utility class for handling the output of preprocessing steps on `DataRec` \n    objects.\n\n    This class provides functionality to build a new `DataRec` from \n    transformation results while updating the processing pipeline accordingly.\n    \"\"\"\n\n    @staticmethod\n    def output(datarec: DataRec, result: pd.DataFrame, step_info: dict) -&gt; DataRec:\n        \"\"\"\n        Create a new `DataRec` object from a transformation result and update \n        the processing pipeline with a new step.\n\n        Args:\n            datarec (DataRec): The original `DataRec` object from which the \n                transformation is derived.\n            result (pd.DataFrame): The result of the transformation.\n            step_info (dict): Metadata of the transformation.\n\n        Returns:\n            (DataRec): A new `DataRec` object wrapping the transformation result\n                with an updated pipeline.\n        \"\"\"\n        pipeline = datarec.pipeline.copy()\n        pipeline.add_step(name='process', operation=step_info['operation'], params=step_info['params'])\n\n        new_datarec = DataRec(\n            RawData(result,\n                    user=datarec.user_col,\n                    item=datarec.item_col,\n                    rating=datarec.rating_col if datarec.rating_col in result.columns else None,\n                    timestamp=datarec.timestamp_col),\n            derives_from=datarec,\n            dataset_name=datarec.dataset_name,\n            pipeline=pipeline\n        )\n\n        return new_datarec\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.processor.Processor.output","title":"<code>output(datarec, result, step_info)</code>  <code>staticmethod</code>","text":"<p>Create a new <code>DataRec</code> object from a transformation result and update  the processing pipeline with a new step.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The original <code>DataRec</code> object from which the  transformation is derived.</p> required <code>result</code> <code>DataFrame</code> <p>The result of the transformation.</p> required <code>step_info</code> <code>dict</code> <p>Metadata of the transformation.</p> required <p>Returns:</p> Type Description <code>DataRec</code> <p>A new <code>DataRec</code> object wrapping the transformation result with an updated pipeline.</p> Source code in <code>datarec/processing/processor.py</code> <pre><code>@staticmethod\ndef output(datarec: DataRec, result: pd.DataFrame, step_info: dict) -&gt; DataRec:\n    \"\"\"\n    Create a new `DataRec` object from a transformation result and update \n    the processing pipeline with a new step.\n\n    Args:\n        datarec (DataRec): The original `DataRec` object from which the \n            transformation is derived.\n        result (pd.DataFrame): The result of the transformation.\n        step_info (dict): Metadata of the transformation.\n\n    Returns:\n        (DataRec): A new `DataRec` object wrapping the transformation result\n            with an updated pipeline.\n    \"\"\"\n    pipeline = datarec.pipeline.copy()\n    pipeline.add_step(name='process', operation=step_info['operation'], params=step_info['params'])\n\n    new_datarec = DataRec(\n        RawData(result,\n                user=datarec.user_col,\n                item=datarec.item_col,\n                rating=datarec.rating_col if datarec.rating_col in result.columns else None,\n                timestamp=datarec.timestamp_col),\n        derives_from=datarec,\n        dataset_name=datarec.dataset_name,\n        pipeline=pipeline\n    )\n\n    return new_datarec\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.rating.FilterByRatingThreshold","title":"<code>FilterByRatingThreshold</code>","text":"<p>               Bases: <code>Processor</code></p> <p>Filters the dataset by removing interactions with a rating below a given threshold.</p> Source code in <code>datarec/processing/rating.py</code> <pre><code>class FilterByRatingThreshold(Processor):\n    \"\"\"\n    Filters the dataset by removing interactions with a rating below a given threshold.\n    \"\"\"\n\n    def __init__(self, rating_threshold: float):\n        \"\"\"\n        Initializes the FilterByRatingThreshold object.\n\n        Args:\n            rating_threshold (float): The minimum rating required for an interaction to be kept.\n\n        Raises:\n            ValueError: If `rating_threshold` is a negative number.\n        \"\"\"\n        if not isinstance(rating_threshold, (int, float)):\n            raise ValueError(\"rating_threshold must be a number.\")\n        if rating_threshold &lt; 0:\n            raise ValueError(\"rating_threshold must be non-negative.\")\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.rating_threshold = rating_threshold\n\n    def run(self, datarec: DataRec) -&gt; DataRec:\n        \"\"\"\n        Filters interactions with a rating below the threshold.\n\n        Args:\n            datarec (DataRec): The input dataset wrapped in a DataRec object.\n\n        Returns:\n            (DataRec): A new DataRec object with the processed dataset.\n        \"\"\"\n\n        dataset = datarec.data\n        filtered_data = dataset[dataset[datarec.rating_col] &gt;= self.rating_threshold]\n\n        return self.output(datarec, filtered_data, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.rating.FilterByRatingThreshold.__init__","title":"<code>__init__(rating_threshold)</code>","text":"<p>Initializes the FilterByRatingThreshold object.</p> <p>Parameters:</p> Name Type Description Default <code>rating_threshold</code> <code>float</code> <p>The minimum rating required for an interaction to be kept.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>rating_threshold</code> is a negative number.</p> Source code in <code>datarec/processing/rating.py</code> <pre><code>def __init__(self, rating_threshold: float):\n    \"\"\"\n    Initializes the FilterByRatingThreshold object.\n\n    Args:\n        rating_threshold (float): The minimum rating required for an interaction to be kept.\n\n    Raises:\n        ValueError: If `rating_threshold` is a negative number.\n    \"\"\"\n    if not isinstance(rating_threshold, (int, float)):\n        raise ValueError(\"rating_threshold must be a number.\")\n    if rating_threshold &lt; 0:\n        raise ValueError(\"rating_threshold must be non-negative.\")\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.rating_threshold = rating_threshold\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.rating.FilterByRatingThreshold.run","title":"<code>run(datarec)</code>","text":"<p>Filters interactions with a rating below the threshold.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The input dataset wrapped in a DataRec object.</p> required <p>Returns:</p> Type Description <code>DataRec</code> <p>A new DataRec object with the processed dataset.</p> Source code in <code>datarec/processing/rating.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; DataRec:\n    \"\"\"\n    Filters interactions with a rating below the threshold.\n\n    Args:\n        datarec (DataRec): The input dataset wrapped in a DataRec object.\n\n    Returns:\n        (DataRec): A new DataRec object with the processed dataset.\n    \"\"\"\n\n    dataset = datarec.data\n    filtered_data = dataset[dataset[datarec.rating_col] &gt;= self.rating_threshold]\n\n    return self.output(datarec, filtered_data, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.rating.FilterByUserMeanRating","title":"<code>FilterByUserMeanRating</code>","text":"<p>               Bases: <code>Processor</code></p> <p>Filters the dataset by removing interactions with a rating below the user's average rating.</p> <p>This filter calculates the average rating given by each user and removes interactions where the rating is below that average.</p> Source code in <code>datarec/processing/rating.py</code> <pre><code>class FilterByUserMeanRating(Processor):\n    \"\"\"\n    Filters the dataset by removing interactions with a rating below the user's average rating.\n\n    This filter calculates the average rating given by each user and removes\n    interactions where the rating is below that average.\n    \"\"\"\n\n    def run(self, datarec: DataRec) -&gt; DataRec:\n        \"\"\"\n        Filters interactions with a rating below the user's mean rating.\n\n        Args:\n            datarec (DataRec): The input dataset wrapped in a DataRec object.\n\n        Returns:\n            (DataRec): A new DataRec object with the processed dataset.\n        \"\"\"\n\n        dataset = datarec.data\n        user_means = dataset.groupby(datarec.user_col)[datarec.rating_col].mean()\n\n        filtered_data = dataset[\n            dataset.apply(lambda row: row[datarec.rating_col] &gt;= user_means[row[datarec.user_col]], axis=1)\n        ]\n\n        return self.output(datarec, filtered_data, {'operation': self.__class__.__name__, 'params': ''})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.rating.FilterByUserMeanRating.run","title":"<code>run(datarec)</code>","text":"<p>Filters interactions with a rating below the user's mean rating.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The input dataset wrapped in a DataRec object.</p> required <p>Returns:</p> Type Description <code>DataRec</code> <p>A new DataRec object with the processed dataset.</p> Source code in <code>datarec/processing/rating.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; DataRec:\n    \"\"\"\n    Filters interactions with a rating below the user's mean rating.\n\n    Args:\n        datarec (DataRec): The input dataset wrapped in a DataRec object.\n\n    Returns:\n        (DataRec): A new DataRec object with the processed dataset.\n    \"\"\"\n\n    dataset = datarec.data\n    user_means = dataset.groupby(datarec.user_col)[datarec.rating_col].mean()\n\n    filtered_data = dataset[\n        dataset.apply(lambda row: row[datarec.rating_col] &gt;= user_means[row[datarec.user_col]], axis=1)\n    ]\n\n    return self.output(datarec, filtered_data, {'operation': self.__class__.__name__, 'params': ''})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.rating.FilterOutDuplicatedInteractions","title":"<code>FilterOutDuplicatedInteractions</code>","text":"<p>               Bases: <code>Processor</code></p> <p>Filters a dataset by removing duplicated (user, item) interactions based on a specified strategy.</p> Source code in <code>datarec/processing/rating.py</code> <pre><code>class FilterOutDuplicatedInteractions(Processor):\n    \"\"\"\n    Filters a dataset by removing duplicated (user, item) interactions based on a specified strategy.\n    \"\"\"\n\n    STRATEGIES = ['first', 'last', 'earliest', 'latest', 'random']\n\n    def __init__(self, keep='first', random_seed=42):\n        \"\"\"\n        Initializes the FilterOutDuplicatedInteractions object.\n\n        Args:\n            keep (str): Strategy to determine which interaction to keep when duplicates are found.\n                Must be one of ['first', 'last', 'earliest', 'latest', 'random'].\n            random_seed (int): Random seed used for reproducibility when using the 'random' strategy.\n\n        Raises:\n            ValueError: If the provided strategy (`keep`) is not among the supported options.\n        \"\"\"\n\n        if keep not in self.STRATEGIES:\n            raise ValueError(f\"Invalid strategy '{keep}'. Choose from {self.STRATEGIES}.\")\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.keep = keep\n        self.random_seed = random_seed\n\n    def run(self, datarec: DataRec, verbose=True) -&gt; DataRec:\n        \"\"\"\n        Filter out duplicated (user, item) interactions in the dataset using the specified strategy.\n\n        Args:\n            datarec (DataRec): An object containing the dataset and metadata (user, item, timestamp columns, etc.)\n            verbose (bool): Whether to print logging information during execution.\n\n        Returns:\n            (DataRec): A new DataRec object with duplicated (user, item) interactions removed according to the selected strategy.\n\n        Raises:\n            ValueError: If Date colum is not provided for 'earliest' and 'latest' strategies.\n            ValueError: If the provided strategy (`keep`) is not among the supported options.\n        \"\"\"\n\n        if verbose:\n            print(f'Running filter-out duplicated interactions with strategy {self.keep}')\n            print(f'Filtering DataRec: {datarec.dataset_name}')\n\n        dataset = datarec.data\n        subset = [datarec.user_col, datarec.item_col]\n\n        # Random strategy\n        if self.keep == 'random':\n            dataset = dataset.sample(frac=1, random_state=self.random_seed).drop_duplicates(subset=subset, keep='first')\n\n        # Ordering-based strategies\n        elif self.keep in ['first', 'last']:\n            dataset = dataset.drop_duplicates(subset=subset, keep=self.keep)\n\n        # Temporal strategies\n        elif self.keep in ['earliest', 'latest']:\n            if datarec.timestamp_col is None:\n                raise ValueError(f\"Date column is required for '{self.keep}' strategy.\")\n            dataset = dataset.sort_values(by=datarec.timestamp_col, ascending=True)\n            if self.keep == 'earliest':\n                dataset = dataset.drop_duplicates(subset=subset, keep='first')\n            else:\n                dataset = dataset.drop_duplicates(subset=subset, keep='last')\n        else:\n            raise ValueError(f\"Invalid strategy '{self.keep}'. Choose from {self.STRATEGIES}.\")\n\n        dataset = dataset.sort_values(by=[datarec.user_col, datarec.item_col], ascending=True)\n\n        return self.output(datarec, dataset, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.rating.FilterOutDuplicatedInteractions.__init__","title":"<code>__init__(keep='first', random_seed=42)</code>","text":"<p>Initializes the FilterOutDuplicatedInteractions object.</p> <p>Parameters:</p> Name Type Description Default <code>keep</code> <code>str</code> <p>Strategy to determine which interaction to keep when duplicates are found. Must be one of ['first', 'last', 'earliest', 'latest', 'random'].</p> <code>'first'</code> <code>random_seed</code> <code>int</code> <p>Random seed used for reproducibility when using the 'random' strategy.</p> <code>42</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided strategy (<code>keep</code>) is not among the supported options.</p> Source code in <code>datarec/processing/rating.py</code> <pre><code>def __init__(self, keep='first', random_seed=42):\n    \"\"\"\n    Initializes the FilterOutDuplicatedInteractions object.\n\n    Args:\n        keep (str): Strategy to determine which interaction to keep when duplicates are found.\n            Must be one of ['first', 'last', 'earliest', 'latest', 'random'].\n        random_seed (int): Random seed used for reproducibility when using the 'random' strategy.\n\n    Raises:\n        ValueError: If the provided strategy (`keep`) is not among the supported options.\n    \"\"\"\n\n    if keep not in self.STRATEGIES:\n        raise ValueError(f\"Invalid strategy '{keep}'. Choose from {self.STRATEGIES}.\")\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.keep = keep\n    self.random_seed = random_seed\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.rating.FilterOutDuplicatedInteractions.run","title":"<code>run(datarec, verbose=True)</code>","text":"<p>Filter out duplicated (user, item) interactions in the dataset using the specified strategy.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>An object containing the dataset and metadata (user, item, timestamp columns, etc.)</p> required <code>verbose</code> <code>bool</code> <p>Whether to print logging information during execution.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataRec</code> <p>A new DataRec object with duplicated (user, item) interactions removed according to the selected strategy.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If Date colum is not provided for 'earliest' and 'latest' strategies.</p> <code>ValueError</code> <p>If the provided strategy (<code>keep</code>) is not among the supported options.</p> Source code in <code>datarec/processing/rating.py</code> <pre><code>def run(self, datarec: DataRec, verbose=True) -&gt; DataRec:\n    \"\"\"\n    Filter out duplicated (user, item) interactions in the dataset using the specified strategy.\n\n    Args:\n        datarec (DataRec): An object containing the dataset and metadata (user, item, timestamp columns, etc.)\n        verbose (bool): Whether to print logging information during execution.\n\n    Returns:\n        (DataRec): A new DataRec object with duplicated (user, item) interactions removed according to the selected strategy.\n\n    Raises:\n        ValueError: If Date colum is not provided for 'earliest' and 'latest' strategies.\n        ValueError: If the provided strategy (`keep`) is not among the supported options.\n    \"\"\"\n\n    if verbose:\n        print(f'Running filter-out duplicated interactions with strategy {self.keep}')\n        print(f'Filtering DataRec: {datarec.dataset_name}')\n\n    dataset = datarec.data\n    subset = [datarec.user_col, datarec.item_col]\n\n    # Random strategy\n    if self.keep == 'random':\n        dataset = dataset.sample(frac=1, random_state=self.random_seed).drop_duplicates(subset=subset, keep='first')\n\n    # Ordering-based strategies\n    elif self.keep in ['first', 'last']:\n        dataset = dataset.drop_duplicates(subset=subset, keep=self.keep)\n\n    # Temporal strategies\n    elif self.keep in ['earliest', 'latest']:\n        if datarec.timestamp_col is None:\n            raise ValueError(f\"Date column is required for '{self.keep}' strategy.\")\n        dataset = dataset.sort_values(by=datarec.timestamp_col, ascending=True)\n        if self.keep == 'earliest':\n            dataset = dataset.drop_duplicates(subset=subset, keep='first')\n        else:\n            dataset = dataset.drop_duplicates(subset=subset, keep='last')\n    else:\n        raise ValueError(f\"Invalid strategy '{self.keep}'. Choose from {self.STRATEGIES}.\")\n\n    dataset = dataset.sort_values(by=[datarec.user_col, datarec.item_col], ascending=True)\n\n    return self.output(datarec, dataset, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.temporal.FilterByTime","title":"<code>FilterByTime</code>","text":"<p>               Bases: <code>Processor</code></p> <p>Filters the dataset based on a time threshold and specified drop condition.</p> <p>This class allows filtering a dataset by a time threshold, either dropping records before or after the specified time.</p> Source code in <code>datarec/processing/temporal.py</code> <pre><code>class FilterByTime(Processor):\n    \"\"\"\n    Filters the dataset based on a time threshold and specified drop condition.\n\n    This class allows filtering a dataset by a time threshold, either dropping\n    records before or after the specified time.\n    \"\"\"\n\n    def __init__(self, time_threshold: float = 0, drop: str = 'after'):\n        \"\"\"  \n        Initializes the FilterByTime object.\n\n        Args:\n            time_threshold (float): The time threshold used for filtering. The dataset\n                                    will be filtered based on this value.\n            drop (str, optional): Specifies whether to drop records 'before' or 'after' the time threshold.\n\n        Raises:\n            ValueError: If `time_threshold` is negative or not a float, or if drop is\n                        neither 'after' nor 'before'.\n        \"\"\"\n        if not isinstance(time_threshold, (int, float)):\n            raise ValueError('time_threshold must be positive number.')\n        if isinstance(time_threshold, float) and time_threshold &lt; 0:\n            raise ValueError('time_threshold must be positive number.')\n\n        if drop not in ['after', 'before']:\n            raise ValueError(f'Drop must be \"after\" or \"before\".')\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n        self.time_threshold = time_threshold\n        self.drop = drop\n\n    def run(self, datarec: DataRec) -&gt; DataRec:\n        \"\"\"\n        Filters the dataset of the given DataRec based on the specified time threshold\n        and drop condition, returning a new DataRec object with the filtered data.\n\n        Args:\n            datarec (DataRec): The input dataset wrapped in a DataRec object.\n\n        Returns:\n            (DataRec): A new DataRec object with the processed dataset.\n\n        Raises:\n            TypeError: If the DataRec does not contain temporal information.\n        \"\"\"\n\n        if datarec.timestamp_col is None:\n            raise TypeError('This DataRec does not contain temporal information')\n\n        dataset = datarec.data\n\n        if self.drop == 'before':\n            data = dataset[dataset[datarec.timestamp_col] &lt; self.time_threshold]\n        else:\n            data = dataset[dataset[datarec.timestamp_col] &gt;= self.time_threshold]\n\n        return self.output(datarec, data, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.temporal.FilterByTime.__init__","title":"<code>__init__(time_threshold=0, drop='after')</code>","text":"<p>Initializes the FilterByTime object.</p> <p>Parameters:</p> Name Type Description Default <code>time_threshold</code> <code>float</code> <p>The time threshold used for filtering. The dataset                     will be filtered based on this value.</p> <code>0</code> <code>drop</code> <code>str</code> <p>Specifies whether to drop records 'before' or 'after' the time threshold.</p> <code>'after'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>time_threshold</code> is negative or not a float, or if drop is         neither 'after' nor 'before'.</p> Source code in <code>datarec/processing/temporal.py</code> <pre><code>def __init__(self, time_threshold: float = 0, drop: str = 'after'):\n    \"\"\"  \n    Initializes the FilterByTime object.\n\n    Args:\n        time_threshold (float): The time threshold used for filtering. The dataset\n                                will be filtered based on this value.\n        drop (str, optional): Specifies whether to drop records 'before' or 'after' the time threshold.\n\n    Raises:\n        ValueError: If `time_threshold` is negative or not a float, or if drop is\n                    neither 'after' nor 'before'.\n    \"\"\"\n    if not isinstance(time_threshold, (int, float)):\n        raise ValueError('time_threshold must be positive number.')\n    if isinstance(time_threshold, float) and time_threshold &lt; 0:\n        raise ValueError('time_threshold must be positive number.')\n\n    if drop not in ['after', 'before']:\n        raise ValueError(f'Drop must be \"after\" or \"before\".')\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n    self.time_threshold = time_threshold\n    self.drop = drop\n</code></pre>"},{"location":"documentation/processing/#datarec.processing.temporal.FilterByTime.run","title":"<code>run(datarec)</code>","text":"<p>Filters the dataset of the given DataRec based on the specified time threshold and drop condition, returning a new DataRec object with the filtered data.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The input dataset wrapped in a DataRec object.</p> required <p>Returns:</p> Type Description <code>DataRec</code> <p>A new DataRec object with the processed dataset.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the DataRec does not contain temporal information.</p> Source code in <code>datarec/processing/temporal.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; DataRec:\n    \"\"\"\n    Filters the dataset of the given DataRec based on the specified time threshold\n    and drop condition, returning a new DataRec object with the filtered data.\n\n    Args:\n        datarec (DataRec): The input dataset wrapped in a DataRec object.\n\n    Returns:\n        (DataRec): A new DataRec object with the processed dataset.\n\n    Raises:\n        TypeError: If the DataRec does not contain temporal information.\n    \"\"\"\n\n    if datarec.timestamp_col is None:\n        raise TypeError('This DataRec does not contain temporal information')\n\n    dataset = datarec.data\n\n    if self.drop == 'before':\n        data = dataset[dataset[datarec.timestamp_col] &lt; self.time_threshold]\n    else:\n        data = dataset[dataset[datarec.timestamp_col] &gt;= self.time_threshold]\n\n    return self.output(datarec, data, {'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/","title":"Splitters Module Reference","text":"<p>This section provides a detailed API reference for all modules related to splitting datasets into training, validation, and test sets.</p>"},{"location":"documentation/splitters/#core-splitting-utilities","title":"Core Splitting Utilities","text":"<p>These modules define the base class and common utilities used by all splitters.</p>"},{"location":"documentation/splitters/#datarec.splitters.splitter.Splitter","title":"<code>Splitter</code>","text":"<p>Base class for dataset splitters.</p> <p>This class provides a common interface for splitting datasets into training, validation, and test sets. Subclasses should implement specific splitting strategies.</p> Source code in <code>datarec/splitters/splitter.py</code> <pre><code>class Splitter:\n    \"\"\"\n    Base class for dataset splitters.\n\n    This class provides a common interface for splitting datasets into training,\n    validation, and test sets. Subclasses should implement specific splitting strategies.\n    \"\"\"\n\n    @staticmethod\n    def output(datarec: DataRec, train: pd.DataFrame, test: pd.DataFrame, validation: pd.DataFrame,\n               step_info: Dict[str, Dict]) -&gt; Dict[str, DataRec]:\n        \"\"\"\n        Creates a dictionary of `DataRec` objects for train, test, and validation splits.\n\n        Args:\n            datarec (DataRec): The original dataset wrapped in a `DataRec` object.\n            train (pd.DataFrame): The training split of the dataset.\n            test (pd.DataFrame): The test split of the dataset.\n            validation (pd.DataFrame): The validation split of the dataset.\n            step_info (Dict[str, Dict]): Metadata of the transformation.\n\n        Returns:\n            (Dict[str, DataRec]): A dictionary containing the split datasets:\n                - 'train': The training dataset as a `DataRec` object (if not empty).\n                - 'test': The test dataset as a `DataRec` object (if not empty).\n                - 'val': The validation dataset as a `DataRec` object (if not empty).\n        \"\"\"\n\n        pipeline = datarec.pipeline.copy()\n        pipeline.add_step(name='split', operation=step_info['operation'], params=step_info['params'])\n\n        result = dict()\n        for k, d in zip(['train', 'test', 'val'], [train, test, validation]):\n            if len(d) &gt; 0:\n                new_datarec = DataRec(RawData(d,\n                                              user=datarec.user_col,\n                                              item=datarec.item_col,\n                                              rating=datarec.rating_col,\n                                              timestamp=datarec.timestamp_col),\n                                      derives_from=datarec,\n                                      dataset_name=datarec.dataset_name,\n                                      pipeline=pipeline.copy())\n                result[k] = new_datarec\n        return result\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.splitter.Splitter.output","title":"<code>output(datarec, train, test, validation, step_info)</code>  <code>staticmethod</code>","text":"<p>Creates a dictionary of <code>DataRec</code> objects for train, test, and validation splits.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The original dataset wrapped in a <code>DataRec</code> object.</p> required <code>train</code> <code>DataFrame</code> <p>The training split of the dataset.</p> required <code>test</code> <code>DataFrame</code> <p>The test split of the dataset.</p> required <code>validation</code> <code>DataFrame</code> <p>The validation split of the dataset.</p> required <code>step_info</code> <code>Dict[str, Dict]</code> <p>Metadata of the transformation.</p> required <p>Returns:</p> Type Description <code>Dict[str, DataRec]</code> <p>A dictionary containing the split datasets: - 'train': The training dataset as a <code>DataRec</code> object (if not empty). - 'test': The test dataset as a <code>DataRec</code> object (if not empty). - 'val': The validation dataset as a <code>DataRec</code> object (if not empty).</p> Source code in <code>datarec/splitters/splitter.py</code> <pre><code>@staticmethod\ndef output(datarec: DataRec, train: pd.DataFrame, test: pd.DataFrame, validation: pd.DataFrame,\n           step_info: Dict[str, Dict]) -&gt; Dict[str, DataRec]:\n    \"\"\"\n    Creates a dictionary of `DataRec` objects for train, test, and validation splits.\n\n    Args:\n        datarec (DataRec): The original dataset wrapped in a `DataRec` object.\n        train (pd.DataFrame): The training split of the dataset.\n        test (pd.DataFrame): The test split of the dataset.\n        validation (pd.DataFrame): The validation split of the dataset.\n        step_info (Dict[str, Dict]): Metadata of the transformation.\n\n    Returns:\n        (Dict[str, DataRec]): A dictionary containing the split datasets:\n            - 'train': The training dataset as a `DataRec` object (if not empty).\n            - 'test': The test dataset as a `DataRec` object (if not empty).\n            - 'val': The validation dataset as a `DataRec` object (if not empty).\n    \"\"\"\n\n    pipeline = datarec.pipeline.copy()\n    pipeline.add_step(name='split', operation=step_info['operation'], params=step_info['params'])\n\n    result = dict()\n    for k, d in zip(['train', 'test', 'val'], [train, test, validation]):\n        if len(d) &gt; 0:\n            new_datarec = DataRec(RawData(d,\n                                          user=datarec.user_col,\n                                          item=datarec.item_col,\n                                          rating=datarec.rating_col,\n                                          timestamp=datarec.timestamp_col),\n                                  derives_from=datarec,\n                                  dataset_name=datarec.dataset_name,\n                                  pipeline=pipeline.copy())\n            result[k] = new_datarec\n    return result\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.utils.random_sample","title":"<code>random_sample(dataframe, seed, n_samples=1)</code>","text":"<p>Randomly selects a specified number of samples from a given DataFrame.</p> <p>This function splits the input DataFrame into two subsets: - One containing <code>n_samples</code> randomly selected rows. - One containing the remaining rows after the selection.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The input DataFrame from which to sample.</p> required <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> required <code>n_samples</code> <code>int</code> <p>The number of samples to extract. Must be at least 1. Default is 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <ul> <li>The first DataFrame contains the remaining data after sampling.</li> <li>The second DataFrame contains the randomly selected samples.</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>n_samples</code> is less than 1 or lesser/greater than the number of rows in the DataFrame.</p> Source code in <code>datarec/splitters/utils.py</code> <pre><code>def random_sample(dataframe: pd.DataFrame, seed: int, n_samples: int = 1) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Randomly selects a specified number of samples from a given DataFrame.\n\n    This function splits the input DataFrame into two subsets:\n    - One containing `n_samples` randomly selected rows.\n    - One containing the remaining rows after the selection.\n\n    Args:\n        dataframe (pd.DataFrame): The input DataFrame from which to sample.\n        seed (int): Random seed for reproducibility.\n        n_samples (int, optional): The number of samples to extract. Must be at least 1. Default is 1.\n\n    Returns:\n        (Tuple[pd.DataFrame, pd.DataFrame]):\n            - The first DataFrame contains the remaining data after sampling.\n            - The second DataFrame contains the randomly selected samples.\n\n    Raises:\n        ValueError: If `n_samples` is less than 1 or lesser/greater than the number of rows in the DataFrame.\n    \"\"\"\n\n    if n_samples &lt; 1:\n        raise ValueError('number of samples must be greater than 1.')\n\n    if n_samples &gt; len(dataframe):\n        raise ValueError('number of samples greater than the number of samples in the DataFrame.')\n\n    samples = dataframe.sample(n=n_samples, random_state=seed)\n\n    if len(samples) != n_samples:\n        raise ValueError('number of samples lesser or greater than the number of rows in the DataFrame.')\n    else:\n        return dataframe.drop(samples.index), samples\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.utils.max_by_col","title":"<code>max_by_col(dataframe, discriminative_column, seed)</code>","text":"<p>Selects the row with the minimum value in the specified column from the given DataFrame. If multiple rows have the same minimum value, one is randomly selected.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <code>discriminative_column</code> <code>str</code> <p>The column used to determine the minimum value.</p> required <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <code>DataFrame</code> <ul> <li>The first DataFrame contains the remaining rows after removing the selected row.</li> </ul> <code>Tuple[DataFrame, DataFrame]</code> <ul> <li>The second DataFrame contains the selected row with the minimum value.</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified column is not present in the DataFrame.</p> <code>ValueError</code> <p>If no candidates are found (should not happen unless DataFrame is empty).</p> <code>ValueError</code> <p>If the random selection fails to return exactly one row.</p> Source code in <code>datarec/splitters/utils.py</code> <pre><code>def max_by_col(dataframe: pd.DataFrame, discriminative_column: str, seed: int) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Selects the row with the minimum value in the specified column from the given DataFrame.\n    If multiple rows have the same minimum value, one is randomly selected.\n\n    Args:\n        dataframe (pd.DataFrame): The input DataFrame.\n        discriminative_column (str): The column used to determine the minimum value.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        (Tuple[pd.DataFrame, pd.DataFrame]):\n        - The first DataFrame contains the remaining rows after removing the selected row.\n        - The second DataFrame contains the selected row with the minimum value.\n\n    Raises:\n        ValueError: If the specified column is not present in the DataFrame.\n        ValueError: If no candidates are found (should not happen unless DataFrame is empty).\n        ValueError: If the random selection fails to return exactly one row.\n    \"\"\"\n\n    if discriminative_column not in dataframe:\n        raise ValueError(f'Column \\'{discriminative_column}\\' must be in the dataframe.')\n\n    max_value = dataframe[discriminative_column].max()\n    candidates = dataframe.loc[dataframe[discriminative_column] == max_value]\n    n_candidates = len(candidates)\n\n    if n_candidates == 0:\n        raise ValueError('No candidate.')\n    elif n_candidates == 1:\n        return dataframe.drop(candidates.index), candidates\n    else:\n        candidates = candidates.sample(n=1, random_state=seed)\n        if len(candidates) != 1:\n            raise ValueError('Number of candidates lesser or greater than 1.')\n        return dataframe.drop(candidates.index), candidates\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.utils.temporal_holdout","title":"<code>temporal_holdout(dataframe, test_ratio, val_ratio, temporal_col)</code>","text":"<p>Splits a dataset into training, validation, and test sets based on temporal ordering.</p> <p>The function sorts the dataset according to a specified timestamp column and assigns the oldest interactions to the training set, followed by the validation set (if applicable), and the most recent interactions to the test set.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The input dataset containing interaction data.</p> required <code>test_ratio</code> <code>float</code> <p>The proportion of the dataset to allocate to the test set. Must be between 0 and 1.</p> required <code>val_ratio</code> <code>float</code> <p>The proportion of the dataset to allocate to the validation set. Must be between 0 and 1.</p> required <code>temporal_col</code> <code>str</code> <p>The name of the column containing timestamp information.</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame, DataFrame]</code> <p>A tuple containing the train, validation, and test sets.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>test_ratio</code> or <code>val_ratio</code> are not in the range [0, 1].</p> Source code in <code>datarec/splitters/utils.py</code> <pre><code>def temporal_holdout(dataframe: pd.DataFrame, test_ratio: float, val_ratio: float, temporal_col: str) \\\n        -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Splits a dataset into training, validation, and test sets based on temporal ordering.\n\n    The function sorts the dataset according to a specified timestamp column and assigns\n    the oldest interactions to the training set, followed by the validation set (if applicable),\n    and the most recent interactions to the test set.\n\n    Args:\n        dataframe (pd.DataFrame): The input dataset containing interaction data.\n        test_ratio (float): The proportion of the dataset to allocate to the test set. Must be between 0 and 1.\n        val_ratio (float): The proportion of the dataset to allocate to the validation set. Must be between 0 and 1.\n        temporal_col (str): The name of the column containing timestamp information.\n\n    Returns:\n        (Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]): A tuple containing the train, validation, and test sets.\n\n    Raises:\n        ValueError: If `test_ratio` or `val_ratio` are not in the range [0, 1].\n    \"\"\"\n\n    if test_ratio &lt; 0 or test_ratio &gt; 1:\n        raise ValueError('test ratio must be between 0 and 1.')\n\n    if val_ratio &lt; 0 or val_ratio &gt; 1:\n        raise ValueError('val ratio must be between 0 and 1.')\n\n    train, test, val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n    total_samples = len(dataframe)\n\n    test_samples = round(total_samples * test_ratio)\n    train_samples = total_samples - test_samples\n    val_samples = round(train_samples * val_ratio)\n\n    train_samples = total_samples - test_samples - val_samples\n\n    assert (train_samples + val_samples + test_samples) == total_samples\n\n    ordered = dataframe.sort_values(by=temporal_col)\n\n    train = ordered.iloc[:train_samples]\n    if val_samples:\n        val = ordered.iloc[train_samples:(train_samples + val_samples)]\n    if test_samples:\n        test = ordered.iloc[(train_samples + val_samples):]\n\n    assert len(train) == train_samples\n    assert len(val) == val_samples\n    assert len(test) == test_samples\n\n    return train, test, val\n</code></pre>"},{"location":"documentation/splitters/#uniform-splitting-strategies","title":"Uniform Splitting Strategies","text":"<p>These splitters operate on the entire dataset globally.</p>"},{"location":"documentation/splitters/#datarec.splitters.uniform.hold_out.RandomHoldOut","title":"<code>RandomHoldOut</code>","text":"<p>               Bases: <code>Splitter</code></p> <p>Implements a random holdout split for recommendation datasets.</p> <p>This splitter partitions the dataset into training, validation, and test sets using a random sampling approach. The proportions of the dataset allocated to the validation and test sets are controlled by <code>val_ratio</code> and <code>test_ratio</code>, respectively.</p> Source code in <code>datarec/splitters/uniform/hold_out.py</code> <pre><code>class RandomHoldOut(Splitter):\n    \"\"\"\n    Implements a random holdout split for recommendation datasets.\n\n    This splitter partitions the dataset into training, validation, and test sets\n    using a random sampling approach. The proportions of the dataset allocated to\n    the validation and test sets are controlled by `val_ratio` and `test_ratio`, respectively.\n\n    \"\"\"\n\n    def __init__(self, test_ratio: float = 0, val_ratio: float = 0, seed: int = 42):\n        \"\"\"\n        Initializes the RandomHoldOut object.\n\n        Args:\n            test_ratio (float, optional): The proportion of the dataset to include in the test set.\n                Must be between 0 and 1. Default is 0.\n            val_ratio (float, optional): The proportion of the training set to include in the validation set.\n                Must be between 0 and 1. Default is 0.\n            seed (int, optional): The random seed for reproducibility. Defaults to 42.\n\n        Raises:\n            ValueError: If `test_ratio` or `val_ratio` is not in the range [0, 1].\n        \"\"\"\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.test_ratio = test_ratio\n        self.val_ratio = val_ratio\n        self.seed = seed\n\n    @property\n    def test_ratio(self) -&gt; float:\n        \"\"\"\n        The proportion of the dataset for the test set.\n        \"\"\"\n        return self._test_ratio\n\n    @test_ratio.setter\n    def test_ratio(self, value: float) -&gt; None:\n        \"\"\"\n        Sets the proportion of the dataset for the test set.\n\n        Args:\n            value (float): The proportion to allocate to the test set.\n                Must be between 0 and 1.\n\n        Raises:\n            ValueError: If `value` is not in the range [0, 1].\n        \"\"\"\n        if value &lt; 0 or value &gt; 1:\n            raise ValueError('ratio must be between 0 and 1')\n        self._test_ratio = value\n\n    @property\n    def val_ratio(self) -&gt; float:\n        \"\"\"\n        The proportion of the dataset for the validation set.\n        \"\"\"\n        return self._val_ratio\n\n    @val_ratio.setter\n    def val_ratio(self, value: float) -&gt; None:\n        \"\"\"\n        Sets the proportion of the dataset for the validation set.\n\n        Args:\n            value (float): The proportion to allocate to the validation set.\n                Must be between 0 and 1.\n\n        Raises:\n            ValueError: If `value` is not in the range [0, 1].\n        \"\"\"\n        if value &lt; 0 or value &gt; 1:\n            raise ValueError('ratio must be between 0 and 1')\n        self._val_ratio = value\n\n    def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n        \"\"\"\n        Splits the dataset into training, validation, and test sets according to the specified ratios, \n        with the val_ratio being applied to the dataset after the test set has been partitioned.\n\n        Args:\n            datarec (DataRec): The dataset to be split.\n\n        Returns:\n            (Dict[str, DataRec]): A dictionary with the following keys:\n                - \"train\": The training dataset (`DataRec`).\n                - \"test\": The test dataset (`DataRec`), if `test_ratio` &gt; 0.\n                - \"val\": The validation dataset (`DataRec`), if `val_ratio` &gt; 0.\n        \"\"\"\n\n        train, val, test = datarec.data, pd.DataFrame(), pd.DataFrame()\n\n        if self.test_ratio:\n            train, test = split(train, test_size=self._test_ratio, random_state=self.seed)\n\n        if self.val_ratio:\n            train, val = split(train, test_size=self._val_ratio, random_state=self.seed)\n\n        return self.output(datarec=datarec, train=train, test=test, validation=val,\n                           step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.uniform.hold_out.RandomHoldOut.test_ratio","title":"<code>test_ratio</code>  <code>property</code> <code>writable</code>","text":"<p>The proportion of the dataset for the test set.</p>"},{"location":"documentation/splitters/#datarec.splitters.uniform.hold_out.RandomHoldOut.val_ratio","title":"<code>val_ratio</code>  <code>property</code> <code>writable</code>","text":"<p>The proportion of the dataset for the validation set.</p>"},{"location":"documentation/splitters/#datarec.splitters.uniform.hold_out.RandomHoldOut.__init__","title":"<code>__init__(test_ratio=0, val_ratio=0, seed=42)</code>","text":"<p>Initializes the RandomHoldOut object.</p> <p>Parameters:</p> Name Type Description Default <code>test_ratio</code> <code>float</code> <p>The proportion of the dataset to include in the test set. Must be between 0 and 1. Default is 0.</p> <code>0</code> <code>val_ratio</code> <code>float</code> <p>The proportion of the training set to include in the validation set. Must be between 0 and 1. Default is 0.</p> <code>0</code> <code>seed</code> <code>int</code> <p>The random seed for reproducibility. Defaults to 42.</p> <code>42</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>test_ratio</code> or <code>val_ratio</code> is not in the range [0, 1].</p> Source code in <code>datarec/splitters/uniform/hold_out.py</code> <pre><code>def __init__(self, test_ratio: float = 0, val_ratio: float = 0, seed: int = 42):\n    \"\"\"\n    Initializes the RandomHoldOut object.\n\n    Args:\n        test_ratio (float, optional): The proportion of the dataset to include in the test set.\n            Must be between 0 and 1. Default is 0.\n        val_ratio (float, optional): The proportion of the training set to include in the validation set.\n            Must be between 0 and 1. Default is 0.\n        seed (int, optional): The random seed for reproducibility. Defaults to 42.\n\n    Raises:\n        ValueError: If `test_ratio` or `val_ratio` is not in the range [0, 1].\n    \"\"\"\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.test_ratio = test_ratio\n    self.val_ratio = val_ratio\n    self.seed = seed\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.uniform.hold_out.RandomHoldOut.run","title":"<code>run(datarec)</code>","text":"<p>Splits the dataset into training, validation, and test sets according to the specified ratios,  with the val_ratio being applied to the dataset after the test set has been partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The dataset to be split.</p> required <p>Returns:</p> Type Description <code>Dict[str, DataRec]</code> <p>A dictionary with the following keys: - \"train\": The training dataset (<code>DataRec</code>). - \"test\": The test dataset (<code>DataRec</code>), if <code>test_ratio</code> &gt; 0. - \"val\": The validation dataset (<code>DataRec</code>), if <code>val_ratio</code> &gt; 0.</p> Source code in <code>datarec/splitters/uniform/hold_out.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n    \"\"\"\n    Splits the dataset into training, validation, and test sets according to the specified ratios, \n    with the val_ratio being applied to the dataset after the test set has been partitioned.\n\n    Args:\n        datarec (DataRec): The dataset to be split.\n\n    Returns:\n        (Dict[str, DataRec]): A dictionary with the following keys:\n            - \"train\": The training dataset (`DataRec`).\n            - \"test\": The test dataset (`DataRec`), if `test_ratio` &gt; 0.\n            - \"val\": The validation dataset (`DataRec`), if `val_ratio` &gt; 0.\n    \"\"\"\n\n    train, val, test = datarec.data, pd.DataFrame(), pd.DataFrame()\n\n    if self.test_ratio:\n        train, test = split(train, test_size=self._test_ratio, random_state=self.seed)\n\n    if self.val_ratio:\n        train, val = split(train, test_size=self._val_ratio, random_state=self.seed)\n\n    return self.output(datarec=datarec, train=train, test=test, validation=val,\n                       step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.uniform.temporal.hold_out.TemporalHoldOut","title":"<code>TemporalHoldOut</code>","text":"<p>               Bases: <code>Splitter</code></p> <p>Implements a temporal hold-out splitting strategy for recommendation datasets.</p> <p>This splitter partitions a dataset into training, validation, and test sets based on the timestamps associated with interactions. The training set contains the oldest interactions, while the test set contains the most recent ones.</p> Source code in <code>datarec/splitters/uniform/temporal/hold_out.py</code> <pre><code>class TemporalHoldOut(Splitter):\n    \"\"\"\n    Implements a temporal hold-out splitting strategy for recommendation datasets.\n\n    This splitter partitions a dataset into training, validation, and test sets based on\n    the timestamps associated with interactions. The training set contains the oldest interactions,\n    while the test set contains the most recent ones.\n\n    \"\"\"\n\n    def __init__(self, test_ratio: float = 0, val_ratio: float = 0):\n\n        \"\"\"\n        Initializes the TemporalHoldOut object.\n        Args:\n            test_ratio (float, optional): The proportion of the dataset to allocate to the test set.\n                Must be between 0 and 1. Default is 0.\n            val_ratio (float, optional): The proportion of the dataset to allocate to the validation set.\n                Must be between 0 and 1. Default is 0.\n\n        Raises:\n            ValueError: If `test_ratio` or `val_ratio` are not in the range [0, 1].\n        \"\"\"\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.test_ratio = test_ratio\n        self.val_ratio = val_ratio\n\n    @property\n    def test_ratio(self) -&gt; float:\n        \"The proportion of the dataset allocated to the test set.\"\n        return self._test_ratio\n\n    @test_ratio.setter\n    def test_ratio(self, value: float) -&gt; None:\n        \"\"\"\n        Sets the test ratio.\n\n        Args:\n            value (float): The proportion of the dataset to allocate to the test set.\n                Must be between 0 and 1.\n\n        Raises:\n            ValueError: If `value` is not in the range [0, 1].\n        \"\"\"\n        if value &lt; 0 or value &gt; 1:\n            raise ValueError('ratio must be between 0 and 1')\n        self._test_ratio = value\n\n    @property\n    def val_ratio(self) -&gt; float:\n        \"\"\"\n        The proportion of the dataset allocated to the validation set.\n        \"\"\"\n        return self._val_ratio\n\n    @val_ratio.setter\n    def val_ratio(self, value: float) -&gt; None:\n        \"\"\"\n        Sets the validation ratio.\n\n        Args:\n            value (float): The proportion of the dataset to allocate to the validation set.\n                Must be between 0 and 1.\n\n        Raises:\n            ValueError: If `value` is not in the range [0, 1].\n        \"\"\"\n        if value &lt; 0 or value &gt; 1:\n            raise ValueError('ratio must be between 0 and 1')\n        self._val_ratio = value\n\n    def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n        \"\"\"\n        Splits the dataset using a temporal hold-out strategy.\n\n        This method partitions the dataset into training, validation, and test sets based on\n        the timestamps present in the `datarec` object. The split is performed such that the\n        training set contains older interactions, while the test set contains more recent ones.\n\n        Args:\n            datarec (DataRec): A DataRec object containing the dataset and a timestamp column.\n\n        Returns:\n            (Dict[str, DataRec]): A dictionary with three keys:\n                - `'train'`: A DataRec object containing the training set.\n                - `'val'`: A DataRec object containing the validation set (if `val_ratio` &gt; 0).\n                - `'test'`: A DataRec object containing the test set (if `test_ratio` &gt; 0).\n\n        Raises:\n            TypeError: If the `datarec` object does not contain a timestamp column.\n        \"\"\"\n\n        if datarec.timestamp_col is None:\n            raise TypeError('This DataRec does not contain temporal information')\n\n        train, test, val = temporal_holdout(dataframe=datarec.data,\n                                            test_ratio=self.test_ratio, val_ratio=self.val_ratio,\n                                            temporal_col=datarec.timestamp_col)\n\n        return self.output(datarec, train, test, val,\n                           step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.uniform.temporal.hold_out.TemporalHoldOut.test_ratio","title":"<code>test_ratio</code>  <code>property</code> <code>writable</code>","text":"<p>The proportion of the dataset allocated to the test set.</p>"},{"location":"documentation/splitters/#datarec.splitters.uniform.temporal.hold_out.TemporalHoldOut.val_ratio","title":"<code>val_ratio</code>  <code>property</code> <code>writable</code>","text":"<p>The proportion of the dataset allocated to the validation set.</p>"},{"location":"documentation/splitters/#datarec.splitters.uniform.temporal.hold_out.TemporalHoldOut.__init__","title":"<code>__init__(test_ratio=0, val_ratio=0)</code>","text":"<p>Initializes the TemporalHoldOut object. Args:     test_ratio (float, optional): The proportion of the dataset to allocate to the test set.         Must be between 0 and 1. Default is 0.     val_ratio (float, optional): The proportion of the dataset to allocate to the validation set.         Must be between 0 and 1. Default is 0.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>test_ratio</code> or <code>val_ratio</code> are not in the range [0, 1].</p> Source code in <code>datarec/splitters/uniform/temporal/hold_out.py</code> <pre><code>def __init__(self, test_ratio: float = 0, val_ratio: float = 0):\n\n    \"\"\"\n    Initializes the TemporalHoldOut object.\n    Args:\n        test_ratio (float, optional): The proportion of the dataset to allocate to the test set.\n            Must be between 0 and 1. Default is 0.\n        val_ratio (float, optional): The proportion of the dataset to allocate to the validation set.\n            Must be between 0 and 1. Default is 0.\n\n    Raises:\n        ValueError: If `test_ratio` or `val_ratio` are not in the range [0, 1].\n    \"\"\"\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.test_ratio = test_ratio\n    self.val_ratio = val_ratio\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.uniform.temporal.hold_out.TemporalHoldOut.run","title":"<code>run(datarec)</code>","text":"<p>Splits the dataset using a temporal hold-out strategy.</p> <p>This method partitions the dataset into training, validation, and test sets based on the timestamps present in the <code>datarec</code> object. The split is performed such that the training set contains older interactions, while the test set contains more recent ones.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>A DataRec object containing the dataset and a timestamp column.</p> required <p>Returns:</p> Type Description <code>Dict[str, DataRec]</code> <p>A dictionary with three keys: - <code>'train'</code>: A DataRec object containing the training set. - <code>'val'</code>: A DataRec object containing the validation set (if <code>val_ratio</code> &gt; 0). - <code>'test'</code>: A DataRec object containing the test set (if <code>test_ratio</code> &gt; 0).</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the <code>datarec</code> object does not contain a timestamp column.</p> Source code in <code>datarec/splitters/uniform/temporal/hold_out.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n    \"\"\"\n    Splits the dataset using a temporal hold-out strategy.\n\n    This method partitions the dataset into training, validation, and test sets based on\n    the timestamps present in the `datarec` object. The split is performed such that the\n    training set contains older interactions, while the test set contains more recent ones.\n\n    Args:\n        datarec (DataRec): A DataRec object containing the dataset and a timestamp column.\n\n    Returns:\n        (Dict[str, DataRec]): A dictionary with three keys:\n            - `'train'`: A DataRec object containing the training set.\n            - `'val'`: A DataRec object containing the validation set (if `val_ratio` &gt; 0).\n            - `'test'`: A DataRec object containing the test set (if `test_ratio` &gt; 0).\n\n    Raises:\n        TypeError: If the `datarec` object does not contain a timestamp column.\n    \"\"\"\n\n    if datarec.timestamp_col is None:\n        raise TypeError('This DataRec does not contain temporal information')\n\n    train, test, val = temporal_holdout(dataframe=datarec.data,\n                                        test_ratio=self.test_ratio, val_ratio=self.val_ratio,\n                                        temporal_col=datarec.timestamp_col)\n\n    return self.output(datarec, train, test, val,\n                       step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.uniform.temporal.threshold.TemporalThresholdSplit","title":"<code>TemporalThresholdSplit</code>","text":"<p>               Bases: <code>Splitter</code></p> <p>Splits a dataset into training, validation, and test sets based on two timestamp thresholds.</p> <p>The dataset is divided such that: - The training set contains interactions occurring strictly before <code>val_threshold</code>. - The validation set contains interactions occurring between <code>val_threshold</code> (inclusive)   and <code>test_threshold</code> (exclusive). - The test set contains interactions occurring at or after <code>test_threshold</code>.</p> Source code in <code>datarec/splitters/uniform/temporal/threshold.py</code> <pre><code>class TemporalThresholdSplit(Splitter):\n    \"\"\"\n    Splits a dataset into training, validation, and test sets based on two timestamp thresholds.\n\n    The dataset is divided such that:\n    - The training set contains interactions occurring strictly before `val_threshold`.\n    - The validation set contains interactions occurring between `val_threshold` (inclusive)\n      and `test_threshold` (exclusive).\n    - The test set contains interactions occurring at or after `test_threshold`.\n    \"\"\"\n\n    def __init__(self, val_threshold: float, test_threshold: float):\n        \"\"\"Initializes the TemporalThresholdSplit object.\n\n        Args:\n            val_threshold (float): The timestamp value that defines the split between training and validation.\n            test_threshold (float): The timestamp value that defines the split between validation and test.\n\n        Raises:\n            ValueError: If `val_threshold` is not strictly less than `test_threshold`.\n        \"\"\"\n\n        if val_threshold &gt;= test_threshold:\n            raise ValueError('val_threshold must be strictly less than test_threshold')\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.val_threshold = val_threshold\n        self.test_threshold = test_threshold\n\n    def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n        \"\"\"\n        Splits the dataset into training, validation, and test sets based on two thresholds.\n\n        Args:\n            datarec (DataRec): A DataRec object containing the dataset with a timestamp column.\n\n        Returns:\n            Dict[str, DataRec]: A dictionary with:\n                - `'train'`: Training set (timestamps &lt; `val_threshold`).\n                - `'val'`: Validation set (timestamps between `val_threshold` and `test_threshold`).\n                - `'test'`: Test set (timestamps &gt;= `test_threshold`).\n\n        Raises:\n            TypeError: If the `datarec` object does not contain a timestamp column.\n        \"\"\"\n\n        if datarec.timestamp_col is None:\n            raise TypeError('This DataRec does not contain temporal information')\n\n        dataset = datarec.data\n\n        train = dataset[dataset[datarec.timestamp_col] &lt; self.val_threshold]\n\n        val = dataset[(dataset[datarec.timestamp_col] &gt;= self.val_threshold) &amp;\n                      (dataset[datarec.timestamp_col] &lt; self.test_threshold)]\n\n        test = dataset[dataset[datarec.timestamp_col] &gt;= self.test_threshold]\n\n        return self.output(datarec, train, test, val,\n                           step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.uniform.temporal.threshold.TemporalThresholdSplit.__init__","title":"<code>__init__(val_threshold, test_threshold)</code>","text":"<p>Initializes the TemporalThresholdSplit object.</p> <p>Parameters:</p> Name Type Description Default <code>val_threshold</code> <code>float</code> <p>The timestamp value that defines the split between training and validation.</p> required <code>test_threshold</code> <code>float</code> <p>The timestamp value that defines the split between validation and test.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>val_threshold</code> is not strictly less than <code>test_threshold</code>.</p> Source code in <code>datarec/splitters/uniform/temporal/threshold.py</code> <pre><code>def __init__(self, val_threshold: float, test_threshold: float):\n    \"\"\"Initializes the TemporalThresholdSplit object.\n\n    Args:\n        val_threshold (float): The timestamp value that defines the split between training and validation.\n        test_threshold (float): The timestamp value that defines the split between validation and test.\n\n    Raises:\n        ValueError: If `val_threshold` is not strictly less than `test_threshold`.\n    \"\"\"\n\n    if val_threshold &gt;= test_threshold:\n        raise ValueError('val_threshold must be strictly less than test_threshold')\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.val_threshold = val_threshold\n    self.test_threshold = test_threshold\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.uniform.temporal.threshold.TemporalThresholdSplit.run","title":"<code>run(datarec)</code>","text":"<p>Splits the dataset into training, validation, and test sets based on two thresholds.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>A DataRec object containing the dataset with a timestamp column.</p> required <p>Returns:</p> Type Description <code>Dict[str, DataRec]</code> <p>Dict[str, DataRec]: A dictionary with: - <code>'train'</code>: Training set (timestamps &lt; <code>val_threshold</code>). - <code>'val'</code>: Validation set (timestamps between <code>val_threshold</code> and <code>test_threshold</code>). - <code>'test'</code>: Test set (timestamps &gt;= <code>test_threshold</code>).</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the <code>datarec</code> object does not contain a timestamp column.</p> Source code in <code>datarec/splitters/uniform/temporal/threshold.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n    \"\"\"\n    Splits the dataset into training, validation, and test sets based on two thresholds.\n\n    Args:\n        datarec (DataRec): A DataRec object containing the dataset with a timestamp column.\n\n    Returns:\n        Dict[str, DataRec]: A dictionary with:\n            - `'train'`: Training set (timestamps &lt; `val_threshold`).\n            - `'val'`: Validation set (timestamps between `val_threshold` and `test_threshold`).\n            - `'test'`: Test set (timestamps &gt;= `test_threshold`).\n\n    Raises:\n        TypeError: If the `datarec` object does not contain a timestamp column.\n    \"\"\"\n\n    if datarec.timestamp_col is None:\n        raise TypeError('This DataRec does not contain temporal information')\n\n    dataset = datarec.data\n\n    train = dataset[dataset[datarec.timestamp_col] &lt; self.val_threshold]\n\n    val = dataset[(dataset[datarec.timestamp_col] &gt;= self.val_threshold) &amp;\n                  (dataset[datarec.timestamp_col] &lt; self.test_threshold)]\n\n    test = dataset[dataset[datarec.timestamp_col] &gt;= self.test_threshold]\n\n    return self.output(datarec, train, test, val,\n                       step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#user-stratified-splitting-strategies","title":"User-Stratified Splitting Strategies","text":"<p>These splitters operate on a per-user basis, ensuring that each user's interaction history is partitioned across the splits.</p>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.hold_out.UserStratifiedHoldOut","title":"<code>UserStratifiedHoldOut</code>","text":"<p>               Bases: <code>Splitter</code></p> <p>Implements a user-stratified holdout split for a recommendation dataset.</p> <p>This splitter ensures that each user's interactions are split into training, validation, and test sets while maintaining the proportion specified by <code>test_ratio</code> and <code>val_ratio</code>.</p> Source code in <code>datarec/splitters/user_stratified/hold_out.py</code> <pre><code>class UserStratifiedHoldOut(Splitter):\n    \"\"\"\n    Implements a user-stratified holdout split for a recommendation dataset.\n\n    This splitter ensures that each user's interactions are split into training, validation,\n    and test sets while maintaining the proportion specified by `test_ratio` and `val_ratio`.\n\n    \"\"\"\n\n    def __init__(self, test_ratio: float = 0, val_ratio: float = 0, seed: int = 42):\n        \"\"\"Initializes the UserStratifiedHoldOut splitter.\n\n        Args:\n            test_ratio (float, optional): The proportion of interactions per user to include in the test set.\n                Must be between 0 and 1. Default is 0.\n            val_ratio (float, optional): The proportion of interactions per user to include in the validation set.\n                Must be between 0 and 1. Default is 0.\n            seed (int, optional): Random seed for reproducibility. Defaults to 42.\n\n         Raises:\n            ValueError: If `test_ratio` or `val_ratio` is not in the range [0, 1].\n        \"\"\"\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.test_ratio = test_ratio\n        self.val_ratio = val_ratio\n        self.seed = seed\n\n    @property\n    def test_ratio(self) -&gt; float:\n        \"\"\"The proportion of interactions per user for the test set.\"\"\"\n        return self._test_ratio\n\n    @test_ratio.setter\n    def test_ratio(self, value: float) -&gt; None:\n        \"\"\"\n        Sets the proportion of interactions per user for the test set.\n\n        Args:\n            value (float): Ratio for the test set. Must be between 0 and 1.\n\n        Raises:\n            ValueError: If the ratio is not between 0 and 1.\n        \"\"\"\n        if value &lt; 0 or value &gt; 1:\n            raise ValueError('ratio must be between 0 and 1')\n        self._test_ratio = value\n\n    @property\n    def val_ratio(self) -&gt; float:\n        \"\"\" \n        The proportion of interactions per user for the validation set.\n        \"\"\"\n        return self._val_ratio\n\n    @val_ratio.setter\n    def val_ratio(self, value: float) -&gt; None:\n        \"\"\"\n        Sets the proportion of remaining interactions per user for the validation set.\n\n        Args:\n            value (float): Ratio for the validation set. Must be between 0 and 1.\n\n        Raises:\n            ValueError: If the ratio is not between 0 and 1.\n        \"\"\"\n        if value &lt; 0 or value &gt; 1:\n            raise ValueError('ratio must be between 0 and 1')\n        self._val_ratio = value\n\n    def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n        \"\"\"\n        Splits the dataset into train, validation, and test sets using a user-stratified holdout approach.\n\n        Each user's interactions are split independently according to `test_ratio` and `val_ratio`, ensuring\n        that the distribution is preserved per user. The function returns a dictionary containing the three\n        resulting subsets.\n\n        Args:\n            datarec (DataRec): The dataset to be split.\n\n        Returns:\n            (Dict[str, DataRec]): A dictionary with the following keys:\n                - \"train\": DataRec containing the training set.\n                - \"test\": DataRec containing the test set, if `test_ratio` &gt; 0.\n                - \"val\": DataRec containing the validation set, if `val_ratio` &gt; 0.\n        \"\"\"\n\n        train, test, val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n        data = datarec.data\n        for u in datarec.users:\n\n            u_train, u_val, u_test = data[data.iloc[:, 0] == u], pd.DataFrame(), pd.DataFrame()\n\n            if self.test_ratio:\n                u_train, u_test = split(u_train, test_size=self._test_ratio, random_state=self.seed)\n            if self.val_ratio:\n                u_train, u_val = split(u_train, test_size=self._val_ratio, random_state=self.seed)\n\n            train = pd.concat([train, u_train], axis=0, ignore_index=True)\n            test = pd.concat([test, u_test], axis=0, ignore_index=True)\n            val = pd.concat([val, u_val], axis=0, ignore_index=True)\n\n        return self.output(datarec=datarec, train=train, test=test, validation=val,\n                           step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.hold_out.UserStratifiedHoldOut.test_ratio","title":"<code>test_ratio</code>  <code>property</code> <code>writable</code>","text":"<p>The proportion of interactions per user for the test set.</p>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.hold_out.UserStratifiedHoldOut.val_ratio","title":"<code>val_ratio</code>  <code>property</code> <code>writable</code>","text":"<p>The proportion of interactions per user for the validation set.</p>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.hold_out.UserStratifiedHoldOut.__init__","title":"<code>__init__(test_ratio=0, val_ratio=0, seed=42)</code>","text":"<p>Initializes the UserStratifiedHoldOut splitter.</p> <p>Parameters:</p> Name Type Description Default <code>test_ratio</code> <code>float</code> <p>The proportion of interactions per user to include in the test set. Must be between 0 and 1. Default is 0.</p> <code>0</code> <code>val_ratio</code> <code>float</code> <p>The proportion of interactions per user to include in the validation set. Must be between 0 and 1. Default is 0.</p> <code>0</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> <p>Raises:     ValueError: If <code>test_ratio</code> or <code>val_ratio</code> is not in the range [0, 1].</p> Source code in <code>datarec/splitters/user_stratified/hold_out.py</code> <pre><code>def __init__(self, test_ratio: float = 0, val_ratio: float = 0, seed: int = 42):\n    \"\"\"Initializes the UserStratifiedHoldOut splitter.\n\n    Args:\n        test_ratio (float, optional): The proportion of interactions per user to include in the test set.\n            Must be between 0 and 1. Default is 0.\n        val_ratio (float, optional): The proportion of interactions per user to include in the validation set.\n            Must be between 0 and 1. Default is 0.\n        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n\n     Raises:\n        ValueError: If `test_ratio` or `val_ratio` is not in the range [0, 1].\n    \"\"\"\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.test_ratio = test_ratio\n    self.val_ratio = val_ratio\n    self.seed = seed\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.hold_out.UserStratifiedHoldOut.run","title":"<code>run(datarec)</code>","text":"<p>Splits the dataset into train, validation, and test sets using a user-stratified holdout approach.</p> <p>Each user's interactions are split independently according to <code>test_ratio</code> and <code>val_ratio</code>, ensuring that the distribution is preserved per user. The function returns a dictionary containing the three resulting subsets.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The dataset to be split.</p> required <p>Returns:</p> Type Description <code>Dict[str, DataRec]</code> <p>A dictionary with the following keys: - \"train\": DataRec containing the training set. - \"test\": DataRec containing the test set, if <code>test_ratio</code> &gt; 0. - \"val\": DataRec containing the validation set, if <code>val_ratio</code> &gt; 0.</p> Source code in <code>datarec/splitters/user_stratified/hold_out.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n    \"\"\"\n    Splits the dataset into train, validation, and test sets using a user-stratified holdout approach.\n\n    Each user's interactions are split independently according to `test_ratio` and `val_ratio`, ensuring\n    that the distribution is preserved per user. The function returns a dictionary containing the three\n    resulting subsets.\n\n    Args:\n        datarec (DataRec): The dataset to be split.\n\n    Returns:\n        (Dict[str, DataRec]): A dictionary with the following keys:\n            - \"train\": DataRec containing the training set.\n            - \"test\": DataRec containing the test set, if `test_ratio` &gt; 0.\n            - \"val\": DataRec containing the validation set, if `val_ratio` &gt; 0.\n    \"\"\"\n\n    train, test, val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n    data = datarec.data\n    for u in datarec.users:\n\n        u_train, u_val, u_test = data[data.iloc[:, 0] == u], pd.DataFrame(), pd.DataFrame()\n\n        if self.test_ratio:\n            u_train, u_test = split(u_train, test_size=self._test_ratio, random_state=self.seed)\n        if self.val_ratio:\n            u_train, u_val = split(u_train, test_size=self._val_ratio, random_state=self.seed)\n\n        train = pd.concat([train, u_train], axis=0, ignore_index=True)\n        test = pd.concat([test, u_test], axis=0, ignore_index=True)\n        val = pd.concat([val, u_val], axis=0, ignore_index=True)\n\n    return self.output(datarec=datarec, train=train, test=test, validation=val,\n                       step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.leave_out.LeaveNOut","title":"<code>LeaveNOut</code>","text":"<p>               Bases: <code>Splitter</code></p> <p>Implements the Leave-N-Out splitting strategy for recommendation datasets.</p> <p>This splitter ensures that for each user, a fixed number of interactions (<code>test_n</code> and <code>validation_n</code>) are randomly selected and moved to the test and validation sets, respectively. The remaining interactions are kept in the training set.</p> Source code in <code>datarec/splitters/user_stratified/leave_out.py</code> <pre><code>class LeaveNOut(Splitter):\n    \"\"\"\n    Implements the Leave-N-Out splitting strategy for recommendation datasets.\n\n    This splitter ensures that for each user, a fixed number of interactions (`test_n` and `validation_n`)\n    are randomly selected and moved to the test and validation sets, respectively. The remaining interactions\n    are kept in the training set.\n    \"\"\"\n\n    def __init__(self, test_n: int = 0, validation_n: int = 0, seed: int = 42):\n        \"\"\"Initializes the LeaveNOut splitter.\n\n        Args:\n            test_n (int, optional): Number of interactions to move to the test set per user. Default is 0.\n            validation_n (int, optional): Number of interactions to move to the validation set per user. Default is 0.\n            seed (int, optional): Random seed for reproducibility. Default is 42.\n\n        Raises:\n            ValueError: If `test_n` or `validation_n` are negative.\n            TypeError: If `test_n` or `validation_n` are not integers.\n        \"\"\"\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.test_n = test_n\n        self.validation_n = validation_n\n        self.seed = seed\n\n    @property\n    def test_n(self) -&gt; int:\n        \"\"\"Number of interactions to move to the test set per user.\"\"\"\n        return self._test_n\n\n    @test_n.setter\n    def test_n(self, value: int) -&gt; None:\n        \"\"\"\n        Sets the number of interactions to move to the test set per user.\n\n        Args:\n            value (int): Number of interactions.\n\n        Raises:\n            ValueError: If `value` is negative.\n            TypeError: If `value` is not an integer.\n        \"\"\"\n        if value &lt; 0:\n            raise ValueError(\"test_n must be greater or equal to 0.\")\n        if isinstance(value, float):\n            raise TypeError(\"test_n must be an integer.\")\n        self._test_n = value\n\n    @property\n    def validation_n(self) -&gt; int:\n        \"\"\"Number of interactions to move to the test set per user.\"\"\"\n        return self._validation_n\n\n    @validation_n.setter\n    def validation_n(self, value: int) -&gt; None:\n        \"\"\"\n        Sets the number of interactions to move to the validation set per user.\n\n        Args:\n            value (int): Number of interactions.\n\n        Raises:\n            ValueError: If `value` is negative.\n            TypeError: If `value` is not an integer.\n        \"\"\"\n        if value &lt; 0:\n            raise ValueError(\"validation_n must be greater or equal to 0.\")\n        if isinstance(value, float):\n            raise TypeError(\"validation_n must be an integer.\")\n        self._validation_n = value\n\n    def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n        \"\"\"\n        Splits the dataset into train, validation, and test sets using a Leave-N-Out approach.\n\n        For each user, `test_n` interactions are randomly assigned to the test set, and `validation_n`\n        interactions are assigned to the validation set. The remaining interactions are used for training.\n\n        Args:\n            datarec (DataRec): The dataset to be split.\n\n        Returns:\n            (Dict[str, DataRec]): A dictionary with the following keys:\n                - \"train\": DataRec containing the training set.\n                - \"test\": DataRec containing the test set, if `test_n` &gt; 0.\n                - \"validation\": DataRec containing the validation set, if `val_n` &gt; 0.\n        \"\"\"\n\n        train, test, val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n        data = datarec.data\n        for u in datarec.users:\n\n            u_train, u_val, u_test = data[data.iloc[:, 0] == u], pd.DataFrame(), pd.DataFrame()\n\n            if self.test_n:\n                u_train, sample = random_sample(dataframe=u_train, n_samples=self.test_n, seed=self.seed)\n                u_test = pd.concat([u_test, sample], axis=0, ignore_index=True)\n\n            if self.validation_n:\n                u_train, sample = random_sample(dataframe=u_train, n_samples=self.validation_n, seed=self.seed)\n                u_val = pd.concat([u_val, sample], axis=0, ignore_index=True)\n\n            train = pd.concat([train, u_train], axis=0, ignore_index=True)\n            test = pd.concat([test, u_test], axis=0, ignore_index=True)\n            val = pd.concat([val, u_val], axis=0, ignore_index=True)\n\n        return self.output(datarec, train, test, val,\n                           step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.leave_out.LeaveNOut.test_n","title":"<code>test_n</code>  <code>property</code> <code>writable</code>","text":"<p>Number of interactions to move to the test set per user.</p>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.leave_out.LeaveNOut.validation_n","title":"<code>validation_n</code>  <code>property</code> <code>writable</code>","text":"<p>Number of interactions to move to the test set per user.</p>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.leave_out.LeaveNOut.__init__","title":"<code>__init__(test_n=0, validation_n=0, seed=42)</code>","text":"<p>Initializes the LeaveNOut splitter.</p> <p>Parameters:</p> Name Type Description Default <code>test_n</code> <code>int</code> <p>Number of interactions to move to the test set per user. Default is 0.</p> <code>0</code> <code>validation_n</code> <code>int</code> <p>Number of interactions to move to the validation set per user. Default is 0.</p> <code>0</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Default is 42.</p> <code>42</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>test_n</code> or <code>validation_n</code> are negative.</p> <code>TypeError</code> <p>If <code>test_n</code> or <code>validation_n</code> are not integers.</p> Source code in <code>datarec/splitters/user_stratified/leave_out.py</code> <pre><code>def __init__(self, test_n: int = 0, validation_n: int = 0, seed: int = 42):\n    \"\"\"Initializes the LeaveNOut splitter.\n\n    Args:\n        test_n (int, optional): Number of interactions to move to the test set per user. Default is 0.\n        validation_n (int, optional): Number of interactions to move to the validation set per user. Default is 0.\n        seed (int, optional): Random seed for reproducibility. Default is 42.\n\n    Raises:\n        ValueError: If `test_n` or `validation_n` are negative.\n        TypeError: If `test_n` or `validation_n` are not integers.\n    \"\"\"\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.test_n = test_n\n    self.validation_n = validation_n\n    self.seed = seed\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.leave_out.LeaveNOut.run","title":"<code>run(datarec)</code>","text":"<p>Splits the dataset into train, validation, and test sets using a Leave-N-Out approach.</p> <p>For each user, <code>test_n</code> interactions are randomly assigned to the test set, and <code>validation_n</code> interactions are assigned to the validation set. The remaining interactions are used for training.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The dataset to be split.</p> required <p>Returns:</p> Type Description <code>Dict[str, DataRec]</code> <p>A dictionary with the following keys: - \"train\": DataRec containing the training set. - \"test\": DataRec containing the test set, if <code>test_n</code> &gt; 0. - \"validation\": DataRec containing the validation set, if <code>val_n</code> &gt; 0.</p> Source code in <code>datarec/splitters/user_stratified/leave_out.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n    \"\"\"\n    Splits the dataset into train, validation, and test sets using a Leave-N-Out approach.\n\n    For each user, `test_n` interactions are randomly assigned to the test set, and `validation_n`\n    interactions are assigned to the validation set. The remaining interactions are used for training.\n\n    Args:\n        datarec (DataRec): The dataset to be split.\n\n    Returns:\n        (Dict[str, DataRec]): A dictionary with the following keys:\n            - \"train\": DataRec containing the training set.\n            - \"test\": DataRec containing the test set, if `test_n` &gt; 0.\n            - \"validation\": DataRec containing the validation set, if `val_n` &gt; 0.\n    \"\"\"\n\n    train, test, val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n    data = datarec.data\n    for u in datarec.users:\n\n        u_train, u_val, u_test = data[data.iloc[:, 0] == u], pd.DataFrame(), pd.DataFrame()\n\n        if self.test_n:\n            u_train, sample = random_sample(dataframe=u_train, n_samples=self.test_n, seed=self.seed)\n            u_test = pd.concat([u_test, sample], axis=0, ignore_index=True)\n\n        if self.validation_n:\n            u_train, sample = random_sample(dataframe=u_train, n_samples=self.validation_n, seed=self.seed)\n            u_val = pd.concat([u_val, sample], axis=0, ignore_index=True)\n\n        train = pd.concat([train, u_train], axis=0, ignore_index=True)\n        test = pd.concat([test, u_test], axis=0, ignore_index=True)\n        val = pd.concat([val, u_val], axis=0, ignore_index=True)\n\n    return self.output(datarec, train, test, val,\n                       step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.leave_out.LeaveOneOut","title":"<code>LeaveOneOut</code>","text":"<p>               Bases: <code>LeaveNOut</code></p> <p>Implements the Leave-One-Out splitting strategy for recommendation datasets.</p> <p>This splitter ensures that for each user, at most one interaction is randomly selected and moved to the test and/or validation set, depending on the specified parameters. The remaining interactions are kept in the training set.</p> <p>This is a special case of <code>LeaveNOut</code> where <code>test_n=1</code> and/or <code>validation_n=1</code> if <code>test</code> and <code>validation</code> are set to <code>True</code>, respectively.</p> Source code in <code>datarec/splitters/user_stratified/leave_out.py</code> <pre><code>class LeaveOneOut(LeaveNOut):\n    \"\"\"\n    Implements the Leave-One-Out splitting strategy for recommendation datasets.\n\n    This splitter ensures that for each user, at most one interaction is randomly selected and moved\n    to the test and/or validation set, depending on the specified parameters. The remaining interactions\n    are kept in the training set.\n\n    This is a special case of `LeaveNOut` where `test_n=1` and/or `validation_n=1` if `test` and `validation`\n    are set to `True`, respectively.\n    \"\"\"\n\n    def __init__(self, test: bool = True, validation: bool = True, seed: int = 42):\n        \"\"\"Initializes the LeaveOneOut splitter.\n\n        Args:\n            test (bool, optional): Whether to include a test set. Defaults to True.\n            validation (bool, optional): Whether to include a validation set. Defaults to True.\n            seed (int, optional): Random seed for reproducibility. Default is 42.\n\n        Raises:\n            TypeError: If `test` or `validation` is not a boolean.\n        \"\"\"\n        if not isinstance(test, bool):\n            raise TypeError(\"test must be a boolean.\")\n        if not isinstance(validation, bool):\n            raise TypeError(\"validation must be an boolean.\")\n\n        test = 1 if test else 0\n        validation = 1 if validation else 0\n\n        super().__init__(test_n=test, validation_n=validation, seed=seed)\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.leave_out.LeaveOneOut.__init__","title":"<code>__init__(test=True, validation=True, seed=42)</code>","text":"<p>Initializes the LeaveOneOut splitter.</p> <p>Parameters:</p> Name Type Description Default <code>test</code> <code>bool</code> <p>Whether to include a test set. Defaults to True.</p> <code>True</code> <code>validation</code> <code>bool</code> <p>Whether to include a validation set. Defaults to True.</p> <code>True</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Default is 42.</p> <code>42</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>test</code> or <code>validation</code> is not a boolean.</p> Source code in <code>datarec/splitters/user_stratified/leave_out.py</code> <pre><code>def __init__(self, test: bool = True, validation: bool = True, seed: int = 42):\n    \"\"\"Initializes the LeaveOneOut splitter.\n\n    Args:\n        test (bool, optional): Whether to include a test set. Defaults to True.\n        validation (bool, optional): Whether to include a validation set. Defaults to True.\n        seed (int, optional): Random seed for reproducibility. Default is 42.\n\n    Raises:\n        TypeError: If `test` or `validation` is not a boolean.\n    \"\"\"\n    if not isinstance(test, bool):\n        raise TypeError(\"test must be a boolean.\")\n    if not isinstance(validation, bool):\n        raise TypeError(\"validation must be an boolean.\")\n\n    test = 1 if test else 0\n    validation = 1 if validation else 0\n\n    super().__init__(test_n=test, validation_n=validation, seed=seed)\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.leave_out.LeaveRatioOut","title":"<code>LeaveRatioOut</code>","text":"<p>               Bases: <code>Splitter</code></p> <p>Splits the dataset into training, test, and validation sets based on a ratio instead of a fixed number of samples.</p> <p>This splitter selects a fraction of interactions for each user to be assigned to the test and validation sets, ensuring that the splits are proportional to the user's total number of interactions.</p> Source code in <code>datarec/splitters/user_stratified/leave_out.py</code> <pre><code>class LeaveRatioOut(Splitter):\n    \"\"\"\n    Splits the dataset into training, test, and validation sets based on a ratio instead of a fixed number of samples.\n\n    This splitter selects a fraction of interactions for each user to be assigned to the test and validation sets,\n    ensuring that the splits are proportional to the user's total number of interactions.\n    \"\"\"\n\n    def __init__(self, test_ratio: float = 0, val_ratio: float = 0, seed: int = 42):\n        \"\"\"Initializes the LeaveRatioOut splitter.\n\n        Args:\n            test_ratio (float, optional): Proportion of each user's interactions assigned to the test set. Default is 0.\n            val_ratio (float, optional): Proportion of each user's interactions assigned to the validation set. Default is 0.\n            seed (int, optional): Random seed for reproducibility. Default is 42.\n\n        Raises:\n            ValueError: If `test_ratio` or `val_ratio` are not in the range [0, 1].\n            ValueError: If the sum of `test_ratio` and `val_ratio` exceeds 1.\n        \"\"\"\n        if not (0 &lt;= test_ratio &lt;= 1):\n            raise ValueError('ratio must be between 0 and 1')\n        if not (0 &lt;= val_ratio &lt;= 1):\n            raise ValueError('ratio must be between 0 and 1')\n        if test_ratio + val_ratio &gt; 1:\n            raise ValueError(\"sum of test_ratio and val_ratio must not exceed 1\")\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.test_ratio = test_ratio\n        self.val_ratio = val_ratio\n        self.seed = seed\n\n    def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n        \"\"\"\n         Splits the dataset into train, test, and validation sets based on the specified ratios.\n\n         The interactions of each user are sampled proportionally to create the test and validation sets.\n         The remaining interactions are used as the training set.\n\n         Args:\n             datarec (DataRec): The dataset containing interactions and user-item relationships.\n\n         Returns:\n             (Dict[str, DataRec]): A dictionary containing the following keys:\n                 - `\"train\"` (`DataRec`): The training dataset.\n                 - `\"test\"` (`DataRec`): The test dataset, if `test_ratio` &gt; 0.\n                 - `\"val\"` (`DataRec`): The validation dataset, if `val_ratio` &gt; 0.\n\n         Raises:\n             ValueError: If an empty dataset is encountered after sampling.\n         \"\"\"\n\n        train, test, val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n        data = datarec.data\n        for u in datarec.users:\n            u_train, u_val, u_test = data[data.iloc[:, 0] == u], pd.DataFrame(), pd.DataFrame()\n\n            user_total = len(u_train)\n\n            test_n_samples = round(self.test_ratio * user_total)\n            val_n_samples = round(self.val_ratio * user_total)\n\n            if test_n_samples &gt; 0:\n                u_train, sample = random_sample(dataframe=u_train, n_samples=min(test_n_samples, len(u_train)),\n                                                seed=self.seed)\n                u_test = pd.concat([u_test, sample], axis=0, ignore_index=True)\n\n            if val_n_samples &gt; 0:\n                u_train, sample = random_sample(dataframe=u_train, n_samples=min(val_n_samples, len(u_train)),\n                                                seed=self.seed)\n                u_val = pd.concat([u_val, sample], axis=0, ignore_index=True)\n\n            train = pd.concat([train, u_train], axis=0, ignore_index=True)\n            test = pd.concat([test, u_test], axis=0, ignore_index=True)\n            val = pd.concat([val, u_val], axis=0, ignore_index=True)\n\n        return self.output(datarec, train, test, val,\n                           step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.leave_out.LeaveRatioOut.__init__","title":"<code>__init__(test_ratio=0, val_ratio=0, seed=42)</code>","text":"<p>Initializes the LeaveRatioOut splitter.</p> <p>Parameters:</p> Name Type Description Default <code>test_ratio</code> <code>float</code> <p>Proportion of each user's interactions assigned to the test set. Default is 0.</p> <code>0</code> <code>val_ratio</code> <code>float</code> <p>Proportion of each user's interactions assigned to the validation set. Default is 0.</p> <code>0</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Default is 42.</p> <code>42</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>test_ratio</code> or <code>val_ratio</code> are not in the range [0, 1].</p> <code>ValueError</code> <p>If the sum of <code>test_ratio</code> and <code>val_ratio</code> exceeds 1.</p> Source code in <code>datarec/splitters/user_stratified/leave_out.py</code> <pre><code>def __init__(self, test_ratio: float = 0, val_ratio: float = 0, seed: int = 42):\n    \"\"\"Initializes the LeaveRatioOut splitter.\n\n    Args:\n        test_ratio (float, optional): Proportion of each user's interactions assigned to the test set. Default is 0.\n        val_ratio (float, optional): Proportion of each user's interactions assigned to the validation set. Default is 0.\n        seed (int, optional): Random seed for reproducibility. Default is 42.\n\n    Raises:\n        ValueError: If `test_ratio` or `val_ratio` are not in the range [0, 1].\n        ValueError: If the sum of `test_ratio` and `val_ratio` exceeds 1.\n    \"\"\"\n    if not (0 &lt;= test_ratio &lt;= 1):\n        raise ValueError('ratio must be between 0 and 1')\n    if not (0 &lt;= val_ratio &lt;= 1):\n        raise ValueError('ratio must be between 0 and 1')\n    if test_ratio + val_ratio &gt; 1:\n        raise ValueError(\"sum of test_ratio and val_ratio must not exceed 1\")\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.test_ratio = test_ratio\n    self.val_ratio = val_ratio\n    self.seed = seed\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.leave_out.LeaveRatioOut.run","title":"<code>run(datarec)</code>","text":"<p>Splits the dataset into train, test, and validation sets based on the specified ratios.</p> <p>The interactions of each user are sampled proportionally to create the test and validation sets. The remaining interactions are used as the training set.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The dataset containing interactions and user-item relationships.</p> required <p>Returns:</p> Type Description <code>Dict[str, DataRec]</code> <p>A dictionary containing the following keys: - <code>\"train\"</code> (<code>DataRec</code>): The training dataset. - <code>\"test\"</code> (<code>DataRec</code>): The test dataset, if <code>test_ratio</code> &gt; 0. - <code>\"val\"</code> (<code>DataRec</code>): The validation dataset, if <code>val_ratio</code> &gt; 0.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an empty dataset is encountered after sampling.</p> Source code in <code>datarec/splitters/user_stratified/leave_out.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n    \"\"\"\n     Splits the dataset into train, test, and validation sets based on the specified ratios.\n\n     The interactions of each user are sampled proportionally to create the test and validation sets.\n     The remaining interactions are used as the training set.\n\n     Args:\n         datarec (DataRec): The dataset containing interactions and user-item relationships.\n\n     Returns:\n         (Dict[str, DataRec]): A dictionary containing the following keys:\n             - `\"train\"` (`DataRec`): The training dataset.\n             - `\"test\"` (`DataRec`): The test dataset, if `test_ratio` &gt; 0.\n             - `\"val\"` (`DataRec`): The validation dataset, if `val_ratio` &gt; 0.\n\n     Raises:\n         ValueError: If an empty dataset is encountered after sampling.\n     \"\"\"\n\n    train, test, val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n    data = datarec.data\n    for u in datarec.users:\n        u_train, u_val, u_test = data[data.iloc[:, 0] == u], pd.DataFrame(), pd.DataFrame()\n\n        user_total = len(u_train)\n\n        test_n_samples = round(self.test_ratio * user_total)\n        val_n_samples = round(self.val_ratio * user_total)\n\n        if test_n_samples &gt; 0:\n            u_train, sample = random_sample(dataframe=u_train, n_samples=min(test_n_samples, len(u_train)),\n                                            seed=self.seed)\n            u_test = pd.concat([u_test, sample], axis=0, ignore_index=True)\n\n        if val_n_samples &gt; 0:\n            u_train, sample = random_sample(dataframe=u_train, n_samples=min(val_n_samples, len(u_train)),\n                                            seed=self.seed)\n            u_val = pd.concat([u_val, sample], axis=0, ignore_index=True)\n\n        train = pd.concat([train, u_train], axis=0, ignore_index=True)\n        test = pd.concat([test, u_test], axis=0, ignore_index=True)\n        val = pd.concat([val, u_val], axis=0, ignore_index=True)\n\n    return self.output(datarec, train, test, val,\n                       step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.temporal.leave_out.LeaveNLast","title":"<code>LeaveNLast</code>","text":"<p>               Bases: <code>Splitter</code></p> <p>Splits the dataset by removing the last <code>n</code> interactions per user based on a timestamp column.</p> <p>This splitter selects the last <code>test_n</code> interactions for the test set and the last <code>validation_n</code> interactions for the validation set while keeping the remaining interactions in the training set.</p> Source code in <code>datarec/splitters/user_stratified/temporal/leave_out.py</code> <pre><code>class LeaveNLast(Splitter):\n    \"\"\"\n    Splits the dataset by removing the last `n` interactions per user based on a timestamp column.\n\n    This splitter selects the last `test_n` interactions for the test set and the last `validation_n`\n    interactions for the validation set while keeping the remaining interactions in the training set.\n    \"\"\"\n    def __init__(self, test_n: int = 0, validation_n: int = 0, seed: int = 42):\n        \"\"\"Initializes the LeaveNLast splitter.\n\n        Args:\n            test_n (int, optional): Number of last interactions for the test set. Defaults to 0.\n            validation_n (int, optional): Number of last interactions for the validation set. Defaults to 0.\n            seed (int, optional): Random seed for reproducibility. Defaults to 42.\n        \"\"\"\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.test_n = test_n\n        self.validation_n = validation_n\n        self.seed = seed\n\n    @property\n    def test_n(self) -&gt; int:\n        \"\"\"The number of last interactions per user for the test set.\"\"\"\n        return self._test_n\n\n    @test_n.setter\n    def test_n(self, value: int) -&gt; None:\n        \"\"\"\n        Sets the number of last interactions per user for the test set.\n\n        Args:\n            value (int): Number of interactions. Must be &gt;= 0.\n\n        Raises:\n            ValueError: If `value` &lt; 0.\n            TypeError: If `value` is not an integer.\n        \"\"\"\n        if value &lt; 0:\n            raise ValueError(\"test_n must be greater or equal than 0.\")\n        if isinstance(value, float):\n            raise TypeError(\"test_n must be an integer.\")\n        self._test_n = value\n\n    @property\n    def validation_n(self) -&gt; int:\n        \"\"\"The number of last interactions per user for the validation set.\"\"\"\n        return self._validation_n\n\n    @validation_n.setter\n    def validation_n(self, value: int) -&gt; None:\n        \"\"\"\n        Sets the number of last interactions per user for the validation set.\n\n        Args:\n            value (int): Number of interactions. Must be &gt;= 0.\n\n        Raises:\n            ValueError: If `value` &lt; 0.\n            TypeError: If `value` is not an integer.\n        \"\"\"\n        if value &lt; 0:\n            raise ValueError(\"validation_n must be greater or equal than 0.\")\n        if isinstance(value, float):\n            raise TypeError(\"validation_n must be and integer.\")\n        self._validation_n = value\n\n    def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n        \"\"\"\n        Splits the dataset into train, test, and validation sets based on the last `n` interactions.\n\n        Args:\n            datarec (DataRec): The dataset containing the interactions and timestamp column.\n\n        Returns:\n            (Dict[str, DataRec]): A dictionary with the following keys:\n                - \"train\": The training dataset (`DataRec`).\n                - \"test\": The test dataset (`DataRec`), if `test_n` &gt; 0.\n                - \"val\": The validation dataset (`DataRec`), if `val_n` &gt; 0.\n\n        Raises:\n            TypeError: If the dataset does not contain a timestamp column.\n        \"\"\"\n\n        if datarec.timestamp_col is None:\n            raise TypeError('This DataRec does not contain temporal information')\n\n        train, test, val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n        data = datarec.data\n        for u in datarec.users:\n\n            u_train, u_val, u_test = data[data.iloc[:, 0] == u], pd.DataFrame(), pd.DataFrame()\n\n            if self.test_n:\n                for _ in range(self.test_n):\n                    u_train, sample = max_by_col(u_train, datarec.timestamp_col, seed=self.seed)\n                    u_test = pd.concat([u_test, sample], axis=0, ignore_index=True)\n\n            if self.validation_n:\n                for _ in range(self.validation_n):\n                    u_train, sample = max_by_col(u_train, datarec.timestamp_col, seed=self.seed)\n                    u_val = pd.concat([u_val, sample], axis=0, ignore_index=True)\n\n            train = pd.concat([train, u_train], axis=0, ignore_index=True)\n            test = pd.concat([test, u_test], axis=0, ignore_index=True)\n            val = pd.concat([val, u_val], axis=0, ignore_index=True)\n\n        return self.output(datarec, train, test, val,\n                           step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.temporal.leave_out.LeaveNLast.test_n","title":"<code>test_n</code>  <code>property</code> <code>writable</code>","text":"<p>The number of last interactions per user for the test set.</p>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.temporal.leave_out.LeaveNLast.validation_n","title":"<code>validation_n</code>  <code>property</code> <code>writable</code>","text":"<p>The number of last interactions per user for the validation set.</p>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.temporal.leave_out.LeaveNLast.__init__","title":"<code>__init__(test_n=0, validation_n=0, seed=42)</code>","text":"<p>Initializes the LeaveNLast splitter.</p> <p>Parameters:</p> Name Type Description Default <code>test_n</code> <code>int</code> <p>Number of last interactions for the test set. Defaults to 0.</p> <code>0</code> <code>validation_n</code> <code>int</code> <p>Number of last interactions for the validation set. Defaults to 0.</p> <code>0</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> Source code in <code>datarec/splitters/user_stratified/temporal/leave_out.py</code> <pre><code>def __init__(self, test_n: int = 0, validation_n: int = 0, seed: int = 42):\n    \"\"\"Initializes the LeaveNLast splitter.\n\n    Args:\n        test_n (int, optional): Number of last interactions for the test set. Defaults to 0.\n        validation_n (int, optional): Number of last interactions for the validation set. Defaults to 0.\n        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n    \"\"\"\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.test_n = test_n\n    self.validation_n = validation_n\n    self.seed = seed\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.temporal.leave_out.LeaveNLast.run","title":"<code>run(datarec)</code>","text":"<p>Splits the dataset into train, test, and validation sets based on the last <code>n</code> interactions.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The dataset containing the interactions and timestamp column.</p> required <p>Returns:</p> Type Description <code>Dict[str, DataRec]</code> <p>A dictionary with the following keys: - \"train\": The training dataset (<code>DataRec</code>). - \"test\": The test dataset (<code>DataRec</code>), if <code>test_n</code> &gt; 0. - \"val\": The validation dataset (<code>DataRec</code>), if <code>val_n</code> &gt; 0.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the dataset does not contain a timestamp column.</p> Source code in <code>datarec/splitters/user_stratified/temporal/leave_out.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n    \"\"\"\n    Splits the dataset into train, test, and validation sets based on the last `n` interactions.\n\n    Args:\n        datarec (DataRec): The dataset containing the interactions and timestamp column.\n\n    Returns:\n        (Dict[str, DataRec]): A dictionary with the following keys:\n            - \"train\": The training dataset (`DataRec`).\n            - \"test\": The test dataset (`DataRec`), if `test_n` &gt; 0.\n            - \"val\": The validation dataset (`DataRec`), if `val_n` &gt; 0.\n\n    Raises:\n        TypeError: If the dataset does not contain a timestamp column.\n    \"\"\"\n\n    if datarec.timestamp_col is None:\n        raise TypeError('This DataRec does not contain temporal information')\n\n    train, test, val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n    data = datarec.data\n    for u in datarec.users:\n\n        u_train, u_val, u_test = data[data.iloc[:, 0] == u], pd.DataFrame(), pd.DataFrame()\n\n        if self.test_n:\n            for _ in range(self.test_n):\n                u_train, sample = max_by_col(u_train, datarec.timestamp_col, seed=self.seed)\n                u_test = pd.concat([u_test, sample], axis=0, ignore_index=True)\n\n        if self.validation_n:\n            for _ in range(self.validation_n):\n                u_train, sample = max_by_col(u_train, datarec.timestamp_col, seed=self.seed)\n                u_val = pd.concat([u_val, sample], axis=0, ignore_index=True)\n\n        train = pd.concat([train, u_train], axis=0, ignore_index=True)\n        test = pd.concat([test, u_test], axis=0, ignore_index=True)\n        val = pd.concat([val, u_val], axis=0, ignore_index=True)\n\n    return self.output(datarec, train, test, val,\n                       step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.temporal.leave_out.LeaveOneLastItem","title":"<code>LeaveOneLastItem</code>","text":"<p>               Bases: <code>LeaveNLast</code></p> <p>Special case of LeaveNLast that removes only the last interaction per user for test and validation.</p> <p>This class sets <code>test_n</code> and <code>validation_n</code> to 1 if their corresponding boolean parameters are True.</p> Source code in <code>datarec/splitters/user_stratified/temporal/leave_out.py</code> <pre><code>class LeaveOneLastItem(LeaveNLast):\n    \"\"\"\n    Special case of LeaveNLast that removes only the last interaction per user for test and validation.\n\n    This class sets `test_n` and `validation_n` to 1 if their corresponding boolean parameters are True.\n\n    \"\"\"\n\n    def __init__(self, test: bool = True, validation: bool = True, seed: int = 42):\n        \"\"\"\n        Initializes the LeaveOneLastItem splitter.\n\n        Args:\n            test (bool, optional): Whether to remove the last interaction for the test set. Defaults to True.\n            validation (bool, optional): Whether to remove the last interaction for the validation set. Defaults to True.\n            seed (int, optional): Random seed for reproducibility. Default is 42.\n\n        Raises:\n            TypeError: If `test` or `validation` are not boolean.\n        \"\"\"\n        if not isinstance(test, bool):\n            raise TypeError(\"test must be a boolean.\")\n        if not isinstance(validation, bool):\n            raise TypeError(\"validation must be an boolean.\")\n\n        test = 1 if test else 0\n        validation = 1 if validation else 0\n\n        super().__init__(test_n=test, validation_n=validation, seed=seed)\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.temporal.leave_out.LeaveOneLastItem.__init__","title":"<code>__init__(test=True, validation=True, seed=42)</code>","text":"<p>Initializes the LeaveOneLastItem splitter.</p> <p>Parameters:</p> Name Type Description Default <code>test</code> <code>bool</code> <p>Whether to remove the last interaction for the test set. Defaults to True.</p> <code>True</code> <code>validation</code> <code>bool</code> <p>Whether to remove the last interaction for the validation set. Defaults to True.</p> <code>True</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Default is 42.</p> <code>42</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>test</code> or <code>validation</code> are not boolean.</p> Source code in <code>datarec/splitters/user_stratified/temporal/leave_out.py</code> <pre><code>def __init__(self, test: bool = True, validation: bool = True, seed: int = 42):\n    \"\"\"\n    Initializes the LeaveOneLastItem splitter.\n\n    Args:\n        test (bool, optional): Whether to remove the last interaction for the test set. Defaults to True.\n        validation (bool, optional): Whether to remove the last interaction for the validation set. Defaults to True.\n        seed (int, optional): Random seed for reproducibility. Default is 42.\n\n    Raises:\n        TypeError: If `test` or `validation` are not boolean.\n    \"\"\"\n    if not isinstance(test, bool):\n        raise TypeError(\"test must be a boolean.\")\n    if not isinstance(validation, bool):\n        raise TypeError(\"validation must be an boolean.\")\n\n    test = 1 if test else 0\n    validation = 1 if validation else 0\n\n    super().__init__(test_n=test, validation_n=validation, seed=seed)\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.temporal.leave_out.LeaveRatioLast","title":"<code>LeaveRatioLast</code>","text":"<p>               Bases: <code>Splitter</code></p> <p>Splits the dataset into training, test, and validation sets by selecting the most recent interactions for each user based on a specified ratio.</p> <p>Unlike <code>LeaveNLast</code>, which selects a fixed number of interactions, this splitter chooses a fraction of the total interactions per user, preserving temporal order.</p> Source code in <code>datarec/splitters/user_stratified/temporal/leave_out.py</code> <pre><code>class LeaveRatioLast(Splitter):\n    \"\"\"\n    Splits the dataset into training, test, and validation sets by selecting the most recent interactions\n    for each user based on a specified ratio.\n\n    Unlike `LeaveNLast`, which selects a fixed number of interactions, this splitter chooses a fraction\n    of the total interactions per user, preserving temporal order.\n\n    \"\"\"\n\n    def __init__(self, test_ratio: float = 0, val_ratio: float = 0, seed: int = 42):\n        \"\"\"\n        Args:\n            test_ratio (float, optional): Proportion of each user's interactions assigned to the test set. Default is 0.\n            val_ratio (float, optional): Proportion of each user's interactions assigned to the validation set. Default is 0.\n            seed (int, optional): Random seed for reproducibility. Default is 42.\n\n        Raises:\n            ValueError: If `test_ratio` or `val_ratio` are not in the range [0, 1].\n            ValueError: If `test_ratio + val_ratio` &gt; 1.\n        \"\"\"\n        if not (0 &lt;= test_ratio &lt;= 1):\n            raise ValueError('ratio must be between 0 and 1')\n        if not (0 &lt;= val_ratio &lt;= 1):\n            raise ValueError('ratio must be between 0 and 1')\n        if test_ratio + val_ratio &gt; 1:\n            raise ValueError(\"sum of test_ratio and val_ratio must not exceed 1\")\n\n        self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n        self.test_ratio = test_ratio\n        self.val_ratio = val_ratio\n        self.seed = seed\n\n    def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n        \"\"\"\n        Splits the dataset into train, test, and validation sets by selecting the last interactions\n        (in chronological order) for each user.\n\n        The most recent interactions are removed first for the test set, then for the validation set,\n        leaving the remaining interactions for training.\n\n        Args:\n            datarec (DataRec): The dataset containing interactions with a timestamp column.\n\n        Returns:\n            (Dict[str, DataRec]): A dictionary containing the following keys:\n                - `\"train\"` (`DataRec`): The training dataset.\n                - `\"test\"` (`DataRec`): The test dataset, if `test_ratio` &gt; 0.\n                - `\"val\"` (`DataRec`): The validation dataset, if `val_ratio` &gt; 0.\n\n        Raises:\n            TypeError: If the dataset does not contain a timestamp column.\n        \"\"\"\n\n        if datarec.timestamp_col is None:\n            raise TypeError('This DataRec does not contain temporal information')\n\n        train, test, val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n        data = datarec.data\n        for u in datarec.users:\n            u_train, u_val, u_test = data[data.iloc[:, 0] == u], pd.DataFrame(), pd.DataFrame()\n\n            user_total = len(u_train)\n\n            test_n_samples = round(self.test_ratio * user_total)\n            val_n_samples = round(self.val_ratio * user_total)\n\n            if test_n_samples &gt; 0:\n                for _ in range(min(test_n_samples, len(u_train))):\n                    u_train, sample = max_by_col(u_train, datarec.timestamp_col, seed=self.seed)\n                    u_test = pd.concat([u_test, sample], axis=0, ignore_index=True)\n\n            if val_n_samples &gt; 0:\n                for _ in range(min(val_n_samples, len(u_train))):\n                    u_train, sample = max_by_col(u_train, datarec.timestamp_col, seed=self.seed)\n                    u_val = pd.concat([u_val, sample], axis=0, ignore_index=True)\n\n            train = pd.concat([train, u_train], axis=0, ignore_index=True)\n            test = pd.concat([test, u_test], axis=0, ignore_index=True)\n            val = pd.concat([val, u_val], axis=0, ignore_index=True)\n\n        return self.output(datarec, train, test, val,\n                           step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.temporal.leave_out.LeaveRatioLast.__init__","title":"<code>__init__(test_ratio=0, val_ratio=0, seed=42)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>test_ratio</code> <code>float</code> <p>Proportion of each user's interactions assigned to the test set. Default is 0.</p> <code>0</code> <code>val_ratio</code> <code>float</code> <p>Proportion of each user's interactions assigned to the validation set. Default is 0.</p> <code>0</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Default is 42.</p> <code>42</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>test_ratio</code> or <code>val_ratio</code> are not in the range [0, 1].</p> <code>ValueError</code> <p>If <code>test_ratio + val_ratio</code> &gt; 1.</p> Source code in <code>datarec/splitters/user_stratified/temporal/leave_out.py</code> <pre><code>def __init__(self, test_ratio: float = 0, val_ratio: float = 0, seed: int = 42):\n    \"\"\"\n    Args:\n        test_ratio (float, optional): Proportion of each user's interactions assigned to the test set. Default is 0.\n        val_ratio (float, optional): Proportion of each user's interactions assigned to the validation set. Default is 0.\n        seed (int, optional): Random seed for reproducibility. Default is 42.\n\n    Raises:\n        ValueError: If `test_ratio` or `val_ratio` are not in the range [0, 1].\n        ValueError: If `test_ratio + val_ratio` &gt; 1.\n    \"\"\"\n    if not (0 &lt;= test_ratio &lt;= 1):\n        raise ValueError('ratio must be between 0 and 1')\n    if not (0 &lt;= val_ratio &lt;= 1):\n        raise ValueError('ratio must be between 0 and 1')\n    if test_ratio + val_ratio &gt; 1:\n        raise ValueError(\"sum of test_ratio and val_ratio must not exceed 1\")\n\n    self.params = {k: v for k, v in locals().items() if k != 'self'}\n\n    self.test_ratio = test_ratio\n    self.val_ratio = val_ratio\n    self.seed = seed\n</code></pre>"},{"location":"documentation/splitters/#datarec.splitters.user_stratified.temporal.leave_out.LeaveRatioLast.run","title":"<code>run(datarec)</code>","text":"<p>Splits the dataset into train, test, and validation sets by selecting the last interactions (in chronological order) for each user.</p> <p>The most recent interactions are removed first for the test set, then for the validation set, leaving the remaining interactions for training.</p> <p>Parameters:</p> Name Type Description Default <code>datarec</code> <code>DataRec</code> <p>The dataset containing interactions with a timestamp column.</p> required <p>Returns:</p> Type Description <code>Dict[str, DataRec]</code> <p>A dictionary containing the following keys: - <code>\"train\"</code> (<code>DataRec</code>): The training dataset. - <code>\"test\"</code> (<code>DataRec</code>): The test dataset, if <code>test_ratio</code> &gt; 0. - <code>\"val\"</code> (<code>DataRec</code>): The validation dataset, if <code>val_ratio</code> &gt; 0.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the dataset does not contain a timestamp column.</p> Source code in <code>datarec/splitters/user_stratified/temporal/leave_out.py</code> <pre><code>def run(self, datarec: DataRec) -&gt; Dict[str, DataRec]:\n    \"\"\"\n    Splits the dataset into train, test, and validation sets by selecting the last interactions\n    (in chronological order) for each user.\n\n    The most recent interactions are removed first for the test set, then for the validation set,\n    leaving the remaining interactions for training.\n\n    Args:\n        datarec (DataRec): The dataset containing interactions with a timestamp column.\n\n    Returns:\n        (Dict[str, DataRec]): A dictionary containing the following keys:\n            - `\"train\"` (`DataRec`): The training dataset.\n            - `\"test\"` (`DataRec`): The test dataset, if `test_ratio` &gt; 0.\n            - `\"val\"` (`DataRec`): The validation dataset, if `val_ratio` &gt; 0.\n\n    Raises:\n        TypeError: If the dataset does not contain a timestamp column.\n    \"\"\"\n\n    if datarec.timestamp_col is None:\n        raise TypeError('This DataRec does not contain temporal information')\n\n    train, test, val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n    data = datarec.data\n    for u in datarec.users:\n        u_train, u_val, u_test = data[data.iloc[:, 0] == u], pd.DataFrame(), pd.DataFrame()\n\n        user_total = len(u_train)\n\n        test_n_samples = round(self.test_ratio * user_total)\n        val_n_samples = round(self.val_ratio * user_total)\n\n        if test_n_samples &gt; 0:\n            for _ in range(min(test_n_samples, len(u_train))):\n                u_train, sample = max_by_col(u_train, datarec.timestamp_col, seed=self.seed)\n                u_test = pd.concat([u_test, sample], axis=0, ignore_index=True)\n\n        if val_n_samples &gt; 0:\n            for _ in range(min(val_n_samples, len(u_train))):\n                u_train, sample = max_by_col(u_train, datarec.timestamp_col, seed=self.seed)\n                u_val = pd.concat([u_val, sample], axis=0, ignore_index=True)\n\n        train = pd.concat([train, u_train], axis=0, ignore_index=True)\n        test = pd.concat([test, u_test], axis=0, ignore_index=True)\n        val = pd.concat([val, u_val], axis=0, ignore_index=True)\n\n    return self.output(datarec, train, test, val,\n                       step_info={'operation': self.__class__.__name__, 'params': self.params})\n</code></pre>"}]}